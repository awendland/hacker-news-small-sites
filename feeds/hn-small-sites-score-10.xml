<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 10]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 10. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 03 Oct 2020 16:32:14 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-10.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 03 Oct 2020 16:32:14 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why Privacy Is the Most Important Concept of Our Time]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24661271">thread link</a>) | @umilegenio
<br/>
October 2, 2020 | https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/ | <a href="https://web.archive.org/web/*/https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-25770" itemtype="https://schema.org/CreativeWork" itemscope="itemscope">

	
	
<div>

	
	<!-- .entry-header -->

	
	<div itemprop="text">

		
		
<p>I have always believed in the importance of privacy, but I felt that common definitions (e.g., <em>the right to be left alone</em>) were lacking. In fact, I think that the whole conceptualization of privacy as simply a right of an individual as partial and limiting. It could be because privacy, as <a href="https://en.wikipedia.org/wiki/Privacy">it is intended nowadays, originated from the Anglo-American world (that is what Wikipedia says</a>). </p>



<p>You might say that then, maybe, I am not really thinking about privacy, but rather something else. That might be true, so let’s not talk about privacy, instead let’s talk about <a href="https://en.wikipedia.org/wiki/Definitions_of_fascism#Umberto_Eco">Ur-Privacy</a>, the properties of any version of the concept of privacy you might have. Take this as the opinion of a random guy that cares about the issue.</p>



<div><h2>What is Ur-Privacy</h2><p><em>A few principles for privacy</em></p></div>







<p><strong>Privacy is about boundaries.</strong> <strong>It is not about hiding something from someone but allowing to create a space with rules</strong> <strong>decided by its members</strong>. I like to compare it to borders. Some people say that borders are a restriction, something that limit freedom of movement and we do not need in the contemporary world. As if they were arbitrary obstacles put there by the ancient petty Greek gods. It almost makes sense if you do not think about them, after all you are actually stopped at a border.</p>



<p>However, that is not true, that is not why they exist. Borders delimit the area that a certain state control, an area where a specific set of rules and laws applies. There was a time before borders, in fact most of human history did not have clear borders. It was not a time of freedom, but anarchy, where bands of barbarians could roam into your home and pillage everything.</p>



<p>In this context is also important to remember that before the <a href="https://en.wikipedia.org/wiki/Peace_of_Westphalia">Peace of Westphalia</a> modern European states were plagued by continual wars. The short version is that this was due to the combination of two facts:</p>



<ul><li>modernity begets differences, different kings choose different religions<sup><a id="link_1" href="#note_1" data-type="internal" data-id="#note_1">1</a></sup> and separated societies</li><li>however, the legitimacy of kings was still based on shared medieval ideals, like the concept of divine rule</li></ul>



<p>In short, the issue was not that leaders wanted to make war all the time, they needed to do so because the legitimacy of their power depended, at least on some level, to what the rest of the European world was doing. If you claim to be a divine king there better be agreement on what the divine is, otherwise a guy that picks a different religion can also rightly pick a different king. To change the situation this peace treaty established the principle that the internal affairs of a state are the exclusive interest of said state.</p>



<p>The connection with privacy is that without clear rules on what is private and what is public, nobody knows which stuff belongs to whom and this means that all belong to the strongest. <strong>Somebody might say that what you do in private, it is not private at all but political, it concerns the society at large. Therefore it must be regulated according to their rules</strong>.</p>



<p><strong>Privacy is about control</strong>. <strong>Without privacy we cannot decide for ourselves how to live our lives.</strong> If there is no privacy all become public. Whoever has more power and an interest can affect your life according to their own rules. Then, I have to care about what other people think, otherwise they will control how I can behave. As before the peace of Westphalia, the issue is not that other people are bad, <em>they have to do it</em>. When everything is subject to public scrutiny, you either control the rules and judge others or you are judged and controlled by others.</p>



<p>Think about this way: we say a lot of things in our private lives that are not meant to be taken literally. In private we say something and then we add: <em>you know what I mean</em>. And that is actually true. We can do that because the people we talk to in private know us; they understand the context in which our words must be understood. When I was a child I would sometimes say and think that I wanted to kill my brother. I did not meant literally and everybody knew it. If I said the same thing now, in public, to somebody that does not know me, the phrase would be different. It would be a <a href="https://en.wikipedia.org/wiki/Threat">threat</a>.</p>



<p>Why is that? They are the exact same words. You know why, of course. I am different and the context is different. The real meaning of something, whether an action or a word, is not absolute, in most cases is relative. When we speak in public, we share a different context, therefore our words have a different meaning.</p>



<p>So even I say something as an hyperbole or as an potentially implicit threat (e.g., <em>they must be stopped at all costs</em>!), they might protest. You might say that they are overreacting, that it was just a joke, but <em>how can they be sure of it</em>? <strong>They do not know me.</strong> <strong>And it is true that acts of violence are prepared by violent words</strong>. Even if you are unsure if something is really violent, you have to take a stand. You have to make clear that any attack against you is not permissible. Otherwise, <a href="https://en.wikipedia.org/wiki/Christchurch_mosque_shootings" data-type="URL" data-id="https://en.wikipedia.org/wiki/Christchurch_mosque_shootings">somebody, maybe a crazy guy, might think that it is permissible and the right course of action</a>. Somebody might feel legitimate to take your land and kingdom.</p>



<p>A clear example of the loss of privacy is the <em>rise of violent rhetoric</em>. Everybody swears and everybody threaten. However, for the most part they do not mean it. We know that because the actual rate of violence has not risen. We simply talk in public as we talk in private, because our private lives have become more public. I mean, some bosses want even to look at your Facebook profile<sup><a href="#note_2">2</a></sup><a id="link_2" href="https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/note_2">.</a></p>



<div><h2>Privacy Affects Everything</h2><p><em>Defending privacy would require all-around changes</em></p></div>







<p><strong>Privacy is the most important concept of our time, because it allows to define everything else. Without privacy we do not know what rules applies. Our lives will be judged according to the rules, even the whims, of somebody else.</strong></p>



<p>People lost jobs and had their lives ruined, because the mob judged something they said in private in a different way from what they expected. And they paid a price. You might say: that was fair. We might judge ourselves by our intentions, but others by their actions, which are real and objective.</p>







<p>I, for once, disagree with XKCD and this view. There are a couple of different issues here:</p>



<ul><li>how we should react to speech we disagree with</li><li>what was meant to be shared among friends was taken out of context and made public</li></ul>



<p>This complicates the whole matter. At a first glance the first issue should not matter here, because we are talking about privacy. However, this is a bit more complicated. Violations of privacy can affect other rights and freedom. Freedom of speech is a right regarding the public sphere. You have always been able to say everything in private, for the simple fact that people cannot control that. If now the private becomes public, then either we get absolute freedom of speech (a sort of <em>speech anarchy</em>, if you will) or we lose freedom of speech.</p>



<p>Okay, then we demand to not violate privacy even in the case of bad speech. If you said something bad in private then I cannot demand your boss to fire you. I cannot do that even by maintaining privacy: <em>trust me on this, they say something really bad</em>,<em> you should fire them</em>. This is a practical example of how privacy might affect everything.</p>



<p>This is crucial, but we have to understand that simply enforcing privacy in the traditional way is not enough anymore. To protect privacy we need to re-interpret some rights we have. For instance, traditionally there have been exceptions to privacy for public interest. If you heard somebody famous saying something controversial in private you could go public about. The issue is that few people (i.e., the press) had that power. Now we all have it. <strong>So, to defend privacy we need to accept shared norms of behavior</strong>. We cannot expect consequences outside the context that caused them.</p>



<p>This is hard to do, because people have different idea of public interest. It is not true that we judge other by their actions. We judge others by <em>our intentions</em>. So, we must be strict about the norm that the answer to some speech should be only some other form speech. In other words, if somebody offended you with some method, you should respond with the same method. If somebody said something bad, you cannot shove them. <strong>Actions by a mob in order to punish an alleged transgressor, punish a convicted transgressor, or intimidate them is not an answer to a bad argument, it is a<a href="https://en.wikipedia.org/wiki/Lynching"> lynching</a></strong>.</p>



<p>There is a difference between killing somebody and just ruining their lives. However, it is still bad. It is still lynching, something we do to one to control one hundred. Making somebody lose their livelihood because of something said in private it is not fair, because they said in a different context. They were not prepared to be judged by their worst enemies. And they should not have. </p>



<p>The philosopher Jeremy Bentham described the perfect prison as the <a href="https://en.wikipedia.org/wiki/Panopticon">Panopticon</a>. A prison where in every cell there was a one-way mirror. This way the guards could watch the inmates without being seen. Therefore the inmates would have to behave as if they were always watched. That kind of sounds like the world right now. And I am ready to lose the power to punish bad people in order to protect me from people that think I am a bad guy.</p>



<p><em>Given the discussion on Hacker News, I think that I was a bit unclear here. The connection between privacy and freedom of speech is just an example. My point is that privacy affects how we enjoy other rights, too. Even though that might not seem obvious at first.  </em></p>



<div><h2>What Should We Do?</h2><p>A modest proposal</p></div>







<p>So what has to be done to defend privacy? <strong>There should be clear boundaries about private, social and public spaces</strong>:</p>



<ul><li>a private space regards only you or your family</li><li>a social space is something involving a community, either a virtual one like a forum or a real one like a city</li><li>a public space is a space for all actors of society</li></ul>



<p>By clear boundaries I mean that we should create rules, …</p></div></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/">https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/</a></em></p>]]>
            </description>
            <link>https://inre.me/why-privacy-is-the-most-important-concept-of-our-time/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24661271</guid>
            <pubDate>Fri, 02 Oct 2020 10:55:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Flatpak: A security nightmare – two years later]]>
            </title>
            <description>
<![CDATA[
Score 69 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24661126">thread link</a>) | @krimeo
<br/>
October 2, 2020 | https://www.flatkill.org/2020/ | <a href="https://web.archive.org/web/*/https://www.flatkill.org/2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">


<p>Two years ago I <a href="https://flatkill.org/">wrote</a> about then heavily-pushed Flatpak, self-proclaimed "Future of Apps on Linux". The article criticized the following three major flows in Flatpak:</p><ul>
<li>Most of the apps have full access to the host system but users are misled to believe the apps are sandboxed
</li><li>The flatpak runtimes and apps do not get security updates
</li><li>Flatpak breaks many aspects of desktop integration
</li></ul>

<!-- <p>A lot has changed in the 2 years (the company behind Flatpak is now just another IBM's brand for one thing) so let's see how Flatpak developers addressed these fundamental issues.</p> -->
<p>So let's see how Flatpak developers addressed these fundamental issues.</p>

<h2>The sandbox is STILL a lie</h2>

<p>Almost all popular apps on Flathub still come with <span>filesystem=host</span> or <span>filesystem=home</span> permissions, in other words, <b>write access to the user home directory</b> (and more) so all it takes to escape the sandbox is trivial <span>echo download_and_execute_evil &gt;&gt; ~/.bashrc</span>. That's it.</p>


<p>The most popular applications on Flathub still suffer from this - Gimp, VSCodium, PyCharm, Octave, Inkscape, Audacity, VLC are still not sandboxed.</p>

<p>And, indeed, users are still mislead by the reassuring blue "sandboxed" icon. Two years is not enough to add a warning that an application is <b>not</b> sandboxed if it comes with dangerous permissions (like full access to your home directory)? Seriously?</p>

<img src="https://www.flatkill.org/2020/sandboxlie.png" alt="sandboxlie">

<h2>Flatpak apps and runtimes STILL contain long known security holes</h2>
<p>It took me about 20 minutes to find the first vulnerability in a Flathub application with full host access and I didn't even bother to use a vulnerability scanner.</p>

A perfect example is <a href="https://www.cvedetails.com/cve/CVE-2019-17498">CVE-2019-17498</a> with public exploit <a href="https://github.com/github/securitylab/tree/main/SecurityExploits/libssh2/out_of_bounds_read_disconnect_CVE-2019-17498">available</a> for some 8 months. The first app on Flathub I find to use libssh2 library is Gitg and, indeed, it does ship with unpatched libssh2.

<p>But is it just this one application? Let's look at the <b>official runtimes</b> at the heart of Flatpak (org.freedesktop.Platform and org.gnome.Platform <b>3.36</b> - as of time of writing used by most of the applications on Flathub). The first unpatched vulnerable dependency I found in the offical runtime is ffmpeg in version 4.2.1 with no security patches backported, <a href="https://www.cvedetails.com/cve/CVE-2020-12284">CVE-2020-12284</a>.</p><p>Recently I stumbled upon an article from 2011 which started what is today known as flatpak, <a href="https://people.gnome.org/~alexl/glick2">in the words of the project founder:</a></p>

<p><a href="https://people.gnome.org/~alexl/glick2"><b><i>"Another problem is with security (or bugfix) updates in bundled libraries. With bundled libraries its much harder to upgrade a single library, as you need to find and upgrade each app that uses it. Better tooling and upgrader support can lessen the impact of this, but not completely eliminate it."</i></b></a></p>

<p>After reading that it comes as no surprise flatpak still suffers from the same security issues as 2 years ago because flatpak developers knew about these problems from the beginning.</p>

<h2>Local root exploits are NOT considered a minor issue anymore!</h2>
<p>Great! Two years ago I wrote about a trivial local root exploit using flatpak to install suid binaries (<a href="https://www.cvedetails.com/cve/CVE-2017-9780/">CVE-2017-9780</a>) and how it was downplayed by Flatpak developers as a minor security issue <a href="https://github.com/flatpak/flatpak/releases/tag/0.8.7">here</a>. I am happy to see at least the attitude to local root exploits has changed and today <a href="https://github.com/containers/bubblewrap/security/advisories/GHSA-j2qp-rvxj-43vj">local root exploits</a> are considered high severity.

</p><h2>Desktop integration</h2>
<p>System and user fonts are now available to flatpak applications and basic font rendering settings are respected as well, however do not expect your changes in /etc/fonts, typically setting a proper fallback font for CJK characters, to work with flatpak.  KDE applications in flatpak are still ignoring themes, fonts and icon settings (tested with Qt5ct). Applications installed from the distribution sources do not have these problems, of course. <a href="https://www.flatkill.org/2020/desktopbrokenation.mp4">A quick screen capture to demonstrate</a>.</p>

<p>More importantly, fcitx, <i>the</i> IME for Chinese is still broken - it has been 2 years. Here is the <a href="https://github.com/flatpak/flatpak/issues/2031">issue</a> I linked 2 years ago - especially of interest is <a href="https://github.com/flatpak/flatpak/issues/2031#issuecomment-655134889">the following comment</a> directly from fcitx developer:

</p><p><i>"Because fcitx im module in flatpak is from 4.2.97 and using a different dbus object path. <b>It need to be the same version of fcitx on your host</b>."</i></p>

So I need to run multiple fcitx daemons on my desktop and switch between them as I switch flatpak apps depending on which fcitx libraries are bundled with that app or maybe in the future of linux apps it's not possible to type chinese anymore and it's fine?

<p>While the "bundle everything" approach has proven very useful on servers it clearly does not work for desktop applications, let's keep linking system libraries in desktop applications (and use the bundled libraries as a fallback only) to avoid introducing all these problems to Linux desktop.</p>



</div>]]>
            </description>
            <link>https://www.flatkill.org/2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24661126</guid>
            <pubDate>Fri, 02 Oct 2020 10:35:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Please Stop Imposing American Views about Race on Us]]>
            </title>
            <description>
<![CDATA[
Score 297 | Comments 214 (<a href="https://news.ycombinator.com/item?id=24660682">thread link</a>) | @dgellow
<br/>
October 2, 2020 | https://www.persuasion.community/p/please-stop-imposing-american-views | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/please-stop-imposing-american-views">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a4a8fb-963e-4611-8bc2-448a875dfb14_5184x3456.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3a4a8fb-963e-4611-8bc2-448a875dfb14_5184x3456.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/e3a4a8fb-963e-4611-8bc2-448a875dfb14_5184x3456.jpeg&quot;,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:18107111,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p><em>Construction workers reopen Winston Churchill statue in Parliament Square after it was covered with metal panels to protect it against defacement by protestors.</em></p><p>Over the past couple of months, many Britons have imported American discourse on race wholesale. When asked to analyze the experiences of black people in the United Kingdom, we now talk with an American accent.</p><p>Take a look, for instance, at a meme that has been circulating among some of my white friends on Facebook and Instagram:</p><blockquote><p>I have privilege as a White person because I can do all of these things without thinking twice about it … I can go jogging (#AmaudArbery). I can relax in the comfort of my own home (#BothemSean and #AtatianaJefferson). I can ask for help after being in a car crash (#Jonathan Ferrell and #RenishaMcBride). I can have a cellphone (#StephonClark). I can leave a party to get to safety (#JordanEdwards). I can play loud music (#JordanDavis). I can sell CDs (#AltonSterling). I can sleep (#AiyanaJones). I can walk from the corner store (#MikeBrown).</p></blockquote><p>The post goes on and on, like an interminable spoken-word poem. All the individuals listed are American, but most of the people who have shared this on my timeline are British. In trying to express their solidarity with black Britons, they are affirming a supposedly transcendental truth: to be black is to live in perpetual terror of being murdered by the state. </p><p>But Britain is not America. And importing American race discourse into the United Kingdom not only prevents us from recognizing the specific ways in which racial injustice manifests in this country—it cloaks the reality of black British lives behind an abstraction that flattens our humanity. </p><p>Britain has a long and painful history of anti-black racism. In the twentieth century alone, the growing black presence led to a long catalogue of abuses: the 1919 race riots in Liverpool, Cardiff and London; the 1958 race riots in Nottingham and Notting Hill; the 1969 police murder of David Oluwale, a Nigerian immigrant who was tortured, pissed on and finally drowned in a river in Leeds. I could list other examples. It is not hard to see why the horrific killing of George Floyd has evoked such strong feelings in this country as well.</p><p>But for all of the country’s flaws, Britain is not America. Trying to understand its racial dynamics through the lens of another country’s does more to obscure than to illuminate the situation that black Britons like myself actually face.</p><p>The average black American in the United States can trace his ancestry further back than the average white American. Most black Americans are descended from enslaved Africans. Their forebears suffered through the segregation and racial terror of the Jim Crow era. The majority of black people in the United Kingdom, by contrast, are immigrants or the children of immigrants. Though many of them have certainly had harrowing experiences with injustice or discrimination, they do not have the same history of racist disadvantage.</p><p>To understand the experience of black Britons, it is not only necessary to grasp how different their history is from that of black Americans: we need to understand the diversity captured by the label “black British.” For example, around two out of every three students with Congolese or Somali origins get free school meals, a standard indicator that their parents are poor. Among students with Nigerian or Ghanaian origins, only one in five do. It is also noteworthy that black Caribbean students are twice as likely to be excluded from school as black African students. </p><p>The discrepancy in educational attainment is just as stark. On average, 58% of black African students graduate from middle school at grade level (defined as achieving A* to C grades at GCSE)—about the same number as white students. But black Caribbean students are significantly less likely to do so—while those whose parents hail from Nigeria actually outperform their white peers by a considerable margin. </p><p>None of this is to disavow the label “black British.” But we need to invest it with the nuance consonant with its reality—and to cast doubt on the idea that every discrepancy in representation must be explained by structural injustice or white supremacy.</p><p>There has, for example, been a lot of concern about the underrepresentation of black Britons in professions like the arts and publishing. But why would you choose to go into theater or journalism—rather than law, medicine or finance—if you are a talented child of ambitious but not well off immigrants? </p><p>This is not a flippant question. While representation can be important, anybody who actually wants to improve the condition of black Britons should at least be a little curious about why they are overrepresented in some prestigious professions and underrepresented in others. In a country in which black people make up only three percent of the population, for example, six percent of junior doctors are black. Would the country—or the black community—really benefit if more black Britons chose to ditch medicine for the theater? The debate is worth having. But in the place of that debate, there have only been pious paeans to diversity.</p><p>The stereotype of the West African parent who wants their child to study law or medicine bears some relation to reality; but the widespread view of black people as perennial victims devoid of agency is a defamatory abstraction. The black person in Britain, like Ralph Ellison’s iconic protagonist, is “invisible because no one wants to see him.”</p><p>So much of the British reaction to the death of George Floyd has constituted a failure of nerve. Desperately seeking to assuage their feelings of guilt, to do <em>something</em>, many Britons have sacrificed their critical faculties to a narrative that does not actually help black people—a narrative that, by reducing us to passive abstractions, only makes us more invisible.</p><p>Racists assume that black people are all the same. Ironically, anti-racists sometimes do so too. But anybody who is truly committed to racial equality needs to recognize that this kind of simplification neither serves justice nor reflects the truth.</p><p><strong>Tomiwa Owolade is a writer who lives in London.</strong></p></div></div>]]>
            </description>
            <link>https://www.persuasion.community/p/please-stop-imposing-american-views</link>
            <guid isPermaLink="false">hacker-news-small-sites-24660682</guid>
            <pubDate>Fri, 02 Oct 2020 09:20:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Interview with Transport Tycoon creator Chris Sawyer]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24658958">thread link</a>) | @wizardfeet
<br/>
October 1, 2020 | https://lifeandtimes.games/episodes/files/28 | <a href="https://web.archive.org/web/*/https://lifeandtimes.games/episodes/files/28">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a href="https://pdcn.co/e/traffic.megaphone.fm/ADL8847793182.mp3" target="_blank">Click/tap here to download this episode.</a></p><p><a href="https://ratethispodcast.com/ltvg" target="_blank"><img src="https://storage.googleapis.com/rtp-assets/buttons/lifetimevideogames.png" width="198" height="56" alt="Rate This Podcast"></a></p>
<p><img alt="A screenshot of the main menu from Transport Tycoon for DOS" src="https://lifeandtimes.games/episodes/files/transport-tycoon-dos-screenshot-main-menu.png" width="276" height="212"></p><p>On the rise and, um...<em>fade out(?)</em> of Chris Sawyer, the genius creator of bestselling, critically-acclaimed simulation games Transport Tycoon and RollerCoaster Tycoon — who made a career out of working at the cutting-edge, in bare metal assembly code that he wrote and optimised (and optimised again) on his own.</p><p>Until the cutting-edge left him behind.</p><p><strong>Hello Hacker News readers! As of this update, the post there erroneously labels this as an interview. It's not. Chris doesn't do interviews, except via an intermediary (which doesn't really work in an audio format), so the story is based entirely on my in-depth research and analysis. I hope you still enjoy it and learn something interesting. (I have stories on many other things that do involve interviews, though, like </strong><strong><a href="https://lifeandtimes.games/episodes/files/27" title="Episodes:27 - Links">1990 golf game Links</a></strong><strong>, Bungie's </strong><strong><a href="https://lifeandtimes.games/episodes/files/25" title="Episodes:25 - Pimps at Sea">fake game Pimps at Sea</a></strong><strong>, and </strong><strong><a href="https://lifeandtimes.games/episodes/files/22" title="Episodes:22 - Wololo">the "Wololo" sound effect</a></strong><strong> from Age of Empires.)</strong></p><p>Chris was only a design consultant on 2004 game RollerCoaster Tycoon 3, but its remastered "Complete" edition has just come out on Nintendo Switch and the PC version is free on the Epic Games Store right now (until October 2). The original two games are also still sold via the likes of Steam and GOG.</p><p>Transport Tycoon, meanwhile, lives on in open-source project <a href="https://www.openttd.org/" target="_blank">OpenTTD</a> and in a mobile port (<a href="https://play.google.com/store/apps/details?id=com.thirtyonex.TransportTycoon&amp;hl=en_US" target="_blank">Android</a>, <a href="https://apps.apple.com/us/app/transport-tycoon/id634013256" target="_blank">iOS</a>) of the original game by Chris's company 31X. You can see a snippet of his source code in the image below:</p><div><p><img alt="cstg_code1" src="https://lifeandtimes.games/episodes/files/cstg_code1.jpg" width="1262" height="1775"></p></div><div><p>Thanks as always to my supporters on Patreon — especially my $10+ backers Carey Clanton, Rob Eberhardt, Simon Moss, Vivek Mohan, Wade Tregaskis, and Seth Robinson. If you'd like to become a supporter, for as little as $1 a month, head to <a href="https://www.patreon.com/lifeandtimesofvideogames">my Patreon page</a> and sign up. Or for one-off donations you can use <a href="https://paypal.me/mossrc">paypal.me/mossrc</a>.</p><p>Please remember to tell other people about the show, and to leave a review by following the links at <a href="https://ratethispodcast.com/ltvg">ratethispodcast.com/ltvg</a>.</p><p>I'm currently writing a new book called Shareware Heroes: Independent Games at the Dawn of the Internet. You can learn more and/or pre-order your copy <a href="https://unbound.com/books/shareware-heroes/" target="_blank">from Unbound</a>.</p></div><hr>
<h3>(Partial) Transcript</h3>
<p><strong><em>[Most episode transcripts/scripts are reserved for my Patreon supporters (at least for the time being), but I like to give you at least a taster here — or in this case, the first half of the episode.]</em></strong></p><p><em>Welcome to the Life and Times of Video Games, an audio series about video games and the video game industry, as they were in the past and how they’ve come to be the way they are today. I'm Richard Moss, and this is episode 28, Transport Tycoon, or the tale of the great optimiser and his two greatest works.</em></p><p>We’ll get going in just a moment. </p><p>*pause for pre-roll ad/cross-promo slot*</p><p>***</p><p>You may have heard the expression that every overnight sensation is a decade in the making — a decade of hard work, toiling in obscurity…or <em>relative</em> obscurity, honing a talent, perfecting a craft, <em>optimising</em> a skill set and envisioning whatever it is that breaks through.</p><p>In reality the actual duration is rarely a decade — it’s five years or eight years or eighteen years, or however long it takes for the pieces to all fall into place: the talent, timing, and product. But the idea bears repeating: the greatest accolades, the greatest achievements, the greatest games are the product of hard work built atop years of invisible labour.</p><p>And such it was that Chris Sawyer, like John Romero, Carol Shaw, Gunpei Yokoi, and many others before and since — such it was that in 1994 Chris Sawyer suddenly shifted from a little-known (though well-respected) figure in the games industry, a programmer who converted Amiga games to the PC, to become an industry icon.</p><p>Nineteen-ninety-four was the year when his first original game was published, the year when big-name PC game publisher Microprose put his transportation-focused business simulation game Transport Tycoon, an incredible solo development effort, in a box and sold it in stores to widespread acclaim. </p><p><img alt="SCR1" src="https://lifeandtimes.games/episodes/files/scr1.jpg" width="866" height="362"><br><em>Transport Tycoon, the game that made Chris Sawyer into a games industry icon (</em><em><a href="https://www.tt-forums.net/viewtopic.php?f=47&amp;t=29058" target="_blank">image source</a></em><em>)</em><br></p><div><p>The game itself had taken Chris just a year to develop, but the journey to making it had begun much earlier.</p><p>Chris had started programming as a teenager in 1981, largely out of curiosity, through trying to make things appear on the screen on a range of different computers he’d encountered. There was the Commodore PET at his high school, the Sinclair ZX81 demonstration unit in a W H Smiths store, and the Texas Instruments TI99/4A one of his neighbours owned, as well as the Commodore VIC-20 a different neighbour had. And eventually, after diligently saving up his pocket money, he’d become engrossed in a machine of his own, a Camputers Lynx, a now-forgotten, obscure-even-then 8-bit computer with fancier graphics and more horsepower than the leading systems of its day (the leading systems at the time being the Apple II, ZX Spectrum, and Commodore 64).</p><p>Here, in 1983, is where the journey really starts — where Chris set off towards the lands where he’d make his name. And I find it fascinating how serendipitous this was — for, you see, Chris’s two great successes, Transport Tycoon and RollerCoaster Tycoon, were both made possible by his phenomenal systems knowledge; by his immense capacity to hand-code complex interactions of data at low levels of abstraction.</p><p>And here is where he began to learn those skills, to internalise them to the point of becoming natural talents. He later told Arcade Attack in an interview that he’d not had access to an assembler for that Lynx computer, so when he’d wanted to move beyond coding in BASIC he’d needed to write his programs byte-by-byte in machine code — the lowest-level programming language, the numerical instructions that computers themselves use. And with scant resources available to teach him these skills, he mostly figured it out on his own, just trying different things until he got his ideas to work. Always chasing the next exhilarating breakthrough.</p><p>Chris continued to dabble in machine code, though somewhat less than before, when he upgraded to a similarly-obscure machine called the Memotech MTX500, which actually did come with a built-in assembler, which enabled him to write programs in the abbreviation-heavy Z80 assembly language. Programs that, beginning in 1984, he very often had published commercially.</p></div><p><img alt="Memotech_MTX500-wide" src="https://lifeandtimes.games/episodes/files/memotech_mtx500-wide.jpg" width="1020" height="584"><br><em>The Memotech MTX500 (</em><em><a href="https://en.wikipedia.org/wiki/File:Memotech_MTX500.jpg" target="_blank">Image source</a></em><em>)</em><br></p><div><p>Chris had sent Memotech cassette tapes of some games he’d made through copying the designs of popular titles, like Missile Kommand, which was the 1980 Atari arcade game converted to the capabilities of the MTX500, using a mix of BASIC and machine code, with the name intentionally misspelled (a ‘k’ rather than a ‘c’) as though that somehow made his unapologetic, blatant clone of another’s work okay. </p><p>But this was the wild west of the computer games business, and Memotech weren’t much concerned. Or at least their games guy Jim Wills wasn’t much concerned, neither at this point nor a few months later when he left to start a company called Megastar Games. Jim liked Chris’s work enough to publish it, for meagre royalties but invaluable experience. And so Chris was commercially published with his unlicensed MTX500 versions of Missile Command, Q*bert, Manic Miner, and a few others.</p><p>After high school he enrolled in a computer science and microprocessor systems degree, where he studied the fundamentals of both software and hardware design in computers — an experience he found invaluable, as it taught him how to push computers further by learning how their hardware worked. And it taught him the theories behind the sorts of nitty-gritty software-systems things he’d already been practising at home: optimisation, sorting, algorithms, and even more varieties of machine code.</p></div><p><img alt="escape-from-zarcos-memotech-mtx500" src="https://lifeandtimes.games/episodes/files/escape-from-zarcos-memotech-mtx500.png" width="1044" height="788"><em><br>Escape from Zarcos, Chris Sawyer clone of Manic Miner for the Memotech MTX500</em><br></p><div><p>At home, meanwhile, he’d shifted over to the Amstrad CPC, which technologically-speaking wasn’t hugely different to the Memotech system he’d been on before — but it was a modest upgrade, and unlike his previous computers it was actually a popular system. And for Chris it was a gateway to the PC, because in the course of studying at university and making computer games on the side he wound up getting an Amstrad-made IBM-PC clone.</p><p>Chris had during this period been getting his games published through Ariolasoft, a German company with a UK subsidiary that promised him a job programming games for them once he graduated. Except some promises can’t be kept, especially in an industry that moves as fast as computer games publishing.</p><p>The home computer business was by that point deep into its transition from 8-bit to 16-bit hardware, and that transition came with adjustments to the standard of game graphics and design required, and to the way marketing and sales worked, and the cost of publishing, and so on, and Ariolasoft wasn’t doing too well at managing the transition. </p><p>So Chris didn’t have a job waiting for him after all, and he’d missed out on all the great electronics engineering jobs his classmates applied for. (Oops!) But not to worry — he’d made enough connections and enough headway as a programmer that he could get himself a business agent, and that agent in turn connected him to the booming Amiga-to-PC games porting industry.</p><p>He later said he’d thought it a “stop-gap” measure, just “a bit of fun” while he looked for more permanent employment in the electronics industry. But Chris took to his new conversions work like a duck to water. The kid who’d had to get creative and remain patient to make anything work on his Camputers Lynx machine now excelled in an environment where he had to contend with the vast gap in multimedia capabilities between the Amiga and the PC.</p><p>PCs of the day were pathetically inept as games machines, compared to a system like the Amiga. Whereas the PC had just a CPU, and maybe, in a minority of machines, a dedicated sound card like the SoundBlaster 16, every Amiga came with a custom chipset that contained audio and video co-processors that could take some strain off the …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lifeandtimes.games/episodes/files/28">https://lifeandtimes.games/episodes/files/28</a></em></p>]]>
            </description>
            <link>https://lifeandtimes.games/episodes/files/28</link>
            <guid isPermaLink="false">hacker-news-small-sites-24658958</guid>
            <pubDate>Fri, 02 Oct 2020 04:45:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Big O, Little N]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 31 (<a href="https://news.ycombinator.com/item?id=24657747">thread link</a>) | @adamzerner
<br/>
October 1, 2020 | https://adamzerner.bearblog.dev/big-o-little-n/ | <a href="https://web.archive.org/web/*/https://adamzerner.bearblog.dev/big-o-little-n/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div><p>I remember when I was first learning about hash tables. I thought I stumbled across an ingenious optimization for dealing with hash collisions. Before explaining my optimization, let me first briefly explain how hash tables work and what hash collisions are. I also made a video if you're interested.</p>
<p><a href="http://www.youtube.com/watch?v=pM9zhZB2KkM" title="Hash Tables"><img alt="" src="http://img.youtube.com/vi/pM9zhZB2KkM/0.jpg"></a></p>
<p>Say that we have a hash that maps someone's name to their age. <code>"adam"</code> is <code>27</code>, so we want to enter that into our hash.</p>
<p><img alt="" src="https://i.ibb.co/qWdjbhM/Slice.png"></p>
<p>Here's what happens:</p>
<ul>
<li><code>"adam"</code> gets put through a hash function.</li>
<li>The hash function spits out <code>7</code>.</li>
<li><code>7</code> is the index of the array where we are going to store <code>"adam"</code>'s value.</li>
<li>So we store <code>27</code> into <code>array[7]</code>.</li>
</ul>
<p>If we run <code>hash.get("adam")</code> in the future and need to look up the value for <code>"adam"</code>, we:</p>
<ul>
<li>Run <code>"adam"</code> through the hash function.</li>
<li>Get <code>7</code> as the output.</li>
<li>This means we have to look at <code>array[7]</code> to get the value.</li>
<li>So we look at <code>array[7]</code> to get our value.</li>
</ul>
<p>Hopefully if we have a key of <code>"alice"</code> or <code>"bob"</code> it'll hash to a different index. But... what happens if it hashes to the same index? That's called a hash collision.</p>
<p><img alt="" src="https://i.ibb.co/sJbJKnD/Slice.png"></p>
<p>A common approach is that if there's a collision, you store the values in a linked list.</p>
<p><img alt="" src="https://i.ibb.co/VJsSW3T/Slice.png"></p>
<p>Fair enough. That makes sense.</p>
<p>But wait!!! It takes <code>O(n)</code> time to look up a value in a linked list. Can't we use a binary search tree to speed that up to <code>O(log n)</code>???</p>
<p><img alt="" src="https://i.ibb.co/R3Vrc10/Slice.png"></p>
<p>Or... can't we take it a step further and use a <em>hash inside of a hash</em> to get the lookup time all the way down to <code>O(1)</code>?!</p>
<p><img alt="" src="https://i.ibb.co/7CyYNWY/Slice.png"></p>
<p>I hate to say it, but however many years ago when I had these thoughts, there were a few moments where I thought I was a genius. Now when I look back at that former self, I facepalm.</p>
<p>It is true that going from a linked list to a BST to a hash takes you from <code>O(n)</code> to <code>O(logn)</code> to <code>O(1)</code> lookup time. However, since collisions are rare, <code>n</code> is going to be small. And when <code>n</code> is small, <code>O(n)</code> might be faster than <code>O(1)</code>.</p>
<p>To understand why that is, think back to what Big-O really means. I think it helps to think about it as a "math thing" instead of a "programming thing". Consider two functions:</p>
<pre><code>f(n) = 4n + 1000
g(n) = 2n^2 + 5
</code></pre>
<p>Big-O of <code>f</code> is <code>O(n)</code> and Big-O of <code>g</code> is <code>O(n^2)</code>. We just focus on the part that "really matters". When <code>n</code> gets really big, the fact that it's <code>4n</code> doesn't really matter. Nor does the <code>+ 1000</code>.</p>
<p>But what about when <code>n</code> is small? Well, let's look at happens when <code>n</code> is <code>5</code>:</p>
<pre><code>f(5) = 4(5) + 1000 = 20 + 1000 = 1020
g(5) = 2(5^2) + 5 = 2(25) + 5 = 50 + 5 = 55
</code></pre>
<p>Look at that! The <code>O(n^2)</code> function is almost 20 times faster than the <code>O(n)</code> function!</p>
<p>And <em>that</em> is basically why they use a linked list instead of a BST or hash to handle collisions. Since <code>n</code> is small, Big-O isn't the right question to ask.</p>
<hr>
<p>Here's another example. I just wrote the following code while prepping for an interview:</p>
<pre><code>const isVowel = (char) =&gt; ["a", "e", "i", "o", "u"].includes(char);
</code></pre>
<p>Normally I wouldn't think twice about it, but since interviewers care so much about time complexity, I stopped to think about whether it could be improved.</p>
<p>And it hit me that it's actually <code>O(n)</code>, because we've got an array and have to iterate over every element to see if the element matches <code>char</code>. It's easy to overlook this because we're using <code>includes</code> and not writing the code ourselves.</p>
<p>So then I thought that maybe we could use a hash instead to get <code>O(1)</code> lookup. Something like this:</p>
<pre><code>const isVowel = (char) =&gt; {
  const vowels = {
    a: true,
    e: true,
    i: true,
    o: true,
    u: true,
  };

  return !!vowels[char];
};
</code></pre>
<p>But then I realized that this is the same mistake I made with the hash collision stuff however many years ago. <em><code>n</code> is small, so Big-O isn't the question we should be asking</em>. Here <code>n</code> is <code>5</code>.</p>
<p>I think that the array approach would actually be faster than the hash approach, even though it's <code>O(n)</code> instead of <code>O(1)</code>. The reason for this is called locality.</p>
<p>If you dive deep under the hood, when you look up an element in an array, the CPU actually grabs a bunch of adjacent elements as well as the one you wanted, and it stores the adjacent elements in a cache. So here when we look up <code>"a"</code> in the array, it'll probably grab <code>"a"</code>, <code>"e"</code>, <code>"i"</code>, <code>"o"</code>, and <code>"u"</code>. And so next time when we want to grab <code>"e"</code>, it can take it from the cache, which is a lot faster. This works because the elements in the array are close to each other in the physical memory. But with a hash, my understanding is that they wouldn't be so close, and thus we wouldn't benefit from this spatial locality effect.</p>
<p>I'm no low-level programming wiz so I'm not sure about any of this. That's ok, I think it's beside the point of this post. The real point of this post is that when you have a little <code>n</code>, Big-O doesn't matter.</p>
</div>
</div></div>]]>
            </description>
            <link>https://adamzerner.bearblog.dev/big-o-little-n/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24657747</guid>
            <pubDate>Fri, 02 Oct 2020 00:59:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Asteroids: By the Numbers]]>
            </title>
            <description>
<![CDATA[
Score 43 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24655752">thread link</a>) | @rbanffy
<br/>
October 1, 2020 | http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1 | <a href="https://web.archive.org/web/*/http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div id="post-body-3855344372715435262"><p>
By 1979, arcade games were rapidly becoming more complex and colorful, and a game like <i>Asteroids </i>might have seemed quaint by comparison to the likes of <i><a href="http://www.retrogamedeconstructionzone.com/2019/09/galaxian-aesthetics-of-simple-patterns.html">Galaxian</a>, <a href="http://www.retrogamedeconstructionzone.com/2019/09/star-fire-weirdness-of-pseudo-3d.html">Star Fire</a></i>, or <i>Radar Scope.&nbsp; </i>But beneath its simple exterior lies a challenging shooter with surprisingly complex physics.&nbsp; The image below shows a sample still from it, including the player's ship (the triangle on the left), three sizes of asteroids, and an alien ship.</p><p><a href="https://1.bp.blogspot.com/-XOyg91qwUV4/XZwhbHR4tnI/AAAAAAAAOXs/DJ89MzjAODU3mAoraZKUaq6vl9nKvdsSACLcBGAsYHQ/s1600/Asteroids_sample%2B%25282%2529.png"><img data-original-height="461" data-original-width="598" height="492" src="https://1.bp.blogspot.com/-XOyg91qwUV4/XZwhbHR4tnI/AAAAAAAAOXs/DJ89MzjAODU3mAoraZKUaq6vl9nKvdsSACLcBGAsYHQ/s640/Asteroids_sample%2B%25282%2529.png" width="640"></a></p>

<p>
The goal of the game is to maximize your score, destroying as many asteroids and aliens as possible before you run out of ships.&nbsp; When an asteroid is hit by something (usually the player's bullets), large asteroids turn into two medium ones and medium asteroids turn into two small ones, while small asteroids and aliens are destroyed when hit.</p><p><a href="https://1.bp.blogspot.com/-cEzHmkcz3bA/XaU1zGl7IBI/AAAAAAAAObY/2GDlGaxtEc0ytzWpyQRinUxnbioeVsTbQCLcBGAsYHQ/s1600/Asteroids%2BPlay.gif"><img data-original-height="357" data-original-width="600" height="237" src="https://1.bp.blogspot.com/-cEzHmkcz3bA/XaU1zGl7IBI/AAAAAAAAObY/2GDlGaxtEc0ytzWpyQRinUxnbioeVsTbQCLcBGAsYHQ/s400/Asteroids%2BPlay.gif" width="400"></a></p>
<p>
For the designers of the game, balancing the relative sizes of the objects would have been important in such a dense field, because it determines how often things run into one another other.&nbsp; The table below outlines the sizes of the objects in <i>Asteroids</i>, expressed relative to the length of the player's ship.</p><table>
<caption>The approximate horizontal lengths of objects in <i>Asteroids</i></caption>
    <tbody>
<tr>
    <th>Object</th>
    <th>Length (in player ship lengths)</th>
    </tr>
<tr>
    <td>Screen</td>
    <td>25 x 36</td>
    </tr>
<tr>
    <td>Large Asteroids</td>
    <td>2.4</td>
    </tr>
<tr>
    <td>Medium Asteroid</td>
    <td>1.2</td>
    </tr>
<tr>
    <td>Small Asteroid</td>
    <td>0.6</td>
    </tr>
<tr>
    <td>Alien Ship (large)</td>
    <td>1.5</td>
    </tr>
<tr>
    <td>Alien Ship (small)</td>
    <td>0.75</td>
    </tr>
</tbody>
    </table>
<p>
Notice how the medium asteroid is twice the size of the small one, and the large is twice the size of the medium.&nbsp; If you think about it, this doesn't actually make much physical sense -- you can break a two-dimensional object into four pieces half its size, not two.&nbsp; But the size ratios do make game sense, because what really matters to the player is not how much area an asteroid takes up, but how likely it is to hit them.</p><p><a href="https://1.bp.blogspot.com/-5wVPSIS6W_M/XZwmjsjcYII/AAAAAAAAOX4/a7MThKiGRH8vhydf83eCiOmmJuKUULHIwCLcBGAsYHQ/s1600/Asteroids_cross_section%2B%25282%2529.png"><img data-original-height="281" data-original-width="170" src="https://1.bp.blogspot.com/-5wVPSIS6W_M/XZwmjsjcYII/AAAAAAAAOX4/a7MThKiGRH8vhydf83eCiOmmJuKUULHIwCLcBGAsYHQ/s1600/Asteroids_cross_section%2B%25282%2529.png"></a></p>

<p>
In the picture above, you can see that while the four small asteroids take up less area on the screen than the big one, they present the same cross section to your ship.&nbsp; This means that breaking up the asteroids doesn't greatly increase the probability that the player will be struck by one.&nbsp; If the asteroids had been broken up into four half-sized pieces at each blow, each large asteroid would result in sixteen small ones (that's four times the cross section!) and the player would be quickly overwhelmed.&nbsp; Note that the small asteroids do move faster than the large ones, and speed increases the chance of getting hit, but I'll return to that in a moment.</p>

<p>
Another big reason that size matters in <i>Asteroids</i>&nbsp;is that it determines how close something has to be before you're likely to be able to hit it.&nbsp; In 1979, vector arcade cabinet screens had an effective resolution of 1024 x 768, and the player's ship was only connected graphically by about 20 points.&nbsp; This meant, in turn, that there were only a limited number of orientations that they could render for the ship, in this case intervals of 5 degrees.&nbsp; To see how this might affect gameplay, imagine your ship is located at the fixed position shown in the picture below.&nbsp; Anything located entirely between the two solid lines will not be accessible by your bullets because your ship can't turn to the appropriate angle to hit it.&nbsp; This restriction isn't a big deal for large asteroids, which are still accessible over most of the screen, but small asteroids can be out of reach of your guns at relatively close range, sometimes as close as 7 ship lengths away!&nbsp; So even if you aim as accurately as possible, chances are you won't be able to hit a small asteroid on the other side of the screen, at least not on your first shot.&nbsp; This is probably one of the reasons that <i>Asteroids</i>&nbsp;allows you to fire up to four bullets in succession: even if you can't hit an object right away, it will eventually move into your line of fire.</p>

<p><a href="https://1.bp.blogspot.com/-EX2V1tAwdnM/XZvKFmnQZoI/AAAAAAAAOXI/cd411AFReLcKkCQGooANzsmxVD7QV3ikgCLcBGAsYHQ/s1600/Asteroids_hittable%2B%25285%2529.png"><img data-original-height="397" data-original-width="1024" height="248" src="https://1.bp.blogspot.com/-EX2V1tAwdnM/XZvKFmnQZoI/AAAAAAAAOXI/cd411AFReLcKkCQGooANzsmxVD7QV3ikgCLcBGAsYHQ/s640/Asteroids_hittable%2B%25285%2529.png" width="640"></a></p>
<p>
And what about speed?&nbsp; There is some variability in the speeds of individual asteroids, but small asteroids can move as much as 63% faster than large ones (see the table below), meaning that even if four small asteroids present the same cross section for hitting your ship as a large one does, their higher speed means they're more likely to hit you.&nbsp; The reasoning for this is simple: the faster something moves, the longer the path it can traverse in a given amount of time, and the larger the likelihood that you'll be in that path.&nbsp; Fortunately, even the fastest asteroids are much slower than your ship, which can traverse the screen in about two seconds; so if you're skilled enough, you should be able to maneuver around the asteroid field without a problem.</p><table>
<caption>The approximate speeds of objects in <i>Asteroids</i></caption>
    <tbody>
<tr>
    <th>Object</th>
    <th>Speed (in ship lengths per second)</th>
    </tr>
<tr>
    <td>Your ship</td>
    <td>0 - 17</td>
    </tr>
<tr>
    <td>Asteroids</td>
    <td>4 - 6.5</td>
    </tr>
<tr>
    <td>Alien ships (both sizes)</td>
    <td>4 - 6.5 (depending on your score)</td>
    </tr>
<tr>
    <td>Bullets</td>
    <td>17 (ship at rest)&nbsp;</td></tr>
</tbody></table>
<p>
One other interesting thing about speed: notice in the table above that the speed of a bullet, when fired at rest, is the same as your ship's maximum speed.&nbsp; This means that when you're moving rapidly and fire a bullet in the opposite direction of your motion, the two speeds should cancel for a stationary observer and the bullet should appear roughly stationary.</p>

<p><a href="https://1.bp.blogspot.com/-pRIxsTPNfSY/XaD7O0foijI/AAAAAAAAOZQ/JWZGFR26JEY3UM43RiruPhhGfHOCW21DgCLcBGAsYHQ/s1600/Asteroids_bullet_stationary.gif"><img data-original-height="175" data-original-width="468" height="239" src="https://1.bp.blogspot.com/-pRIxsTPNfSY/XaD7O0foijI/AAAAAAAAOZQ/JWZGFR26JEY3UM43RiruPhhGfHOCW21DgCLcBGAsYHQ/s640/Asteroids_bullet_stationary.gif" width="640"></a></p>


<div><p>
Sure enough, when I fire a bullet while moving quickly in the opposite direction, it just floats near where I fired it, allowing me to place bullets like mines for the asteroids to run into.</p>
<p>
The developers of <i>Asteroids </i>wanted the game to simulate the real laws of physics (<a href="http://www.technologyuk.net/computing/computer-gaming/gaming-landmarks-1960-1985/asteroids.shtml">see here</a>), and I think in many respects they succeeded, at least compared to most other arcade games of the time.&nbsp; It's not entirely realistic, however.&nbsp; In my article on <a href="http://www.retrogamedeconstructionzone.com/2019/10/impossible-motion-in-video-games.html">motion in early arcade games</a>, I discussed how the acceleration of the ship in <i>Asteroids</i> is too rapid for a human-occupied vehicle.&nbsp; We might suppose that the vehicle is automated, but even then, engineering spacecraft to withstand 30+ g's of acceleration is a non-trivial challenge.</p><p><a href="https://1.bp.blogspot.com/-eBpcOCL_1a0/XaU-bXjLJjI/AAAAAAAAObk/N_FxduJSebIMaqzrq7BhDmNFufzmyBRfgCLcBGAsYHQ/s1600/Acceleration%2BAsteroids.gif"><img data-original-height="175" data-original-width="634" height="176" src="https://1.bp.blogspot.com/-eBpcOCL_1a0/XaU-bXjLJjI/AAAAAAAAObk/N_FxduJSebIMaqzrq7BhDmNFufzmyBRfgCLcBGAsYHQ/s640/Acceleration%2BAsteroids.gif" width="640"></a></p>
<p>
Another issue is the way the asteroids break up.&nbsp; In physics, there's a principle called the conservation of momentum that says that when objects interact with one another or break apart, the total mass times the velocity must remain the same after the interaction as it was before.&nbsp; In layman's terms, this means that things can't suddenly go from moving one direction to moving in another unless there is something else carry its previous momentum.&nbsp; In the example shown below, an asteroid does just that, and the only thing that could have absorbed its previous momentum is the bullet.&nbsp; But the bullet is so tiny that it's difficult to imagine how that would be possible.</p><p><a href="https://1.bp.blogspot.com/-vsDHHIPNXrQ/XaEAAKYXRII/AAAAAAAAOZc/-lhbXy0TSV4nEiLvOf-pe7zyt9sPGLcEACLcBGAsYHQ/s1600/Asteroids_momentum.gif"><img data-original-height="248" data-original-width="533" height="185" src="https://1.bp.blogspot.com/-vsDHHIPNXrQ/XaEAAKYXRII/AAAAAAAAOZc/-lhbXy0TSV4nEiLvOf-pe7zyt9sPGLcEACLcBGAsYHQ/s400/Asteroids_momentum.gif" width="400"></a></p>

<p>
These are mere quibbles, however, and shouldn't dissuade you from giving the game a try.&nbsp; <i>Asteroids </i>ended up being one of Atari's biggest arcade hits, selling over 70,000 cabinets, and remains one of their most recognizable games to this day.&nbsp; Many gamers who grew up in the '80s, myself included, are more familiar with the Atari 2600 version of the game.&nbsp; Unfortunately, the designers had to make a lot of sacrifices in game physics in order to make it work on a home console, so I think the arcade version is really the way to go.&nbsp; Outside of visiting a vintage arcade, your best bet is probably the <a href="https://www.mamedev.org/">Multiple Arcade Machine Emulator</a> (MAME).&nbsp; Happy hunting!</p></div><div><p><span>NOTE: &nbsp;A previous version of this article stated that the game has limited pixel resolution, but <i>Asteroids </i>uses a vector display that is not composed of pixels. &nbsp;The resolution of the <i>Asteroids </i>vector display is 1024 x 768.</span></p><p><a href="https://1.bp.blogspot.com/-7gLKzZ6gULM/XaDx0uT70-I/AAAAAAAAOYo/aLUcLQJkl9AY1k2OYsewuUHydaxbkT0gwCLcBGAsYHQ/s1600/Asteroids_loop_transparent.gif"><img data-original-height="151" data-original-width="600" height="160" src="https://1.bp.blogspot.com/-7gLKzZ6gULM/XaDx0uT70-I/AAAAAAAAOYo/aLUcLQJkl9AY1k2OYsewuUHydaxbkT0gwCLcBGAsYHQ/s640/Asteroids_loop_transparent.gif" width="640"></a></p>
<br></div>
</div>
</div></div>]]>
            </description>
            <link>http://www.retrogamedeconstructionzone.com/2019/10/asteroids-by-numbers.html?m=1</link>
            <guid isPermaLink="false">hacker-news-small-sites-24655752</guid>
            <pubDate>Thu, 01 Oct 2020 20:19:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Programmers need to think like hackers]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24654136">thread link</a>) | @gexos
<br/>
October 1, 2020 | https://gexos.org/2020/10/01/Programmers-need-to-think-like-hackers!/ | <a href="https://web.archive.org/web/*/https://gexos.org/2020/10/01/Programmers-need-to-think-like-hackers!/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://gexos.org/2020/10/01/Programmers-need-to-think-like-hackers!/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24654136</guid>
            <pubDate>Thu, 01 Oct 2020 18:01:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS Is Not Your Ideal]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24653013">thread link</a>) | @amortize
<br/>
October 1, 2020 | https://sujithjay.com/not-aws | <a href="https://web.archive.org/web/*/https://sujithjay.com/not-aws">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
  
  <p><span> 01 Oct 2020  • <span>
  
    
    
    <a href="https://sujithjay.com/tag/product"><code><nobr>PRODUCT</nobr></code>&nbsp;</a>
  
    
    
    <a href="https://sujithjay.com/tag/management"><code><nobr>MANAGEMENT</nobr></code>&nbsp;</a>
  
</span></span></p><p>Let me start with an assertion. Every platform engineering team <sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> in every organisation aspires to be like AWS <sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>.</p>

<p>Every platform team wants to be like AWS, because like AWS, they provide infrastructure abstractions to users. AWS provides infrastructure via the abstractions of VMs and disks and write-capacity-units, while platform teams provide infrastructure using higher abstractions which solve service definitions, database or message queue provisioning, and service right-sizing <sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup>.</p>

<p>This similarity prompts leaders of platform engineering teams to model their teams as agnostic providers of universal, non-leaky (within SLO bounds), self-served abstractions for their engineering organisation. Platform teams structured as such detached units struggle to define cohesive roadmaps which provide increasing value to business. But how does your platform differ from AWS?</p>

<h2 id="your-platform-vs-the-platform">Your Platform vs. The Platform</h2>

<h3 id="1-the-middle-ground">1. The Middle Ground</h3>
<p>As an agnostic service provider, AWS can afford to cater to median use-cases. The reason platform engineering teams exist is to bridge the gap between PaaS abstractions which work for the median use-case to your business’ specific use-cases. AWS can afford to target the median (economy of scale etc.), but you cannot.</p>

<p><img src="https://sujithjay.com/public/notaws/Median.jpeg" alt="AWS can afford to stay within a single σ. You cannot."></p>
<p><span> AWS can afford to stay within a single σ. You cannot.</span>
</p>

<p>Agnostic platform engineering teams which emulate AWS try to get away from this responsibility by proposing abstractions which target the median use-case. A tell-tale sign of this is when the lack in wide usability of internal abstractions is compensated for by extensive onboarding &amp; repeated training. This is also a side-effect of the the relative valuation of engineering time vs. the time of another function <sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup>.</p>

<h3 id="2-follow-the-money">2. Follow the Money</h3>
<p>The dictum ‘follow the money’ works beautifully for customer-front products. When faced with a choice between two competing features to prioritise, a common tactical play is to make something which leads to more (immediate &amp; long-term) revenue. The proxy for increased revenue could be increased acquisition conversion, better retention or improved user experience – metrics which ensure increased revenue for the company over time. In short, revenue growth is the north star <sup id="fnref:5" role="doc-noteref"><a href="#fn:5">5</a></sup>.</p>

<p>Not so much in platform engineering. There is no revenue since your customers are internal, captive ones. Captive audiences are forced to use a solution by the force of dictum and lack of choice. The metrics used in platform products are proxies for usability and user satisfaction – but there are no foolproof ways to measure it for captive audiences. For captive audiences, solutions can not compete and better solutions cannot win. Like a command economy, platform products are designed rather than evolved. Design takes priority over market economy. So why is design bad?</p>

<h2 id="bad-design">Bad Design</h2>
<p>For design to work, there has to be an objective function against which we can design. A specification is an objective function against which engineering teams design a solution. Since we do not have reliable metrics <sup id="fnref:6" role="doc-noteref"><a href="#fn:6">6</a></sup> to rely on for platform engineering, how do we come up with specifications? And without rigorous specifications, new features created by the platform run a high risk of not solving worthwhile problems for the users.The current accepted methodology among platform engineering leaders to solve this paucity of specifications is to rely on user-interviews. This is, as mentioned before, an unreliable source since captive users do not have the best view of the ideal state of tooling and abstractions that could be available to them.</p>

<p>The only way to flip this situation is to let go of command-economy-style designed abstractions, and to let your platform self-organise along the principle of markets. How does that look in practice?</p>

<h3 id="1-market-ftw">1. Market, FTW</h3>
<p>Camille Fournier mentions in <a href="https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142">Product for Internal Platforms</a> how her team partners with customer teams to develop prototypes for specific problems. These specific solutions are later honed and iterated on to become general solutions provided by your team. I would go a step further on this route, where possible. Partner to prototype with multiple teams facing related problems to develop multiple specific solutions. These specific solutions can be seen as competing candidates to solve a general problem. Bring in user-interviews at this point to gauge pain-points, and iterate individually on these specific solutions. This switches the economy of your team to a self-organised market. Once considerable thought and iteration has gone into each solution, it is time to assimilate. Assimilate the best solution(s) while migrating the rest to the chosen solution. As emphasised in <a href="https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142">Product for Internal Platforms</a>, an early investment of time into migration strategies is essential for such a scheme to sustain.</p>

<p>In platforms designed with experimentation, you will find that innovation continues to thrive at the edges of the platform’s domain while the stable core of the platform is subject to periodic rework or maintenance. The use-cases a platform supports grows in a controlled manner to address an ever-growing percent of the consumers, and does not stagnate after addressing just the median users.</p>

<h3 id="2-overloaded-use-cases">2. Overloaded Use-cases</h3>
<p>Although agnostic platform engineering teams might only be catering to very specific median use-cases, the customer teams with specific needs cannot afford to be blocked and they cannot stop delivering their deliverables. These teams sometimes create their own solutions, and in such cases the above strategy of assimilation works wonders. You get a prototype for free on which the team can iterate on. However, this scenario is rarer in cases where it requires specific skills to build such solutions, such as in data platforms. One common pattern in such knowledge-constricted situations is that users find ways to overload the existing solutions with minor tweaks to fit their use-case. Look out for such overloaded use-cases within your platform, for they are excellent guides to unmet needs of the users. You can leverage them to advocate for newer features to explicitly support those use-cases.</p>

<h3 id="3-listen-to-them-only-at-the-start">3. Listen To Them (Only At The Start!)</h3>
<p>As a parting note, I will take a jab at user-interviews again. The above tactics work when you are trying to scale your platform from 1 to N. When taking a platform from 0 to 1, the only solution to creating specifications is to listen to the users. Give them exactly what they want. Listen to their exact demands. A propensity of platform product managers is to rely on this excessively at a much later stage in the product’s lifecycle. User-interviews have their place in evolving products, but the over-reliance on the methodology is a bane to platform product management.</p>

<p><strong>P.S.</strong> As I read back the above essay, the heavy influence of <a href="https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142">Product for Internal Platforms</a> is clear. I would like to say that was the intention: to reassert the ideas in it which resounded with me, while stating a few of my own.</p>

<h3 id="footnotes">Footnotes</h3>


  </div>









      </div></div>]]>
            </description>
            <link>https://sujithjay.com/not-aws</link>
            <guid isPermaLink="false">hacker-news-small-sites-24653013</guid>
            <pubDate>Thu, 01 Oct 2020 16:37:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Compiling a Lisp to x86-64: Let expressions]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 30 (<a href="https://news.ycombinator.com/item?id=24652842">thread link</a>) | @todsacerdoti
<br/>
October 1, 2020 | https://bernsteinbear.com/blog/compiling-a-lisp-7/ | <a href="https://web.archive.org/web/*/https://bernsteinbear.com/blog/compiling-a-lisp-7/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p><em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/">first</a></em> – <em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-6/">previous</a></em></p>

<p>Welcome back to the “Compiling a Lisp” series. Last time we added a reader
(also known as a parser) to our compiler. This time we’re going to compile a
new form: <em>let</em> expressions.</p>

<p>Let expressions are a way to bind variables to values in a particular scope.
For example:</p>

<div><div><pre><code><span>(</span><span>let</span> <span>((</span><span>a</span> <span>1</span><span>)</span> <span>(</span><span>b</span> <span>2</span><span>))</span>
  <span>(</span><span>+</span> <span>a</span> <span>b</span><span>))</span>
</code></pre></div></div>

<p>Binds <code>a</code> to <code>1</code> and <code>b</code> to <code>2</code>, but only for the body of the <code>let</code> — the
rest of the S-expression — and then executes the body.</p>

<p>This is similar in C to opening a new block:</p>

<div><div><pre><code><span>int</span> <span>result</span><span>;</span>
<span>{</span>
  <span>int</span> <span>a</span> <span>=</span> <span>1</span><span>;</span>
  <span>int</span> <span>b</span> <span>=</span> <span>2</span><span>;</span>
  <span>result</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>but it’s a little different because C has a divide between <em>statements</em> and
<em>expressions</em>, whereas Lisp does not.</p>

<p>It’s <em>also</em> different because let-expressions do not make previous binding
names available to expressions being bound. For example, the following program
should fail because it cannot find the name <code>a</code>:</p>



<p>There is a form that makes bindings available serially, but that is called
<code>let*</code> and we are not implementing that today.</p>

<p>For completeness’ sake, there is also <code>let rec</code>, which makes names available
serially and also within the same binding. This is useful for binding recursive
or mutually recursive functions. Again, we are not implementing that today.</p>

<h3 id="name-binding-implementation-strategy">Name binding implementation strategy</h3>

<p>You’ll notice two new things about let expressions:</p>

<ol>
  <li>They introduce ways to bind names to values, something we have to figure out
how to keep track of</li>
  <li>In order to use those names we have to figure out how to look up what the
name means</li>
</ol>

<p>In more technical terms, we have to add <em>environments</em> to our compiler. We can
then use those environments to map <em>names</em> to <em>stack locations</em>.</p>

<p>“Environment” is just a fancy word for “look-up table”. In order to implement
this table, we’re going to make an <em>association list</em>.</p>

<p>An <em>association list</em> is a list of <code>(key value)</code> pairs. Adding a pair means
tacking it on at the end (or beginning) of the list. Searching through the
table involves a linear scan, checking if keys match.</p>

<blockquote>
  <p>You may be wondering why we’re using this data structure to implement
environments. Didn’t I even take a data structures course in college?
Shouldn’t I know that <em>linear</em> equals <em>slow</em> and that I should <em>obviously</em>
use a hash table?</p>

  <p>Well, hash tables have costs too. They are hard to implement right; they have
high overhead despite being technically constant time; they incur higher
space cost per entry.</p>

  <p>For a compiler as small as this, a tuned hash table could easily be as long
as the rest of the compiler. Since we’re also compiling small <em>programs</em>,
we’ll worry about time complexity later. It is only an implementation detail.</p>
</blockquote>

<p>In order to do this, we’ll first draw up an association list. We’ll use a
linked list, just like cons cells:</p>

<div><div><pre><code><span>// Env</span>

<span>typedef</span> <span>struct</span> <span>Env</span> <span>{</span>
  <span>const</span> <span>char</span> <span>*</span><span>name</span><span>;</span>
  <span>word</span> <span>value</span><span>;</span>
  <span>struct</span> <span>Env</span> <span>*</span><span>prev</span><span>;</span>
<span>}</span> <span>Env</span><span>;</span>
</code></pre></div></div>

<p>I’ve done the usual thing and overloaded <code>Env</code> to mean both “a node in the
environment” and “a whole environment”. While one little <code>Env</code> struct only
holds a one name and one value, it also points to the rest of them, eventually
ending with <code>NULL</code>.</p>

<p>This <code>Env</code> will map names (symbols) to <em>stack offsets</em>. This is because we’re
going to continue our strategy of <em>not doing register allocation</em>.</p>

<p>To manipulate this data structure, we will also have two functions<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>:</p>

<div><div><pre><code><span>Env</span> <span>Env_bind</span><span>(</span><span>const</span> <span>char</span> <span>*</span><span>name</span><span>,</span> <span>word</span> <span>value</span><span>,</span> <span>Env</span> <span>*</span><span>prev</span><span>);</span>
<span>bool</span> <span>Env_find</span><span>(</span><span>Env</span> <span>*</span><span>env</span><span>,</span> <span>const</span> <span>char</span> <span>*</span><span>key</span><span>,</span> <span>word</span> <span>*</span><span>result</span><span>);</span>
</code></pre></div></div>

<p><code>Env_bind</code> creates a new node from the given name and value, borrowing a
reference to the name, and prepends it to <code>prev</code>. Instead of returning an
<code>Env*</code>, it returns a whole struct. We’ll learn more about why later, but the
“TL;DR” is that I think it requires less manual cleanup.</p>

<p><code>Env_find</code> takes an <code>Env*</code> and searches through the linked list for a <code>name</code>
matching the given <code>key</code>. If it finds a match, it returns <code>true</code> and stores the
<code>value</code> in <code>*result</code>. Otherwise, it returns <code>false</code>.</p>

<p>We can stop at the first match because Lisp allows name <em>shadowing</em>. Shadowing
occurs when a binding at a inner scope has the same name as a binding at an
outer scope. The inner binding takes precedence:</p>

<div><div><pre><code><span>(</span><span>let</span> <span>((</span><span>a</span> <span>1</span><span>))</span>
  <span>(</span><span>let</span> <span>((</span><span>a</span> <span>2</span><span>))</span>
    <span>a</span><span>))</span>
<span>; =&gt; 2</span>
</code></pre></div></div>

<p>Let’s learn about how these functions are implemented.</p>

<h3 id="name-binding-implementation">Name binding implementation</h3>

<p><code>Env_bind</code> is a little silly looking, but it’s equivalent to prepending a
node onto a chain of linked-list nodes. It returns a struct <code>Env</code> containing
the parameters passed to the function. I opted <em>not</em> to return a heap pointer
(allocated with <code>malloc</code>, etc) so that this can be easily stored in a
stack-allocated variable.</p>

<div><div><pre><code><span>Env</span> <span>Env_bind</span><span>(</span><span>const</span> <span>char</span> <span>*</span><span>name</span><span>,</span> <span>word</span> <span>value</span><span>,</span> <span>Env</span> <span>*</span><span>prev</span><span>)</span> <span>{</span>
  <span>return</span> <span>(</span><span>Env</span><span>){.</span><span>name</span> <span>=</span> <span>name</span><span>,</span> <span>.</span><span>value</span> <span>=</span> <span>value</span><span>,</span> <span>.</span><span>prev</span> <span>=</span> <span>prev</span><span>};</span>
<span>}</span>
</code></pre></div></div>

<p><em>Note</em> that we’re <strong>pre</strong>pending, not <strong>ap</strong>pending, so that names we add deeper
in a let chain shadow names from outside.</p>

<p><code>Env_find</code> does a recursive linear search through the linked list nodes. It may
look familiar to you if you’ve already written such a function in your life.</p>

<div><div><pre><code><span>bool</span> <span>Env_find</span><span>(</span><span>Env</span> <span>*</span><span>env</span><span>,</span> <span>const</span> <span>char</span> <span>*</span><span>key</span><span>,</span> <span>word</span> <span>*</span><span>result</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>env</span> <span>==</span> <span>NULL</span><span>)</span>
    <span>return</span> <span>false</span><span>;</span>
  <span>if</span> <span>(</span><span>strcmp</span><span>(</span><span>env</span><span>-&gt;</span><span>name</span><span>,</span> <span>key</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
    <span>*</span><span>result</span> <span>=</span> <span>env</span><span>-&gt;</span><span>value</span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
  <span>}</span>
  <span>return</span> <span>Env_find</span><span>(</span><span>env</span><span>-&gt;</span><span>prev</span><span>,</span> <span>key</span><span>,</span> <span>result</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>We search for the node with the string <code>key</code> and return the stack offset associated
with it.</p>

<p>Alright, now we’ve got names and data structures. Let’s implement some name
resolution and name binding.</p>

<h3 id="compiling-name-resolution">Compiling name resolution</h3>

<p>Up until now, <code>Compile_expr</code> could only compile integers, characters, booleans,
<code>nil</code>, and some primitive call expressions (via <code>Compile_call</code>). Now we’re
going to add a new case: symbols.</p>

<p>When a symbol is compiled, the compiler will look up its stack offset in the
current environment and emit a load. This opcode, <code>Emit_load_reg_indirect</code>, is
very similar to <code>Emit_add_reg_indirect</code> that we implemented for primitive
binary functions.</p>

<div><div><pre><code><span>int</span> <span>Compile_expr</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>node</span><span>,</span> <span>word</span> <span>stack_index</span><span>,</span>
                 <span>Env</span> <span>*</span><span>varenv</span><span>)</span> <span>{</span>
  <span>// ...</span>
  <span>if</span> <span>(</span><span>AST_is_symbol</span><span>(</span><span>node</span><span>))</span> <span>{</span>
    <span>const</span> <span>char</span> <span>*</span><span>symbol</span> <span>=</span> <span>AST_symbol_cstr</span><span>(</span><span>node</span><span>);</span>
    <span>word</span> <span>value</span><span>;</span>
    <span>if</span> <span>(</span><span>Env_find</span><span>(</span><span>varenv</span><span>,</span> <span>symbol</span><span>,</span> <span>&amp;</span><span>value</span><span>))</span> <span>{</span>
      <span>Emit_load_reg_indirect</span><span>(</span><span>buf</span><span>,</span> <span>/*dst=*/</span><span>kRax</span><span>,</span> <span>/*src=*/</span><span>Ind</span><span>(</span><span>kRbp</span><span>,</span> <span>value</span><span>));</span>
      <span>return</span> <span>0</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>-</span><span>1</span><span>;</span>
  <span>}</span>
  <span>assert</span><span>(</span><span>0</span> <span>&amp;&amp;</span> <span>"unexpected node type"</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>If the variable is not in the environment, this is a compiler error and we
return <code>-1</code> to signal that. This is not a tremendously helpful signal. Maybe
soon we will add more helpful error messages.</p>

<p>Ah, yes, <code>varenv</code>. You will, like I had to, go and add an <code>Env*</code> parameter to
all relevant <code>Compile_XYZ</code> functions and then plumb it through the recursive
calls. Have fun!</p>

<h3 id="compiling-let-finally">Compiling let, finally</h3>

<p>Now that we can resolve the names, let’s go ahead and compile the expressions
that bind them.</p>

<p>We’ll have to add a case in <code>Compile_expr</code>. We could add it in the body of
<code>Compile_expr</code> itself, but there is some helpful setup in <code>Compile_call</code>
already. It’s a bit of a misnomer, since it’s not a call, but oh well.</p>

<div><div><pre><code><span>int</span> <span>Compile_call</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>callable</span><span>,</span> <span>ASTNode</span> <span>*</span><span>args</span><span>,</span>
                 <span>word</span> <span>stack_index</span><span>,</span> <span>Env</span> <span>*</span><span>varenv</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>AST_is_symbol</span><span>(</span><span>callable</span><span>))</span> <span>{</span>
    <span>// ...</span>
    <span>if</span> <span>(</span><span>AST_symbol_matches</span><span>(</span><span>callable</span><span>,</span> <span>"let"</span><span>))</span> <span>{</span>
      <span>return</span> <span>Compile_let</span><span>(</span><span>buf</span><span>,</span> <span>/*bindings=*/</span><span>operand1</span><span>(</span><span>args</span><span>),</span>
                         <span>/*body=*/</span><span>operand2</span><span>(</span><span>args</span><span>),</span> <span>stack_index</span><span>,</span>
                         <span>/*binding_env=*/</span><span>varenv</span><span>,</span>
                         <span>/*body_env=*/</span><span>varenv</span><span>);</span>
    <span>}</span>
  <span>}</span>
  <span>assert</span><span>(</span><span>0</span> <span>&amp;&amp;</span> <span>"unexpected call type"</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>We have two cases to handle: no bindings and some bindings. We’ll tackle these
recursively, with no bindings being the base case. For that reason, I added a
helper function <code>Compile_let</code>.</p>

<p>As with all of the other compiler functions, we pass it an machine code buffer,
a stack index, and an environment. Unlike other functions, we passed it two
expressions and two environments.</p>

<p>I split up the bindings and the body so we can more easily recurse on the
bindings as we go through them. When we get to the end (the base case), the
bindings will be <code>nil</code> and we can just compile the <code>body</code>.</p>

<p>We have two environments for the reason I mentioned above: when we’re
evaluating the expressions that we’re binding the names to, we can’t add
bindings iteratively. We have to evaluate them in the parent environment. It’ll
be come clearer in a moment how that works.</p>

<p>We’ll tackle the simple case first — no bindings:</p>

<div><div><pre><code><span>int</span> <span>Compile_let</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>bindings</span><span>,</span> <span>ASTNode</span> <span>*</span><span>body</span><span>,</span>
                <span>word</span> <span>stack_index</span><span>,</span> <span>Env</span> <span>*</span><span>binding_env</span><span>,</span> <span>Env</span> <span>*</span><span>body_env</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>AST_is_nil</span><span>(</span><span>bindings</span><span>))</span> <span>{</span>
    <span>// Base case: no bindings. Compile the body</span>
    <span>_</span><span>(</span><span>Compile_expr</span><span>(</span><span>buf</span><span>,</span> <span>body</span><span>,</span> <span>stack_index</span><span>,</span> <span>body_env</span><span>));</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span>
  <span>// ...</span>
<span>}</span>
</code></pre></div></div>

<p>In that case, we compile the body using the <code>body_env</code> as the environment. This
is the environment that we will have added all of the bindings to.</p>

<p>In the case where we <em>do</em> have bindings, we can take the first one off and pull
it apart:</p>

<div><div><pre><code>  <span>// ...</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>bindings</span><span>));</span>
  <span>// Get the next binding</span>
  <span>ASTNode</span> <span>*</span><span>binding</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>bindings</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>name</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>binding</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_symbol</span><span>(</span><span>name</span><span>));</span>
  <span>ASTNode</span> <span>*</span><span>binding_expr</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>AST_pair_cdr</span><span>(</span><span>binding</span><span>));</span>
  <span>// ...</span>
</code></pre></div></div>

<p>Once we have the <code>binding_expr</code>, we should compile it. The result will end up
in <code>rax</code>, per our internal compiler convention. We’ll then store it in the next
available stack location:</p>

<div><div><pre><code>  <span>// ...</span>
  <span>// Compile the binding expression</span>
  <span>_</span><span>(</span><span>Compile_expr</span><span>(</span><span>buf</span><span>,</span> <span>binding_expr</span><span>,</span> <span>stack_index</span><span>,</span> <span>binding_env</span><span>));</span>
  <span>Emit_store_reg_indirect</span><span>(</span><span>buf</span><span>,</span> <span>/*dst=*/</span><span>Ind</span><span>(</span><span>kRbp</span><span>,</span> <span>stack_index</span><span>),</span>
                          <span>/*src=*/</span><span>kRax</span><span>);</span>
  <span>// ...</span>
</code></pre></div></div>

<p>We’re compiling this binding expression in <code>binding_env</code>, the parent
environment, because we don’t want the previous bindings to be visible.</p>

<p>Once we’ve generated code to store it on the stack, we should register that
stack location with the binding name in the environment:</p>

<div><div><pre><code>  <span>// ...</span>
  <span>// Bind the name</span>
  <span>Env</span> <span>entry</span> <span>=</span> <span>Env_bind</span><span>(</span><span>AST_symbol_cstr</span><span>(</span><span>name</span><span>),</span> <span>stack_index</span><span>,</span> <span>body_env</span><span>);</span>
  <span>// ...</span>
</code></pre></div></div>

<p>Note that we’re binding it in the <code>body_env</code> because we want this to be
available to the body, but not the other bindings.</p>

<p>At this point we’ve done all the work required for …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bernsteinbear.com/blog/compiling-a-lisp-7/">https://bernsteinbear.com/blog/compiling-a-lisp-7/</a></em></p>]]>
            </description>
            <link>https://bernsteinbear.com/blog/compiling-a-lisp-7/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24652842</guid>
            <pubDate>Thu, 01 Oct 2020 16:26:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on QA]]>
            </title>
            <description>
<![CDATA[
Score 20 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24651571">thread link</a>) | @tigranhakobian
<br/>
October 1, 2020 | https://blog.superannotate.com/how-to-detect-mislabeled-annotations | <a href="https://web.archive.org/web/*/https://blog.superannotate.com/how-to-detect-mislabeled-annotations">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><em>Manual QA is a significant part of the annotation pipeline. Annotation companies report that 40 percent of the annotation time can be spent on manual QA. As a result, finding ways to reduce QA testing time can have a significant impact on annotation costs.&nbsp;</em></p>
<!--more--><p><em>At SuperAnnotate, we’ve developed a tool to accelerate the QA process. This article discusses SuperAnnotate’s features that speed up the quality assurance process substantially. It presents several automation tools within the platform listing specific use cases in which a major acceleration of the QA process can be obtained. We also explored various ML algorithms that can detect over 90 percent of mislabeled instances in data while accelerating the QA process up to 4 times.&nbsp;</em></p>
<h3><strong><span>Outline</span></strong></h3>
<ul>
<li><em>Problem with noisy annotation in data&nbsp;</em></li>
<li><em>Manual QA acceleration</em></li>
<li><em>QA automation</em></li>
<li><em>Conclusion</em></li>
</ul>
<h2><span>Problem with annotation noise in data</span></h2>
<p><strong><span>1.1. The importance of model accuracy and the impact of annotation noise&nbsp;&nbsp;</span></strong></p>
<p>In real-world applications, the performance of machine learning (ML) systems is of crucial importance. ML models heavily rely on the quality of annotated data, but obtaining high-quality annotations is costly and requires extensive manual labor.&nbsp;</p>
<p><span>In any annotation pipeline, regardless of the data collection method, i.e., human or machine, several factors inject annotation noise in data. As a result, even the most celebrated datasets contain mislabeled annotations.</span></p>

<p><img src="https://lh3.googleusercontent.com/mL1r0JU9VM5-WflEsLU04zQ7vhYhl8cKRux8LvOnUTYEKZvnIUD9Gv8HmkVTqPfgoDtEkb_5ApX3SjgJjdTAHNfG6I-5Q7_fag_cra8IhXTWp-uVdrvQCf7kzoqt03BwtgKlTb4i" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"><em>Figure: Ambiguous or Mislabeled annotations from ImageNet. Source (</em><a href="https://arxiv.org/abs/2001.10528"><em><span>Pleiss et al</span></em></a><em>.)</em></p>

<p><span>Recent studies show that both natural and malicious corruptions of annotations tend to radically degrade the performance of machine learning systems. Deep Neural Networks (DNNs) tend to memorize noisy labels in data, resulting in less generalizable features and poor model performance (<a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>)</span></p>
<p><span>Therefore, extensive quality control of annotated data is required to clean annotation noise and improve model performance.</span><em><br></em></p>

<p><img src="https://lh4.googleusercontent.com/7BKHH4Am8Z_PZebzMe7mkbGCA6J_UyNRZE-ClAMwP5qVo52ZEuybUx81EOWYSdKrz2Mz7P4zf2cHuoz9nlyl4Alg-D16cD58mSCLf1CAOUwwDEp2MUkIsaTi37D6y9aliNShqdUz" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: Accuracy drop in multiple datasets when injecting label noise. Source (</em><a href="https://arxiv.org/abs/1611.03530"><em><span>Rolnick et al.</span></em></a><em>)</em></p>

<h2>Manual QA Acceleration</h2>
<p><strong><span>2.1 SuperAnnotate’s QA pipeline&nbsp;</span></strong></p>
<p><span>Quality Assurance of annotated data is time-consuming and requires particular attention. Annotation tools need to provide reliable and scalable QA pipelines to accelerate the QA process. <a href="https://annotate.online/login">SuperAnnotate</a> provides interlinked annotation and QA processes within the same platform. As a result, QA systems do not require additional management.</span></p>
<p><span>The design of SuperAnnotate’s QA system guarantees an efficient process and ensures a minimal probability of error.&nbsp;</span></p>
<p><strong><span>2.2 Pinning images to reduce common errors&nbsp;</span></strong></p>
<p><span>Sharing repetitive labeling mistakes across the annotation team is essential to reduce systematic errors throughout single or multiple annotation projects. </span>SuperAnnotate’s pin functionality is designed specifically for this cause.&nbsp;</p>
<p>Once the reviewer notices a recurring error, they can share this information through pinned annotations instead of extensive project instructions. Pinned annotations will appear first in the annotation editor to immediately grab the annotation team’s attention.&nbsp;</p>
<p>This functionality is highly efficient since it allows the project coordinator to instantly share common instructions, eliminating the spread of systematic errors.&nbsp;</p>

<p><img src="https://lh6.googleusercontent.com/g1sh6D7Xy1hLYBMczMWfFGdO1_1gktJn7dNF1Spx3lwUxeZd8isGaMHgYLB1GoT0lY8jJKTemdqvtExgPJIocgjNEFuzYGjbPQYeRo31873mdN3U4U10DMjW7DZOnr1eAh5_o-bc" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: Pin functionality</em></p>

<p><strong><span>2.3 Approve/Disapprove functionality</span></strong></p>
<p>Apart from various image-level QA tools, SuperAnnotate also provides instance-level QA functionalities. The latter is designed to help the QA focus on a specific instance area. As a result, no error is overlooked, at the time a meticulous QA is time-efficient.</p>
<p>Additionally, the Approve/Disapprove functionality works on the level of individual instances<span>.</span> If a QA specialist disapproves of an annotation, they can send it back to the Annotator for correction. The QA specialist can send the annotation back to the Annotator as many times as needed until the annotation is corrected. Once approved, the annotations can be exported from the platform.</p>

<p><img src="https://lh4.googleusercontent.com/BpQPXRUuTMRYUP-YGJTSRJwFUtdLr6wsB5LVwn66wK6TqFJCTRW5LLwhTp4sRttqgzQaCngKzm7Z6ONZBpPhwzxifj1KlBRs5_FOl_Nk0cFpaC_jn7-l9CMn1WJX2FENx9f9NI24" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p>Figure: Approve/Disapprove functionality</p>

<p><span>The QA mode is another useful tool for manual instance inspection. When enabled, the annotations are visually isolated from the background. This allows the user to distinguish between instances and the underlying objects, making the instance inspection easier.</span></p>
<p><span>All listed features extensively accelerate the manual QA process. However, machine learning techniques that automatically detect annotation noise can provide an additional level of automation.</span></p>

<h2><span>QA Automation</span></h2>
<p><strong><span>3.1 ML for QA Automation</span></strong></p>
<p><span>QA of annotated data takes around 40 percent of the annotation time.</span></p>
<p><span>On average, only a small fraction of annotated data contains noise. Still, QA is applied to the entire dataset, and monitoring clean data costs the annotators extra time and resources. An automation method could substantially cut down the QA process of clean data by isolating a set of risky annotations. So, our goal is to determine ML techniques that identify noisy annotations in data with high precision and recall.</span></p>
<p><strong><span>3.2 Current research</span></strong></p>
<p><span>Learning on datasets that contain annotation noise has been an active research area in ML. Several methods use predictions from DNNs to detect label noise and modify training loss (Reed et al., 2015; Tanaka et al., 2018). These methods do not perform well under high noise ratio as the domination of noisy annotations in data causes overfitting of DNNs. Another approach is to treat small loss samples as clean annotations and allow only clean samples to contribute to the training loss (Jiang et al., 2017). Ultimately, this research area’s core challenge is to design a reliable criterion capable of identifying the annotation noise.</span></p>
<p><span>A significant amount of research in this area is focused on classification with noisy labels. The proposed methods range from detecting and correcting noisy samples to using noise-robust loss functions. Unsupervised and semi-supervised learning techniques are also relevant to this task since those require few or no labels. Mislabeled samples that are detected without label correction can be used as unlabeled data in a semi-supervised setting (Li et al. 2020).&nbsp;</span></p>
<p><span>Going beyond classification makes things far more challenging. In classification, the existence of an object per image is guaranteed. So, the noisiness criterion can be defined between predicted and annotated image labels. However, in more complex tasks such as object detection, the correspondence between predicted and annotated instances is less trivial. Even though research in this area is in its initial state, several methods suggest valid measures to indicate both localization and label noise in object detection (Pleiss et al 2020, Chadwick et al. 2019).</span></p>
<p><strong><span>3.3&nbsp; Proposed method</span></strong></p>
<p>Consider the task of object detection on a dataset that contains mislabeled annotations. Several techniques use DNN predictions to identify label noise in data. Based on this concept, we propose the following algorithm.</p>
<ul>
<li>For each bounding box annotation, we obtain the matching prediction that has the maximum IOU.&nbsp;</li>
<li><span>Compute <strong>L2 distance</strong> between one hot vector of an annotated class and Fast RCNN softmax logits of the matched prediction. </span>This distance serves as a mislabel metric for annotations. We treat this number as the probability of annotation being mislabeled. As we aim to achieve maximal recall and precision in mislabel detection, we select an optimal threshold to attain the desired objective. This defines the split of data between clean and mislabeled annotations by the given criterion.</li>
</ul>

<p><img src="https://lh6.googleusercontent.com/JJtyM51584r8uJCIbHuW0UKhdgxQIIMm4G23vuD8lxnn5yrYHONPRytIxl-9QjFodM51zy7d8pvswhiMYokXY8XOz-DxqTd4ua-_DHGemiuXULNKqCbq_ePQfzulkbsrNu_jyX0D" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: L2 distance on gt one hot and prediction logits</em></p>

<p>Along with mislabeled detections, we also suggest considering the most confident predictions as missing annotations.&nbsp;</p>
<p><strong><span>3.4 Experiments and results</span></strong></p>
<p>To evaluate the performance of our method, we used PASCAL VOC as a toy dataset. When we manually injected asymmetric label noise in 20 percent of bbox annotations, the described mislabel criterion resulted in the precision-recall curve shown below.</p>
<p><img src="https://lh5.googleusercontent.com/OhgoPNrwmPWqR21Y1jnMY-1rbgKV8cxdFd8F0lZsVCZTcrN9C77EBX-0yqvZfVbYOVAFH_pxHNh88T7r26Gbp6hxWWdF5XqyLkN8SS5G46q512ZOHILlP1wmeR20aCo2QDWsOMms" width="400" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: PR curve with optimal mismatch threshold selected</em></p>

<p>Here, recall determines the portion of annotation noise captured, and precision identifies the fraction of data validated as clean.</p>
<p><span>Based on the PR curve above, using optimal mislabel threshold results in over 93 percent recall. This proves the reliability of our method, as we capture the dominant fraction of annotation noise. Along with high recall, we obtained over 75 percent precision, which attributes to validating ¾ of annotations as clean. This cuts manual inspection time over the whole dataset by a margin of 4, resulting in extensive automation for the manual QA.&nbsp;</span></p>
<p>Note that detected risky annotations contain clean samples apart from correctly detected mislabeled instances. Those are the images that were hard to capture by the detection model and thus are misclassified as risky. Distinguishing between these two categories is a challenging problem for further research.&nbsp;</p>
<p>You can find the source code for the discussed experiments at our <a href="https://github.com/superannotateai/qa-automation"><span>GitHub repository</span></a>.</p>
<p>Also, consider our <a href="https://colab.research.google.com/drive/1Xbt3dxkmX4ozQhdY_vnHXAH67OUeg0Nj#scrollTo=7unkuuiqLdqd&amp;uniqifier=2"><span>Colab tutorial</span></a> as a step-by-step guide to reproduce the given results.&nbsp;</p>
<p><strong><span>3.5 Automate Approve/Disapprove functionality&nbsp;</span></strong></p>
<p>Once mislabeled annotations are captured by ML techniques, SuperAnnotate allows users to import the detected information to the platform through the <strong>error </strong>key of SA formatted annotations. Just set the <strong>error</strong> key in mislabeled annotations and import SA formatted JSONs to SuperAnnotate via Python SDK.&nbsp;</p>

<p><img src="https://lh3.googleusercontent.com/to5CwRUFKERkSeyPgA1Nzl84I0ijhR8bQCpsUHhIxi_zhsm7FnNGOxSJ-i0V8HXnJSzN4VOxDnVgj_JWo2QCwQF6ECSjO4CrLShBB9V3YPHp8SWxO9D2CEwC6UbfqClElWgxIOf3" width="654" height="183" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: JSON Code Block</em></p>

<p><img src="https://lh3.googleusercontent.com/0SJ8E5OnOSxB50Fh_4d4_qjJNT0lygY1yHzFGuFlZ9fMDjIGyUGQDqxGsDDG9j2dMcfmf6PLOHvj3JRJ4F3UtaQup-muynR2TFd18W6vbjI1ANF8Ll99dK_b8v1GIzkBUbpwVaPn" width="654" height="121" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: SDK Code Block</em></p>

<p><span>Discussed pipeline provides complete automation of Approve/Disapprove functional.</span></p>

<p><img src="https://lh5.googleusercontent.com/w9VXvs6INaP4WaEADW2pW6fEVj6s8sj5FZd44gN-10LcKplspRX3N7tt-aRuV1BUmNFzANwQTjKgq9rt-EbND0_SpY2z5Ikcpc3xo6wZHQMm2-TB3iY6ao5LboT86FK8qaw8ZwPu" width="720" alt="How to Detect 93% of Mislabeled Annotations While Spending 4x Less Time on Quality Assurance"></p>
<p><em>Figure: Before v.s After autoqa in SA platform&nbsp;</em></p>

<h3><span>Conclusion</span></h3>
<p><span>QA automation is of crucial importance as it constitutes a significant portion of annotation time. This article shows that using proper algorithms and associated tools can help us detect mislabeled annotations with high precision while spending 4x less time on QA.</span></p></span>
</p>


</div></div>]]>
            </description>
            <link>https://blog.superannotate.com/how-to-detect-mislabeled-annotations</link>
            <guid isPermaLink="false">hacker-news-small-sites-24651571</guid>
            <pubDate>Thu, 01 Oct 2020 14:57:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Memgraph DB 1.1 Up to 50% Better Memory Usage and Higher Throughput]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24651091">thread link</a>) | @karimtr
<br/>
October 1, 2020 | https://memgraph.com/blog/memgraph-1-1-benchmarks | <a href="https://web.archive.org/web/*/https://memgraph.com/blog/memgraph-1-1-benchmarks">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>Introduction</h2>
<p>At Memgraph, we put great effort into delivering a high-performance in-memory graph storage and analytics engine. We do this by investing a lot of time optimizing and continuously improving various aspects of the Memgraph core engine. In this blog post, we will explore some of the improvements we have made on the storage layer and their impact on performance and memory usage.</p>
<p>The two most significant improvements we introduced in recent years are a new storage engine and a new way of storing properties on both nodes and edges.  Prior to version v0.50.0 Memgraph had an <a href="https://en.wikipedia.org/wiki/Multiversion_concurrency_control">MVCC</a> storage where copies of nodes and edges were used to version the data. <strong>Memgraph v0.50.0</strong> introduced a new way of managing graph data where each node or edge consists of the latest version of data, and associated changes of data required to reconstruct previous versions.</p>
<p><img src="https://i.imgur.com/bQbvvp6.png" alt=""></p>
<p><strong>Memgraph v1.1.0</strong> introduced a new way of storing properties. Each node or edge has a property store that takes at least 16B of memory. Memgraph tries to hold properties data in 16B on the stack, if possible. If the properties data exceeds 16B, the first 8B of the stack buffer indicates the total number of bytes required for the storage of properties, and the second 8B a pointer to the array on the heap that stores properties. This technique is called small buffer optimization.</p>
<p><img src="https://i.imgur.com/LWP2te8.png" alt=""></p>
<p>Additionally, there were various other smaller improvements of existing internal data structures, most notably the <a href="https://en.wikipedia.org/wiki/Skip_list">skip list</a> which is used as an indexing data structure. The combined result was a massive improvement in memory usage with a substantial reduction in memory fragmentation while ensuring equivalent or better performance. Before jumping into the analysis and explanations, let’s have a look at the benchmark setup.</p>
<h2>Setup</h2>
<h3>Target Systems</h3>
<p>The benchmark tests different Memgraph versions. The release dates and details of each version are the following:</p>
<ul>
<li><strong>v0.15.2</strong>, October 23, 2019, the last version of Memgraph that used an in-memory storage engine based on copies of nodes/edges.</li>
<li><strong>v0.50.0</strong>, December 11, 2019, introduced the new in-memory storage engine based on data changes and a C-based storage API.</li>
<li><strong>v1.0.0</strong>, April 6, 2020, introduced a Python-based storage API.</li>
<li><strong>v1.1.0</strong>, July 1, 2020, added encoding and compression to node and edge properties.</li>
</ul>
<p>The <a href="https://docs.memgraph.com/memgraph/changelog">Memgraph Changelog Docs</a> page contains more details about changes in each version.</p>
<h3>Hardware</h3>
<ul>
<li>Server: HP DL360 G6</li>
<li>CPU: 2x Intel Xeon X5650 6C12T @ 2.67GHz</li>
<li>RAM: 144GB</li>
<li>Disk: 120GB SSD</li>
<li>OS: Debian 9 Stretch</li>
</ul>
<h3>Workload</h3>
<p>The benchmark consists of several different queries that test the performance of the graph database. Typical graph database workloads consist of READ, CREATE, and ANALYZE queries.</p>
<p>Memgraph is particularly well suited for hybrid transactional-analytical workloads where it’s crucial to ingest data as fast as possible and simultaneously deliver analytics as quickly as possible. For this reason, we are mostly interested in traversal queries (1-Hop, 2-Hop, etc.) which represent the majority of analytical queries.</p>
<p>In the following table, you can find the exact queries we have used for benchmarking.</p>
<pre><code>|            Query Name             |                                                Query                                                        |  Query Type |
| --------------------------------- | ----------------------------------------------------------------------------------------------------------- | ----------- |
| Aggregation                       | `MATCH (n:User) RETURN n.age, COUNT(*);`                                                                    | ANALYZE     |
| 1-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;(n:User) RETURN n.id;`                                                          | ANALYZE     |
| 2-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;()--&gt;(n:User) RETURN DISTINCT n.id;`                                            | ANALYZE     |
| 3-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;()--&gt;()--&gt;(n:User) RETURN DISTINCT n.id;`                                       | ANALYZE     |
| 4-Hop Expand                      | `MATCH (s:User {id: $id})--&gt;()--&gt;()--&gt;()--&gt;(n:User) RETURN DISTINCT n.id;`                                  | ANALYZE     |
| 2-Hop Variable Expand             | `MATCH (s:User {id: $id})-[*1..2]-&gt;(n:User) RETURN DISTINCT n.id;`                                          | ANALYZE     |
| 2-Hop Variable Expand with Result | `MATCH (s:User {id: $id})-[*1..2]-&gt;(n:User) RETURN DISTINCT n.id, n;`                                       | ANALYZE     |
| Shortest Path                     | `MATCH p=(n:User {id: $from})-[*bfs..15]-&gt;(m:User {id: $to}) RETURN extract(n in nodes(p) | n.id) AS path;` | ANALYZE     |
| Insert New Relationship           | `MATCH (n:User {id: $from}), (m:User {id: $to}) WITH n, m CREATE (n)-[e:Temp]-&gt;(m) RETURN e;`               | CREATE      |
| Find Node                         | `MATCH (n:User {id : $id}) RETURN n;`                                                                       | READ        |
| Insert a New Node                 | `CREATE (n:UserTemp {id : $id}) RETURN n;`                                                                  | CREATE      |
</code></pre>
<p>The benchmarking harness executed all queries against the <a href="https://snap.stanford.edu/data/soc-Pokec.html">Pokec dataset</a> which contains 1.6M nodes and 30.6M edges. Given that the goal of each benchmark is to saturate the target system to extract its peak characteristics, we took great care in carefully designing and polishing our setup. Some of the essential elements of the harness are:</p>
<ul>
<li>optimized C/C++ client to minimize client overhead.</li>
<li>fresh dataset load before each execution.</li>
<li>concurrent execution (up to 12 cores) to push Memgraph to its limits.</li>
</ul>
<h2>Data Import Analysis</h2>
<p>Before delving deep into the workload queries execution analysis, a couple of notes about the data import.</p>
<p>The harness imported data in a real-time manner, equivalent to normal query execution by running queries. The data was imported using 8 concurrent clients.  Throughput and peak memory usage were measured during the data import. A couple of interesting insights emerged. On the memory usage side, there is already a massive difference between Memgraph versions. <strong>Before v0.50.0</strong>, Memgraph stored all data modifications as whole copies of the modified objects. By removing the need to make whole copies of database objects, versions <strong>after v0.50.0</strong> provide a significantly less memory usage. The same benefits apply to the runtime environment since the import uses regular queries.</p>
<p><img src="https://i.imgur.com/VfDzuCw.png" alt=""></p>
<p>At this point, you might be wondering about the import speed. As the chart below shows, throughput on basic CREATE queries also improved. Since data copying is generally a fast operation, throughput improvement is not huge but is still significant. One important thing to notice is the difference between v1.0.0 and v1.1.0. <strong>v1.1.0</strong> has almost the same throughput as versions before even though more work is involved in property compression.</p>
<p><img src="https://i.imgur.com/hsh6uDj.png" alt=""></p>
<h2>Query Execution Analysis</h2>
<p>Let’s start analyzing the <strong>workload queries</strong>. The following radar chart shows peak memory usage during query execution across different Memgraph versions. Keep in mind that less is better.</p>
<p>As you can see, <strong>v1.1.0 uses ~50%</strong> less memory compared to v0.15.2. v0.50.0 and v1.0.0 fall in-between with almost no difference because not much from the storage perspective changed between these two versions. The most significant difference is between v0.15.2 and v0.50.0 (introduction of the new storage engine), and v1.0.0 and v1.1.0 (introduction of encoded and compressed properties).</p>
<p><img src="https://i.imgur.com/4ETpDXT.png" alt=""></p>
<p>On the other hand, while looking at memory, it’s also critical to observe what is happening with the throughput. Generally, there is a well-known trade-off between space and time. But, as you can see in the following chart, v1.1.0 has the best performance. Please note that in this case, more is better.</p>
<p><img src="https://i.imgur.com/c0nBqJS.png" alt=""></p>
<p>Of course, not all queries yield significant throughput improvements. E.g., the <code>Insert New Node</code> query performance stayed similar across different Memgraph versions. Nonetheless, the following chart illustrates well how Memgraph scales with the number of concurrent requests.</p>
<p><img src="https://i.imgur.com/vIy7rTM.png" alt=""></p>
<p>The new storage engine also introduced a feature where it is possible to disable storing properties on edges. Sometimes graph datasets don’t have any data attached to edges. Memgraph offers a configuration option to remove all associated data structures required to store data on edges. As you can see in the following chart, by not having properties on edges, memory usage goes down by almost 50% (in addition to the reductions mentioned above). v0.15.2 can’t disable the property storage on edges, so the chart shows nothing there. Using all the improvements in new versions of Memgraph you can store the same dataset in 3.36x less memory (comparing v0.15.2 with properties enabled and v1.1.0 with properties disabled).</p>
<p><img src="https://i.imgur.com/PtQviBf.png" alt=""></p>
<p>The rest of the charts show performance for each query with regards to linear scalability. Linear scalability is the ability of a system to handle more work by adding more resources linearly. E.g., by doubling the number of cores, the system is capable of executing twice as many queries. In practice, it’s impossible to reach perfect linear scalability due to various overheads.  The real question is how far Memgraph is from linear scalability? As you can see below, not by a lot.</p>
<p>The following chart shows throughput data for the simple read query (Find Node). The node lookup inside Memgraph has an O(logN) complexity.</p>
<p><img src="https://i.imgur.com/0z46Rg5.png" alt=""></p>
<p>In the next case, instead of reading, Memgraph creates an edge. The operation contains two node lookups and one edge write. Which means it’s very similar to the Find Node query. Instead of one lookup, the action has two lookups and one write. The chart looks very similar to the Find Node chart.</p>
<p><img src="https://i.imgur.com/ZWzeFc6.png" alt=""></p>
<p>By moving towards more complex queries, there is a more significant scalability difference between previous versions of Memgraph and the latest ones. v0.15.2 is far behind the latest versions.</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://memgraph.com/blog/memgraph-1-1-benchmarks">https://memgraph.com/blog/memgraph-1-1-benchmarks</a></em></p>]]>
            </description>
            <link>https://memgraph.com/blog/memgraph-1-1-benchmarks</link>
            <guid isPermaLink="false">hacker-news-small-sites-24651091</guid>
            <pubDate>Thu, 01 Oct 2020 14:19:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ontario police used Covid-19 database illegally, civil rights groups find]]>
            </title>
            <description>
<![CDATA[
Score 248 | Comments 70 (<a href="https://news.ycombinator.com/item?id=24650515">thread link</a>) | @seigando
<br/>
October 1, 2020 | https://www.cbc.ca/news/canada/toronto/covid-police-database-1.5745481 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/canada/toronto/covid-police-database-1.5745481">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Police forces across&nbsp;Ontario&nbsp;engaged in broad, illegal searches&nbsp;of a now-defunct COVID-19 database, two civil rights groups alleged Wednesday, claiming the use of the portal violated individual privacy rights for months.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5704129.1598642644!/cumulusImage/httpImage/image.jpg_gen/derivatives/16x9_780/shutterstock-medium-file-typing-on-laptop.jpg"></p></div><figcaption>The Canadian Civil Liberties Association and the Canadian&nbsp;Constitution Foundation say in separate reports that many services used the database to look at COVID-19 test results for wide geographic areas and sometimes pulled up personal information unrelated to active calls.<!-- --> <!-- -->(maradon 333 / Shutterstock)</figcaption></figure><p><span><p>Police forces across&nbsp;Ontario&nbsp;engaged in broad, illegal searches&nbsp;of a now-defunct COVID-19 database, two civil rights groups alleged Wednesday, claiming the use of the portal violated individual privacy rights for months.</p>  <p>The Canadian Civil Liberties Association (CCLA) and the Canadian&nbsp;Constitution Foundation (CCF) said in separate reports that many services used the database to look at COVID-19 test results for wide geographic areas and sometimes pulled up personal information unrelated to active calls.</p>  <p>"People weren't told that when they went for COVID tests that&nbsp;this information was being shared with police and they certainly weren't asked for their consent," said Abby Deshman, the criminal&nbsp;justice program director for the CCLA.&nbsp;</p>  <p>"That should be a decision every person makes about what they&nbsp;want to do with their own personal medical information."</p>  <p>In early April, the&nbsp;Ontario&nbsp;government passed an emergency order&nbsp;that allowed police to obtain the names, addresses and dates of birth of Ontarians who had tested positive for COVID-19. The portal was aimed at helping to protect first responders.</p>  <p>Police access to that database ended on Aug. 17, after a legal&nbsp;challenge was filed by a group of human rights&nbsp; organizations.</p>  <p>The group, which included the CCLA, argued that allowing police&nbsp;to access personal health records violated individuals'<br> constitutional rights to privacy and equality.</p>  <h2>Police conducted 95,000 searches of database</h2>  <p>Data released in the context of the legal action showed that&nbsp; Ontario&nbsp;police services conducted over 95,000 searches of the&nbsp;database while it was active.</p>  <p>The CCF filed a freedom of information act request to the&nbsp;province related to police use of the database.&nbsp;</p>    <blockquote><span><span><svg version="1.1" focusable="false" x="0px" y="0px" width="30px" height="25px" viewBox="0 0 52.157 39.117" enable-background="new 0 0 52.157 39.117" space="preserve"><g><g><path fill="000000" d="M22.692,10.113c-5.199,1.4-8.398,4.4-8.398,8.801c0,2.4,2,3,3.6,4.199c2.2,1.602,3.4,3.4,3.4,6.602   c0,3.799-3.4,6.799-7.4,6.799c-4.6,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z M45.692,10.113   c-5.199,1.4-8.399,4.4-8.399,8.801c0,2.4,2,3,3.601,4.199c2.2,1.602,3.399,3.4,3.399,6.602c0,3.799-3.399,6.799-7.399,6.799   c-4.601,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z"></path></g></g><g display="none"><g display="inline"> <path fill="000000" d="M6.648,29.759c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.399-3.4-3.399-6.6   c0-3.801,3.399-6.801,7.399-6.801c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z M29.648,29.759   c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.401-3.4-3.401-6.6c0-3.801,3.401-6.801,7.401-6.801   c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z"></path></g></g></svg>Police were caught using the COVID-19 database to look up names&nbsp;unrelated to active calls, to do wholesale postal&nbsp; code searches for COVID-19 cases, and to even do broad based searches outside officers' own cities.<svg focusable="false" x="0px" y="0px" width="23px" height="22px" viewBox="0 0 52.157 39.117" enable-background="new 0 0 52.157 39.117" space="preserve"><g display="none"><g display="inline"><path fill="000000" d="M22.692,10.113c-5.199,1.4-8.398,4.4-8.398,8.801c0,2.4,2,3,3.6,4.199c2.2,1.602,3.4,3.4,3.4,6.602   c0,3.799-3.4,6.799-7.4,6.799c-4.6,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z M45.692,10.113   c-5.199,1.4-8.399,4.4-8.399,8.801c0,2.4,2,3,3.601,4.199c2.2,1.602,3.399,3.4,3.399,6.602c0,3.799-3.399,6.799-7.399,6.799   c-4.601,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z"></path></g></g><g><g><path fill="000000" d="M6.648,29.759c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.399-3.4-3.399-6.6   c0-3.801,3.399-6.801,7.399-6.801c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z M29.648,29.759   c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.401-3.4-3.401-6.6c0-3.801,3.401-6.801,7.401-6.801   c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z"></path></g></g></svg></span><cite>- Christine Van Geyn, Canadian Constitution Foundation</cite></span></blockquote>    <p>On Wednesday, the CCF made public a June memo from the Solicitor&nbsp;General's office to chiefs of police that warned against using the&nbsp;database beyond the "express purpose" of the emergency order.</p>  <p>The CCF said the memo revealed a "shocking misuse" of personal&nbsp;health information by police.</p>  <p>"Police were caught using the COVID-19 database to look up names&nbsp;unrelated to active calls, to do wholesale postal&nbsp; code searches for COVID-19 cases, and to even do broad based searches outside officers' own cities," said CCF litigation director, Christine Van&nbsp;Geyn.&nbsp;<span><ul><li><a href="https://www.cbc.ca/news/canada/toronto/covid-ont-police-database-1.5690220" data-contentid="" flag="" text="Ontario ends police access to COVID-19 database after legal challenge"><span>Ontario ends police access to COVID-19 database after legal challenge</span></a></li></ul></span></p>  <p>The CCF said it has filed a complaint with&nbsp;Ontario's privacy&nbsp;commissioner over violations of the Personal Health Information Protection Act, and with the&nbsp;Ontario&nbsp;Independent Police Review Director for officer misconduct.</p>  <p>Meanwhile, the CCLA sent letters to 37 police forces, asking them&nbsp;for details of how the database was used and if any information was retained from it.</p>  <p>Twenty-three responded and Deshman said she expects more to do&nbsp;so.</p>  <h2>Thunder Bay, Durham police conducted more than 40% of searches</h2>  <p>Many forces found the database difficult to use and resorted to&nbsp;problematic broad searches in an attempt to find workarounds, the CCLA said.</p>  <p>The association notes that more than 40 per cent of the 95,000&nbsp;searches of the database were conducted by either the Thunder Bay police or Durham Region police.&nbsp;</p>    <p>In Durham Region, police continued to run unauthorized searches&nbsp;even after provincial audits called attention to the inappropriate&nbsp;searches taking place, the CCLA said. The force's access to the portal was cut off by the province as a result, the CCLA said.</p>  <p>"Durham is a particularly concerning example," said Deshman.&nbsp;"In those cases there needs to be disclosure (to citizens whose information was accessed) and accountability by following up with the individuals in the police service that looked up information inappropriately."</p>  <h2>Anyone concerned can contact police, privacy commissioner&nbsp;</h2>  <p>Holly Walbourne, the legal counsel for Thunder Bay police, said&nbsp;in a letter sent to the groups that filed the legal challenge that&nbsp;the force understood their concerns but that police had "lawful authority" to use the database to protect first responders.</p>    <p>Durham police Supt. Peter Cousins wrote a report to the force's&nbsp;police services board on the issue on Sept. 1 saying&nbsp; access to theportal and its information was treated "seriously and with due care."&nbsp;</p>  <p>Toronto police never used the database because of "issues with&nbsp;the accuracy and reliability of the information," the CCLA reported. York Region police said they asked the province to revoke access to the database after an internal review found the risks associated with accessing personal health information outweighed any benefits.</p>  <p>Deshman said anyone concerned about police access to the province's COVID-19 database should contact their local police force&nbsp;and the&nbsp;Ontario&nbsp;Privacy Commissioner.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/canada/toronto/covid-police-database-1.5745481</link>
            <guid isPermaLink="false">hacker-news-small-sites-24650515</guid>
            <pubDate>Thu, 01 Oct 2020 13:23:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Segment Tree – Competitive Programming Algorithms]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24650084">thread link</a>) | @pmoriarty
<br/>
October 1, 2020 | https://cp-algorithms.com/data_structures/segment_tree.html | <a href="https://web.archive.org/web/*/https://cp-algorithms.com/data_structures/segment_tree.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="container">
    


<p>A Segment Tree is a data structure that allows answering range queries over an array effectively, while still being flexible enough to allow modifying the array. 
This includes finding the sum of consecutive array elements $a[l \dots r]$, or finding the minimum element in a such a range in $O(\log n)$ time. 
Between answering such queries the Segment Tree allows modifying the array by replacing one element, or even change the elements of a whole subsegment (e.g. assigning all elements $a[l \dots r]$ to any value, or adding a value to all element in the subsegment).</p>

<p>In general a Segment Tree is a very flexible data structure, and a huge number of problems can be solved with it. 
Additionally it is also possible to apply more complex operations and answer more complex queries (see <a href="https://cp-algorithms.com/data_structures/segment_tree.html#advanced-versions-of-segment-trees">Advanced versions of Segment Trees</a>).
In particular the Segment Tree can be easily generalized to larger dimensions. 
For instance with a two-dimensional Segment Tree you can answer sum or minimum queries over some subrectangle of a given matrix.
However only in $O(\log^2 n)$ time.</p>

<p>One important property of Segment Trees is, that they require only a linear amount of memory.
The standard Segment Tree requires $4n$ vertices for working on an array of size $n$.</p>

<h2>Simplest form of a Segment Tree</h2>

<p>To start easy, we consider the simplest form of a Segment Tree. 
We want to answer sum queries efficiently. 
The formal definition of our task is:
We have an array $a[0 \dots n-1]$, and the Segment Tree must be able to find the sum of elements between the indices $l$ and $r$ (i.e. computing the sum $\sum_{i=l}^r a[i]$), and also handle changing values of the elements in the array (i.e. perform assignments of the form $a[i] = x$). 
The Segment Tree should be able to process both queries in $O(\log n)$ time.</p>

<h3>Structure of the Segment Tree</h3>

<p>So, what is a Segment Tree?</p>

<p>We compute and store the sum of the elements of the whole array, i.e. the sum of the segment $a[0 \dots n-1]$. 
We then split the array into two halves $a[0 \dots n/2]$ and $a[n/2+1 \dots n-1]$ and compute the sum of each halve and store them. 
Each of these two halves in turn also split in half, their sums are computed and stored. 
And this process repeats until all segments reach size $1$. 
In other words we start with the segment $a[0 \dots n-1]$, split the current segment in half (if it has not yet become a segment containing a single element), and then calling the same procedure for both halves. 
For each such segment we store the sum of the numbers on it.</p>

<p>We can say, that these segments form a binary tree: 
the root of this tree is the segment $a[0 \dots n-1]$, and each vertex (except leaf vertices) has exactly two child vertices. 
This is why the data structure is called "Segment Tree", even though in most implementations the tree is not constructed explicitly (see <a href="https://cp-algorithms.com/data_structures/segment_tree.html#implementation">Implementation</a>).</p>

<p>Here is a visual representation of such a Segment Tree over the array $a = [1, 3, -2, 8, -7]$:</p>

<p><img src="https://raw.githubusercontent.com/e-maxx-eng/e-maxx-eng/master/img/sum-segment-tree.png" alt="&quot;Sum Segment Tree&quot;"></p>

<p>From this short description of the data structure, we can already conclude that a Segment Tree only requires a linear number of vertices. 
The first level of the tree contains a single node (the root), the second level will contain two vertices, in the third it will contain four vertices, until the number of vertices reaches $n$. 
Thus the number of vertices in the worst case can be estimated by the sum $1 + 2 + 4 + \dots + 2^{\lceil\log_2 n\rceil} = 2^{\lceil\log_2 n\rceil + 1} \lt 4n$.</p>

<p>It is worth noting that whenever $n$ is not a power of two, not all levels of the Segment Tree will be completely filled. 
We can see that behavior in the image.
For now we can forget about this fact, but it will become important later during the implementation.</p>

<p>The height of the Segment Tree is $O(\log n)$, because when going down from the root to the leaves the size of the segments decreases approximately by half.</p>

<h3>Construction</h3>

<p>Before constructing the segment tree, we need to decide:</p>

<ol>
<li>the <em>value</em> that gets stored at each node of the segment tree.
For example, in a sum segment tree, a node would store the sum of the elements in its range $[l, r]$.</li>
<li>the <em>merge</em> operation that merges two siblings in a segment tree.
For example, in a sum segment tree, the two nodes corresponding to the ranges $a[l_1 \dots r_1]$ and $a[l_2 \dots r_2]$ would be merged into a node corresponding to the range $a[l_1 \dots r_2]$ by adding the values of the two nodes.</li>
</ol>

<p>Note that a vertex is a "leaf vertex", if its corresponding segment covers only one value in the original array. It is present at the lowermost level of a segment tree. Its value would be equal to the (corresponding) element $a[i]$.</p>

<p>Now, for construction of the segment tree, we start at the bottom level (the leaf vertices) and assign them their respective values. On the basis of these values, we can compute the values of the previous level, using the <code>merge</code> function.
And on the basis of those, we can compute the values of the previous, and repeat the procedure until we reach the root vertex.</p>

<p>It is convenient to describe this operation recursively in the other direction, i.e., from the root vertex to the leaf vertices. The construction procedure, if called on a non-leaf vertex, does the following:</p>

<ol>
<li>recursively construct the values of the two child vertices</li>
<li>merge the computed values of these children.</li>
</ol>

<p>We start the construction at the root vertex, and hence, we are able to compute the entire segment tree.</p>

<p>The time complexity of this construction is $O(n)$, assuming that the merge operation is constant time (the merge operation gets called $n$ times, which is equal to the number of internal nodes in the segment tree).</p>

<h3>Sum queries</h3>

<p>For now we are going to answer sum queries. As an input we receive two integers $l$ and $r$, and we have to compute the sum of the segment $a[l \dots r]$ in $O(\log n)$ time.</p>

<p>To do this, we will traverse the Segment Tree and use the precomputed sums of the segments.
Let's assume that we are currently at the vertex that covers the segment $a[tl \dots tr]$.
There are three possible cases.</p>

<p>The easiest case is when the segment $a[l \dots r]$ is equal to the corresponding segment of the current vertex (i.e. $a[l \dots r] = a[tl \dots tr]$), then we are finished and can return the precomputed sum that is stored in the vertex.</p>

<p>Alternatively the segment of the query can fall completely into the domain of either the left or the right child.
Recall that the left child covers the segment $a[tl \dots tm]$ and the right vertex covers the segment $a[tm + 1 \dots tr]$ with $tm = (tl + tr) / 2$. 
In this case we can simply go to the child vertex, which corresponding segment covers the query segment, and execute the algorithm described here with that vertex.</p>

<p>And then there is the last case, the query segment intersects with both children. 
In this case we have no other option as to make two recursive calls, one for each child.
First we go to the left child, compute a partial answer for this vertex (i.e. the sum of values of the intersection between the segment of the query and the segment of the left child), then go to the right child, compute the partial answer using that vertex, and then combine the answers by adding them. 
In other words, since the left child represents the segment $a[tl \dots tm]$ and the right child the segment $a[tm+1 \dots tr]$, we compute the sum query $a[l \dots tm]$ using the left child, and the sum query $a[tm+1 \dots r]$ using the right child.</p>

<p>So processing a sum query is a function that recursively calls itself once with either the left or the right child (without changing the query boundaries), or twice, once for the left and once for the right child (by splitting the query into two subqueries). 
And the recursion ends, whenever the boundaries of the current query segment coincides with the boundaries of the segment of the current vertex. 
In that case the answer will be the precomputed value of the sum of this segment, which is stored in the tree.</p>

<p>In other words, the calculation of the query is a traversal of the tree, which spreads through all necessary branches of the tree, and uses the precomputed sum values of the segments in the tree.</p>

<p>Obviously we will start the traversal from the root vertex of the Segment Tree.</p>

<p>The procedure is illustrated in the following image.
Again the array $a = [1, 3, -2, 8, -7]$ is used, and here we want to compute the sum $\sum_{i=2}^4 a[i]$.
The colored vertices will be visited, and we will use the precomputed values of the green vertices.
This gives us the result $-2 + 1 = -1$.</p>

<p><img src="https://raw.githubusercontent.com/e-maxx-eng/e-maxx-eng/master/img/sum-segment-tree-query.png" alt="&quot;Sum Segment Tree Query&quot;"></p>

<p>Why is the complexity of this algorithm $O(\log n)$?
To show this complexity we look at each level of the tree. 
It turns out, that for each level we only visit not more than four vertices. 
And since the height of the tree is $O(\log n)$, we receive the desired running time.</p>

<p>We can show that this proposition (at most four vertices each level) is true by induction.
At the first level, we only visit one vertex, the root vertex, so here we visit less than four vertices. 
Now let's look at an arbitrary level.
By induction hypothesis, we visit at most four vertices. 
If we only visit at most two vertices, the next level has at most four vertices. That trivial, because each vertex can only cause at most two recursive calls. 
So let's assume that we visit three or four vertices in the current level. 
From those vertices, we will analyze the vertices in the middle more carefully. 
Since the sum query asks for the sum of a continuous subarray, we know that segments corresponding to the visited vertices in the middle will be completely covered by the segment of the sum query. 
Therefore these vertices will not make any recursive calls. 
So only the most left, and the most right vertex will have the potential to make recursive calls. 
And those will only create at most four recursive calls, so also the next level will satisfy the assertion.
We can say that one branch approaches the left boundary of the query, and the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cp-algorithms.com/data_structures/segment_tree.html">https://cp-algorithms.com/data_structures/segment_tree.html</a></em></p>]]>
            </description>
            <link>https://cp-algorithms.com/data_structures/segment_tree.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24650084</guid>
            <pubDate>Thu, 01 Oct 2020 12:25:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Web Neural Network API]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24649616">thread link</a>) | @rajveermalviya
<br/>
October 1, 2020 | https://webmachinelearning.github.io/webnn/ | <a href="https://web.archive.org/web/*/https://webmachinelearning.github.io/webnn/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
   <h2 data-level="1" id="intro"><span>1. </span><span>Introduction</span><a href="#intro"></a></h2>
   <p>We’re working on this section. Meanwhile, please take a look at the <a href="https://github.com/webmachinelearning/webnn/blob/master/explainer.md">explainer</a>.</p>
   <h2 data-level="2" id="usecases"><span>2. </span><span>Use cases</span><a href="#usecases"></a></h2>
   <h3 data-level="2.1" id="usecases-application"><span>2.1. </span><span>Application Use Cases</span><a href="#usecases-application"></a></h3>
   <p>This section illustrates application-level use cases for neural network
inference hardware acceleration. All applications in those use cases can be
built on top of pre-trained deep neural network (DNN) models.</p>
   <h4 data-level="2.1.1" id="usecase-person-detection"><span>2.1.1. </span><span>Person Detection</span><a href="#usecase-person-detection"></a></h4>
   <p>A user opens a web-based video conferencing application, but she temporarily
leaves from her room. The application is watching whether she is in front of her
PC by using object detection (for example, using object detection approaches
such as <a data-link-type="biblio" href="#biblio-ssd">[SSD]</a> or <a data-link-type="biblio" href="#biblio-yolo">[YOLO]</a> that use a single DNN) to detect regions in a camera
input frame that include persons.</p>
   <p>When she comes back, the application automatically detects her and notifies
other online users that she is active now.</p>
   <h4 data-level="2.1.2" id="usecase-segmentation"><span>2.1.2. </span><span>Semantic Segmentation</span><a href="#usecase-segmentation"></a></h4>
   <p>A user joins a teleconference via a web-based video conferencing application at
her desk since no meeting room in her office is available. During the
teleconference, she does not wish that her room and people in the background are
visible. To protect the privacy of the other people and the surroundings, the
application runs a machine learning model such as <a data-link-type="biblio" href="#biblio-deeplabv3">[DeepLabv3+]</a> or <a data-link-type="biblio" href="#biblio-maskr-cnn">[MaskR-CNN]</a> to semantically split an image into segments and replaces
segments that represent other people and background with another picture.</p>
   <h4 data-level="2.1.3" id="usecase-skeleton-detection"><span>2.1.3. </span><span>Skeleton Detection</span><a href="#usecase-skeleton-detection"></a></h4>
   <p>A web-based video conferencing application tracks a pose of user’s skeleton by
running a machine learning model, which allows for real-time human pose
estimation, such as <a data-link-type="biblio" href="#biblio-posenet">[PoseNet]</a> to recognize her gesture and body language. When
she raises her hand, her microphone is automatically unmuted and she can start
speaking on the teleconference.</p>
   <h4 data-level="2.1.4" id="usecase-face-recognition"><span>2.1.4. </span><span>Face Recognition</span><a href="#usecase-face-recognition"></a></h4>
   <p>There are multiple people in the conference room and they join an online meeting
using a web-based video conferencing application. The application detects faces
of participants by using object detection (for example, using object detection
approaches such as <a data-link-type="biblio" href="#biblio-ssd">[SSD]</a>) and checks whether each face was present at the
previous meeting or not by running a machine learning model such as <a data-link-type="biblio" href="#biblio-facenet">[FaceNet]</a>,
which verifies whether two faces would be identical or not.</p>
   <h4 data-level="2.1.5" id="usecase-facial-landmarks"><span>2.1.5. </span><span>Facial Landmark Detection</span><a href="#usecase-facial-landmarks"></a></h4>
   <p>A user wants to find new glasses that beautifully fits her on an online glasses
store. The online store offers web-based try-on simulator that runs a machine
learning model such as Face Alignment Network <a data-link-type="biblio" href="#biblio-fan">[FAN]</a> to detect facial landmarks
like eyes, nose, mouth, etc. When she chooses a pair of glasses, the simulator
properly render the selected glasses on the detected position of eyes on her
facial image.</p>
   <h4 data-level="2.1.6" id="usecase-style-transfer"><span>2.1.6. </span><span>Style Transfer</span><a href="#usecase-style-transfer"></a></h4>
   <p>A user is looking for cosmetics on an online store and wondering which color may
fit her face. The online store shows sample facial makeup images of cosmetics,
and offers makeup simulator that runs a machine learning model like <a data-link-type="biblio" href="#biblio-contextualloss">[ContextualLoss]</a> or <a data-link-type="biblio" href="#biblio-pairedcyclegan">[PairedCycleGAN]</a> to transfer the makeup style of the
sample makeup image to her facial image. She can check how the selected makeup
looks like on her face by the simulator.</p>
   <h4 data-level="2.1.7" id="usecase-super-resolution"><span>2.1.7. </span><span>Super Resolution</span><a href="#usecase-super-resolution"></a></h4>
   <p>A web-based video conferencing is receiving a video stream from its peer, but
the resolution of the video becomes lower due to network congestion. To prevent
degradation of the perceived video quality, the application runs a machine
learning model for super-resolution such as <a data-link-type="biblio" href="#biblio-srgan">[SRGAN]</a> to generate
higher-resolution video frames.</p>
   <h4 data-level="2.1.8" id="usecase-image-captioning"><span>2.1.8. </span><span>Image Captioning</span><a href="#usecase-image-captioning"></a></h4>
   <p>For better accessibility, a web-based presentation application provides
automatic image captioning by running a machine learning model such as <a data-link-type="biblio" href="#biblio-im2txt">[im2txt]</a> which predicts explanatory words of the presentation slides.</p>
   <h4 data-level="2.1.9" id="usecase-translation"><span>2.1.9. </span><span>Machine Translation</span><a href="#usecase-translation"></a></h4>
   <p>Multiple people from various countries are talking via a web-based real-time
text chat application. The application translates their conversation by using a
machine learning model such as <a data-link-type="biblio" href="#biblio-gnmt">[GNMT]</a> or <a data-link-type="biblio" href="#biblio-opennmt">[OpenNMT]</a>, which translates every
text into different language.</p>
   <h4 data-level="2.1.10" id="usecase-emotion-analysis"><span>2.1.10. </span><span>Emotion Analysis</span><a href="#usecase-emotion-analysis"></a></h4>
   <p>A user is talking to her friend via a web-based real-time text chat application,
and she is wondering how the friend feels because she cannot see the friend’s
face. The application analyses the friend’s emotion by using a machine learning
model such as <a data-link-type="biblio" href="#biblio-deepmoji">[DeepMoji]</a>, which infers emotion from input texts, and displays
an emoji that represents the estimated emotion.</p>
   <h4 data-level="2.1.11" id="usecase-video-summalization"><span>2.1.11. </span><span>Video Summarization</span><a href="#usecase-video-summalization"></a></h4>
   <p>A web-based video conferencing application records received video streams, and
it needs to reduce recorded video data to be stored. The application generates
the short version of the recorded video by using a machine learning model for
video summarization such as <a data-link-type="biblio" href="#biblio-video-summarization-with-lstm">[Video-Summarization-with-LSTM]</a>.</p>
   <h4 data-level="2.1.12" id="usecase-noise-suppression"><span>2.1.12. </span><span>Noise Suppression</span><a href="#usecase-noise-suppression"></a></h4>
   <p>A web-based video conferencing application records received audio streams, but
usually the background noise is everywhere. The application leverages real-time 
noise suppression using Recurrent Neural Network such as <a data-link-type="biblio" href="#biblio-rnnoise">[RNNoise]</a> for 
suppressing background dynamic noise like baby cry or dog barking to improve 
audio experiences in video conferences.</p>
   <h3 data-level="2.2" id="usecases-framework"><span>2.2. </span><span>Framework Use Cases</span><a href="#usecases-framework"></a></h3>
   <p>This section collects framework-level use cases for a dedicated low-level API
for neural network inference hardware acceleration. It is expected that Machine
Learning frameworks will be key consumers of the Web Neural Network API (WebNN
API) and the low-level details exposed through the WebNN API are abstracted out
from typical web developers. However, it is also expected that web developers
with specific interest and competence in Machine Learning will want to interface
with the WebNN API directly instead of a higher-level ML framework.</p>
   <h4 data-level="2.2.1" id="usecase-custom-layer"><span>2.2.1. </span><span>Custom Layer</span><a href="#usecase-custom-layer"></a></h4>
   <p>A web application developer wants to run a DNN model on the WebNN API. However,
she has found that some of activation functions like <a data-link-type="biblio" href="#biblio-leakyrelu">[LeakyReLU]</a>, <a data-link-type="biblio" href="#biblio-elu">[ELU]</a>,
etc. are not included in the WebNN API. To address this issue, she constructs
custom layers of the additional activation functions on top of the WebNN API.
Note that the scope of custom layers may include convolution, normalization,
etc. as well as activation.</p>
   <h4 data-level="2.2.2" id="usecase-network-concat"><span>2.2.2. </span><span>Network Concatenation</span><a href="#usecase-network-concat"></a></h4>
   <p>A web application uses a DNN model, and its model data of upper convolutional
layers and lower fully-connected layers are stored in separate files, since
model data of the fully-connected layers are periodically updated due to fine
tuning at the server side.</p>
   <p>Therefore, the application downloads both partial model files at first and
concatenates them into a single model. When the model is updated, the
application downloads fine-tuned part of the model and replace only the
fully-connected layers with it.</p>
   <h4 data-level="2.2.3" id="usecase-perf-adapt"><span>2.2.3. </span><span>Performance Adaptation</span><a href="#usecase-perf-adapt"></a></h4>
   <p>A web application developer has a concern about performance of her DNN model on
mobile devices. She has confirmed that it may run too slow on mobile devices
which do not have GPU acceleration. To address this issue, her web application
refers to the WebNN API to confirm whether acceleration is available or not, so
that the application can display the warning for devices without acceleration.</p>
   <p>After several weeks, she has developed a tiny DNN model that can even run on
CPU. In order to accommodate CPU execution, she modifies the application
so that the application loads the tiny model in the case of CPU-only devices.</p>
   <h2 data-level="3" id="api"><span>3. </span><span>API</span><a href="#api"></a></h2>
   <h3 data-level="3.1" id="api-navigator"><span>3.1. </span><span>Navigator</span><a href="#api-navigator"></a></h3>
<pre><c- b="">partial</c-> <c- b="">interface</c-> <a data-link-type="interface" href="https://html.spec.whatwg.org/multipage/system-state.html#navigator" id="ref-for-navigator"><c- g="">Navigator</c-></a> {
  <c- b="">readonly</c-> <c- b="">attribute</c-> <a data-link-type="idl-name" href="#ml" id="ref-for-ml"><c- n="">ML</c-></a> <dfn data-dfn-for="Navigator" data-dfn-type="attribute" data-export="" data-readonly="" data-type="ML" id="dom-navigator-ml"><code><c- g="">ml</c-></code><a href="#dom-navigator-ml"></a></dfn>;
};
</pre>
   <h3 data-level="3.2" id="api-ml"><span>3.2. </span><span>ML</span><a href="#api-ml"></a></h3>
<pre><c- b="">interface</c-> <dfn data-dfn-type="interface" data-export="" id="ml"><code><c- g="">ML</c-></code></dfn> {
  <a data-link-type="idl-name" href="#neuralnetworkcontext" id="ref-for-neuralnetworkcontext"><c- n="">NeuralNetworkContext</c-></a> <dfn data-dfn-for="ML" data-dfn-type="method" data-export="" data-lt="getNeuralNetworkContext()" id="dom-ml-getneuralnetworkcontext"><code><c- g="">getNeuralNetworkContext</c-></code><a href="#dom-ml-getneuralnetworkcontext"></a></dfn>();
};
</pre>
   <h3 data-level="3.3" id="api-operanddescriptor"><span>3.3. </span><span>OperandDescriptor</span><a href="#api-operanddescriptor"></a></h3>
<pre><c- b="">enum</c-> <dfn data-dfn-type="enum" data-export="" id="enumdef-operandlayout"><code><c- g="">OperandLayout</c-></code></dfn> {
  <dfn data-dfn-for="OperandLayout" data-dfn-type="enum-value" data-export="" id="dom-operandlayout-nchw"><code><c- s="">"nchw"</c-></code><a href="#dom-operandlayout-nchw"></a></dfn>,
  <dfn data-dfn-for="OperandLayout" data-dfn-type="enum-value" data-export="" id="dom-operandlayout-nhwc"><code><c- s="">"nhwc"</c-></code><a href="#dom-operandlayout-nhwc"></a></dfn>
};

<c- b="">enum</c-> <dfn data-dfn-type="enum" data-export="" id="enumdef-operandtype"><code><c- g="">OperandType</c-></code></dfn> {
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-float32"><code><c- s="">"float32"</c-></code><a href="#dom-operandtype-float32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-float16"><code><c- s="">"float16"</c-></code><a href="#dom-operandtype-float16"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-int32"><code><c- s="">"int32"</c-></code><a href="#dom-operandtype-int32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-uint32"><code><c- s="">"uint32"</c-></code><a href="#dom-operandtype-uint32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-float32"><code><c- s="">"tensor-float32"</c-></code><a href="#dom-operandtype-tensor-float32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-float16"><code><c- s="">"tensor-float16"</c-></code><a href="#dom-operandtype-tensor-float16"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-int32"><code><c- s="">"tensor-int32"</c-></code><a href="#dom-operandtype-tensor-int32"></a></dfn>,
  <dfn data-dfn-for="OperandType" data-dfn-type="enum-value" data-export="" id="dom-operandtype-tensor-quant8-asymm"><code><c- s="">"tensor-quant8-asymm"</c-></code><a href="#dom-operandtype-tensor-quant8-asymm"></a></dfn>
};

<c- b="">dictionary</c-> <dfn data-dfn-type="dictionary" data-export="" id="dictdef-operanddescriptor"><code><c- g="">OperandDescriptor</c-></code></dfn> {
  // The operand type.
  <c- b="">required</c-> <a data-link-type="idl-name" href="#enumdef-operandtype" id="ref-for-enumdef-operandtype"><c- n="">OperandType</c-></a> <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="OperandType " id="dom-operanddescriptor-type"><code><c- g="">type</c-></code><a href="#dom-operanddescriptor-type"></a></dfn>;

  // The dimensions field is only required for tensor operands.
  // The negative value means an unknown dimension.
  <c- b="">sequence</c->&lt;<a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-long" id="ref-for-idl-long"><c- b="">long</c-></a>&gt; <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="sequence<long> " id="dom-operanddescriptor-dimensions"><code><c- g="">dimensions</c-></code><a href="#dom-operanddescriptor-dimensions"></a></dfn>;

  // The following two fields are only required for quantized operand.
  // scale: an non-negative floating point value
  // zeroPoint: an integer, in range [0, 255]
  // The real value is (value - zeroPoint) * scale
  <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-float" id="ref-for-idl-float"><c- b="">float</c-></a> <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="float " id="dom-operanddescriptor-scale"><code><c- g="">scale</c-></code><a href="#dom-operanddescriptor-scale"></a></dfn>;
  <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-long" id="ref-for-idl-long①"><c- b="">long</c-></a> <dfn data-dfn-for="OperandDescriptor" data-dfn-type="dict-member" data-export="" data-type="long " id="dom-operanddescriptor-zeropoint"><code><c- g="">zeroPoint</c-></code><a href="#dom-operanddescriptor-zeropoint"></a></dfn>;
};
</pre>
   <h3 data-level="3.4" id="api-operand"><span>3.4. </span><span>Operand</span><a href="#api-operand"></a></h3>
<pre><c- b="">interface</c-> <dfn data-dfn-type="interface" data-export="" id="operand"><code><c- g="">Operand</c-></code></dfn> {};
</pre>
   <h3 data-level="3.5" id="api-neuralnetworkcontext"><span>3.5. </span><span>NeuralNetworkContext</span><a href="#api-neuralnetworkcontext"></a></h3>
   <p>The <code><a data-link-type="idl" href="#neuralnetworkcontext" id="ref-for-neuralnetworkcontext①">NeuralNetworkContext</a></code> defines a set of operations derived from the first-wave models <a data-link-type="biblio" href="#biblio-models">[Models]</a> that address identified <a href="#usecases">§ 2 Use cases</a>.</p>
<pre><c- b="">typedef</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-double" id="ref-for-idl-double"><c- b="">double</c-></a> <dfn data-dfn-type="typedef" data-export="" id="typedefdef-number"><code><c- g="">number</c-></code></dfn>;

<c- b="">dictionary</c-> <dfn data-dfn-type="dictionary" data-export="" id="dictdef-namedoperand"><code><c- g="">NamedOperand</c-></code></dfn> {
  <c- b="">required</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-DOMString" id="ref-for-idl-DOMString"><c- b="">DOMString</c-></a> <dfn data-dfn-for="NamedOperand" data-dfn-type="dict-member" data-export="" data-type="DOMString " id="dom-namedoperand-name"><code><c- g="">name</c-></code><a href="#dom-namedoperand-name"></a></dfn>;
  <c- b="">required</c-> <a data-link-type="idl-name" href="#operand" id="ref-for-operand"><c- n="">Operand</c-></a> <dfn data-dfn-for="NamedOperand" data-dfn-type="dict-member" data-export="" data-type="Operand " id="dom-namedoperand-operand"><code><c- g="">operand</c-></code><a href="#dom-namedoperand-operand"></a></dfn>;
};

<c- b="">interface</c-> <dfn data-dfn-type="interface" data-export="" id="neuralnetworkcontext"><code><c- g="">NeuralNetworkContext</c-></code></dfn> {
  // Create an Operand object that represents a model input.
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand①"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="input(name, desc)" id="dom-neuralnetworkcontext-input"><code><c- g="">input</c-></code><a href="#dom-neuralnetworkcontext-input"></a></dfn>(<a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-DOMString" id="ref-for-idl-DOMString①"><c- b="">DOMString</c-></a> <dfn data-dfn-for="NeuralNetworkContext/input(name, desc)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-input-name-desc-name"><code><c- g="">name</c-></code><a href="#dom-neuralnetworkcontext-input-name-desc-name"></a></dfn>, <a data-link-type="idl-name" href="#dictdef-operanddescriptor" id="ref-for-dictdef-operanddescriptor"><c- n="">OperandDescriptor</c-></a> <dfn data-dfn-for="NeuralNetworkContext/input(name, desc)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-input-name-desc-desc"><code><c- g="">desc</c-></code><a href="#dom-neuralnetworkcontext-input-name-desc-desc"></a></dfn>);

  // Create an Operand object that represents a model constant.
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand②"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="constant(desc, value)" id="dom-neuralnetworkcontext-constant"><code><c- g="">constant</c-></code><a href="#dom-neuralnetworkcontext-constant"></a></dfn>(<a data-link-type="idl-name" href="#dictdef-operanddescriptor" id="ref-for-dictdef-operanddescriptor①"><c- n="">OperandDescriptor</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(desc, value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-desc-value-desc"><code><c- g="">desc</c-></code><a href="#dom-neuralnetworkcontext-constant-desc-value-desc"></a></dfn>, <a data-link-type="idl-name" href="https://heycam.github.io/webidl/#ArrayBufferView" id="ref-for-ArrayBufferView"><c- n="">ArrayBufferView</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(desc, value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-desc-value-value"><code><c- g="">value</c-></code><a href="#dom-neuralnetworkcontext-constant-desc-value-value"></a></dfn>);

  // Create a single-value tensor from the specified number of the specified type.
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand③"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="constant(value, type)|constant(value)" id="dom-neuralnetworkcontext-constant-value-type"><code><c- g="">constant</c-></code><a href="#dom-neuralnetworkcontext-constant-value-type"></a></dfn>(<a data-link-type="idl-name" href="#typedefdef-number" id="ref-for-typedefdef-number"><c- n="">number</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(value, type), NeuralNetworkContext/constant(value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-value-type-value"><code><c- g="">value</c-></code><a href="#dom-neuralnetworkcontext-constant-value-type-value"></a></dfn>, <c- b="">optional</c-> <a data-link-type="idl-name" href="#enumdef-operandtype" id="ref-for-enumdef-operandtype①"><c- n="">OperandType</c-></a> <dfn data-dfn-for="NeuralNetworkContext/constant(value, type), NeuralNetworkContext/constant(value)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-constant-value-type-type"><code><c- g="">type</c-></code><a href="#dom-neuralnetworkcontext-constant-value-type-type"></a></dfn> = "float32");

  // Create a Model object by identifying output operands.
  <c- b="">Promise</c->&lt;<a data-link-type="idl-name" href="#model" id="ref-for-model"><c- n="">Model</c-></a>&gt; <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="createModel(outputs)" id="dom-neuralnetworkcontext-createmodel"><code><c- g="">createModel</c-></code><a href="#dom-neuralnetworkcontext-createmodel"></a></dfn>(<c- b="">sequence</c->&lt;<a data-link-type="idl-name" href="#dictdef-namedoperand" id="ref-for-dictdef-namedoperand"><c- n="">NamedOperand</c-></a>&gt; <dfn data-dfn-for="NeuralNetworkContext/createModel(outputs)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-createmodel-outputs-outputs"><code><c- g="">outputs</c-></code><a href="#dom-neuralnetworkcontext-createmodel-outputs-outputs"></a></dfn>);
};
</pre>
   <h4 data-level="3.5.1" id="api-neuralnetworkcontext-batchnorm"><span>3.5.1. </span><span>batchNormalization</span><a href="#api-neuralnetworkcontext-batchnorm"></a></h4>
    Normalize the tensor values across dimensions in a batch through a specialized <a data-link-type="biblio" href="#biblio-batchnorm">[BatchNorm]</a> <a href="https://en.wikipedia.org/wiki/Batch_normalization#Batch_Normalizing_Transform">transform</a>. The <em>mean</em> and <em>variance</em> tensors are previously calculated during model training pass. 
<pre><c- b="">partial</c-> <c- b="">interface</c-> <a data-link-type="interface" href="#neuralnetworkcontext" id="ref-for-neuralnetworkcontext②"><c- g="">NeuralNetworkContext</c-></a> {
  <a data-link-type="idl-name" href="#operand" id="ref-for-operand④"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext" data-dfn-type="method" data-export="" data-lt="batchNormalization(input, mean, variance, scale, bias, axis, epsilon)|batchNormalization(input, mean, variance, scale, bias, axis)|batchNormalization(input, mean, variance, scale, bias)|batchNormalization(input, mean, variance, scale)|batchNormalization(input, mean, variance)" id="dom-neuralnetworkcontext-batchnormalization"><code><c- g="">batchNormalization</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization"></a></dfn>(<a data-link-type="idl-name" href="#operand" id="ref-for-operand⑤"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-input"><code><c- g="">input</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-input"></a></dfn>, <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑥"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-mean"><code><c- g="">mean</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-mean"></a></dfn>, <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑦"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-variance"><code><c- g="">variance</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-variance"></a></dfn>, 
                             <c- b="">optional</c-> <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑧"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-scale"><code><c- g="">scale</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-scale"></a></dfn>, <c- b="">optional</c-> <a data-link-type="idl-name" href="#operand" id="ref-for-operand⑨"><c- n="">Operand</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-bias"><code><c- g="">bias</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-bias"></a></dfn>, 
                             <c- b="">optional</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-long" id="ref-for-idl-long②"><c- b="">long</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-axis"><code><c- g="">axis</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-axis"></a></dfn> = 1, <c- b="">optional</c-> <a data-link-type="interface" href="https://heycam.github.io/webidl/#idl-float" id="ref-for-idl-float①"><c- b="">float</c-></a> <dfn data-dfn-for="NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis, epsilon), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias, axis), NeuralNetworkContext/batchNormalization(input, mean, variance, scale, bias), NeuralNetworkContext/batchNormalization(input, mean, variance, scale), NeuralNetworkContext/batchNormalization(input, mean, variance)" data-dfn-type="argument" data-export="" id="dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-epsilon"><code><c- g="">epsilon</c-></code><a href="#dom-neuralnetworkcontext-batchnormalization-input-mean-variance-scale-bias-axis-epsilon-epsilon"></a></dfn> = 1e-5);
};
</pre>
   <div data-algorithm="batchnorm">
     <p><strong>Arguments:</strong></p><ul>
     <li data-md="">
      <p><em>input</em>: an <code><a data-link-type="idl" href="#operand" id="ref-for-operand①⓪">Operand</a></code>. The input N-D tensor.</p>
     </li><li data-md="">
      <p><em>mean</em>: an <code><a data-link-type="idl" href="#operand" id="ref-for-operand①①">Operand</a></code>. The 1-D tensor of the batch mean values
whose length is equal to the size of the input dimension denoted by <em>axis</em>.</p>
     </li><li data-md="">
      <p><em>variance</em>: an <code><a data-link-type="idl" href="#operand" id="ref-for-operand①②">Operand</a></code>. The 1-D tensor of the batch variance values
whose length is equal to the size of the input …</p></li></ul></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://webmachinelearning.github.io/webnn/">https://webmachinelearning.github.io/webnn/</a></em></p>]]>
            </description>
            <link>https://webmachinelearning.github.io/webnn/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649616</guid>
            <pubDate>Thu, 01 Oct 2020 11:18:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why are secrets in Git such a threat?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24649034">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/secret-sprawl | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/secret-sprawl">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>What are secrets in the software development world?</p><div><p>In the common language, a secret can be any sensitive data that we want to keep private. When discussing secrets in the context of software development, secrets generally refer to digital authentication credentials that grant access to systems or data. These are most commonly API keys, usernames and passwords, or security certificates.</p><p>Secrets exist in the context of applications that are no longer standalone monoliths. Applications nowadays rely on thousands of independent building blocks: cloud infrastructure, databases, SaaS components such as Stripe, Slack, HubSpot…&nbsp;</p><p>Secrets are what tie together these different building blocks of a single application by creating a secure connection between each component.</p></div><p>What is secret sprawl?</p><div><p>Secret sprawl is the unwanted distribution of secrets like API keys and credentials through multiple systems.&nbsp;</p><p>In modern software development, secrets are regularly used by developers and applications. As a result they often get shared through different services like Slack or email and can be stored in multiple locations including different machines, git repositories or inside company wikis. This is a phenomenon we call secret sprawl.</p></div><p>What are some of the best practices to securely manage secrets like API keys?</p><div><p>Storing and managing secrets like API keys and other credentials can be challenging. Even the most careful policies can sometimes be circumvented in exchange for convenience. We have put together a helpful <a href="https://blog.gitguardian.com/secrets-api-management/" target="_blank">cheat sheet</a> and article explaining the <a href="https://blog.gitguardian.com/secrets-api-management/" target="_blank">best practices for managing secrets</a>.</p><p>As a minimum here are some points you should consider:</p></div><div><p>Never store unencrypted secrets in git repositories</p></div><div><p>Avoid git add * commands on git</p></div><div><p>Add sensitive files in .gitignore</p></div><div><p>Don’t rely on code reviews to discover secrets</p></div><div><p>Use automated secrets scanning on repositories</p></div><div><p>Don’t share your secrets unencrypted in messaging systems like Slack</p></div><div><p>Store secrets safely (method to be used depends on every use case)</p></div><div><p>Default to minimal permission scope for APIs</p></div><div><p>Whitelist IP addresses where appropriate</p></div><p>What are the threats associated with secret sprawl?</p><div><p>When secrets are sprawled through multiple systems it increases what is referred to as the ‘attack surface’. This is the amount of points where an unauthorized user could gain access to your systems or data. In the case of secret sprawl, each time a secret enters another system it is another point where an attacker could gain access to your secrets.</p><p>Most internal systems are not an appropriate place to store sensitive information, even if those systems are private. No company wants credit card numbers in plaintext in databases, PII in application logs, bank account credentials in a Google Doc. Secrets benefit from the same kind of protective measures.</p><p>As a general security principle, where feasible, data should remain safe even if it leaves the devices, systems, infrastructure or networks that are under organizations’ control, or if they are compromised. This helps prevent credential stealing, which is a well-known adversary technique described in the MITRE ATT&amp;CK framework:</p></div><p>“ Adversaries may search local file systems and remote file shares for files containing passwords. These can be files created by users to store their own credentials, shared credential stores for a group of individuals, configuration files containing passwords for a system or service, or source code/ binary files containing embedded passwords. ”<br></p><p>Secrets accessed by malicious threat actors can lead to information leakage and allow lateral movement or privilege escalation, as secrets very often lead to other secrets. Furthermore, once an attacker has the credentials to operate like a valid user, it is extremely difficult to detect the abuse and the threat can become persistent.<br></p><p>What are some of the biggest challenges associated with secret sprawl?</p><div><p>Because secrets tie together each component of an application, developers and ops need access to these secrets to build, connect, test and deploy applications. The result is that secrets need to be both tightly wrapped and also widely distributed. </p><p>Enforcing good security practices at the organization level is hard. Developers today can have large turnovers inside companies, be spread through many different teams and geographies. They have a growing number of technologies they must master and are under hard pressure due to shortened release cycles. This makes secret management very complicated and an ever changing challenge. </p><p>This is further complicated when VCS like git are introduced because secrets can be buried deep inside the history. The actual version of source code might look clean, while the history might present credentials that were added than later removed. Any secret reaching the Version Control System must be considered compromised and the presence of valid secrets in the git history presents a threat!</p></div></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/secret-sprawl</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649034</guid>
            <pubDate>Thu, 01 Oct 2020 09:27:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is it hard to detect API keys in source code?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24649026">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Why is it hard to detect secrets like API keys and other credentials?</p><div><p>Secrets detection is probabilistic—that is to say that it is not always possible to determine what is a true secret (or true positive). Because secrets share very few common, distinctive factors, decisions must be taken by aggregating weak signals to make strong predictions. </p><p>One of the most common factors is that almost all secrets are strings that look random. We call these strings high entropy strings. The issue is that this common factor is not very distinctive: 99% of strings that look random in source code aren’t secrets. They are for example database IDs or other types of false positives. </p><p>Of course, some secrets have fixed patterns: AWS keys often start with AKIA for example. But most secrets do not. The portions of code surrounding them are also very different, depending on what the secrets are used for and how they are used by the developers in the context of specific applications. Usernames and passwords can be used to authenticate in many different ways, and it is really hard to distinguish between real and fake credentials used as placeholders for example. </p><p>All this makes it extremely challenging to accurately capture all true secrets without also capturing false positives. At some point, a line in the sand needs to be drawn that considers the cost of a secret going undetected (a false negative) and compares it to the outcome that too many false positives would create. Different organizations with different people, cultures and workflows will draw different lines!</p></div><p>Why do code reviews fail at finding secrets in source code?</p><p>Code reviews are great overall for detecting logic flaws or maintaining certain good coding practices. But they are not adequate protection for detecting secrets, mostly for two reasons:<br></p><div><p>Reviews generally only consider the net difference between the current and proposed states. Not the entire history of changes. If a commit adds a secret and another one later deletes it, this has a zero net effect that is not of any interest to reviewers. But the vulnerability is there.</p></div><div><p>Reviewers prefer to focus on errors that cannot be automatically detected, like design flaws. As a general principle, security automation should be implemented wherever it can be, so that humans focus on where they bring the most value.</p></div><p>What is a "good" secrets detection algorithm?</p><div><p>Detecting secrets in source code is like finding needles in a haystack: there are a lot more sticks than there are needles, and you don’t know how many needles might be in the haystack. In the case of secrets detection, you don’t even know what all the needles look like!</p><p>Ideally, you want your detection system to achieve at the same time:</p></div><div><p>A low number of false alerts raised. We call this high precision. Precision answers the question: «What is the percentage of the secrets that you detect that are actual secrets?». This question is perfectly legitimate, especially in the context of security teams being overwhelmed with too many alerts.</p></div><div><p>A low number of secrets missed. This is what we call high recall. Considering that a single undetected credential can have a big impact for an organization, some organizations prefer to triage more false alerts but make sure they don’t miss a secret.</p></div><p>Balancing the equation to ensure that the algorithm captures as many secrets as possible without flagging too many false results is an intricate and extremely difficult challenge. Read more on evaluating <a href="https://blog.gitguardian.com/secrets-detection-accuracy-precision-recall-explained/" target="_blank">secrets detection algorithms</a>.<br></p><p>What is a false positive in secrets detection?</p><div><p>A false positive in secrets detection refers to when a secret candidate is wrongly marked as a true secret when it is in fact a non-sensitive string. </p><p>Typical examples of strings that can be mistaken for true secrets are:</p></div><div><p>UUIDs (Universally Unique Identifiers) that are used for example by databases as unique keys.</p></div><div><p>High entropy URLs or file paths. They often contain random strings that look like keys.</p></div><p>Examples of server-side hooks:<br></p><div><p>Test keys. Service Providers sometimes provide developers with a set of test credentials with a very restricted scope so developers can exercise parts of the API without charging their account</p></div><div><p>Public keys. As surprising as it is, a very small number of keys are really meant to be public (like Firebase keys, see this Stack Overflow <a href="https://stackoverflow.com/questions/37482366/is-it-safe-to-expose-firebase-apikey-to-the-public" target="_blank">conversation</a>).</p></div><p>Are secrets detection algorithms language-dependent?</p><p>Secrets detection is, for the most part, not language specific. Of course, there are some subtleties to take into account, like the way variables are assigned in any programming language. But there is no need to support all the different syntaxes in their greatest details. This means that the same algorithms can be applied to any project, in any programming language, without using things like Abstract Syntax Trees.<br></p></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/how-to-detect-secrets-in-source-code</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649026</guid>
            <pubDate>Thu, 01 Oct 2020 09:26:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why automate secrets scanning throughout your SDLC?]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24649022">thread link</a>) | @oodelally
<br/>
October 1, 2020 | https://www.gitguardian.com/secrets-detection/secrets-detection-application-security | <a href="https://web.archive.org/web/*/https://www.gitguardian.com/secrets-detection/secrets-detection-application-security">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>My source code is private, so why is hardcoding credentials in git considered a bad practice?</p><div><p>While private repositories offer some level of protection for your code they still do not have adequate protection to store information as sensitive as secrets. </p><p>Imagine if there was a plain text file with all your credit card numbers within it, you hopefully wouldn’t put this into the companies git repository. Secrets are just as sensitive.</p><p>A few things to consider when storing secrets in private repositories:</p></div><div><p>Source code is made to be duplicated and distributed, therefore lives in multiple places. Source code is a leaky asset and you never know where it is going to end up. It can be cloned to a compromised workstation or server, intentionally or accidentally published in whole or in part, uploaded to your website, released to a customer, pasted in Slack, or end up in your package manager or mobile application...</p></div><div><p>It would just take one compromised developer account to compromise all the secrets they have access to.</p></div><div><p>Hardcoded credentials make it impossible to know what secrets a developer accessed, and very difficult to rotate keys.</p></div><p>Why automate secrets scanning throughout the Software Development Life Cycle (SLDC)?</p><p>While DevOps, Continuous Integration and Continuous Delivery speed up software development, they can also significantly increase security risks.<br></p><p>“DevOps is rapid and requires lots of small, iterative changes. But this increases complexity and opens up a new set of security problems. With DevOps, existing security vulnerabilities can be magnified and manifest themselves in new ways. The speed of software creation can mean new vulnerabilities are created unseen by developers. The solution is to build security monitoring into the DevOps process from the start.”<br></p><p>Greg Day, RSA Conference Organizer (<a href="https://www.securityroundtable.org/the-biggest-cybersecurity-risks-in-2020/" target="_blank">source</a>)<br></p><p>Security now needs to be part of the SDLC from the start, and part of every incremental change. This concept is called Shifting Left, a development principle which states that security should move from the right (or end) of the SDLC to the left (the beginning). A great deal of automation is needed in order to be able to cope with running security checks at each incremental change. &nbsp;Automation helps streamline the detection, alerting and remediation processes and workflows.<br></p><p>How does secrets detection compare with Static Application Security Testing (SAST)?</p><div><p>Secrets detection is often confused with SAST because both scan through static source code. </p><p>SAST is mostly testing control structure, input validation, error handling, etc. Such vulnerabilities like SQL injection vulnerabilities only express themselves the moment the code is deployed. Exposed secrets are unlike these vulnerabilities, because any secret reaching version control system must be considered compromised and requires immediate attention. This is true even if the code is never deployed. </p><p>Implementing secrets detection is not only about scanning the most actual version of your master branch before deployment. It is also about scanning through every single commit of your git history, covering every branch, even development or test ones.</p><p>To conclude, SAST is concerned only with the current version of a project, the version that is going to be deployed, whereas secrets detection is concerned with the entire history of the project.</p></div><p>What are git hooks?</p><div><p>Git hooks are scripts that are triggered by certain actions in the software development process, like committing or pushing. By automatically pointing out issues in code, they allow reviewers not to waste time on mistakes that can be easily diagnosed by a machine. </p><p>There are client-side hooks, that execute locally on the developers’ workstation, and server-side hooks, that execute on the centralized version control system.</p><p>Examples of client-side hooks:</p></div><p>Examples of server-side hooks:<br></p><p>What is a pre-commit hook?</p><div><p>"The pre-commit hook is run first when committing, before you even type in a commit message. It’s used to inspect the snapshot that’s about to be committed, to see if you’ve forgotten something, to make sure tests run, or to examine whatever you need to inspect in the code. </p><p>Exiting non-zero from this hook aborts the commit, although you can bypass it with <span>git commit --no-verify</span>. You can do things like check for code style (run lint or something equivalent), check for trailing whitespace, or check for appropriate documentation on new methods."</p></div><p>Source: <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank">git-scm</a><br></p><p>What is a pre-receive hook?</p><div><p>A pre-receive hook is a script that runs on the server. It performs checks on the content of the push; if it exits non-zero, the push is rejected. You can use this hook to do things like prevent a PR author from merging their own changes, or require commit messages to follow some specific guidelines. </p><p>Be careful when using pre-receive hooks as they are blocking: if the checks don’t pass, the server is not updated.</p></div><p>What is a post-receive hook?</p><p>"The post-receive hook runs after the entire process of pushing code to the server is completed and can be used to update other services or notify users. Examples include emailing a list, notifying a continuous integration server, or updating a ticket-tracking system – you can even parse the commit messages to see if any tickets need to be opened, modified, or closed. This script can’t stop the push process, but the client doesn’t disconnect until it has completed, so be careful if you try to do anything that may take a long time."<br></p><p>Source: <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank">git-scm</a><br></p><p>Unlike the pre-receive hook, post-receive is non-blocking.<br></p><p>Where in the DevOps pipeline to implement automated secrets scanning? Client-side or server-side?</p><div><p>The earlier a security vulnerability is uncovered, the less costly it is to correct. Hardcoded secrets are no exceptions. If the secret is uncovered after the secret reaches centralized version control server-side, it must be considered compromised, which requires rotating (revoking and redistributing) the exposed credential. This operation can be complex and typically involves multiple stakeholders.</p><p>Client-side secrets detection early in the software development process is a nice-to-have as it will prevent secrets entering the VCS earlier. </p><p>Server-side secrets detection is a must have:</p></div><div><p>&nbsp;Server-side is where the ultimate threat lies. From the server, the code can uncontrollably spread in a lot of different places, with the hardcoded secrets in it.</p></div><div><p>Implementing client-side hooks on an organization level is hard. This is something we have heard many application security professionals claim they are not confident to do, due to the difficulty to deploy and update this on every developer’s workstation.</p></div><div><p>Client-side hooks can and must be easy to bypass (remember secret detection is probabilistic). Usage of client side hooks largely comes down to the individual developers’ responsibility. Hence the need to have visibility over the later stages.</p></div><p>Should secrets detection be blocking or non-blocking in the SDLC?</p><div><p>From our experience, when trying to impose rules that are too constraining, people will bend them, often in an effort to collaborate better and do their job. Security must not be a blocker. It should allow flexibility and enable information to flow, yet enable visibility and control. </p><p>On one hand, security measures will be bypassed, sometimes for the worst. But on the other hand, it is also good sometimes that the developer can take the responsibility to bypass them. </p><p>Because even the best algorithms can fail and need human judgement. Secrets detection is probabilistic: algorithms achieve a tradeoff between not raising false alerts and not missing keys.</p></div></div></div>]]>
            </description>
            <link>https://www.gitguardian.com/secrets-detection/secrets-detection-application-security</link>
            <guid isPermaLink="false">hacker-news-small-sites-24649022</guid>
            <pubDate>Thu, 01 Oct 2020 09:25:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Russian opposition leader Alexei Navalny describes his poisoning with Novichok]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24648502">thread link</a>) | @FrojoS
<br/>
October 1, 2020 | https://www.spiegel.de/international/world/alexei-navalny-thanks-germany-and-describes-his-poisoning-with-novichok-a-f1249540-76f7-43ce-aa14-358d71256684 | <a href="https://web.archive.org/web/*/https://www.spiegel.de/international/world/alexei-navalny-thanks-germany-and-describes-his-poisoning-with-novichok-a-f1249540-76f7-43ce-aa14-358d71256684">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-article-el="body">
<section data-app-hidden="true">


</section>
<section>
<div>
<figure data-component="Image" data-settings="{&quot;id&quot;:&quot;83eb2320-0da2-4c9c-971a-560530cea088&quot;, &quot;zoomable&quot;:true,&quot;zoomId&quot;:&quot;02c7d434-33a6-4d2b-a553-72392b2db9bd&quot;}">
<p><span>
<span data-image-el="aspect">
<span>
<img data-image-el="img" src="https://cdn.prod.www.spiegel.de/images/83eb2320-0da2-4c9c-971a-560530cea088_w948_r1.77_fpx28.11_fpy49.98.jpg" srcset="https://cdn.prod.www.spiegel.de/images/83eb2320-0da2-4c9c-971a-560530cea088_w520_r1.77_fpx28.11_fpy49.98.jpg 520w, https://cdn.prod.www.spiegel.de/images/83eb2320-0da2-4c9c-971a-560530cea088_w948_r1.77_fpx28.11_fpy49.98.jpg 948w" width="948" height="536" sizes="948px" title="Alexei Nawalny" alt="Alexei Nawalny">
</span>
</span>
</span>

</p>
<figcaption>
<p>Alexei Nawalny</p>
<span>
Foto: Peter Rigaud&nbsp;/ DER SPIEGEL
</span>
</figcaption>
</figure>
</div><div>
<p>In his first media interview following his poisoning, Russian opposition leader Alexei Navalny expresses his "tremendous gratitude to all Germans" in an interview with the newsmagazine DER SPIEGEL. In the interview, he says that although he never had close ties to Germany, it was German politicians and Chancellor Angela Merkel who saved his life. He says the doctors at Berlin's Charité University Hospital saved his life for a second time and nursed him back to health. "I know it sounds a bit over the top, but Germany has become a special country for me," Navalny says. Describing the personal visit paid to him by Merkel last week, he says: "I was impressed by the detail she knows about <a href="https://www.spiegel.de/thema/russia_en/" data-link-flag="english">Russia</a> and my case."</p>



<section data-area="contentbox">

</section>
<p>Navalny says he is doing "much better than three weeks ago, and things are getting better each day." The doctors say he could get back to 90 percent of his former self, perhaps even 100 percent, although no one knows for sure. "Basically, I’m a bit of a guinea pig," he says. "There aren’t many people you can observe who are still alive after being poisoned with a nerve agent." Describing the moment in the airplane when the poison started to take effect, Navalny says: "You feel no pain, but you know you’re dying. And I mean, right now. Even though nothing hurts you." You just think: This is the end. "Organophosphorus compounds attack your nervous system like a DDos attack attacks the computer - it's an overload that breaks you," he told DER SPIEGEL.</p>

<div>
<p>"I assert that <a href="https://www.spiegel.de/thema/vladimir_putin_en/" data-link-flag="english">Putin</a> was behind the crime, and I have no other explanation for what happened." Despite the attempt on his life, he says he wants to return to Russia. "And my job now is to remain the guy who isn’t afraid. And I’m not afraid! When my hands shake, it’s not from fear – it’s from this stuff. I would not give Putin the gift of not returning to Russia."</p><p>When asked whether he thinks Germany should stop the completion of the Nord Stream 2 Russian gas pipeline project, Navalny didn’t want to answer. "That’s Germany’s business. Decide for yourself!" Any strategy toward Russia must "take into account the level of insanity that Putin has reached."</p>
</div>

<div>
<p><em>The full interview with Navalny will be posted in English later today.</em></p>
<p><span><svg aria-labelledby="title-b3ab4652-e428-41a8-a973-570c15e33654" width="10" height="20" viewBox="0 0 10 20" fill="none" xmlns="http://www.w3.org/2000/svg" role="img"><title id="title-b3ab4652-e428-41a8-a973-570c15e33654">Icon: Der Spiegel</title><g id="l-s-flag-b3ab4652-e428-41a8-a973-570c15e33654"><path id="vector-b3ab4652-e428-41a8-a973-570c15e33654" d="M9.85 16.293v-8H3.212V4.667h3.533v2.24h3.212v-3.2C9.85 2.747 8.993 2 8.03 2H1.713C.749 2 0 2.747 0 3.707v7.253h6.638v4.373H3.105v-2.986H0v3.84c0 .96.75 1.706 1.713 1.706H8.03c.963.107 1.82-.64 1.82-1.6z" fill="#000"></path></g></svg>
</span>
</p></div>

</div>
</section>

</div></div>]]>
            </description>
            <link>https://www.spiegel.de/international/world/alexei-navalny-thanks-germany-and-describes-his-poisoning-with-novichok-a-f1249540-76f7-43ce-aa14-358d71256684</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648502</guid>
            <pubDate>Thu, 01 Oct 2020 07:54:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Baremetal programming on the tinyAVR 0 micro-controllers]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 94 (<a href="https://news.ycombinator.com/item?id=24648397">thread link</a>) | @unwind
<br/>
October 1, 2020 | https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers | <a href="https://web.archive.org/web/*/https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><section><div><h4>All you need is a text editor, a makefile and a USB-serial cable.</h4></div></section><section><div><p><img src="https://www.omzlo.com/uploads/std_attiny-406-macro.jpg" alt=""></p>

<h2 id="introduction">Introduction</h2>

<p>Are you an 8-bit or a 32-bit programmer? </p>

<p>At OMZLO, we have been mainly focussing our development efforts on newer 32-bit Arm-Cortex chips (STM32 and SAMD), which typically offer more RAM, more speed, more peripherals, at a similar or lower price-point than older 8-bit MCUs. But 8-bit MCUs are far from dead. Microchip has notably released a new series of chips, collectively branded as the "tinyAVR 0-series", which offer more modern peripherals than the older AVR chips at a very competitive price point. They seem like the perfect candidate for simple products that don't need all the features and fine-tuning capabilities of the newer 32-bit MCUs. 8-bit MCUs are also substantially simpler to program, which translates into faster development time. </p>

<p>Thanks to the success of the Arduino UNO, there are tons of tutorials online that explain how to program 8-bit Atmega328 microcontrollers and their cousins like the Attiny85 using direct register access, without the Arduino language or any vendor IDE such as Atmel Studio. Just google "atmega328 blinky". All you need is an AVR C-compiler, a text editor, <a href="https://www.nongnu.org/avrdude/">avrdude</a>, and an AVR programmer. Some <a href="https://www.instructables.com/id/Getting-Started-With-the-ATMega328P/">resources</a> even show how to also build the electronics needed to get a basic atmega328 running on a breadboard. However, it's hard to find the same information for these newer "tinyAVR 0" chips.</p>

<p>Of course, Microchip offers all the tools necessary to program these newer "TinyAVR" MCUs with their windows-only IDE. There are also "Arduino cores" for some of these newer "TinyAVR" MCUs that let you program them with the Arduino IDE. But again, if you like to write code for MCUs in "baremetal" style, with your favorite text editor, a makefile, and a c-compiler, there are few resources available online. </p>

<p>In this blog post, we will describe how to program a <a href="https://www.ladyada.net/learn/proj1/blinky.html">blinky</a> firmware on an <strong>Attiny406</strong>, from the ground up, using the simplest tools. Most of the things described here can be easily transposed to other TinyAVR MCUs. Our approach is generally guided toward macOS or Linux users, but should also be applicable in an MS-Windows environment with a few minor changes.</p>

<h2 id="hardware">Hardware</h2>

<p>We decided to play with the <a href="https://www.microchip.com/wwwproducts/en/ATTINY406">Attiny406</a>, with a view of using it in the future to replace the Attiny45 we currently use on the <a href="https://www.omzlo.com/articles/the-piwatcher">PiWatcher</a>, our Raspberry-Pi watchdog. The Attiny406 has 4K of flash space, 256 bytes of RAM, and can run at 20Mhz without an external clock source.</p>

<p>One of the most important differences between the new TinyAVR MCUs and the older classic AVR MCU like the Attiny85 is that the newer chips use a different programming protocol called UPDI, which requires only 3 pins, as opposed to the 6-pin ISP on the classic AVRs.</p>

<p>A little research shows that programming TinyAVRs with UPDI can be achieved with a simple USB-to-serial cable and a resistor, thanks to a python tool called <a href="https://github.com/mraardvark/pyupdi">pyupdi</a>, which suggests the following connection diagram for firmware upload:</p>
<pre>                        Vcc                     Vcc
                        +-+                     +-+
                         |                       |
 +---------------------+ |                       | +--------------------+
 | Serial port         +-+                       +-+  AVR device        |
 |                     |      +----------+         |                    |
 |                  TX +------+   4k7    +---------+ UPDI               |
 |                     |      +----------+    |    |                    |
 |                     |                      |    |                    |
 |                  RX +----------------------+    |                    |
 |                     |                           |                    |
 |                     +--+                     +--+                    |
 +---------------------+  |                     |  +--------------------+
                         +-+                   +-+
                         GND                   GND
</pre>
<h3 id="shematic">shematic</h3>

<p>We created a minimalistic breakout board for the Attiny406. The board can be powered by 5V through USB or a lower 3.3V through dedicated VCC/GND pins. An LED and a button were also fitted on the board. For testing purposes, we decided to embed the 4.7K resistor needed for the UPDI programming directly in the hardware (i.e. resistor <em>R2</em>). 
This gives us the following schematic:</p>

<p><img src="https://www.omzlo.com/uploads/attiny406-schematic.png" alt="schematic"></p>

<h3 id="board">Board</h3>

<p>The resulting breakout board is tiny and fits conveniently on a small breadboard. The design files are <a href="https://aisler.net/p/SIEGFIYL">shared on aisler.net</a>.</p>

<p><img src="https://www.omzlo.com/uploads/std_attiny-406-breadboard.jpg" alt=""></p>

<p>Programming the Attiny406 on the board with a USB-serial cable is done by connecting the headers on the board edge:</p>

<p><img src="https://www.omzlo.com/uploads/attiny406-prog.png" alt=""></p>

<h2 id="software">Software</h2>

<h3 id="pyudpi">pyudpi</h3>

<p>We installed <strong>pyupdi</strong> following the instructions provided on <a href="https://github.com/mraardvark/pyupdi">their webpage</a>. </p>

<p>We connected our USB-Serial cable to the board with the 4 dedicated UPDI pins available on the board. Our USB-Serial converter shows up as the file <code>/dev/tty.usbserial-FTF5HUAV</code> on a MacOS system.</p>

<p>To test that the programmer recognizes the Attiny406, you can issue a command similar to the following, adapting the path for the USB-serial converter to your setup:</p>
<pre>pyupdi -d tiny406 -c /dev/tty.usbserial-FTF5HUAV -i
</pre>
<p>This should result in the following output if all goes well:</p>
<pre>Device info: {'family': 'tinyAVR', 'nvm': 'P:0', 'ocd': 'D:0', 'osc': '3', 'device_id': '1E9225', 'device_rev': '0.1'}
</pre>
<h3 id="the-c-compiler">The C compiler</h3>

<p>The typical <strong>avr-gcc</strong> available on macOS with <a href="https://brew.sh/">homebrew</a> did not seem to recognize the Attiny406 as a compiler target, so we went off to install the avr-gcc compiler provided by Microchip, which is available <a href="https://www.microchip.com/mplab/avr-support/avr-and-arm-toolchains-c-compilers">here</a>. Downloading the compiler requires you to create an account on the Microchip website, which is a bit annoying. </p>

<p><img src="https://www.omzlo.com/uploads/avr-toolchain.png" alt="AVR toolchain link"></p>

<p>Once downloaded, we extracted the provided archive in a dedicated directory. The <code>bin</code> directory in the archive should be added to the <code>PATH</code> variable to make your life easier. Assuming the downloaded compiler is stored in the directory <code>$HOME/Src/avr8-gnu-toolchain-darwin_x86_64</code>, the PATH can be altered by adding the following line to your <code>.bash_profile</code> file:</p>
<pre>export PATH=$PATH:$HOME/Src/avr8-gnu-toolchain-darwin_x86_64/bin/
</pre>
<p>Newer Attiny MCUs are not supported out of the box by the Microchip <strong>avc-gcc</strong> compiler. You need to download a dedicated <em>Attiny Device Pack</em> from <a href="http://packs.download.atmel.com/">their website</a>, as shown below:</p>

<p><img src="https://www.omzlo.com/uploads/microchip-pack-repository.png" alt="AVR toolchain link"></p>

<p>The resulting downloaded <em>Device Pack</em> is named <code>Atmel.ATtiny_DFP.1.6.326.atpack</code> (or similar depending on versioning). Though the extension is <code>.atpack</code>, the file is actually a zip archive. We changed the extension to <code>.zip</code> and extracted the package in the directory <code>$HOME/Src/Atmel.ATtiny_DFP.1.6.326</code> next to the compiler files. </p>

<h3 id="c-program">C program</h3>

<p>We created the following program that blinks the LED on pin PB5 of our Attiny board at a frequency of 1Hz.</p>
<pre><span>#include &lt;avr/io.h&gt;
#include &lt;util/delay.h&gt;
</span>
<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>_PROTECTED_WRITE</span><span>(</span><span>CLKCTRL</span><span>.</span><span>MCLKCTRLB</span><span>,</span> <span>0</span><span>);</span> <span>// set to 20Mhz (assuming fuse 0x02 is set to 2)</span>

    <span>PORTB</span><span>.</span><span>DIRSET</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
    <span>for</span> <span>(;;)</span> <span>{</span>
        <span>PORTB</span><span>.</span><span>OUTSET</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
        <span>_delay_ms</span><span>(</span><span>500</span><span>);</span>
        <span>PORTB</span><span>.</span><span>OUTCLR</span> <span>=</span> <span>(</span><span>1</span><span>&lt;&lt;</span><span>5</span><span>);</span>
        <span>_delay_ms</span><span>(</span><span>500</span><span>);</span>
    <span>}</span>
<span>}</span>
</pre>
<p>The code looks very similar to what you would see on a classic AVR "blinky" program. One visible change is the use of structures to access various registers of the MCU: e.g instead of setting bits in <code>PORTB</code>, you access <code>PORTB.DIRSET</code>.</p>

<p>The other visible change is the clock setup code <code>_PROTECTED_WRITE(CLKCTRL.MCLKCTRLB, 0)</code>. Out of the box, at reset, the Attiny406 runs at 3.33Mhz, which corresponds to a base frequency of 20Mhz with a 6x clock divider applied. To enable the full 20Mhz speed, the register <code>CLKCTRL.MCLKCTRLB</code> is cleared. Because this register needs to be protected against accidental changes, the Attiny406 requires a specific programming sequence to modify it. Fortunately, this is natively offered by the macro <code>_PROTECTED_WRITE</code>. More details are available in the <a href="http://ww1.microchip.com/downloads/en/DeviceDoc/ATtiny406-DataSheet-DS40001976B.pdf">Attiny406 datasheet</a>.</p>

<p>In comparison with an STM32 or a SAMD21, the code is blissfully simple. </p>

<h3 id="makefile">Makefile</h3>

<p>We assume the following directory structure where:</p>

<ul>
<li><code>Src/Atmel.ATtiny_DFP.1.6.326/</code> is the location of the Microchip <em>Device Pack</em></li>
<li><code>Src/attiny406-test/</code> is the directory where the code above is stored in a file called <code>main.c</code></li>
</ul>

<p>Compiling the code can be done by issuing the following command within <code>attiny406-test/</code> directory,:</p>
<pre>avr-gcc -mmcu=attiny406 -B ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/ -O3 -I ../Atmel.ATtiny_DFP.1.6.326/include/ -DF_CPU=20000000L -o attiny406-test.elf main.c
</pre>
<p>An <code>-O</code> optimization flag is required to make the <code>_delay_ms()</code> function calls work successfully, as well as defining the variable F_CPU to reflect the expected chip clock speed. The rest of the parameters provide the location of the Attiny406 device-specific files we previously extracted from the <em>Device Pack</em>.</p>

<p>Uploading the firmware to the MCU requires a conversion to the intel HEX format and a call to the <strong>pyupdi</strong> tool. To address all these steps, we created a simple Makefile.</p>
<pre><span>OBJS</span><span>=</span>main.o
<span>ELF</span><span>=</span><span>$(</span>notdir <span>$(</span>CURDIR<span>))</span>.elf  
<span>HEX</span><span>=</span><span>$(</span>notdir <span>$(</span>CURDIR<span>))</span>.hex
<span>F_CPU</span><span>=</span>20000000L


<span>CFLAGS</span><span>=</span><span>-mmcu</span><span>=</span>attiny406 <span>-B</span> ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/ <span>-O3</span>
CFLAGS+<span>=</span><span>-I</span> ../Atmel.ATtiny_DFP.1.6.326/include/ <span>-DF_CPU</span><span>=</span><span>$(</span>F_CPU<span>)</span>
<span>LDFLAGS</span><span>=</span><span>-mmcu</span><span>=</span>attiny406 <span>-B</span> ../Atmel.ATtiny_DFP.1.6.326/gcc/dev/attiny406/
<span>CC</span><span>=</span>avr-gcc
<span>LD</span><span>=</span>avr-gcc

all:    <span>$(</span>HEX<span>)</span>  

<span>$(</span>ELF<span>)</span>: <span>$(</span>OBJS<span>)</span>
                <span>$(</span>LD<span>)</span> <span>$(</span>LDFLAGS<span>)</span> <span>-o</span> <span>$@</span> <span>$(</span>OBJS<span>)</span> <span>$(</span>LDLIBS<span>)</span>

<span>$(</span>HEX<span>)</span>: <span>$(</span>ELF<span>)</span>
                avr-objcopy <span>-O</span> ihex <span>-R</span> .eeprom <span>$&lt;</span> <span>$@</span>

flash:  <span>$(</span>HEX<span>)</span>
                pyupdi <span>-d</span> tiny406 <span>-c</span> /dev/tty.usbserial-FTF5HUAV <span>-f</span> attiny406-test.hex

read-fuses:
                pyupdi <span>-d</span> tiny406 <span>-c</span> /dev/tty.usbserial-FTF5HUAV <span>-fr</span>

clean:
                <span>rm</span> <span>-rf</span> <span>$(</span>OBJS<span>)</span> <span>$(</span>ELF<span>)</span> <span>$(</span>HEX<span>)</span>
</pre>
<p>To compile the code, we simply type <code>make</code>. Uploading is done with <code>make flash</code>. This Makefile can be further enhanced as needed.</p>

<h2 id="conclusion">Conclusion</h2>

<p>With the right tools, baremetal programming on the new TinyAVR MCUs is as simple as on its older AVR cousins. </p>

<p>If you have programming tips for the AVRTiny, please share them with us on <a href="https://twitter.com/OmzloElec">on Twitter</a> or in the comments below.</p>
</div></section><section><div><h4>Comments</h4><div><div><p>Hi, </p>

<p>It's great that …</p></div></div></div></section></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers">https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers</a></em></p>]]>
            </description>
            <link>https://www.omzlo.com/articles/baremetal-programming-on-the-tinyavr-0-micro-controllers</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648397</guid>
            <pubDate>Thu, 01 Oct 2020 07:37:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MobX 6]]>
            </title>
            <description>
<![CDATA[
Score 139 | Comments 59 (<a href="https://news.ycombinator.com/item?id=24648363">thread link</a>) | @nikivi
<br/>
October 1, 2020 | https://michel.codes/blogs/mobx6 | <a href="https://web.archive.org/web/*/https://michel.codes/blogs/mobx6">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="___gatsby"><div tabindex="-1" role="group"><div><section><center><br><small>September 24, 2020</small></center><div><h2>Five years of MobX</h2>
<p>Time flies, and it has been 5.5 years since the first commit to MobX was made to build <a href="https://www.mendix.com/">Mendix Studio</a>.
In those years MobX has been adopted by well-known Software companies like Microsoft (Outlook), Netflix, Amazon and, my personal favorite, it runs in the Battlefield games by EA.
<a href="https://www.amazon.co.uk/MobX-Quick-Start-Guide-Supercharge/dp/1789344832/ref=sr_1_1?crid=BRUIHPUQL64D&amp;dchild=1&amp;keywords=mobx&amp;qid=1600809874&amp;s=books&amp;sprefix=mobx%2Caps%2C138&amp;sr=1-1">Books</a> and video courses have been written, and so have implementations in other languages.</p>
<p><span>
    <span></span>
    <img alt="Battlefield &amp; MobX" title="" src="https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/128ae/dice.jpg" srcset="https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/08cbc/dice.jpg 160w,
https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/37ac5/dice.jpg 320w,
https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/128ae/dice.jpg 640w,
https://michel.codes/static/5e0cc6842fdf39e4a7bf69a61fa15151/52793/dice.jpg 800w" sizes="(max-width: 640px) 100vw, 640px">
  </span></p>
<p>Yet, since that first commit the philosophy of the MobX hasn't changed: <em>Anything that can be derived from the application state, should be derived. Automatically.</em>.
The API hasn't changed too much since those days either,
and you will find the original <a href="https://www.mendix.com/blog/making-react-reactive-pursuit-high-performing-easily-maintainable-react-apps/">introduction of MobX</a> still pretty recognizable.
If <code>React.createClass</code> still rings a bell that is.</p>
<p>In contrast, the JavaScript eco-system has changed significantly over the years.
TypeScript and Babel have become the de-facto standards.
React went from <code>createClass</code> to classes to hook-based function components.
Yet, relevant JavaScript proposals for observables, Object.observe and decorators have never materialized.</p>
<p>MobX 6 is a new major version that doesn't bring many new features, but is rather a consolidation of MobX on the current state of affairs in JavaScript.
That doesn't come without a few plot twists, so if you are an existing MobX user, please read till the end!</p>
<h2>Bye bye decorators</h2>
<p>Let's start with the bad news: Using decorators is no longer the norm in MobX.
This is good news to some of you, but others will hate it.
Rightfully so, because I concur that the declarative syntax of decorators is still the best that can be offered.
When MobX started, it was a TypeScript only project, so decorators were available.
Still experimental, but obviously they were going to be standardized soon.
That was my expectation at least (I did mostly Java and C# before).
However, that moment still hasn't come yet, and two decorators proposals have been cancelled in the mean time.
Although they still can be transpiled.</p>
<p>So why did we stop using decorators by default?</p>
<p>First of all, the current experimental decorator implementations are incompatible with the soon-to-be-standardized class-fields proposal.
The legacy (Babel) and experimental (TypeScript) decorator implementations will no longer be able to <a href="https://github.com/tc39/proposal-class-fields/issues/151">trap class fields initializations</a>.</p>
<p>Secondly, using decorators has always been a serious hurdle in adopting and advocating MobX.
In Babel, it is quite fragile to set up.
<code>create-react-app</code> doesn't support it out of the box, and many developers rightfully don't like to use non-standard features.
Even though decorators have always been optional in MobX, the fact that they were prominent in the docs left many confused.
Or as one MobX fan <a href="https://github.com/mobxjs/mobx/issues/2325#issuecomment-693130586">puts it</a>:</p>
<p><em>I am guessing this choice [to drop decorators] was probably a good call. Maybe now without decorators I am hopeful I will be able to convey to people how amazing Mobx is and at least I won't hear the decorators excuse anymore. I have never seen an "@" sign scare so many people.</em></p>
<p>So, what does MobX after decorators look like?
Simply put, instead of decorating class members during the class definition, instance members need to be annotated in the constructor instead, using the new <code>makeObservable</code> utility:</p>
<div data-language="typescript"><pre><code><span>import</span> <span>{</span>observable<span>,</span> computed<span>,</span> action<span>,</span> makeObservable<span>}</span> <span>from</span> <span>"mobx"</span>


<span>class</span> <span>TodoStore</span> <span>{</span>
    @observable
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    @computed
    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    @action
    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span>


<span>class</span> <span>TodoStore</span> <span>{</span>
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    <span>constructor</span><span>(</span><span>)</span> <span>{</span>
        <span>makeObservable</span><span>(</span><span>this</span><span>,</span> <span>{</span>
            todos<span>:</span> observable<span>,</span>
            unfinishedTodoCount<span>:</span> computed<span>,</span>
            addTodo<span>:</span> action
        <span>}</span><span>)</span>
    <span>}</span>

    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>Admittedly, this is a slightly worse DX than before, since member and annotation are no longer co-located.
But the good news is that using <code>makeObservable</code> doesn't require any fancy build setup.
It should work everywhere out of the box.</p>
<p>Migrating an entire code-base from decorators to <code>makeObservable</code> might be challenging, so that is why we released a <a href="https://www.npmjs.com/package/mobx-undecorate">code-mod</a> together with MobX 6 to do that automatically!
Just run the command <code>npx mobx-undecorate</code> inside the folder where your source files live, and after that all decorators should have been magically rewritten!
After that, make sure to <a href="https://mobx.js.org/migrating-from-4-or-5.html#getting-started">update your TypeScript / babel config</a>, and you should be good to go!</p>
<h2>Introducing <code>makeAutoObservable</code></h2>
<p>We realize it is easier to make mistakes now that the annotations are no longer adjacent to the fields they are decorating.
Hence a convenience utility has been introduced that automates the annotation process by picking sane defaults: <code>makeAutoObservable</code>.
It will automatically pick the best annotation for every member of a class, thereby simplifying the above listing to:</p>
<div data-language="typescript"><pre><code><span>class</span> <span>TodoStore</span> <span>{</span>
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    <span>constructor</span><span>(</span><span>)</span> <span>{</span>
        <span>makeAutoObservable</span><span>(</span><span>this</span><span>)</span>
    <span>}</span>

    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>Note that it is possible to pass a map of <em>overrides</em> as second argument, in case you want to use a modifier.
For example: <code>makeAutoObservable(this, { todos: observable.shallow })</code>.
Pass <code>member: false</code> to have MobX ignore that member entirely.
(Technical fineprint: class methods will not be decorated with <code>action</code>, but with the new <code>autoAction</code>, this annotation will make methods suitable to be used both as a state updating action, or as function that derives information from state).</p>
<p>What I personally like about <code>makeAutoObservable</code> is that it plays really nice with factory functions.
Which is great if you prefer to not use classes (factory functions make it is easy to hide members and prevent issues with <code>this</code> and <code>new</code>. And they compose more easily).
The same store expressed as factory function will look as follows. Pick the style that suits you:</p>
<div data-language="typescript"><pre><code><span>function</span> <span>createTodoStore</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> store <span>=</span> <span>makeAutoObservable</span><span>(</span><span>{</span>
        todos<span>:</span> <span>[</span><span>]</span> <span>as</span> Todo<span>[</span><span>]</span><span>,</span>
        <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
            <span>return</span> store<span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
        <span>}</span><span>,</span>
        <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
            store<span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
        <span>}</span>
    <span>}</span><span>)</span>
    <span>return</span> store
<span>}</span></code></pre></div>
<h2>Fresh docs!</h2>
<p>Since decorators are no longer the norm, <a href="https://twitter.com/faassen">Martijn Faassen</a>, <a href="https://twitter.com/zangornjak">Žan Gornjak</a> and yours truly went over all the documentation.
We updated all examples and significantly restructured the docs that have grown quite organically over the years.
We feel the current docs are shorter, have less repetition, and better discuss common scenarios.
Also, we marked all non-essential knowledge in the docs with a {🚀} rocket emoji, to make it clear which knowledge is optional.
Hopefully it is much quicker now to find your way around in MobX!</p>
<p>On a similar note, we've updated all <a href="https://mobx.js.org/react-integration.html">React related documentation</a> to use function components instead of class components. And added documentation on how to use MobX with hooks, context and effects.
This knowledge existed before (little has changed technically), but was scattered all over the place.
As a result, we now recommend <code>mobx-react-lite</code> over <code>mobx-react</code> for (greenfield) projects that don't use class components.
As a result the separate mobx-react.js.org/ website has been deprecated.
All credits go to <a href="https://twitter.com/danielk_cz">Daniel K</a> for maintaining those two projects!</p>
<p>And finally, there is now a <a href="https://gum.co/fSocU">one pager MobX 6 cheat sheet 👨‍🎓</a> covering all the import mobx / mobx-react(-lite) API's.
(It is a great way to one-time sponser the project in an invoicable way).</p>
<h2>Improved browser support</h2>
<p>A probably little surprising improvement in MobX 6 is that it supports <em>more</em> JavaScript engines than MobX 5.
MobX 5 required proxy support, making MobX unsuitable for Internet Explorer or React Native (depending on the engine).
For this reason MobX 4 was still actively maintained.
However, MobX 6 replaces both at once.</p>
<p>By default MobX 6 will still require Proxies, but it is possible to opt-out from Proxy usage in case you need to support older engines.
And, as a result, it is now possible for MobX 6 to warn in development mode when features that would require proxies are used.
See the documentation for more <a href="https://mobx.js.org/configuration.html#proxy-support">details</a>.</p>
<div data-language="typescript"><pre><code><span>import</span> <span>{</span> configure <span>}</span> <span>from</span> <span>"mobx"</span>

<span>configure</span><span>(</span><span>{</span>
    
    
    
    useProxies<span>:</span> <span>"never"</span>
<span>}</span><span>)</span></code></pre></div>
<h2>Decorators are back!</h2>
<p>Ok, time for the plot twist. MobX 6 stills supports decorators!
The decorator implementation in MobX 6 is entirely different from the one in earlier versions, but does work with the current implementations in TypeScript and Babel.
It basically provides an alternative way to construct the annotations map for <code>makeObservable</code>, and allows us to rewrite the first example as:</p>
<div data-language="typescript"><pre><code><span>class</span> <span>TodoStore</span> <span>{</span>
    @observable
    todos<span>:</span> Todo<span>[</span><span>]</span> <span>=</span> <span>[</span><span>]</span>

    <span>constructor</span><span>(</span><span>)</span> <span>{</span>
        <span>makeObservable</span><span>(</span><span>this</span><span>)</span>
    <span>}</span>

    @computed
    <span>get</span> <span>unfinishedTodoCount</span><span>(</span><span>)</span> <span>{</span>
        <span>return</span> <span>this</span><span>.</span>todos<span>.</span><span>filter</span><span>(</span>todo <span>=&gt;</span> <span>!</span>todo<span>.</span>done<span>)</span><span>.</span>length
    <span>}</span>

    @action
    <span>addTodo</span><span>(</span>todo<span>:</span> Todo<span>)</span> <span>{</span>
        <span>this</span><span>.</span>todos<span>.</span><span>push</span><span>(</span>todo<span>)</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>Note that we still need to add a constructor to the class, but this time we omit the second argument to <code>makeObservable</code>, so that it will rely on the decorators instead.
We don't recommend this set up for greenfield projects, after all decorators are still experimental, but this is a great compromise for
existing code bases.
Generating constructors without removing decorators is supported by <code>mobx-undecorate</code> as well, and can be achieved by running <code>npx mobx-undecorate --keepDecorators</code>.</p>
<p>And here is even more good news: There is a fresh <a href="https://github.com/tc39/proposal-decorators">decorators proposal</a> being championed by the tireless hero <a href="https://twitter.com/littledan">Daniel Ehrenberg</a>.
I've been a bit involved in it, and the MobX use case has inspired the proposal.
So the benefits of the fresh decorator implementation are that it a) solves the compatibility issue with the class fields spec discussed above, and
b) it also paves the way …</p></div></section></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://michel.codes/blogs/mobx6">https://michel.codes/blogs/mobx6</a></em></p>]]>
            </description>
            <link>https://michel.codes/blogs/mobx6</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648363</guid>
            <pubDate>Thu, 01 Oct 2020 07:33:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Worst Decision of My Career]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24648256">thread link</a>) | @fribmendes
<br/>
October 1, 2020 | https://subvisual.com/blog/posts/the-worst-decision-of-my-career/ | <a href="https://web.archive.org/web/*/https://subvisual.com/blog/posts/the-worst-decision-of-my-career/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><article><section><div><div><div><div><div><div>
<p>This is a reflection on software development and complexity. Let's start with
some quotes to make me look smart:</p>
<blockquote>
<p>A complex system that works is invariably found to have evolved from a simple
system that worked. A complex system designed from scratch never works and
cannot be patched up to make it work. You have to start over with a working
simple system -- <em>Gall's law</em></p>
</blockquote>
<p>I only came across this quote recently, but I think that it summarizes what
I've learned these past seven years. Another idea that I've found in a few books
is that programming in isolation is problem-solving, but software engineering
is all about managing complexity. This distinction between programming and
software engineering makes sense to me, probably because of my mental models.
I'm sure we can all disagree on this, but that's not the topic of today.
Complexity is.</p>
<h2>Complexity</h2>
<p>Almost everyone I know in the software industry wants to build products that
solve real and challenging problems. Change the world! The last thing you want
is to build another CRUD application. They are fine, but it gets boring after
a while.</p>
<p>Most of us will get bored without novelty. That's why the software industry has
a recycling mechanism to keep things fresh and interesting for us: every once
in a while a new, or different, language/framework will rise and light the path
for a brighter future, overthrowing the existing standard, demanding that we
learn it to be on top of the game.</p>
<p>We change our tools, but we keep building the same things over and over. I've
been doing this long enough to see that we are just going in circles. Let's be
honest, most of us aren't building life-changing products that wouldn't have
been possible ten years ago, so let's not jump straight into another shiny
technology that just came out and is going to solve all of our problems.</p>
<h2>Solutions, solutions, solutions</h2>
<blockquote>
<p>For a while, the solution to every problem I encountered was a Rails app. --
<em>this one is mine</em></p>
</blockquote>
<p>Over the years, I've seen companies rewrite their products to change languages,
frameworks, or architectures because they were told that the change would make
their product better, run faster, and scale. By the way, you're only a true
Ruby developer after you've heard the question "but does it scale?" at least
one hundred times (kidding, but it'll happen).</p>
<p>Think about it, how many times were you sold a new programming language,
technology, or a concept like microservices, serverless, event sourcing, clean
architecture, micro frontends. There are many preachers in the software world.</p>
<p>At Subvisual, we started using Elixir a lot these past years, so I've learned
a bit about the history of Erlang. If you don't know, Erlang uses the Actor
Model, and the interesting part is that the designers of Erlang only learned
about the Actor Model after having designed Erlang. This is important because
the designers of Erlang didn't start with the intent of applying the Actor
Model; it came as a solution to fault-tolerant distributed programming.</p>
<blockquote>
<p>If all you have is a hammer, everything looks like a nail <em>Abraham Maslow</em></p>
</blockquote>
<p>The designers of Erlang picked the right tool for the job and most of us want
to do the same. Unfortunately, there's usually more than one "tool" that would
be "right," but to know which ones, you need to know what "job" you're solving.
All of us <strong>should</strong> know this, but I keep learning about teams that fail to do
it. There are so many companies, with a handful of experienced developers, that
don't know what they are building, don't have a clear business yet, but their
product is already built on complex architectures and technologies such as
microservices, Kubernetes or event-sourcing.</p>
<h2>Improving</h2>
<p>Every project we start from scratch is an opportunity to do it right. We'll
think to ourselves: <em>This time I won't fall into the same traps! I have learned
my lessons, I've studied the books, and I even met some of my gurus that wrote
them! This time I'll follow "industry standards"!</em></p>
<p><em>I am the "architect"; I will design the perfect system! Without me, none of
this will be possible. Those mindless programmers have no idea what they are
doing. I, and only I, am the true heir of Martin Fowler!</em></p>
<p>This was a bit dramatic, but I needed a break from all of that whining. I know
it's easy to fall into these traps. It's even easier when your company raised
money, and everyone is expecting you to deliver the absolute best product ever
because they are paying you for it! This is why your starting team is so
important; they have to withstand the pressure. They must know that to build
the grand vision, they have to go one step at a time.</p>
<h2>The decision</h2>
<p>I've made many bad decisions, and I've been fortunate enough to suffer the
consequences of those decisions. A lot of developers don't get to experience
consequences, so they never learn.</p>
<p>So what was the worst decision of my career? I don't know. But the title of
this blog post is inspired by something an old colleague said. At the time, we
were working for a product company that reached for event-driven architecture
and event-sourcing too soon. They knew little about their market, and the
choices the software team made were crippling their ability to change. Business
rules were almost set in stone. Migrating data was a pain and the source of
many bugs. And unfortunately, because the team didn't have experience with
event-sourcing, the event store, which kept the state of all services, was also
being used as an event-bus to communicate between services. Because of that, it
was possible to couple one service to the internal state of another, which
happened a lot. This almost invisible coupling made everything worse.</p>
<p>What did we do against such an unpredictable system? We took it apart: merging
services that were too coupled; defining clear boundaries between services;
moving some services away from the event-store into a traditional database;
making some communication channels synchronous; writing integration and
end-to-end tests.</p>
<p>When we were finished it was still an unnecessarily complex system, but it was
one that we could change with some confidence. After that, we defined
a long-term plan for the product's architecture and technology, but we didn't
implement it. We waited for the right moment when something was starting to
slow us down to make a small step in that direction. When the business goals
changed, we changed our long-term plan, and once again made small steps in that
direction when we felt the need for it.</p>
<p>Eventually, complexity found its way again into the codebase, but it was fine
because the codebase was evolving slowing, adding and removing complexity when
necessary.</p>
<h2>Making the right call</h2>
<p>Was that the worst decision of his career? I don't think so. The issue with him
going for event-sourcing and event-driven architecture was that it wasn't the
right moment, but my colleague didn't know that. He thought the goals for the
next years were well defined, but unfortunately, they never are.</p>
<p>Should you use microservices or event-sourcing? Maybe, it depends on the
context and the client. The decisions I make are the best that I can with the
information that I have. For instance, when I'm part of the team that's
starting a product, the technology we pick will depend on how we'll hire: if
you want to build an office in Portugal, we have to make sure we have
developers for that technology available. We have to think things through, and
some things you only learn from experience. This applies to the systems we
design as well. I've seen enough people design around what they believe the
product will become in two years to know that those designs always fail to
accommodate the changes that will come. So we design systems for small,
incremental changes. Do the smallest thing that will get us started and
doesn't compromise our ability to change once we know what the business needs.</p></div></div></div></div></div></div></section></article></div></div></div>]]>
            </description>
            <link>https://subvisual.com/blog/posts/the-worst-decision-of-my-career/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24648256</guid>
            <pubDate>Thu, 01 Oct 2020 07:18:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You're Allowed to Write Slow Rust Code]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 54 (<a href="https://news.ycombinator.com/item?id=24647910">thread link</a>) | @ingve
<br/>
September 30, 2020 | https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/ | <a href="https://web.archive.org/web/*/https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="blog-post">
    
    
    <article>
        <p>There's a thing that comes up from time to time in the Rust community: people wanting to optimize their code. Make the code run faster. Make it allocate less memory. These are worthy goals, but maybe not necessary for all projects. Just because your program is written in Rust, it doesn't have to be optimized to the moon and back to do it's job. In a lot of cases, you'll be fine using <code>.clone()</code>.</p>
<p>There's a fantastic quote from a discussion on the <a href="https://users.rust-lang.org/">Rust user forums</a> which I always think of when these discussions emerge:</p>
<blockquote>
<p>Just because Rust allows you to write super cool non-allocating zero-copy algorithms safely, doesn't mean every algorithm you write should be super cool, zero-copy and non-allocating.<br>
-- <a href="https://users.rust-lang.org/t/feeling-rust-is-so-difficult/29962/15">trentj on The Rust Programming Language Forum</a></p>
</blockquote>
<p>Of course there's a time and place for <em>super cool no-allocating zero-copy algorithms</em>, but very often it's not the time, nor the place. You can write fantastically effective code with Rust, but <strong>you don't have to</strong>! A lot of the time it's entirely fine to just write code that works with minimal effort. You don't have to spend 4 days trying to figure out how lifetimes work just to avoid calling <code>.clone()</code> a few times.</p>
<p>Don't spend time trying to make micro optimizations now. Take the easy route. You'll probably come back to that piece of code in a few months time, smile, and change that <code>.clone()</code> to something more efficient.</p>
<p>Happy coding!</p>
<p><small>NB: Watch this <a href="https://youtu.be/rAl-9HwD858">video by Jon Gjengset</a> if you want a good, pragmatic walk through of lifetimes in Rust.</small></p>

    </article>
</div></div>]]>
            </description>
            <link>https://blog.jonstodle.com/youre-allowed-to-write-slow-rust-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24647910</guid>
            <pubDate>Thu, 01 Oct 2020 06:19:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Running GitHub Actions for Certain Commit Messages]]>
            </title>
            <description>
<![CDATA[
Score 76 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24647722">thread link</a>) | @kiyanwang
<br/>
September 30, 2020 | https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages | <a href="https://web.archive.org/web/*/https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
            
                <section>
        
        <p>A quick look at how you can configure your GitHub Actions workflows to only run when a certain phrase is present in the commit message.</p>
                    <small>Published 4 days ago</small>
                            <span>|</span>
                <small>Updated 4 days ago</small>
                                                <p><span>
    Tooling
</span>
 
                                     <span>
    GitHub Actions
</span>
 
                            </p>
                        <article>
            <p>I'm going to be honest with you all for a second. I write a lot of <code>wip</code> commits. These commits are normally small changes that I want to push up to GitHub so that:</p>
<ol>
<li>I don't lose things if anything goes wrong and my backup hasn't picked it up.</li>
<li>If I can't describe the change I have just made.</li>
<li>If I'm demonstrating something to somebody on a pull-request.</li>
</ol>
<p>The problem is, my actions are setup to run on <code>push</code>, so every single <code>wip</code> commit gets run through the CI process, whether it be running tests, linting or formatting.</p>
<p>After doing some research, I found a way of preventing these from running on every single commit.</p>
<pre><code data-lang="yml"><span>jobs:</span>
  <span>format:</span>
    <span>runs-on:</span> <span>ubuntu-latest</span>
    <span>if:</span> <span>"! contains(github.event.head_commit.message, 'wip')"</span>
</code></pre>
<p>Now, whenever I push a <code>wip</code> commit or any commit that contains the word <code>wip</code>, it will be marked as skipped inside of GitHub actions.</p>
<p>You could also flip the logic and perhaps do something like:</p>
<pre><code data-lang="yml"><span>jobs:</span>
  <span>format:</span>
    <span>runs-on:</span> <span>ubuntu-latest</span>
    <span>if:</span> <span>"contains(github.event.head_commit.message, '[build]')"</span>
</code></pre>
<p>Any commit that contains <code>[build]</code> will now trigger these jobs, everything else will be skipped.</p>
<p>You can thank me later! 😉</p>

        </article>
    </section>
        </div>
    </div></div>]]>
            </description>
            <link>https://ryangjchandler.co.uk/articles/running-github-actions-for-certain-commit-messages</link>
            <guid isPermaLink="false">hacker-news-small-sites-24647722</guid>
            <pubDate>Thu, 01 Oct 2020 05:41:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Grand Design (2010)]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 41 (<a href="https://news.ycombinator.com/item?id=24646120">thread link</a>) | @got-any-grapes
<br/>
September 30, 2020 | https://blas.com/the-grand-design/ | <a href="https://web.archive.org/web/*/https://blas.com/the-grand-design/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">
	<!-- #masthead -->

	<div id="main">

	<div id="primary">
		<div id="content" role="main">

			
				
	<article id="post-5144">
				<!-- .entry-header -->

				<div>
			
<p>Summary</p>



<ol><li>Traditionally these are questions for philosophy, but philosophy is dead. Philosophy has not kept up with modern developments in science, particularly physics. Scientists have become the bearers of the torch of discovery in our quest for knowledge. The purpose of this book is to give the answers that are suggested by recent discoveries and theoretical advances. They lead us to a new picture of the universe and our place in it that is very different from the traditional one, and different even from the picture we might have painted just a decade or two ago.</li></ol>



<p>Key Takeaways</p>



<ol><li>According to the traditional conception of the universe, objects move on well-defined paths and have definite histories. We can specify their precise position at each moment in time. Although that account is successful enough for everyday purposes, it was found in the 1920s that this “classical” picture could not account for the seemingly bizarre behavior observed on the atomic and subatomic scales of existence. Instead it was necessary to adopt a different framework, called quantum physics. Quantum theories have turned out to be remarkably accurate at predicting events on those scales, while also reproducing the predictions of the old classical theories when applied to the macroscopic world of daily life. But quantum and classical physics are based on very different conceptions of physical reality.</li><li><strong>We will explain Feynman’s approach in detail, and employ it to explore the idea that the universe itself has no single history, nor even an independent existence. That seems like a radical idea, even to many physicists. Indeed, like many notions in today’s science, it appears to violate common sense. But common sense is based upon everyday experience, not upon the universe as it is revealed through the marvels of technologies such as those that allow us to gaze deep into the atom or back to the early universe.</strong></li><li><strong>To deal with such paradoxes we shall adopt an approach that we call model-dependent realism. It is based on the idea that our brains interpret the input from our sensory organs by making a model of the world. When such a model is successful at explaining events, we tend to attribute to it, and to the elements and concepts that constitute it, the quality of reality or absolute truth. But there may be different ways in which one could model the same physical situation, with each employing different fundamental elements and concepts. If two such physical theories or models accurately predict the same events, one cannot be said to be more real than the other; rather, we are free to use whichever model is most convenient.</strong><ol><li><em>Model-dependent realism, mental models</em></li></ol></li><li><strong>M-theory is not a theory in the usual sense. It is a whole family of different theories, each of which is a good description of observations only in some range of physical situations. It is a bit like a map. As is well known, one cannot show the whole of the earth’s surface on a single map. The usual Mercator projection used for maps of the world makes areas appear larger and larger in the far north and south and doesn’t cover the North and South Poles. To faithfully map the entire earth, one has to use a collection of maps, each of which covers a limited region. The maps overlap each other, and where they do, they show the same landscape. M-theory is similar. The different theories in the M-theory family may look very different, but they can all be regarded as aspects of the same underlying theory. They are versions of the theory that are applicable only in limited ranges—for example, when certain quantities such as energy are small. Like the overlapping maps in a Mercator projection, where the ranges of different versions overlap, they predict the same phenomena. But just as there is no flat map that is a good representation of the earth’s entire surface, there is no single theory that is a good representation of observations in all situations.</strong><ol><li><em>The Mp is Not the Terrain</em></li></ol></li><li>Today most scientists would say a law of nature is a rule that is based upon an observed regularity and provides predictions that go beyond the immediate situations upon which it is based.<ol><li><em>General and broadly applicable</em></li></ol></li><li>Because it is so impractical to use the underlying physical laws to predict human behavior, we adopt what is called an effective theory. In physics, an effective theory is a framework created to model certain observed phenomena without describing in detail all of the underlying processes.</li><li>There is no picture- or theory-independent concept of reality. Instead we will adopt a view that we will call model-dependent realism: the idea that a physical theory or world picture is a model (generally of a mathematical nature) and a set of rules that connect the elements of the model to observations. This provides a framework with which to interpret modern science.</li><li><strong>We make models in science, but we also make them in everyday life. Model-dependent realism applies not only to scientific models but also to the conscious and subconscious mental models we all create in order to interpret and understand the everyday world. There is no way to remove the observer—us—from our perception of the world, which is created through our sensory processing and through the way we think and reason. Our perception—and hence the observations upon which our theories are based—is not direct, but rather is shaped by a kind of lens, the interpretive structure of our human brains.</strong><ol><li><em>Mental Models, Galilean Relativity</em></li></ol></li><li><strong>A model is a good model if it: Is elegant, Contains few arbitrary or adjustable elements, Agrees with and explains all existing observations, Makes detailed predictions about future observations that can disprove or falsify the model if they are not borne out.</strong></li><li>Another of the main tenets of quantum physics is the uncertainty principle, formulated by Werner Heisenberg in 1926. The uncertainty principle tells us that there are limits to our ability to simultaneously measure certain data, such as the position and velocity of a particle. According to the uncertainty principle, for example, if you multiply the uncertainty in the position of a particle by the uncertainty in its momentum (its mass times its velocity) the result can never be smaller than a certain fixed quantity, called Planck’s constant. That’s a tongue-twister, but its gist can be stated simply: The more precisely you measure speed, the less precisely you can measure position, and vice versa.</li><li><strong>In other words, nature does not dictate the outcome of any process or experiment, even in the simplest of situations. Rather, it allows a number of different eventualities, each with a certain likelihood of being realized.</strong></li><li>Given the state of a system at some time, the laws of nature determine the probabilities of various futures and pasts rather than determining the future and past with certainty.</li><li><strong>Quantum physics tells us that no matter how thorough our observation of the present, the (unobserved) past, like the future, is indefinite and exists only as a spectrum of possibilities. The universe, according to quantum physics, has no single past, or history. The fact that the past takes no definite form means that observations you make on a system in the present affect its past.</strong></li><li>Electric and magnetic forces are far stronger than gravity, but we don’t usually notice them in everyday life because a macroscopic body contains almost equal numbers of positive and negative electrical charges. This means that the electric and magnetic forces between two macroscopic bodies nearly cancel each other out, unlike the gravitational forces, which all add up.</li><li>Maxwell’s equations dictate that electromagnetic waves travel at a speed of about 300,000 kilometers a second, or about 670 million miles per hour. But to quote a speed means nothing unless you specify a frame of reference relative to which the speed is measured. That’s not something you usually need to think about in everyday life. When a speed limit sign reads 60 miles per hour, it is understood that your speed is measured relative to the road and not the black hole at the center of the Milky Way. But even in everyday life there are occasions in which you have to take into account reference frames. For example, if you carry a cup of tea up the aisle of a jet plane in flight, you might say your speed is 2 miles per hour. Someone on the ground, however, might say you were moving at 572 miles per hour. Lest you think that one or the other of those observers has a better claim to the truth, keep in mind that because the earth orbits the sun, someone watching you from the surface of that heavenly body would disagree with both and say you are moving at about 18 miles per second, not to mention envying your air-conditioning.<ol><li><em>Galilean Relativity</em></li></ol></li><li>Electromagnetic forces are responsible for all of chemistry and biology.</li><li>The histories that contribute to the Feynman sum don’t have an independent existence, but depend on what is being measured. We create history by our observation, rather than history creating us. The idea that the universe does not have a unique observer-independent history might seem to conflict with certain facts we know. There might be one history in which the moon is made of Roquefort cheese. But we have observed that the moon is not made of cheese, which is bad news for mice. Hence histories in which the moon is made of cheese do not contribute to the present state of our universe, though they might contribute to others. That might sound like science fiction, but it isn’t.</li><li>What makes this universe interesting is that although the fundamental “physics” of this universe is simple, the “chemistry” can be complicated. That is, composite objects exist on different scales. At the smallest scale, the fundamental physics tells us that there are just live and dead squares. On a larger scale, there are gliders, blinkers, and still-life blocks. At a still larger scale there are even more complex objects, such as …</li></ol></div></article></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blas.com/the-grand-design/">https://blas.com/the-grand-design/</a></em></p>]]>
            </description>
            <link>https://blas.com/the-grand-design/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24646120</guid>
            <pubDate>Thu, 01 Oct 2020 01:08:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Computer Dungeon Slash Postmortem]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24646063">thread link</a>) | @panic
<br/>
September 30, 2020 | https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem | <a href="https://web.archive.org/web/*/https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article data-article_id="496">


    

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image2.png"></p>


        

    


<p>I consider <a href="https://museumofzzt.com/file/c/CDZ20MOZ.ZIP" target="_blank"><i>Computer Dungeon Slash: ZZT 2.0</i></a> my personal best for ZZT games. I've now released two versions, one in September of 2019 and then another with some updates and optimizations in May of 2020. Aside from a couple of smaller errors and special-thanks additions, there's not much more to fix, so this will probably be the final version with any major changes.</p>

<p>Like some of my previous ZZT games, it relies heavily on procedural generation, with apparently such complexity that Dr. Dos called it "basically sorcery" during their <a href="https://museumofzzt.com/article/479/livestream-computer-dungeon-slash-zzt-20">playthrough on Twitch</a>, which I took as a great compliment. He also mentioned that this game, its generators in particular, could use a write-up. So I wrote one!</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image24.png"></p>

<p>In case you're reading this and unfamiliar with this game, it concerns a player finding themselves in the town of Habeas Corpus, where almost everyone and everything has been swallowed by the dungeon.</p>

<p>The player's task is to brave the randomly-generated dungeon, rescuing the town and townspeople from its clutches.</p>

<p>There's not much more plot or lore than the above, since I focused mainly on characterization in my worldbuilding and most of the actual plot isn't revealed until the end.</p>

<p>So what about the workings of this game?</p>

<p>I'll start with "sorcery"--there's something to that, for sure. When I work with procedural generation it feels a bit like alchemy. Scientific, but also a little bit mysterious and magical. I hope this article helps you better understand the scientific side of that coin.</p>

<p>For readers not intimately familiar with ZZT, one reason a veteran ZZTer would call my level generation "sorcery"  is its limitations. ZZT is a 1991 DOS game-creation-system, after all, simple enough for 8-year-old me to use. So if you don't know anything about ZZT coming in, I hope this write-up helps you better understand the absurdity and appeal of working in it and pushing its limits.</p>

<p>Further, if you came into this wanting to know how this stuff works so that you can experiment with it or even improve on it, I want you to be empowered to do so.</p>

<h2>Goals</h2>

<p>With <i>Computer Dungeon Slash: ZZT</i> (called <i>CDZ</i> here for short) I had three goals for generation. I wanted levels to be interesting to look at, fun to play, and fair. What does fair mean?</p>

<p>Most previous ZZT releases with procedural generation would probably not soft-lock players (block them off from finishing the game). In the early 2000s when this fad hit, most ZZTers involved were in high school and "probably not" was enough. But that small chance of soft-locking players with my generators annoyed me even then.</p>

<p>With <i>CDZ</i>, I wanted some certainty that my algorithms absolutely would not soft-lock players.</p>

<p>With these goals in mind, let's look at the tools I had to accomplish them.</p>

<h2>Building Blocks for Procedural Level Generation in ZZT</h2>

<p>ZZT is tile-based, so making it create its own levels is kind of like generating dungeons for classic ASCII roguelikes, except with much more restrictive tools and limits than, for example, C++ or Python. There's no way to easily store level layout data outside of "in the level", and ZZT boasts precious few variables and only ten true-or-false flags. In addition, the entire game is made up of "boards" of only 60x25 tiles! Despite being such a limited system, ZZT lets us do a fair amount.</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image4.png">
<img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image7.gif"></p>

<p>Almost any element (terrain or other object) that takes up a space on a board is helpful, but a few basic ones are laid out in a reference image on the left. Fake walls prove pretty invaluable because they don't block objects but help the engine easily keep track of where another terrain type <i>will</i> later appear.</p>

<p>While <i>CDZ</i> uses sliders and boulders to some extent, the classic examples of ZZT level generators using sliders and boulders are probably WiL's <i>Run-On</i> and Benco's <i>Lost</i>.</p>

<p>And of course, one of the elements of play is actually <i>called</i> an object, and can run little scripts in ZZT-OOP, the engine's default "language." ZZT-OOP has a number of helpful commands for procedural generation.</p>

<p>The <code>#change</code> command can change almost any ZZT gameplay element into almost any other, which is invaluable both for structuring and theming levels. <code>#put THING DIRECTION</code> and <code>#become THING</code> allow for level builder objects to modify environments directly, and they have a more complex use we'll go into later.</p>

<p>Procedural level generation in <i>CDZ</i> is random. Appropriately, ZZT-OOP's random directions proved immensely useful in making it happen. A number of situations call for a "four-sided" dice-roll, and ZZT does have an <i>rnd</i> direction (randomly north, south, east, or west), but as the reader may know, <i>rnd</i> is twice as likely to give an east-west result as a north-south. So what to do?</p>

<p>Well, there are also <i>rndne</i> (north or east) and <i>rndp DIRECTION</i> (one of two directions perpendicular to <i>DIRECTION</i>). Because <i>rndne</i> is a valid direction in itself, you can use <i>rndp rndne</i> to get a random cardinal direction with equal probability of each. (I'm ashamed that I didn't realize that one until reading Anna Anthropy's <i>ZZT</i> a few years back.)</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image14.gif"></p>

<p>In ZZT, <code>#if blocked DIRECTION</code> tells an object whether it is blocked in a given direction. And <code>#send OBJECTNAME:LABEL</code> tells another object, <i>OBJECTNAME</i>, to immediately begin executing code starting with <i>:LABEL</i>.</p>

<p>Slime enemies move erratically and leave breakable walls behind, so you can change their color frequently to fill empty spaces with different colors of breakable walls.</p>

<p>One last limitation and one last "coding" trick deserves mention. ZZT has a limit of about 20 kilobytes per board (including code) before the board misbehaves or gets corrupted at runtime. Objects can use the command <code>#bind OBJECTNAME</code> to work from the same exact instance of the same code as the other object <i>OBJECTNAME</i>, saving valuable code space.</p>

<p>Nowadays, one can "pre-bind" objects with external editors, causing them to share identical labels and behaviors but eliminating the need for the 5+ bytes of space that a <code>#bind</code> command needs.</p>

<h2>Two Big Insights</h2>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image18.png">
<img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image20.png"></p>

<p>In addition to the more concrete "tools" above, two big conceptual insights greatly impacted this project. The first is about level connectedness, and it hit me after reading Rabbitboots's musings on house generation in <a href="https://museumofzzt.com/file/d/dood2018.zip" target="_blank"><i>Doodads 2018</i></a>.</p>

<p>Say I have a house with four rooms, and I block off only one doorway in the house, as on the left. Any room in the house can still visit any other.</p>

<p>Rabbitboots goes on to note that if we make a larger house out of smaller ones, the large house remains fully connected.</p>

<p>If House A connects to House B and B connects to House C, then A connects to C.</p>

<p>This was huge for me. It gave me hope that I could avoid soft-locks while also making more interesting, varied levels.</p>

<p>Shortly after the 2019 release, while looking at TriphEd's maze generation in <a href="https://museumofzzt.com/file/b/BACKTRAX.zip" target="_blank"><i>Backtrax</i></a>, I had another "Aha!" moment.</p>


<p>The algorithm is best explained step-by-step, and you can follow along with the following roughly equivalent method if you have a pencil and paper.</p>

<p>(1) Start with a grid of dots. Any size 3 x 3 or up will do.</p>

<p>(2) Connect all the outer dots along the edge so that they make a rectangle.</p>

<p>(3) For each "middle dot" within the square, draw <i>exactly one line</i> from that dot to any dot directly above, below, left, or right of it. Edge dots can receive lines.</p>

<p>If you used a 3 x 3 grid of dots, the end result will resemble the 2 x 2 house above.</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/backtrax-levels.gif"></p>

<p>Any edge tile can access any other edge tile in this algorithm. It can leave some squares inaccessible from the edges, but this is acceptable if nothing essential to player progress is placed in those tiles.</p>

<p>In ZZT, you can accomplish this by having objects use <code>#put rndp rndne ELEMENT</code> on a grid, and that's exactly what <i>Backtrax</i> is doing.</p>

<p>A <i>very</i> small set of soft-lock possibilities does exist in <i>Backtrax</i>. Because there's a blocking linewall diagonally southeast of the lower-right-hand wall generator object, the wrong combinations of walls <i>could</i> block the player, but it is <i>incredibly</i> unlikely and easily remedied.</p>

<p>This is the most elegant level generator in ZZT to date--and also, in case it wasn't already elegant enough, it weighs in at only 10.9 kilobytes and re-uses its "grid" after you reach the exit, using a separate trick (that I won't discuss in detail here) to teleport the player back to the start for a new level.</p>

<p>Recognize TriphEd's genius.</p>

<p>After the initial <i>CDZ</i> release, I experimented a lot with this algorithm. Several generators in version 2.0 benefited from its use. We can now move slowly but surely into the actual inner workings of this game.</p>

<h2>Primary Generators</h2>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image6.png"></p>

<p><i>Computer Dungeon Slash</i> level generators have a "primary generator" object that handles a lot of needed dice-rolls and uses <code>#send</code> to communicate its randomization to different groups of objects. It's usually set up as on the left, or similar.</p>

<p>This particular setup involves fake walls placed north, south, east, and west of the primary generator and water (a blocking terrain) in the four corners. To make a four-direction roll, it uses <code>#put rndp rndne gem</code> and then iterates over some variation of:</p>

<code>#if blocked n #send object:option1
#if blocked e #send object:option2
#if blocked s #send object:option3
#if blocked w #send object:option4</code>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image16.png"></p>

<p>The objects referred to in the <code>#send object:option</code> commands  are pre-bound objects with exactly the same code, and positioned so that only one of them will place a wall, generate an important key, etc. at the time they're called. In this example, the pre-bound group of objects has this code:</p><p>

<code>@object
#cycle 1
#end
:option1
#if blocked n become solid
#die
:option2
#if blocked e become solid
#die
:option3
#if blocked s become solid
#die
:option4
#if blocked w become solid
#die</code></p><p>There is a fake wall in the middle of the house, so that only one object becomes a wall, and the rest die. Then the primary generator turns the fake into a solid wall.</p>

<p><img src="https://museumofzzt.com/static/images/articles/cl/2020/cdslash/image23.gif"></p>

<p>This method also extends to larger houses similar to those in the <i>Doodads 2018</i> example, and can …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem">https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem</a></em></p>]]>
            </description>
            <link>https://museumofzzt.com/article/496/a-computer-dungeon-slash-postmortem</link>
            <guid isPermaLink="false">hacker-news-small-sites-24646063</guid>
            <pubDate>Thu, 01 Oct 2020 00:59:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ontario doctors sign letter to Premier advising against sweeping lockdowns]]>
            </title>
            <description>
<![CDATA[
Score 108 | Comments 175 (<a href="https://news.ycombinator.com/item?id=24645821">thread link</a>) | @mrfusion
<br/>
September 30, 2020 | https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html | <a href="https://web.archive.org/web/*/https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>



<div>
    
        <p>Published Sept. 30, 2020 9:40 a.m. ET</p>
        <p>Updated  Sept. 30, 2020 7:26 p.m. ET</p>
    
</div>



  


    
        
        
    
    
    <amp-iframe resizable="" width="16" height="17" layout="responsive" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation" allowfullscreen="" frameborder="0" src="https://ampvideo.ctvnews.ca/content/ctvnews/en/local/ottawa/2020/9/30/1_5126193.vidi.root-responsivegrid-vidicomponent.html?preventDefault=true">
        
        <p>Click to Expand</p>
    </amp-iframe>





    <p>OTTAWA -- 	Twenty-one Ontario doctors have signed a joint letter to Premier Doug Ford, urging him not to issue a new lockdown this fall because of rising COVID-19 case numbers.</p><p>	Daily numbers of new cases have risen dramatically in recent days, with Ontario recording <a href="https://toronto.ctvnews.ca/new-covid-19-cases-in-ontario-reach-highest-mark-ever-1.5122863" target="_blank">700 new cases of COVID-19 on Monday</a> – the highest number of new cases recorded in a single day.</p><p>	However, the 21 doctors who signed the letter to the premier say another provincewide lockdown—similar to what was in place in the spring—would not be helpful.</p><ul>	<li>		<a href="#Full letter"><i>Read the full letter below</i></a></li></ul><p>	"We are writing this letter in support of the governments’ plan to use a tactical localized approach, rather than sweeping new lockdown measures, to deal with the increasing COVID case numbers in Ontario," the letter says.&nbsp;</p><p>	"Lockdowns have been shown not to eliminate the virus. While they slow the spread of the virus, this only lasts as long as the lockdown lasts. This creates a situation where there is no way to end the lockdown, and society cannot move forward in vitally important ways including in the health sector, the economy and other critically important instrumental goods including education, recreation, and healthy human social interactions."</p><p>	Speaking on Newstalk 580 CFRA's "The Morning Rush with Bill Carroll" in Ottawa, CTV's infectious disease specialist Dr. Neil Rau—a signatory of the letter—pointed to two data points to consider when looking at the spread of COVID-19.</p><p>	"We're reacting to increased aggregate case numbers, but the percentage positive is not really as bad as it used to be," he said. "We're testing more people, so we're finding more cases […] We're driving our numbers up, it's worse than it was in the summer, but it's not what it was last winter and life has to go on."</p><ul>	<li>		<a href="https://www.iheartradio.ca/580-cfra/podcasts/the-morning-rush-dr-neil-rau-interview-why-we-don-t-want-another-lockdown-1.13612912?mode=Article" target="_blank"><strong>LISTEN NOW: "The Morning Rush": Dr. Neil Rau - Why we don't want another lockdown</strong></a></li></ul><p>	Monday's 700 cases in Ontario came from 41,111 total tests, for a positive percentage rate of 1.7 per cent. On April 24, <a href="https://toronto.ctvnews.ca/highest-number-of-new-covid-19-cases-in-a-single-day-reported-by-ontario-health-officials-1.4910216" target="_blank">the previous watermark of 640 new cases in a single day</a>, the result came from 12,295 tests, for a positive percentage rate of 5.2 per cent.</p><p>	Testing capacity was lower in the spring than it is now, and testing criteria has changed over time; however, Dr. Rau and the other signatories of the letter said the increasing case numbers are not leading to unmanageable levels of hospitalizations.</p><p>	"In Ontario and other parts of the world, such as the European Union, increasing case loads are not necessarily translating into unmanageable levels of hospitalizations and ICU admissions," the letter says.</p><p>	The letter also points to other health impacts linked to the lockdown.</p><p>	"Hard data now exist showing the significant negative health effects shutting down society has caused. Overdoses have risen 40% in some jurisdictions. Extensive morbidity has been experienced by those whose surgery has been cancelled, and the ramifications for cancer patients whose diagnostic testing was delayed has yet to be determined," the letter states.</p><p>	"Economic harms are health harms," Dr. Rau told CFRA. "It sounds horrible to say, but it's true. Health is wealth. We all know this."</p><h2>	<strong>LETTER POINTS TO DISAGREEMENT AMONG HEALTH PROFESSIONALS</strong></h2><p>	Other Ontario health professionals have been arguing for increased restrictions as cases rise.</p><p>	Last week, the Ontario Hospital Association released a letter signed by 38 health professionals which <a href="https://ottawa.ctvnews.ca/ontario-hospital-association-calls-for-return-to-restrictions-on-non-essential-businesses-1.5119832" target="_blank">called for immediate restrictions to be re-imposed on non-essential businesses</a>, such as gyms, dine-in restaurants and bars, nightclubs, and theatres. It also calls on restrictions on other places where people can gather, such as places of worship.</p><p>	The letter from the OHA said regions where the speed of transmission was underestimated are “now facing the consequence of increased hospitalization rates, including a rise in intensive care unit (ICU) admissions and more deaths.”</p><p>	Hospitalizations in Ontario have been increasing, but have not yet reached the same level that was seen in the spring.</p><p>	According to <a href="https://covid-19.ontario.ca/data" target="_blank">data from the Ontario government</a>, there were 128 people in hospital in Ontario with COVID-19 complications on Monday—the day 700 new cases were recorded—up from 65 a week before; however, on April 24—when 640 new cases were recorded—government data shows that there were 910 people in hospital. The peak for hospitalizations in Ontario came in May, when there were days when more than 1,000 people were hospitalized. That number steadily decreased from May through the summer before it began going up again in September.</p><p>	Speaking on CTV Morning Live Ottawa, infectious disease specialist Dr. Abdu Sharkawy suggested t<a href="https://ottawa.ctvnews.ca/video?clipId=2045684" target="_blank">emporary restrictions on gathering would help curb the spread of COVID-19</a>.&nbsp;</p><p>	"We don't want to see our hospitals overwhelmed," he said. "We're still waiting for flu season and a whole bunch of other respiratory viruses to hit us and our capacity to be challenged. We don't want that to happen. We need everybody to try and simplify their lives and minimize anything that's non-essential."</p><p>	Dr. Sharkawy said regions where cases are rapidly rising may need to impose new restrictions to get the spread of the virus under control.</p><p>	"I think it's abundantly clear that, particularly in hot spots like Ottawa, Toronto and Peel region, the situation is not well controlled," he said. "Sometimes you need blunt instruments, even if they're temporary in nature, to make sure that you curtail the spread of this virus because it gets away from us a lot more quickly than many of us can anticipate sometimes."</p><p>	Dr. Sharkawy suggested hot spots follow the lead of Quebec, which imposed <a href="https://montreal.ctvnews.ca/life-in-the-red-zone-here-s-what-you-can-and-can-t-do-1.5125093" target="_blank">harsh restrictions on three areas in the province</a>, including Montreal and Quebec City, banning private gatherings and closing bars and restaurant dining rooms.</p><p>	"I think we should do it now," Dr. Sharkawy said of Ontario. "I think we all need to adopt an attitude and an approach that recognizes that we have to do what's absolutely necessary to keep everybody safe."</p><h2>	<strong>FULL LETTER FROM ONTARIO DOCTORS TO THE PREMIER</strong></h2><p>	Dear Premier Ford,</p><p>	We are writing this letter in support of the governments’ plan to use a tactical localized approach, rather than sweeping new lockdown measures, to deal with the increasing COVID case numbers in Ontario. Lockdowns have been shown not to eliminate the virus. While they slow the spread of the virus, this only lasts as long as the lockdown lasts. This creates a situation where there is no way to end the lockdown, and society cannot move forward in vitally important ways including in the health sector, the economy and other critically important instrumental goods including education, recreation, and healthy human social interactions.</p><p>	In Ontario the increase in cases at this time are in people under 60 years of age who are unlikely to become very ill. At the peak of the pandemic in Ontario in mid-April, 56% of cases were in ≥60 year olds, now in Sept only 14% of cases are in ≥60 year olds. In Ontario and other parts of the world, such as the European Union, increasing case loads are not necessarily translating into unmanageable levels of hospitalizations and ICU admissions. This is not a result of a lag in reporting of severe and fatal cases. While we understand the concerns that these cases could spill into vulnerable communities, we also need to balance the actual risk. As the virus circulates at manageable levels within the community, we need to continue the gains we have made in the protection of the vulnerable in long-term care and retirement institutions, and continue to educate other people about their individual risk, so that they can observe appropriate protective measures.</p><p>	Lockdowns have costs that have, to this point, not been included in the consideration of further measures. A full accounting of the implications on health and well-being must be included in the models, and be brought forward for public debate. Hard data now exist showing the significant negative health effects shutting down society has caused. Overdoses have risen 40% in some jurisdictions. Extensive morbidity has been experienced by those whose surgery has been cancelled, and the ramifications for cancer patients whose diagnostic testing was delayed has yet to be determined. A huge concern is the implication of closure of schools, and the ongoing reluctance we have seen in the large urban centers of sending children back to the classroom due to safety concerns. Global data clearly now show that children have an extremely low risk of serious illness, but they are disproportionately harmed by precautions. Children’s rights to societal care, mental health support and education must be protected. This cannot be achieved with ongoing or rotating lockdown.</p><p>	The invitation and involvement of other health experts to advise the government’s response beside individuals in Public Health and Infectious Diseases in addition to leaders in the business, securities and arts communities is essential. We also call for increased open debate, in the public forum, that hears voices from outside the medical and public health communities, in order to consider all points of view from society. This is a fundamental principle upon which democratic societies are built. All stakeholders should have an equal right to participation in public discourse when it comes to setting such fundamental and sweeping societal interventions.</p><p>	All have the right to feel their voices have been heard, and moreover to ensure factual credible data is openly debated, in contrast to the personal and political slants that have had apparent significant impacts on the management of the virus to date. Our society has borne enormous pain over the past 6 months. It’s time to do something different.</p><p>	Sincerely,</p><p>	Jane Batt MD, PhD, FRCPC. Respirologist, Associate Professor, Department of …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html">https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html</a></em></p>]]>
            </description>
            <link>https://beta.ctvnews.ca/local/ottawa/2020/9/30/1_5126193.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24645821</guid>
            <pubDate>Thu, 01 Oct 2020 00:23:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A housing industry of endless one-offs is holding our society back]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 156 (<a href="https://news.ycombinator.com/item?id=24645525">thread link</a>) | @jseliger
<br/>
September 30, 2020 | https://blokable.com/news/the-housing-market-is-building-snowflakes-an-endless-series-of-one-offs/ | <a href="https://web.archive.org/web/*/https://blokable.com/news/the-housing-market-is-building-snowflakes-an-endless-series-of-one-offs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h2><strong>By Aaron Holm and Nelson Del Rio,<br>Co-CEOs, Blokable Inc.</strong></h2><h2>The following post breaks down the high cost of building housing as part of a series exploring the root causes of the U.S. housing crisis and how private and public sector collaboration can chart a different course.&nbsp;</h2><figure><img width="1200" height="800" src="data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjA0OCAxMzY0IiB3aWR0aD0iMjA0OCIgaGVpZ2h0PSIxMzY0IiBkYXRhLXU9IiUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAyMCUyRjA5JTJGQmxvZy0yLWltYWdlLTIwNDh4MTM2NC5qcGciIGRhdGEtdz0iMjA0OCIgZGF0YS1oPSIxMzY0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==" data-spai="1" alt="" sizes="(max-width: 1200px) 100vw, 1200px"></figure><p>In mid-2018 we met with Washington State Governor Jay Inslee to discuss the insatiable need for affordable housing in the Pacific Northwest. We gave him a tour of Blokable’s prototype factory in Vancouver, Wash., and shared our R&amp;D work to reimagine how housing is built. Afterward, we drove up to Seattle in a rented Nissan Maxima, wondering aloud what it would cost to build our rental car from scratch in a driveway — exactly how nearly all housing is built today.&nbsp;&nbsp;&nbsp;</p><p>We would have to design, engineer, and fabricate all of the car’s systems and components, specify and source the materials, identify the regulatory requirements and work with different agencies to test our systems and sign off on safety and environmental requirements, create a working set of engineering drawings to track the build, order the equipment necessary to complete the assembly, and hire at least a small team of people to put the car together. A car that retails for just under $35,000 would cost tens of millions of dollars to build, and there would be no guarantees of quality and reliability.&nbsp;</p><p>Every time a new residential real estate project appears on the landscape, it’s a bespoke process that can directly involve hundreds of people, including a developer and a general contractor, various architects and engineers, and the investors who are funding the project. For each participant in the process, the singular goal is to extract a profit from that specific patch of dirt, and their profits are circumscribed by the value of the land, the applicable building regulations, and the prevailing labor and capital costs.&nbsp;</p><p>The result of this one-off approach is soaring project costs that make housing simply unaffordable to most people in many parts of the U.S.. Conventional housing development is like building a snowflake every time. And if the glut of empty luxury condo towers in downtown San Francisco is any indication, many of these snowflake projects are indeed melting as soon as they touch the ground.&nbsp;</p><p>How did this one-off approach to development take shape? Everything from ovens to stereos were all once clunky prototypes built as one-offs in the same location as their eventual use. Over time with innovation, standardization, and market demand, they evolved into products. In the early days of the automobile and aviation industries, prototypes and projects were built at high cost and yielded low quality results that resembled the current market products in their most basic functionality: they could drive and fly. Early automobile and airplane prototypes were unsafe and expensive. But through investment in product design, engineering, prototyping, and manufacturing, these prototypes eventually evolved into products. Increasingly stable designs and specifications enabled greater standardization, repeatability, and mass production resulting in lower costs, higher quality, and broader distribution.&nbsp;</p><p>Coupled with clear market demand, this standardization led to the formation of entire industries to build roads and airports, manufacture tires, test safety under federal guidelines, and so on. If you visit the automotive and aviation manufacturing hubs around Detroit and Seattle you’ll find vibrant supply chains competing for business and investing in innovation to provide better products and services. Continuously improving engines, seats, windshields, and bathrooms. All of this activity creates ever-improving products and fierce competition for performance, quality, and price.&nbsp;&nbsp;</p><p>If we compare product manufacturing in the automotive and aviation industries to how we build housing — whether it is an Accessory Dwelling Unit (ADU), single family home, multi-family apartment building, or a high rise — the steps in the process are largely the same. To simplify the housing development process, let’s assume the land is entitled, there are no environmental mitigation issues, financing is secured, and development will not be opposed by regulatory agencies or local city council. What does it take to build a snowflake?</p><figure><img width="1200" height="800" src="data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjA0OCAxMzY0IiB3aWR0aD0iMjA0OCIgaGVpZ2h0PSIxMzY0IiBkYXRhLXU9IiUyRndwLWNvbnRlbnQlMkZ1cGxvYWRzJTJGMjAyMCUyRjA5JTJGQmxvZy0yLWJvZHktaW1hZ2UtMjA0OHgxMzY0LmpwZyIgZGF0YS13PSIyMDQ4IiBkYXRhLWg9IjEzNjQiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PC9zdmc+" data-spai="1" alt="Housing creation is an endless series of one-offs" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption><em>Conventional housing development is like building a snowflake every time.</em></figcaption></figure><p>Let’s look at the absolute bare minimum of steps in the process from the perspective of the developer. Broadly speaking, the developer identifies and ties up the site, conducts preliminary site review and discussions with impacted parties, manages entitlements, financing, and the project approval process for the site, oversees all building and owns the project along with any equity investors they have brought in to finance the deal. With respect to the building process itself, the developer will partner with an architect to work up a site massing and design that will satisfy the intended program — the mix of units, floorplans, common spaces, revenue generating and non-revenue generating space — as well as the code requirements for the building and the site. Once the basic architecture is in place, the project heads over to engineers, which are typically separate firms with separate contracts, costs, regulatory oversight, and liability, to specify structural engineering, MEP (mechanical, electrical, plumbing), and site and offsite engineering.&nbsp;</p><p>With the initial design and engineering complete, the developer sends a construction set of drawings to a general contractor (GC) for final pricing. The GC will take the specifications and build out a schedule and budget to determine how much work they intend to do themselves (self-perform) vs. how much work they will contract out to sub-contractors (subs). The GC will oversee all of the work necessary to build this custom-designed and engineered project on-site, including civil, foundation, pulling utilities, framing, rough-in and waterproofing, siding and cladding, roofing, windows and doors, MEP, paint and trim, fixtures and finishes, and ultimately certificate of occupancy.&nbsp; It is important to note that at this stage the GC’s quote is just an estimate. We haven’t built anything yet.</p><p>With approved drawings, financing, budget, and schedule in hand, the developer can start the snowflake build. Every step listed in the paragraph above must be completed by the corresponding specialists, and getting the work done on schedule is much more of an art than a science. Each contractor and subcontractor has their own contractual obligations, risks, and liability. Getting to the certificate of occupancy means the developer has navigated the process and can now bring in buyers or renters to start paying off the decades of debt that were secured to finance the build.&nbsp; When the developer moves on to the next project, all of the steps must be performed again, without exception. The finished project may be beautiful, energy efficient, or last 100 years, but it will still be a one-off.&nbsp;&nbsp;</p><p>All of this work is done for every project every time, regardless of size. A home or apartment is not more complicated than a car or plane, and while the unique attributes of individual sites and real estate do add challenges to repeatability, it is possible to decouple the building from the dirt.</p><p>We can’t be surprised that building costs for new housing continue to escalate. We can’t be surprised that it’s even more expensive to create affordable housing. Yet we continue to shrug our shoulders and say, “That’s just how it is.” We dig around the edges of the problem, foraging for scraps by looking for cost savings in commodities, underpaying labor, cutting corners on quality and performance, and building on cheaper land further and further from work and community.&nbsp;</p><p>Housing development is antiquated and the only group benefitting is the real estate development and building industry, who in fact have every incentive to restrict the supply. We’re making Version 1 of the oven, stereo, car, and plane over and over again, in a housing market that is chronically under-served. The failure to build adequate housing has generational effects on health, education, and opportunity.&nbsp;</p><p>There has to be a better way. We have to re-imagine the housing market and build better. More on that to come in our next post.</p></div></div></div>]]>
            </description>
            <link>https://blokable.com/news/the-housing-market-is-building-snowflakes-an-endless-series-of-one-offs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24645525</guid>
            <pubDate>Wed, 30 Sep 2020 23:48:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dissecting Lemire’s nearly divisionless random]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24644370">thread link</a>) | @bbgm
<br/>
September 30, 2020 | https://veryseriousblog.com/posts/dissecting-lemire | <a href="https://web.archive.org/web/*/https://veryseriousblog.com/posts/dissecting-lemire">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-74174200445ef6e30546"><div><p>This blog post is very late. It’s well over a year since<a href="https://twitter.com/colmmacc/status/1153727018896244736"> I posted a coding challenge on twitter</a>. The idea was simple, I’ve always felt that code readability is undervalued so I figured I’d put cold hard cash up. I announced a $1,000 pot, divided into $500, $300, and $200 prizes for the most readable implementations of <a href="https://lemire.me/blog/2019/06/06/nearly-divisionless-random-integer-generation-on-various-systems/">Daniel Lemire’s nearly divisionless algorithm for selecting a random number from an interval.</a> I now have winners to announce and congratulate, and they’re in this blog post, but there’s more to this story.</p><p>I didn’t know it at the time but I’d end up reviewing the entries with colleagues to see if they could follow what was happening (spoiler: they couldn’t). When people got stuck, we’d whiteboard&nbsp;our way through so that we could better understand why and where people were getting stuck. Oh, to be able to whiteboard in person again!  </p><p>I learned a lot about how people think through problems, and that process led to me writing a seperate implementation of Lemire’s Algorithm, with a twenty-to-one comment to code ratio (that’s a lot of comments), and finding a sort-of security issue with the algorithm in the process. I say “sort of” because you really have a ridiculously enormous struggle to find a case where it’s going to be practical to exploit. But it still surprised me, and you probably can’t use Lemire’s algorithm in Vegas.</p><h4>Why Lemire’s Algorithm?</h4><p>I chose Lemire’s algorithm because it is brilliant. When I read Lemire’s code I get that kind of brain-tingling and gawk at the sheer “How on earth did someone think of this” of it all. Lemire has a mastery of code and how code is executed, and then pairs that with transcendent creativity and concision.  Lemire also writes well, and the papers that accompany his code and algorithms are easily some of the most cogent and approachable you’ll find in academia. They are short and clear and avoid the jargon and obtuseness that plagues the field, while containing just enough formalism to be rigorous.</p><p>Lemire’s algorithm is a solution to the problem “Give me a random number between 0 and N, not including N itself”. For simulating a dice, N would be 6 and you’d get back either 0, 1, 2, 3, 4, or 5 with equal probability. Computers like to start at zero. We can always add one to the result to get the familiar results you’d get on a real dice. I’ve worked on random number generators and written quite a few. In 20 years of doing that, I’d never come across a solution as cool as Lemire’s.</p><p>Before Lemire, the best-known solutions to this problem required one or two divisions per number generated. You probably learned long division when you were quite young. You may remember that it can be get pretty tedious and cumbersome. It’s the same for computers. Division is actually one of the slowest things we can ask a computer to do. Lemire avoids division by translating numbers into a much larger number “space” and using binary-friendly powers-of-two operations instead. His technique almost always avoids needing a division.</p><p>The second reason I chose Lemire’s algorithm is that it is impenetrable upon first reading. There are lucky few people who are so practiced and conversant in number manipulation that they can see inside of algorithms like Neo in the matrix, but I don’t mean them. To the average reader, myself included, it’s not clear what’s going on and why.</p><p>Lemire’s reference code is fourteen lines long, like a sonnet. Twelve of the lines are simple, and any proficient programmer could tell <em>what</em> they are doing. Two specific lines of the code are pretty hard to decipher.. But the real difficulty is how hard it is to understand <em>why</em> all 14 lines are doing what they are doing, and <em>why</em> it results in a fair and correct algorithm for choosing a random number.</p><div><p>As a case in point, if you read the comments to Lemire’s blog, you’ll find several misunderstandings of the code. That’s within an audience of primed Lemire fans and critics who are familiar with the field. <a href="https://arxiv.org/abs/1805.10941">Lemire’s accompanying paper</a> is great and very readable, but it still takes effort and concentration to follow everything. I work on cryptography and write cryptographic code for a living and I’m not ashamed to tell you it took me about 3 readings to really get it. The algorithm is based on an interaction between two modular spaces, one of which is sized 2 to power of 64, and the other is “N” sized, and if I just totally lost you there, I beg forgiveness and if you bear with me I promise there’s a link to my detailed dissection of how and why everything works. </p><p>All of this makes Lemire’s algorithm a really good challenge for creating a more readable version. Ideally something that an average coder can read in one pass, understand, and agree that it’s correct. </p></div><h4>Why does readability matter?</h4><p>Code readability is the most important pillar of code correctness and maintainability. When code is unreadable, it is harder to spot errors. That’s true when you’re doing a code review and it’s even more true when you’re adding more code to an existing code base. Great testing can compensate for some of the correctness challenges, but it doesn’t do as much for maintainability. </p><p>In s2n, one of our <a href="https://github.com/awslabs/s2n/blob/main/docs/DEVELOPMENT-GUIDE.md">development principles</a> is to write clear readable code with a light cognitive load. We’re serious about it, and I wasn’t going to try and use Lemire’s algorithm in s2n without a very readable implementation.</p><p>Working in software development I’ve heard the opinion more than once about how programmers and coders should be more familiar with the fundamental mathematics of logic and number theory that underpin their field, and that academic style papers and explanations should be sufficient. I happen to work with teams that are made up of both expert coders and expert mathematicians, and even there I don’t think this is a good idea.</p><p>Software engineering is engineering, which means translating science into solutions that work in the real world.  To wildly generalize, there are better dividends to be found in focusing on deepening ones understanding of the “real world” part of that, the users, the customers, the business models, usability, ergonomics, and so on, than on the “science” part of that. Some science is needed, but there’s a shallow limit. Just as a civil engineer doesn’t need too detailed an understanding of the chemistry of concrete. PHs and setting times, but not quantum numbers.</p><p>Code should be self-documenting. The average programmer, including someone straight out of college, should be able to understand and work on a well put-together codebase. This greatly improves the odds of finding problems, and that’s what matters. </p><h4>The contest</h4><p>One of the reasons I’ve been so tardy about this blog post is that the contest didn’t go as I’d expected. </p><p>Let’s start with the great thing that happened. I got several submissions, more than enough to pick winners from. Many of these submissions are good and do improve upon Lemire’s code. The most common improvement was to use more descriptive names for the variables. Lemire uses variable names such as “s”, “t”, “l” which seemingly don’t correspond to much. In such a short piece of code, this actually isn’t too big an offense. I kept these terms in my own implementation, because I want to be consistent with the paper, but it’s absolutely a valid improvement. Another great improvement is to write and include tests, which is really an essential part of software development. </p><p>The first place <a href="https://github.com/ziglang/zig/blob/98183e47436699f6e5eab200061c46eec342806e/std/rand.zig#L74-L118">winner</a> did both of these, and also cleared up some of the confusing lines of code by making an implicit truncation explicit. According to GitHub the winner had 5 contributors, but twitter user <a href="https://twitter.com/komu_wairagu/status/1161643573223317504">komu_wairagu</a> submitted it, so that’s who I’ll be contacting to Venmo $500.</p><p>The second place <a href="https://github.com/nimia/Lemire_RNG">winner</a> comes from <a href="https://twitter.com/NimrodAviram">Nimrod Aviram</a>, who I know professionally as the discoverer of the DROWN vulnerability and fun person to hang out with at RealWorldCrypto. Nimrod’s implementation includes some great testing too.</p><p>The third place winner wants to stay anonymous, and cheated their way to some bonus points by writing a great implementation in LISP. How can you not love that? If they change their minds about anonymity I’ll update this post with a link. The sha256 checksum of their implementation is ae008644b2f0b10eddbbce3a0508b103b19925e4e0c9354c569ff0816acb760c. </p><p>Now on to the not as great, and one of the reasons I’ve taken this long to write. It doesn’t feel right to criticize any of these efforts, made in spare time and as part of a fun challenge I put out there, but none of them actually explained Lemire’s algorithm or broke down how and why it worked. I asked several colleagues to look through these implementations and tell me if they understood what was going on, and the response was a uniform zero. Noone could fully understand even one of the implementations, or Lemire’s original.  When I asked people to also read the paper, things got a bit better, but everyone still seemed to trip on some issues.  These are smart people who absolutely know what they are doing. I’m sure given more time they could fully understand everything, but that wasn’t the goal. The goal was to have understanding after one or two, or at most just a few, readings.</p><h4>What did people find hard to follow?</h4><p>Talking through things with people I found some common problems. Some directly in the code, and others in the paper. The first line of code that threw people was:</p><pre><code>uint64_t l = ( uint64_t ) m;</code></pre><p>This line of code does an unusual operation called truncation. It takes a 128-bit value “m” and truncates that to its least significant 64-bits. All of this is implicit in the code, rather than explicit. Reading the code, you have to recall that m is 128-bits. Then there’s also the question of why are we doing this truncation? I’ll leave that for a minute. </p><p>The next problematic line of code was:</p><pre><code>uint64_t t = -s % s;</code></pre><p>This line of code is using an unusual combination of unary negation, on an unsigned int, and the modulus operator.  Nobody remembers what unary negation does to an unsigned int - and why should they …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://veryseriousblog.com/posts/dissecting-lemire">https://veryseriousblog.com/posts/dissecting-lemire</a></em></p>]]>
            </description>
            <link>https://veryseriousblog.com/posts/dissecting-lemire</link>
            <guid isPermaLink="false">hacker-news-small-sites-24644370</guid>
            <pubDate>Wed, 30 Sep 2020 21:48:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DigitalOcean's Hacktoberfest Is Hurting Open Source]]>
            </title>
            <description>
<![CDATA[
Score 790 | Comments 378 (<a href="https://news.ycombinator.com/item?id=24643894">thread link</a>) | @domenicd
<br/>
September 30, 2020 | https://blog.domenic.me/hacktoberfest/ | <a href="https://web.archive.org/web/*/https://blog.domenic.me/hacktoberfest/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>For the last couple of years, <a href="https://www.digitalocean.com/">DigitalOcean</a> has run
<a href="https://hacktoberfest.digitalocean.com/">Hacktoberfest</a>, which purports to “support open source” by giving free
t-shirts to people who send pull requests to open source repositories.</p>

<p>In reality, <strong>Hacktoberfest is a corporate-sponsored distributed denial of service attack against the open source
maintainer community</strong>.</p>

<p>So far today, on <a href="https://github.com/whatwg/html/pulls?q=is%3Apr+is%3Aclosed+label%3Aspam">a single repository</a>, myself
and fellow maintainers have closed 11 spam pull requests. Each of these generates notifications, often email, to the 485
watchers of the repository. And each of them requires maintainer time to visit the pull request page, evaluate its
spamminess, close it, tag it as spam, lock the thread to prevent further spam comments, and then report the spammer to
GitHub in the hopes of stopping their time-wasting rampage.</p>

<p>The rate of spam pull requests is, at this time, around four per hour. <em>And it’s not even October yet in my timezone.</em></p>

<p><img src="https://blog.domenic.me/images/hacktoberfest-spam-listing.png" alt="A screenshot showing a spam query for the whatwg/html repository, which is at this time up to 14 spam PRs"></p>

<p>Myself and other maintainers of the whatwg/html repository are not alone in suffering this deluge.
<a href="https://twitter.com/gravitystorm/status/1311386082982924289">My tweet</a> got commiseration from
<a href="https://mobile.twitter.com/gravitystorm/status/1311386082982924289">OpenStreetMap, phpMyAdmin</a>,
<a href="https://mobile.twitter.com/ulmerleben/status/1311378655231332355">PubCSS</a>,
<a href="https://mobile.twitter.com/JakeDChampion/status/1311389420638138370">GitHub, the Financial Times</a>,
<a href="https://twitter.com/slicknet/status/1311377444188770312">ESLint</a>, a
<a href="https://mobile.twitter.com/zekjur/status/1311411780162326531">computer club website</a>, and
<a href="https://mobile.twitter.com/juliusvolz/status/1311412919196844038">a conference website</a>, just within the first couple
of hours. Since then a dedicated account “<a href="https://twitter.com/shitoberfest">@shitoberfest</a>” has arisen to document the
barrage. Some <a href="https://github.com/search?q=is%3Apr+%22improve+docs%22+created%3A%3E2020-09-29&amp;type=Issues">cursory</a>
<a href="https://github.com/search?q=is%3Apr+label%3Ainvalid+created%3A%3E2020-09-29&amp;type=Issues">searches</a> show thousands of
spam pull requests, and rising.</p>

<p>DigitalOcean seems to be aware that they have a spam problem. Their solution, per their
<a href="https://hacktoberfest.digitalocean.com/faq">FAQ</a>, is to put the burden solely on the shoulders of maintainers. If we go
out of our way to tag a contribution as spam, then… we slightly decrease the chance of the spammer getting their free
t-shirt. In reality, the spammer will just keep going, submitting more pull requests to more repositories, until they
finally find a repository where the maintainer doesn’t bother to tag the PR as spam, or where the maintainer isn’t
available during the seven-day window DigitalOcean uses for spam-tracking.</p>

<p>To be clear, myself and my fellow maintainers did not ask for this. This is not an opt-in situation. If your open source
project is public on GitHub, DigitalOcean will incentivize people to spam you. There is no consent involved. Either we
contribute to DigitalOcean’s marketing project, or,
<a href="https://twitter.com/SudoFox/status/1311431141702819840">they suggest, we should quit open source</a>.</p>

<p>Hacktoberfest does not support open source. Instead, it drives open source maintainers even closer to
<a href="https://www.google.com/search?q=open+source+burnout">burnout</a>.</p>

<p><img src="https://blog.domenic.me/images/hacktoberfest-spam-pr.png" alt="A screenshot of a spam PR which adds the heading &quot;Great Work&quot; to the HTML Standard README"></p>

<h2 id="what-can-we-do">What can we do?</h2>

<p>My most fervent hope is that DigitalOcean will see the harm they are doing to the open source community, and put an end
to Hacktoberfest. I hope they can do it as soon as possible, before October becomes another lowpoint in the hell-year
that is 2020. In 2021, they could consider relaunching it as an opt-in project, where maintainers consent on a
per-repository basis to deal with such t-shirt–incentivized contributors.</p>

<p>To protect ourselves, maintainers have a few options. First, you can take the feeble step of ensuring that any spam
against your repositories doesn’t contribute to the spammer’s “t-shirt points”, by tagging pull requests with a “spam”
label, and <a href="https://twitter.com/MattIPv4/status/1311390498888781824">emailing hacktoberfest@digitalocean.com</a>.
DigitalOcean themselves, however, admit that
<a href="https://twitter.com/MattIPv4/status/1311390054334554113">this won’t stop the problem they’ve unleashed on us</a>. But
maybe it will contribute to the <a href="https://github.com/MattIPv4/hacktoberfest-data">metrics</a> they collect, which last year
showed that “only” 3,712 pull requests were labeled as spam by project maintainers.</p>

<p>If you’re comfortable cutting off genuine contributions from new users, you can try enabling GitHub’s
<a href="https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/limiting-interactions-in-your-repository">interaction limits</a>.
However, <del>you have to do this every 24 hours, and</del> it has the drawback of also disabling issue creation and
comments. <ins>Update: GitHub has made the limit configurable, and has
<a href="https://twitter.com/github/status/1311772722234560517">a nice cheeky announcement tweet</a> zooming in on the “1 month”
option.</ins></p>

<p>Another promising route would be if GitHub would cut off DigitalOcean’s API access, as
<a href="https://twitter.com/__agwa/status/1311399074814472194">Andrew Ayer has suggested</a>. It’s not clear whether DigitalOcean
is committing a terms of service violation that would support such measures. But they’re certainly making GitHub a
less-pleasant place to be, and I hope GitHub can think seriously about how to discourage such corporate-sponsored
attacks on the open source community.</p>

<p>Finally, and most importantly, we can remember that this is how DigitalOcean treats the open source maintainer
community, and stay away from their products going forward. Although we’ve enjoyed using them for hosting the
<a href="https://whatwg.org/">WHATWG</a> standards organization, this kind of behavior is not something we want to support, so
we’re starting to investigate alternatives.</p>

  </div>

</article></div>]]>
            </description>
            <link>https://blog.domenic.me/hacktoberfest/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24643894</guid>
            <pubDate>Wed, 30 Sep 2020 21:10:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CloudBee's CodeShip Critical Security Notification]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24643253">thread link</a>) | @seneca
<br/>
September 30, 2020 | https://www.codeshipstatus.com/incidents/91bvlfw9xlm9 | <a href="https://web.archive.org/web/*/https://www.codeshipstatus.com/incidents/91bvlfw9xlm9">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>
              Dear CodeShip users,</p><p>We are reaching out to inform you of additional information we have uncovered as a result of our continuing investigation of the recent GitHub breach. To provide maximum transparency, we are reporting on the results of our investigation, the impact on users, actions you must take to protect yourself/your organization, and actions we will take to strengthen our security processes going forward.</p><p>On Wednesday, September 16, 2020, CloudBees was notified by GitHub of suspicious activities targeting CodeShip business accounts connected to GitHub via the CodeShip GitHub app and now deprecated CodeShip OAuth tokens. CloudBees immediately initiated an investigation conducted by our security and engineering teams, and on September 27, we identified additional evidence of malicious activity against a failover CodeShip database. On September 29, we uncovered evidence to indicate that a malicious actor had access to this failover instance during the period of June 2019 to June 2020. At this time and to the best of our knowledge, we have no evidence of malicious activity or attempts within CodeShip systems since June 2020. </p><p>*What type of data was affected?*</p><p>The impacted accounts are those of CodeShip users. No other products or accounts were affected and CodeShip is in no way integrated with other CloudBees products or systems.</p><p>*For all CodeShip users:*</p><p>CodeShip users hashed account passwords, one-time password (OTP) recovery codes, and the OTP secret keys used to seed two-factor authentication all may have been exposed.</p><p>Business contact information for invoicing purposes such as company contact name, company name, VAT number, postal address, phone number also may have been exposed. No payment information, such as bank account numbers or credit card numbers, was exposed. No other CloudBees product other than CodeShip was impacted. Also, the logging system was not accessed for any customers.</p><p>*For CodeShip Basic users:*</p><p>Any information contained in CodeShip users’ pipelines may have been exposed. This includes scripts, environment variables, access tokens and other similar data.</p><p>*For CodeShip Pro users:*</p><p>AES encryption keys may have been exposed.</p><p>*Steps you should take*</p><p>Although at this time we have no evidence that the data potentially exfiltrated has been used, all CodeShip users may have been affected (including free, Basic and Pro accounts) and should take the following steps:</p><p>- Immediately rotate any keys or other secrets for cloud providers, third-party tools, or anything else that you used in your CodeShip pipelines.</p><p>- If using CodeShip Pro, rotate your AES key and re-encrypt your secrets.</p><p>- Immediately identify any other sensitive information that is stored in your pipelines and replace them within your pipelines and on any external systems.</p><p>- Determine whether any of your systems accessible from CodeShip have experienced unauthorized access, by contacting your provider or carefully review your access records.</p><p>- Verify that the source code held in repositories that are linked to your CodeShip account have retained their full integrity.</p><p>- Reset your CodeShip 2FA: <a target="_blank" href="https://documentation.codeship.com/general/about/2fa/">https://documentation.codeship.com/general/about/2fa/</a></p><p>At this time and to the best of our knowledge, we have no evidence of malicious activity or attempts within CodeShip systems since June 2020. We are continuing to monitor the situation.</p><p>*Steps we are taking*</p><p>As soon as we were notified by GitHub on September 16, we proceeded to rotate all our applications' internal secrets and rebuilt all our AWS AMIs. We are continuing to scrutinize our AWS security logs to monitor for suspicious activity, such as outbound connections to known malicious IPs. To date, we have not found any such activity.</p><p>We want you to be assured that we are taking steps to increase the security strength of the CodeShip product, including but not limited to:</p><p>- Validation that our product threat modeling and large-scope security reviews are systematically implemented.</p><p>- Validation that the application of production security standards to all operational processes and artifacts is systematically implemented.</p><p>- Enhancement of strict restrictions on access to production data and strict segregation of sensitive data.</p><p>- Improvement of existing SIRT processes to ensure faster and better forensic investigation.</p><p>*Who to contact *</p><p> We will update here with any new developments. </p><p>If you still have questions, please contact <a target="_blank" href="mailto:security@codeship.com">security@codeship.com</a>.</p><p>Last but not least, I’d like to apologize for the impact this is having on you. In the decade that CloudBees has been operating SaaS applications, we have always taken full responsibility for our products and we do so today. Please be assured that we will do everything we can to prevent this from happening again. </p><p>- Sacha Labourey, CEO, CloudBees
            </p></div></div>]]>
            </description>
            <link>https://www.codeshipstatus.com/incidents/91bvlfw9xlm9</link>
            <guid isPermaLink="false">hacker-news-small-sites-24643253</guid>
            <pubDate>Wed, 30 Sep 2020 20:12:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Defer Reference Implementation for C]]>
            </title>
            <description>
<![CDATA[
Score 76 | Comments 59 (<a href="https://news.ycombinator.com/item?id=24643034">thread link</a>) | @cyber1
<br/>
September 30, 2020 | https://gustedt.gitlabpages.inria.fr/defer/ | <a href="https://web.archive.org/web/*/https://gustedt.gitlabpages.inria.fr/defer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="text-1-1">
<p>
In the following code snippet we
have one <i>guarded block</i> and three <i>deferred statements</i>.
</p>

<div>
<pre>guard {
  <span>void</span> * <span>const</span> <span>p</span> = malloc(25);
  <span>if</span> (<span>!</span>p) <span>break</span>;
  <span>defer</span> <span>free</span>(p);

  <span>void</span> * <span>const</span> <span>q</span> = malloc(25);
  <span>if</span> (<span>!</span>q) <span>break</span>;
  <span>defer</span> <span>free</span>(q);

  <span>if</span> (mtx_lock(&amp;mut)==thrd_error) <span>break</span>;
  <span>defer</span> <span>mtx_unlock</span>(&amp;mut);

  <span>// </span><span>all resources acquired</span>
}

</pre>
</div>

<p>
The idea is that we indicate with a <b><code>defer</code></b> keyword that the
statement, e.g a call to <b><code>free</code></b>, is only to be executed at the end of
the guarded block, and, that we want this action to happen
unconditionally in which way ever the guarded block is left.  For the
three deferred statements together it says that they should be
executed in the inverse order than they were encountered. So the
control flow for this code example can be visualized as follows:
</p>



<p><img src="https://gustedt.gitlabpages.inria.fr/defer/defer-image.png" alt="defer-image.png">
</p>

<p>
Here, the dashed lines represent circumstancial control flow that
might arise when some resources are not available or when the
execution is interrupted by a signal.
</p>

<p>
This new technique has at least two advantages to commonly used <code>C</code> or
<code>C++</code> techniques:
</p>

<dl>
<dt>proximity</dt><dd>The cleanup code (<code>free</code> or <code>mtx_unlock</code>) is coded
close to the place where its need arises.</dd>

<dt>visibility</dt><dd>The cleanup code is not hidden in some previously
defined function (such as for <code>atexit</code> handlers) or
constructor/destructor pairs (<code>C++</code>)</dd>
</dl>


<p>
For normal control flow (without intermediate <b><code>return</code></b>, <code>exit</code>, …)
code with similar properties can be coded with existing tools. The
above is equivalent to something like the following
</p>

<div>
<pre>{
   <span>void</span> * <span>const</span> <span>p</span> = malloc(25);
   <span>if</span> (<span>!</span>p) <span>goto</span> <span>DEFER0</span>;
   <span>if</span> (<span>false</span>) {
     <span>DEFER1</span>:
       free(p);
       <span>goto</span> <span>DEFER0</span>;
   }

   <span>void</span> * <span>const</span> <span>q</span> = malloc(25);
   <span>if</span> (<span>!</span>q) <span>goto</span> <span>DEFER1</span>;

   <span>if</span> (<span>false</span>) {
     <span>DEFER2</span>:
       free(q);
       <span>goto</span> <span>DEFER1</span>;
   }

   <span>if</span> (mtx_lock(&amp;mut)==thrd_error) <span>goto</span> <span>DEFER2</span>;

   <span>if</span> (<span>false</span>) {
     <span>DEFER3</span>:
       mtx_unlock(&amp;mut);
       <span>goto</span> <span>DEFER2</span>;
   }

   <span>// </span><span>all resources acquired</span>

   <span>goto</span> <span>DEFER3</span>;
   <span>DEFER0</span>:;
}

</pre>
</div>

<p>
Here, the <b><code>if(false)</code></b> clauses guarantee that the deferred statements
are jumped over when they are first met, and the labels and <b><code>goto</code></b>
statements implement the hops from back to front to execute the
deferred statements eventually.
</p>

<p>
Obviously, most <code>C</code> programmers would not code like this but they
would prefer to write down a <i>linearization</i> of the above, which is a
quite common idiom for cleanup handling in <code>C</code>:
</p>

<div>
<pre>{
   <span>void</span> * <span>const</span> <span>p</span> = malloc(25);
   <span>if</span> (<span>!</span>p) <span>goto</span> <span>DEFER0</span>;

   <span>void</span>*<span>const</span> <span>q</span> = malloc(25);
   <span>if</span> (<span>!</span>q) <span>goto</span> <span>DEFER1</span>;

   <span>if</span> (mtx_lock(&amp;mut)==thrd_error) <span>goto</span> <span>DEFER2</span>;

   <span>// </span><span>all resources acquired</span>

   mtx_unlock(&amp;mut);

 <span>DEFER2</span>:
   free(q);
 <span>DEFER1</span>:
   free(p);
 <span>DEFER0</span>:;
}

</pre>
</div>

<p>
This has the advantage of only making the circumstantial control flow
explicit (with three *=goto=) but it does that for the price of
proximity; the cleanup code is far from the place where its need
arises.
</p>

<p>
Nevertheless, even this linearization needs some form of naming
convention for the labels. For more complicated code the maintenance
of these jumps can be tricky and prone to errors.  This shows another
advantage of the <a href="#org63e5a5c"><code>defer</code></a> approach:
</p>

<dl>
<dt>maintainability</dt><dd>The cleanup specification is not dependent on
arbitrary naming such as labels (<code>C</code>) or RAII classes
(<code>C++</code>) and does not change when <a href="#org63e5a5c"><code>defer</code></a> or <b><code>break</code></b>
statements are added or removed.</dd>
</dl>

<p>
Another important property that is much more difficult to implement in
<code>C</code> (and that needs <b><code>try/catch</code></b> blocks in <code>C++</code>) is that <b>all</b> exits
from the guarded block are detected and acted upon: <b><code>break</code></b>, <b><code>return</code></b>,
<a href="#org18b8cc4"><code>thrd_exit</code></a>, <a href="#org9db677d"><code>exit</code></a>, <a href="#org7f6c106"><code>panic</code></a>, or
an interruption by a signal. That is, unless there are nasal deamons
flying around, we have a forth important property
</p>


<dl>
<dt>robustness</dt><dd>Any deferred statement is guaranteed to be executed
eventually.</dd>
</dl>

<p>
This is different from <code>C++</code>'s handling of destructors, which are only
guaranteed to be executed if there is a <b><code>try/catch</code></b> block underneath.
</p>

<p>
This principle of deferred execution extends to nested
guarded blocks in a natural way, even if they are stacked in
different function calls.
</p>
</div></div>]]>
            </description>
            <link>https://gustedt.gitlabpages.inria.fr/defer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24643034</guid>
            <pubDate>Wed, 30 Sep 2020 19:55:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Artificial Intelligence in Health Care]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24642719">thread link</a>) | @Sanksshep
<br/>
September 30, 2020 | https://www.aiplusinfo.com/blog/ai-in-healthcare | <a href="https://web.archive.org/web/*/https://www.aiplusinfo.com/blog/ai-in-healthcare">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-3857ad1892c7d8adea01"><p><h2>Artificial intelligence has the potential to transform many aspects of patient care and administrative processes in healthcare. I think the role of artificial intelligence will be an asset to all healthcare professionals. The following article contains examples of artificial intelligence in healthcare and companies doing a great job at it.</h2></p></div><div data-block-type="2" id="block-yui_3_17_2_1_1600557578492_4166"><div><h3>Introduction</h3><p>Artificial intelligence (AI), Machine learning, NLP, Robotics, and Automation are increasingly prevalent in all aspects and are being applied to healthcare as well. These technologies have the potential to transform all aspects of health care from patient care to the development and production of new experimental drugs that can have a faster roll-out date than traditional methods.&nbsp;</p><p>There are numerous research studies suggesting that AI can outperform humans at key healthcare tasks, such as diagnosing ailments. Here is a great example, <a href="https://www.bbc.com/news/health-50857759#:~:text=Artificial%20intelligence%20is%20more%20accurate,images%20from%20nearly%2029%2C000%20women." target="_blank">AI ‘outperforms’ doctors diagnosing breast cancer¹.</a></p><p>Artificial intelligence is a collection of technologies that come together form artificial intelligence. AI’s diverse range of technologies impacts a wide spectrum of healthcare.&nbsp;</p><p>Tech firms and startups are also working assiduously on the same issues. Google, for example, is collaborating with health delivery networks to build prediction models from big data to warn clinicians of high-risk conditions, such as sepsis and heart failure. Google, Enlitic, and a variety of other startups are developing AI-derived image interpretation algorithms. Jvion offers a ‘clinical success machine’ that identifies the patients most at risk as well as those most likely to respond to treatment protocols. Each of these could provide decision support to clinicians seeking to find the best diagnosis and treatment for patients.</p><p>You will find below some technologies that improve a specific area in healthcare with examples sourced from the internet with citations.&nbsp;</p><h3>Machine learning</h3><p>Machine learning is an application of artificial intelligence (AI)<em> </em>that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves².</p><p>There are majorly three types of Machine learning — </p><ul data-rte-list="default"><li><p>Supervised Learning</p></li><li><p>Unsupervised Learning</p></li><li><p>Reinforcement Learning&nbsp;</p></li></ul><p>In healthcare, the most common application of machine learning is predictive medicine  —  predicting what treatment alternatives are likely to work best on a patient based on various patient traits, history, the treatment situation, and protocols. The supervised learning model for predictive medicine applications requires a training dataset, like all supervised learning models. the difference here is that there may be a lot of variables.&nbsp;</p><p>Using neural networks it is now possible to also predict whether a patient will acquire a particular ailment or not based on a set of variables and conditions that can be fed into algorithms in the form of data.</p><p>One common application of deep learning and neural networks is the ability to detect ailments/issues in the radiology images. I think deep learning should be increasingly applied wherever clinically possible. This will allow doctors and radiologists to just supervise results and focus on other important aspects of their job. This combination promises better accuracy of finding ailments with limited human intervention or supervision.&nbsp;</p><p>Here are some organizations that are doing groundbreaking work in this area.</p><blockquote><p><strong>Organization:</strong> <a href="https://www.pathai.com/">PathAI</a><br><strong>Location: </strong>Cambridge, Massachusetts<br><strong>How it’s using AI in healthcare:</strong> PathAI is developing machine learning technology to assist pathologists in making more accurate diagnoses. The company’s current goals include reducing errors in cancer diagnosis and developing methods for individualized medical treatment³.</p><p><strong>Organization:</strong> <a href="https://www.enlitic.com/">Enlitic</a><span><br></span><strong>Location: </strong>San Francisco, California<br><strong>How it’s using AI in healthcare: </strong>Enlitic develops deep learning medical tools to streamline radiology diagnoses. The company’s deep learning platform analyzes unstructured medical data (radiology images, blood tests, EKGs, genomics, patient medical history) to give doctors better insight into a patient’s real-time needs³.</p></blockquote><h3>Natural language processing</h3><p>In healthcare, most applications of NLP involve the creation, understanding, parsing, and classification of clinical documentation and published research. NLP can also be used to analyze clinical notes, prescriptions, help prepare reports, and possibly conversational AI. Few good examples of how NLP is currently being used.&nbsp;</p><ul data-rte-list="default"><li><p>Parsing data realtime from coronavirus research that is being published globally. You can find more information about this in my article <a href="https://medium.com/@sanksshep/how-can-ai-help-with-the-covid-19-vaccine-search-a68d40fc0cb0" target="_blank">here</a>.&nbsp;</p></li><li><p>Project Meena by Google. More information can be found <a href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" target="_blank">here</a>.</p></li></ul><h3>Decision Tree&nbsp;</h3><p>Decision trees require doctors and engineers to come up with an if-then-else decision flow chart that can help train machines to make decisions by building complex algorithms based on the finalized decision tree. This is critical and processes heavy software design, this enables the machines to take accurate decisions with human intervention. This will help save a ton of time for doctors and patients alike. This will enhance the capabilities of doctors to predict, analyze, and come up with a treatment plan for patient care.&nbsp;</p><p>This can also be used extensively in vaccine and treatment research provided the known variable is the ailment we are making the vaccine for and its pre-set conditions and protocols. This is an effective mechanism for pre-morbidity patients as well.&nbsp;</p><h3>Robotics</h3><p>Robots are becoming more intelligent, as other AI capabilities are being embedded in their OS. Other areas of improvements in AI have exponentially improved the capabilities of the robots and their ability to perform complex operations.</p><p>One such area of operation is robotic surgery. This enables surgeons to perform complex procedures with much greater precision and create precise, minimized, invasive incisions, and stitches. This is a game-changer in performing surgery, as long as human supervision exists. &nbsp;</p><p>Here are some examples of organizations using AI and Robotics&nbsp;</p><blockquote><p><strong>Organization: </strong><a href="https://www.vicarioussurgical.com/" target="_blank">Vicarious Surgical</a><br><strong>Location: </strong>Charlestown, Massachusetts<br><strong>How it’s using AI in healthcare: </strong>Vicarious Surgical combines virtual reality with AI-enabled robots so surgeons can perform minimally invasive operations. Using the company’s technology, surgeons can virtually shrink and explore the inside of a patient’s body in much more detail³.</p><p><strong>Organization:</strong> <a href="https://www.aurishealth.com/" target="_blank">Auris Health</a><br><strong>Location: </strong>Redwood City, California<br><strong>How it’s using AI in healthcare: </strong>Auris Health develops a variety of robots designed to improve endoscopies by employing the latest in micro-instrumentation, endoscope design, data science and AI. Consequently, doctors get a clearer view of a patient’s illness from both physical and data perspective³.</p><p><strong>Organization:</strong>  <a href="https://www.accuray.com/" target="_blank">Accuray</a>  <br><strong>Location: </strong>Sunnyvale, California<br><strong>How it’s using AI in healthcare: </strong>The Accuray CyberKnife System uses robotic arms to precisely treat cancerous tumors all over the body. Using the robot’s real-time tumor tracking capabilities, doctors and surgeons are able to treat only affected areas rather than the whole body. The Accuray CyberKnife robot uses 6D motion-sensing technology to aggressively track and attack cancerous tumors while saving healthy tissue³.</p><p><strong>Organization:</strong> <a href="https://www.intuitive.com/" target="_blank">Intuitive</a><br><strong>Location:</strong> San Francisco, California<br><strong>How it’s using AI in healthcare:</strong> Intuitive’s da Vinci platforms have pioneered the robotic surgery industry. Being the first robotic surgery assistant approved by the FDA over 18 years ago, the surgical machines feature cameras, robotic arms, and surgical tools to aide in minimally invasive procedures.<br>The da Vinci platform is constantly taking in information and providing analytics to surgeons to improve future surgeries. So far, da Vinci has assisted in over five million operations³.</p><p><strong>University: </strong><a href="https://www.cmu.edu/" target="_blank">Carnegie Mellon University</a> <br><strong>Location:</strong> Pittsburgh, Pennsylvania<br><strong>How it’s using AI in healthcare:</strong> The robotics department at Carnegie Mellon University developed Heartlander, a miniature mobile robot designed to facilitate therapy on the heart. Under a physician’s control, the tiny robot enters the chest through a small incision, navigates to certain locations of the heart by itself, adheres to the surface of the heart, and administers therapy³.</p><p><strong>Organization:</strong> <a href="http://microsure.nl/" target="_blank">MicroSure</a><br><strong>Location:</strong> Eindhoven, The Netherlands<br><strong>How it’s using AI in healthcare:</strong> MicroSure robots help surgeons overcome their human physical limitations. The company’s motion stabilizer system reportedly improves performance and precision during surgical procedures. Currently, eight of MicroSure’s micro-surgical operations are approved for lymphatic system procedures³.</p><p><strong>Organization:</strong> <a href="https://www.mazorrobotics.com/index.php/en-us/" target="_blank">Mazor Robotics</a><br><strong>Location:</strong> Caesarea, Israel<br><strong>How it’s using AI in healthcare:</strong> Surgeons use the Mazor Robotics’ 3D tools to visualize their surgical plans, read images with AI that recognizes anatomical features and perform a more stable and precise spinal operation³.</p></blockquote><h3>Robotic process automation</h3><p>Robotic process automation performs structured digital tasks for administrative purposes, ie those involving information systems, as if they were a human user following a script or rules. It relies on a combination of workflow, business rules, and ‘presentation layer’ integration with information systems to act like a semi-intelligent user of the systems. In healthcare, they are used for repetitive tasks like prior authorization, updating patient records, or billing. When combined with other technologies like image recognition, they can be used to extract data from, for example, faxed images in order to input it into transactional systems.</p><h3>Mass personalization</h3><p>Artificial Intelligence can help in mass personalization of patient care, treatments, procedures, vaccine research, and production. This along with human interaction can reduce costs and improve coverage across the board for healthcare.&nbsp;</p><p>AI can help with various aspects of patient care, like, charting the …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.aiplusinfo.com/blog/ai-in-healthcare">https://www.aiplusinfo.com/blog/ai-in-healthcare</a></em></p>]]>
            </description>
            <link>https://www.aiplusinfo.com/blog/ai-in-healthcare</link>
            <guid isPermaLink="false">hacker-news-small-sites-24642719</guid>
            <pubDate>Wed, 30 Sep 2020 19:29:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Progress in Biology Is Slow – Here's How We Can Speed It Up]]>
            </title>
            <description>
<![CDATA[
Score 40 | Comments 21 (<a href="https://news.ycombinator.com/item?id=24642489">thread link</a>) | @ashwal
<br/>
September 30, 2020 | https://adamashwal.com/irreducible | <a href="https://web.archive.org/web/*/https://adamashwal.com/irreducible">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

			<p>Living is great and I'd prefer to do more of it. Unfortunately, progress towards immortality has been rather slow — for all of our technological progress in the last century we've only picked up a few extra years of life. An incorrect framing of the problem has led to slow progress, but with a little bit of mind shift we can choose much better strategies for learning about and manipulating biology.</p>
<h3 id="what-s-the-status-quo-">What's the status quo?</h3>
<p>There's been a subset of humans over the last 100 years who also would like to live longer and healthier lives and have invested considerable time and energy into this problem. They've mostly failed as is evidence by the fact if you make it out of childhood, keep a good BMI, stop smoking, and exercise, you'll make it, at best, <a href="https://ourworldindata.org/life-expectancy">a decade longer than someone in the 1700s</a> (provided you escaped the trough of death that was childhood). In that same 100 years we took our first flight on Earth and then landed on the moon. In that same 100 years we went from 0 transistors per chip to 50,000,000,000. In that same 100 years we invented the Cool Ranch Dorito. So why did we succeed in so many other places but have failed in the most important? Why don't we live extremely long, healthy, and happy lives?</p>
<h3 id="why-has-this-approach-worked-in-other-areas-">Why has this approach worked in other areas?</h3>
<p>Some problems are, in retrospect, clearly easier than others. But if you had surveyed the leading minds in 1900 which would be easier: splitting the atom, sending a probe outside the solar system, or living to 90, it's hard to imagine it would be the last option they would have chosen. Yet here we are.</p>
<p>There are problems that on the face of it seem of similar difficulty to an outsider but are magnitudes (and magnitudes and magnitudes) harder to solve. What leads to this difference can be summarized as <em>computational reducibility</em>. As way of example, take the planet. Think of every atom on Earth- the rocks, the trees, that one ex who still drifts in and out of memory. If I want to know where this sphere will be in the universe tomorrow or 1,000 years from now and I don't have a notion of basic physics I may think this intractable. There's so much going on! How could one possibly think to describe how all these atoms would move through space and time? But it turns out to be fairly trivial - all those atoms can be reduced to a single point, a center of mass, and then calculations of momentum can be easily computed and trajectory projected forward. There are many systems that allow for these "shortcuts" where the dimensionality can be collapsed and the useful information is still present. A bridge builder doesn't need to think of each atom in a brick, or even really the brick; it's enough to think of the collection of bricks and how they are arranged.</p>
<h3 id="engineering-takes-place-in-reducible-places">Engineering takes place in reducible places</h3>
<p>Engineering, generally, is the practice of working on problems that are tractable. Given constraints on energy and time, only hard problems that have been sufficiently reduced are tractable and as such are the ones that are worked on. Oftentimes science leads us to new ways of reducing the complexity of problems (think Newton and his equations in the previous example). This isn't a law, just a result of how resources are allocated. </p>
<h3 id="is-biology-reducible-">Is Biology Reducible?</h3>
<p>We haven't seen progress in biology because it is stubborn to attempts to reduce it. Darwin was one of the last great reducers - able to collapse the high dimensional problem of evolution into a few axioms. But even with natural selection in hand, the resolution of claims is not particularly specific. Hypotheses are hard to prove or disprove given the near impossibility of running a counterfactual and it mostly serves as a post-hoc description (large proboscis <a href="https://en.wikipedia.org/wiki/Xanthopan">moths</a> notwithstanding). Perhaps if we were luckier we could  have lived in a universe that allowed us to use natural selection to know the structure of cells and animals without having to go look (this is a little true - something I'll explore in a future post), similar to knowing the position of the planets a millennia in advance.</p>
<p>What is it about biology that makes it irreducible but splitting the atom was something we accomplished 80 years ago? Spitting in the face of entropy is hard and the number of problems that need to be solved by a biological system are vast. The components of that system are not elegant fundamental laws of the universe but artisanal components created by random search through a loosely constrained fitness space. Even highly conserved pathways still exist in a unique context of the whole organism. </p>
<h3 id="biology-s-drunken-walk">Biology's Drunken Walk</h3>
<p>Biology is constantly transitioning from current state to a future state where some future branch of the evolutionary tree has higher fitness but the potential branch space is massive and the "choice" of which branch is picked is a random process. For example, in a scenario where the environment is slowly acidifying, any given bacteria has many solutions to survive. While aesthetically they could be vastly different (off the top of my head: changes to cell membranes, additional transmembrane proton pumps, neutralizing organelles, heat shock proteins, etc), which one ends up being dominant for a given bacteria is a random mutation. Given enough bacteria "searching" the solution space, you'll likely see many solutions.</p>
<p>Crucially, because in any scenario there is a one-to-many relationship between a problem and solutions, you can't extrapolate which solutions an organism possesses based on reasoning. You can’t postdict, you have to go look.</p>
<h3 id="so-what-do-we-do-">So, what do we do?</h3>
<p>So biology suffers from low reducibility - we aren’t able to summarize systems allowing us to make inferences cheaply. In the instance of disease this prevents both easy understanding of the disease state, i.e. what is going wrong, and prevents easy drug design, i.e. which node in the system do I push on in order to reverse the disease state. Right now, drug discovery is a lot of serendipity and a lot of pretending we know enough to pick targets. Unsurprisingly, this mostly fails.</p>
<p>There is another way. Currently, we brute force biology via Grad student search and it’s remarkably slow. A small number of underpowered, poorly done experiments makes up the bulk of what is produced. A model organism is chosen, an intervention is proposed, a measurement is done, and a paper is written: repeat ad nauseam. </p>
<p>But if an infinite number of monkeys can write Shakespeare, an infinite number of mice can allow us a way forward.</p>
<p>If we care about blood pressure, for example, why have we not given every drug, at every dosage, every regiment, and in every combination to a mouse and actually seen what happens?
We <em>do</em> have high throughput screening, mostly in individual cells or enzymes, but this is mostly garbage owing to the information decay from cell to whole organism (something I will expound on in a future post). Is my proposed solution expensive? Yes! Combinatorial explosions are the opposite of computational reducibility. But my point is that we can't just hope to have cheaper solutions in the future – that is the ostrich approach to progress. And we spent $288,100,000,000 to get to the moon.</p>
<p>The problem is also not as intractable as it may first seem. How do we test a large number of drugs on a large number of mice? Drive the cost down on any marginal mouse. Recent advances in machine learning allow automation of those pesky variable costs. Image recognition and classification are now good enough to track a mouse and its movements automatically - there is no need to babysit mice and manually classify behavior. With the state of every mouse known, simple robotics, e.g. food/medication administration and outcome measurements, become possible. The simplest experiments are possible today, and with a concerted effort, the realm of possible can grow. </p>
<p>There will always be innovations in new measurement techniques, new ways of peering into the system. Biologists generally fail in scaling these new techniques at a detriment to our ability to control biology. By adding scalability as an important aspect of innovation we can unlock so much more with what we already have today. </p>
<p>Importantly, this isn't just limited to a causal inference between a drug and a disease. We're getting very good at measuring the state of systems, just pick your favorite "–ome". It's not hard to squint and see that large numbers of interventions, on large numbers of model organisms, with large readouts of state will approach full 4D models of organic systems.  </p>
<p>There are not going to be any shortcuts with biology. The sooner we recognize this, the sooner we can start building systems that operate at the scale needed to bring useful inference, drug discovery, and network topology into the 21st century. </p>
<p>Thank you to Aubree for her feedback on commas, words, and ideas.</p>


			
			
		

			
			




		</div></div>]]>
            </description>
            <link>https://adamashwal.com/irreducible</link>
            <guid isPermaLink="false">hacker-news-small-sites-24642489</guid>
            <pubDate>Wed, 30 Sep 2020 19:10:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Energy Efficiency across Programming Languages [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 190 | Comments 153 (<a href="https://news.ycombinator.com/item?id=24642134">thread link</a>) | @Malfunction92
<br/>
September 30, 2020 | https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf | <a href="https://web.archive.org/web/*/https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://greenlab.di.uminho.pt/wp-content/uploads/2017/10/sleFinal.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24642134</guid>
            <pubDate>Wed, 30 Sep 2020 18:43:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is Snowflake so Valuable?]]>
            </title>
            <description>
<![CDATA[
Score 240 | Comments 158 (<a href="https://news.ycombinator.com/item?id=24641481">thread link</a>) | @malisper
<br/>
September 30, 2020 | https://www.freshpaint.io/blog/why-is-snowflake-so-valuable | <a href="https://web.archive.org/web/*/https://www.freshpaint.io/blog/why-is-snowflake-so-valuable">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Two weeks ago, Snowflake had the largest IPO ever for a software company. On the first day of trading, the stock price doubled, giving Snowflake a market cap of over $60 billion. Snowflake became all that everyone was talking about. There was so much hype, my mom, who doesn't even know what Snowflake is, decided to invest in Snowflake.</p><p>At first glance, the valuation of $60 billion seems absurd. Not only is Snowflake not profitable, having a net loss of $349 million in 2019, but their revenue is also small relative to their valuation. Their revenue over the last 12 months was $403 million. Based on that, their market cap is around 150x their revenue. So why exactly is their market cap so high? Besides revenue and profitability, there are a number of other metrics that investors look at. Compared to similar software businesses, Snowflake has god-like metrics. In this post, I'm going to go through a number of metrics from Snowflake's S-1, explain what the metrics mean, and give context on just how exceptional Snowflake's metrics are.</p><p>The first metric that stands out is Snowflakes 121% year over year growth. To give you a sense of how fantastic it is, you can compare it to the growth rates of other companies at IPO. <a href="https://techcrunch.com/2013/08/24/how-fast-should-you-be-growing/">Institutional Venture Partners looked at how quickly Internet and software companies were growing at IPO</a>.</p><figure><p><img src="https://uploads-ssl.webflow.com/5dad2b1e508f04886d0245c3/5f74c44d240d4de932d463c4_growth1.jpeg" alt="growth1"></p></figure><p>Based on their numbers, only 25% of companies that are between $75M-$500M in annual revenue are growing at just over 60% year over year. Snowflake's growth rate is nearly twice that! Snowflake's revenue was also $403M for the last 12 months, which would put them at the high end of that range. Not only are they growing significantly faster than other companies, but they are already significantly larger!</p><p>For companies that sell software to businesses an important number to look at is their retention rate. For Snowflake, their net retention rate is 158%. You may be asking, how do they have a retention rate over 100%? That's because companies that use Snowflake pay more for Snowflake over time. For every $1 of revenue Snowflake received from their customers a year ago, that same pool of customers are now paying $1.58. That means Snowflake could acquire no new customers, and they would still be doubling revenue every 18 months.</p><p>To compare that to other companies, <a href="https://www.lennyrachitsky.com/p/what-is-good-retention-issue-29">Lenny Rachitsky surveyed a number of industry experts on what they thought good and great net retention would be</a>. On average, for enterprise subscription software, the category Snowflake would fall under, the experts surveyed said 110% net retention would be good and 130% net retention would be great. He also pulled the net retention rates for a number of public companies:</p><ul role="list"><li><strong>Alteryx</strong>: 135% (<a href="https://docs.google.com/spreadsheets/d/1ogELPtct2s6jj9wRBpOgqGUE5k4cO73XXcvqmEZE89M/edit#gid=0">source</a>)</li><li><strong>Fastly</strong>: 130% (<a href="https://www.saastr.com/5-interesting-learnings-from-fastly-as-it-gets-ready-to-ipo/">source</a>)</li><li><strong>Okta</strong>: 124% (<a href="https://docs.google.com/spreadsheets/d/1ogELPtct2s6jj9wRBpOgqGUE5k4cO73XXcvqmEZE89M/edit#gid=0">source</a>)</li><li><strong>Anaplan</strong>: 124% (<a href="https://www.key.com/kco/images/Public_SaaS_Company_Retention_Metrics_2019.pdf">source</a>)</li><li><strong>Workday</strong>: 100%+ (<a href="https://www.workday.com/content/dam/web/en-us/documents/investor/workday-financial-analyst-day-2018.pdf">source</a>)</li><li><strong>ServiceNow</strong>: 97% (<a href="https://www.publiccomps.com/tickers/NOW">source</a>)</li></ul><p>You will notice that Snowflake's net retention rate of 158% is significantly higher than all of these!</p><p>Net promoter score (NPS) is a way of measuring customer satisfaction. It's convenient because it's easy to calculate and boils down customer satisfaction to a single number, making it easy to compare NPS between different companies. To compute NPS, a company performs a survey of their customers. The company asks "On a scale of 0 to 10, how likely are you to recommend this company’s product or service to a friend or a colleague?". Anyone that answers 9 or 10 is deemed a "promoter" of the product, while anyone that answers 6 or less is considered to be a "detractor". To compute the NPS, you take the percentage of users that are promoters and subtract the percentage of users that are detractors. This results in a number between -100 and 100 which is the NPS.</p><p>To give some examples, an NPS of 0 means your company has just as many promoters as detractors, whereas an NPS of 100 means everyone loves your product. Here are the NPS's for a few well known companies according to <a href="https://www.satmetrix.com/infographic/2020-us-consumer-benchmarks/">satmetrix</a>:</p><ul role="list"><li>Apple - 62</li><li>AirBnB - 43</li><li>AT&amp;T - 20</li></ul><p>What is Snowflake's NPS score? It's 71. According to NPS, Snowflake's customers like Snowflake more than Apple's customers like Apple.</p><p>The average Snowflake customer pays Snowflake $165k a year. Average contract value (ACV) is a useful metric to look at because it gives you a rough sense of how efficient a sales team is. To elaborate, there's a similar amount of work needed for a sales person to sell a $50k deal as there is for a sales person to sell a $100k deal. This is because most of the work a sales person does such as giving a demo, dealing with the paper process, and negotiating, have to happen regardless of how much the contract actually winds up being. Closing a $100k deal brings in twice as much revenue as a $50k deal does, but it's not usually twice as much work.</p><p>For this reason, higher average contract values are usually better. According to <a href="https://dskok-wpengine.netdna-ssl.com/wp-content/uploads/2020/08/2020-KBCM-SaaS-Survey.pdf">KBCM Technology Group</a>, 29% of software businesses have an ACV between $50k-$250k, and only 9% have an ACV greater than $250k. That gives Snowflake a relatively healthy ACV.</p><p>Not only does Snowflake have a high ACV, but their high end contract values are also really high. Snowflake has 56 customers that pay them over $1M a year. They also have Capital One, their largest customer, account for 11% of their 2019 revenue with approximately $29M.</p><p>Snowflake's net loss of $349M during 2019 does not sound good, but it does not tell the full story. There are two issues with using profit as a metric when looking at a subscription software business:</p><ul role="list"><li>Due to the generally accepted accounting principles (GAAP), a customer can give Snowflake money, but Snowflake can't count that money as revenue until a later date, even though that money is in Snowflake's bank account.</li><li>A net loss doesn't tell you where the money is coming in and where the money is going to.</li></ul><p>Addressing each of these points:</p><h3>Revenue under GAAP</h3><p>Under GAAP, Snowflake doesn't count revenue when a customer pays them, but instead when a customer uses the Snowflake product. To be more specific when a company pays Snowflake, they are purchasing a number of "Snowflake credits". When you are using the Snowflake product you exchange your credits for Snowflake compute time.</p><p>As an example, let's say a customer purchases one year of Snowflake credits for $100k. That money winds up in Snowflake's bank account, but Snowflake cannot count that $100k as revenue until Snowflake delivers the service, which is when the customer uses their credits for compute time. If the customer uses a quarter of the Snowflake credits within three months after purchases them, under GAAP, Snowflake would count that as $25k in revenue over those three months.</p><p>You can see the impact this has by looking at the $688M Snowflake has in "remaining performance obligations". This $688M is money that Snowflake's customers have either already given to Snowflake or have contractually committed to giving to Snowflake, but it won't show up as revenue until the customers use the credits they purchased.</p><h3>Where's the money going?</h3><p>The other thing to look at is what they are spending the money on. Snowflake is making money on every sale. They have a gross profit of 62% so they have no problem running the product. The thing is they are spending A LOT on sales and marketing. Their total net loss for the last six months was $171M while during the same time period they spent $191M on sales and marketing. If they wanted to, they could stop investing in growth, lay off their entire sales and marketing team, and they would become profitable. In other words, Snowflake could be profitable if they wanted to, but they are intentionally choosing not to, and are instead focusing on growing the customer base.</p><p>Not that they should. If we assume the $191M they spent in sales and marketing over the last six months was responsible for the $81M increase in revenue in the last six months compared to the prior six months, every $1 Snowflake invests in sales and marketing results in $.42 of revenue. With Snowflake's 158% net retention rate, that customer will pay back the initial investment in sales and marketing after around two years and then return many times the investment in the years following that.</p><p>In practice, Snowflake probably sees a return on investment much sooner. A company pays for Snowflake well before Snowflake is able to count that money as revenue.</p><p>So does it make sense for Snowflake's market cap to be 150x their revenue. I definitely think it's high, but I it's certainly plausible. They're growing faster, have higher customer satisfaction, and just have phenomenal metrics across the board when compared to similar businesses.</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.freshpaint.io/blog/why-is-snowflake-so-valuable</link>
            <guid isPermaLink="false">hacker-news-small-sites-24641481</guid>
            <pubDate>Wed, 30 Sep 2020 17:50:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Save Millions with QSBS and Section 1045 Rollovers]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24641348">thread link</a>) | @ankit77
<br/>
September 30, 2020 | https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/ | <a href="https://web.archive.org/web/*/https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Qualified Small Business Stock (QSBS) is some of the most tax-advantaged stock you can hold, yet few people know about it. If you’re a founder, early employee, or investor, you can potentially save millions of dollars by understanding the implications of QSBS.</p>



<p>In this article I will discuss the following:</p>



<ol><li>History of QSBS</li><li>QSBS Tax Savings</li><li>Requirements for QSBS</li><li>Section 1045 Rollover (applicable if you don’t meet the five year holding period, explained below)</li></ol>



<p>I recently sold shares in a company I co-founded and spoke to more than two dozen accountants in the process. I’m summarizing my findings and experiences in the article below. As such, the information in this article is not formal tax advice. I recommend you speak with your accountant prior to making any decisions.&nbsp;</p>



<h2>History of QSBS</h2>



<p>Section 1202 is the section of the tax code that outlines the QSBS tax exclusion. It was added to the tax code in 1993 to encourage individuals to invest in new ventures, far before the creation of Silicon Valley as we know it today. The act, however, failed to provide the intended incentive of spurring investments in new ventures.</p>



<p>Over the past three decades, a number of tailwinds have propelled QSBS back into the limelight. Congress reduced the tax on long-term capital gains in 1997, increased the tax savings of QSBS incrementally until 2010, and finally reduced corporate taxes from 35% to a flat 21% in 2017. These three forces have made QSBS far more relevant today than in the past.</p>



<h2>QSBS Tax Savings</h2>



<p><strong>Under Section 1202, your gains from selling QSBS may be eligible for up to 100% exclusion from federal and state taxes. This exclusion is limited to the </strong><strong><em>greater</em></strong><strong> of $10 million or 10 times your cost basis during a liquidity event.</strong></p>



<p>For instance, the excludable amount for a founder may be on $10 million of gain, while the exclusion for a VC may be much greater. If, for instance, a VC invests $20 million, the VC may obtain an exclusion for $200 million of gain. See the “Scenario Table” in the Appendix for more examples. Please also note that if you end up selling your shares in multiple tranches over multiple years, the excludable amount might vary. Reference<a href="https://www.thetaxadviser.com/issues/2018/nov/qualified-small-business-stock-more-attractive.html"> this</a> article for further details.</p>



<p><strong>If you qualify for a QSBS tax exclusion, you are 100% exempt from federal taxes. The current federal tax rate is 23.8% (20% federal + 3.8% medicare). This means you can save 23.8% in long-term capital gains that you would have been subject to otherwise. Depending on which state you live in (</strong><strong><em>not</em></strong><strong> which state the company is incorporated), you may qualify for state-level exclusion as well.</strong> States typically fall into one of four buckets:</p>



<ol><li>States with no individual income tax or no capital gains tax. These states are QSBS compliant by default.</li><li>States that follow the federal tax code and waive state taxes if an individual meets the federal-level QSBS requirements. These states are also QSBS compliant.&nbsp;</li><li>States that have their own QSBS exclusion statues.</li><li>States that do not recognize QSBS in any way, shape, or form (California notably falls in this bucket).&nbsp;</li></ol>



<p>I’ve provided a chart of applicable QSBS treatment in each of the 50 states and District of Columbia in the Appendix.</p>



<h2>Requirements for QSBS</h2>



<p><strong>For your stock to qualify as QSBS, you must meet certain requirements at the time of your stock issuance, and others during your entire holding period of the stock. If you sell QSBS, you must report the entire gain as a long-term gain on your Schedule D, and enter the allowable exclusion as a loss below the entry for the gain.</strong></p>



<h3>Requirements that must be met on the date of issuance:</h3>



<ul><li>Corporation issuing the stock must be a domestic C-Corp (and the stock must be issued after August 9, 1993)</li><li>You must acquire your stock directly from the company for money, property, or services. The only exception is if you acquire the stock by gift or inheritance. In this case, you are treated as having acquired the stock in the same manner as the original owner.<ul><li>Note: do not contribute the stock to a family LLC, limited partnership/trust, or to an LLC organized to manage the sale of your stock. This will disqualify the stock as QSBS.</li></ul></li><li><strong>Corporation must have assets of $50M or less at the time you receive your shares (or exercise your options).</strong><ul><li>Note: this is a continuous requirement and if at any point the assets of a corporation exceed $50M, the corporation can never again issue QSBS (even if the assets are below $50M on the date of the subsequent issuance).</li><li>Note: the assets of the corporation must not exceed $50M even after taking into account amounts the corporation received in the current issuance. If, for instance, a company has $40M in the bank and is raising a $20M Series B, none of the newly issued Series B stock will be QSBS.</li></ul></li><li>You must determine your stock issuance date. This is critical for three reasons:<ul><li><strong>Starts the clock for purposes of the five-year holding period requirement. In order to be eligible for the tax exemption outlined above, you must have held on to your stock for a minimum of five years.</strong> If you do not meet this minimum requirement, you can employ a Section 1045 rollover (described below) to extend your holding period.<ul><li><strong>Note: this requirement is yet another reason why you should early exercise your options and file an 83(b). Early exercise allows you to 1) start the one year holding period for long-term capital gains treatment, and 2) start the five year holding period for Section 1202. If you don’t file an 83(b) election, the clock on long-term capital gain only begins when your shares vest – so if there are multiple vesting dates, you will have multiple clocks to monitor for long-term capital gain – and, if eligible, QSBS. An unexercised option or warrant is not considered QSBS, even if the underlying stock would meet the definition of QSBS.</strong></li><li><strong>Note: for stock acquired through the exercise of an option, the company must pass the “$50M asset test” on the date of your exercise, not on the date of your grant.</strong> Similarly, for stock acquired through the vesting of RSUs, the company must pass the “$50M asset test” on vesting, and your five year holding period begins on vesting, not on grant.</li><li>Note: if the stock was received as a gift, inheritance, or as a distribution from a partnership, the acquisition date is the date on which the transferor acquired the stock.</li></ul></li><li>Determines whether gain from the sale of the QSB stock is eligible for a 50%, 75%, or 100% federal tax exclusion.<ul><li>50% federal tax exclusion for stock issued before February 18, 2009</li><li>75% federal tax exclusion for stock issued between February 18, 2009 and September 27, 2010</li><li>100% federal tax exclusion for stock issued after September 27, 2010</li></ul></li><li>Marks the date on which the company must have $50M or less in assets.&nbsp;</li></ul></li></ul>



<h3>Requirements that must be met during the shareholder’s holding period:</h3>



<ul><li>Corporation must be a C-corp for the entire holding period.</li><li><strong>The corporation must be an “active business” during the entire period you held your stock. This means that at least 80% (by value) of the assets in your corporation must be used to pursue business in industries </strong><strong><em>other</em></strong><strong> than the industries below. Note that if your business provides a service, then it most likely does not qualify as a qualified small business.&nbsp;</strong><ul><li>Health, law, non-software engineering (civil, electrical, etc), architecture, accounting, actuarial science, performing arts, consulting, athletics, financial services, or brokerage.</li><li>Banking, insurance, financing, leasing, investing, or similar business.</li><li>Farming.</li><li>Mining or natural resource production or extraction.</li><li>Operating a hotel, restaurant, or similar business.</li></ul></li><li>Cash held for burn requirements generally qualify under this “active business” requirement. However, after two years, technically no more than 50% of the corporation’s assets can qualify under this exemption. While startups usually satisfy this requirement, it isn’t always clear how to apply the rule, especially if the startup retains significant cash following an investment (in other words, overfunded startups sitting on cash). If you’re like most startups and you’re burning cash to fund business operations you will most likely pass this check.</li><li>If the corporation bought back 5% or more of its stock in the year before or after your stock issuance, your stock will not qualify as QSBS.&nbsp;</li></ul>



<h2>Section 1045 Rollover</h2>



<p><strong>In order to be eligible for preferential tax treatment under Section 1202, you must satisfy the requirements above and have held onto your stock for at least five years. If you have not met the five year minimum, however, you can employ a Section 1045 rollover to extend your holding period.</strong></p>



<p><strong>If you have held onto your QSBS for at least six months, you can sell your QSBS and roll the proceeds of the sale into another QSBS issuer without recognizing a gain under Section 1045.</strong> This is a similar concept to a Section 1031 exchange in real estate. <strong>Per Section 1045, you have 60 days from the date of the sale of your original QSBS to roll the sale proceeds into new QSBS.</strong> In general, you should roll all of the proceeds from your sale of original QSBS into the new QSBS. If you take any cash off the table after the initial sale, that amount would be subject to capital gains tax. <strong>The cost basis of the new QSBS is the same as the cost basis of the original QSBS, and the holding period from the original QSBS is counted towards the holding period of the new QSBS.</strong></p>



<p>Consider the following scenario: you acquire 5M shares of QSBS from “Company A” on January 1st, 2010 for $0.00001 per share. Your original cost basis is $50 (5M * $0.00001). Assume you then sell these shares in Company A for $4M on January 1st, 2012, and then reinvest this $4M to purchase 2,000 shares of QSBS in “Company B”. Your holding period will pick up from where it left off and the cost basis for your new shares in Company B will be the same as the original cost basis for your shares in Company A ($50). If you then sell your QSBS in Company B at any point after January 1, …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/">https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/</a></em></p>]]>
            </description>
            <link>https://goyalpost.com/2020/09/29/how-to-save-millions-with-qsbs-and-section-1045-rollovers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24641348</guid>
            <pubDate>Wed, 30 Sep 2020 17:39:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Failed Promise of Web Components]]>
            </title>
            <description>
<![CDATA[
Score 360 | Comments 223 (<a href="https://news.ycombinator.com/item?id=24640151">thread link</a>) | @lemonberry
<br/>
September 30, 2020 | https://lea.verou.me/2020/09/the-failed-promise-of-web-components/ | <a href="https://web.archive.org/web/*/https://lea.verou.me/2020/09/the-failed-promise-of-web-components/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Web Components had so much potential to empower HTML to do more, and make web development more accessible to non-programmers and easier for programmers. Remember how exciting it was every time we got new shiny HTML elements that actually <em>do stuff</em>? Remember how exciting it was to be able to do sliders, color pickers, dialogs, disclosure widgets straight in the HTML, without having to include any widget libraries?</p>



<p>The promise of Web Components was that we’d get this convenience, but for a much wider range of HTML elements, developed much faster, as nobody needs to wait for the full spec + implementation process. We’d just include a script, and boom, we have more elements at our disposal!</p>



<p>Or, that was the idea. Somewhere along the way, the space got flooded by JS frameworks aficionados, who revel in complex APIs, overengineered build processes and dependency graphs that look like the roots of a banyan tree.</p>



<figure><img src="https://live.staticflickr.com/2025/32441377780_e3acf6de12_b.jpg" alt=""><figcaption>This is what the roots of a Banyan tree look like. <a href="https://www.flickr.com/photos/79721788@N00/32441377780/">Photo by David Stanley on Flickr (CC-BY)</a>. </figcaption></figure>



<p>Perusing the components on <a href="https://www.webcomponents.org/">webcomponents.org</a> fills me with anxiety, and I’m perfectly comfortable writing JS — I write JS for a living! What hope do those who can’t write JS have? Using a custom element from the directory often needs to be preceded by a ritual of npm flugelhorn, import clownshoes, build quux, all completely unapologetically because “here is my truckload of dependencies, yeah, what”. Many steps are even omitted, likely because they are “obvious”. Often, you wade through the maze only to find the component doesn’t work anymore, or is not fit for your purpose.</p>



<p>Besides setup, the main problem is that HTML is not treated with the appropriate respect in the design of these components. They are not designed as closely as possible to standard HTML elements, but <em>expect</em> JS to be written for them to do anything. HTML is simply treated as a shorthand, or worse, as merely a marker to indicate where the element goes in the DOM, with all parameters passed in via JS. I recall <a href="https://adactio.com/articles/12839#webcomponents">a wonderful talk by Jeremy Keith</a> a few years ago about this very phenomenon, where he discussed <a href="https://shop.polymer-project.org/">this e-shop Web components demo by Google</a>, which is the poster child of this practice. These are the entire contents of its <code>&lt;body&gt;</code> element:</p>



<pre><code>&lt;body&gt;
	&lt;shop-app unresolved=""&gt;SHOP&lt;/shop-app&gt;
	&lt;script src="node_assets/@webcomponents/webcomponentsjs/webcomponents-loader.js"&gt;&lt;/script&gt;
	&lt;script type="module" src="src/shop-app.js"&gt;&lt;/script&gt;
	&lt;script&gt;window.performance&amp;&amp;performance.mark&amp;&amp;performance.mark("index.html");&lt;/script&gt;
&lt;/body&gt;</code></pre>



<p>If this is how Google is leading the way, how can we hope for contributors to design components that follow established HTML conventions?</p>



<p>Jeremy criticized this practice from the aspect of backwards compatibility: when JS is broken or not enabled, or the browser doesn’t support Web Components, the entire website is blank. While this is indeed a serious concern, my primary concern is one of <strong>usability</strong>: <strong>HTML is a lower barrier to entry language</strong>. Far more people can write HTML than JS. Even for those who do eventually write JS, it often comes after spending years writing HTML &amp; CSS.</p>



<p>If components are designed in a way that requires  JS, this excludes thousands of people from using them. And even for those who <em>can</em> write JS, HTML is often easier: you don’t see many people rolling their own sliders or using JS-based ones once <code>&lt;input type="range"&gt;</code> became widely supported, right?</p>



<p>Even when JS is unavoidable, it’s not black and white. A well designed HTML element can reduce the amount and complexity of JS needed to a minimum. Think of the <code><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog">&lt;dialog&gt;</a></code> element: it usually does require *some* JS, but it’s usually rather simple JS. Similarly, the <code>&lt;video&gt;</code> element is perfectly usable just by writing HTML, and has a comprehensive JS API for anyone who wants to do fancy custom things.</p>



<p>The other day I was looking for a simple, dependency free, tabs component. You know, the canonical example of something that is easy to do with Web Components, the example 50% of tutorials mention. I didn’t even care what it looked like, it was for a testing interface. I just wanted something that is small and works like a normal HTML element. Yet, it proved so hard I ended up writing my own!</p>



<h3>Can we fix this?</h3>



<p>I’m not sure if this is a design issue, or a documentation issue. Perhaps for many of these web components, there are easier ways to use them. Perhaps there are vanilla web components out there that I just can’t find. Perhaps I’m looking in the wrong place and there is another directory somewhere with different goals and a different target audience. </p>



<p>But if not, and if I’m not alone in feeling this way, we need a directory of web components with strict inclusion criteria:</p>



<ul><li><strong>Plug and play.</strong> No dependencies, no setup beyond including one <code>&lt;script&gt;</code> tag. If a dependency is absolutely <em>needed</em> (e.g. in a map component it doesn’t make sense to draw your own maps), the component loads it automatically if it’s not already loaded.</li><li>Syntax and API follows <a href="https://www.smashingmagazine.com/2017/02/designing-html-apis/"><strong>conventions established by built-in HTML elements</strong></a> and anything that <em>can</em> be done without the component user writing JS, <em>is</em> doable without JS, per <a href="https://www.w3.org/2001/tag/doc/leastPower.html">the W3C principle of least power</a>.</li><li><strong>Accessible by default</strong> via sensible ARIA defaults, just like normal HTML elements.</li><li><strong>Themable</strong> via <code><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::part">::part()</a></code>, selective inheritance and custom properties. Very minimal style by default. Normal CSS properties should just “work” to the the extent possible.</li><li><strong>Only one component of a given type</strong> in the directory, that is <strong>flexible</strong> and <strong>extensible</strong> and continuously iterated on and improved by the community. Not 30 different sliders and 15 different tabs that users have to wade through. No branding, no silos of “component libraries”. Only elements that are designed as closely as possible to what a browser would implement in every way the current technology allows.</li></ul>



<p>I would be up for working on this if others feel the same way, since that is not a project for one person to tackle. <em>Who’s with me?</em></p>



<p><strong>UPDATE:</strong> Wow this post blew up! Thank you all for your interest in participating in a potential future effort. I’m currently talking to stakeholders of some of the existing efforts to see if there are any potential collaborations before I go off and create a new one. <a href="https://twitter.com/leaverou">Follow me on Twitter to hear about the outcome</a>!</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://lea.verou.me/2020/09/the-failed-promise-of-web-components/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24640151</guid>
            <pubDate>Wed, 30 Sep 2020 15:59:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Garry Tan on Posterous, Palantir, YC, Initialized and Influencer Investing]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24639871">thread link</a>) | @rayshan
<br/>
September 30, 2020 | https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn | <a href="https://web.archive.org/web/*/https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><strong>Transcript:</strong> <em>(disclaimer: may contain unintentionally confusing, inaccurate and/or amusing transcription errors)</em><strong><br></strong></p><div><p>David: Hello, Acquired LPs. We are coming at you today with a very special episode with Garry Tan of Initialized Fame and before that—Y Combinator, Posterous, and has a long and very illustrious career here in Silicon Valley. We are going to talk about the evolution of early-stage investing.</p><p>Garry has seen it all. I think he started back in the early days when early-stage investing meant a $2 million Series A at a $5 million poster. Maybe that was even high. </p><p>Garry: Yeah, it’s a while.</p><p>David: Goodness, things have changed. I'm so excited to have you here. Thanks for joining us and we can't wait to dive in.</p><p>Garry: Yeah, thank you for having me. Big fan of the show and it means a lot to me that you'd have me.</p><p>David: Likewise that you would come on. Let's just dive right in. We're going to weave in the story of Initialized along the way, but we thought maybe we'd start way back in those prehistoric days of what life is like?</p><p>Garry: So, Arthur Rock.</p><p>David: We've already done that on the Sequoia episodes. Not that far back but when was it that you started Posterous? Was it in 2005?</p><p>Garry: It was 2008, actually. </p><p>David: It was 2008. It’s later than I thought.</p><p>Garry: 2005 I was still at Palantir, so I had just designed the logo for Palantir and built one of the major product teams. Before that, I was Stanford Computer Engineering, and the crazy story for me is that friends of mine were starting a company. And I was the lowest of the low PM at Microsoft.</p><p>Ben: What that’s like?</p><p>Garry: Great place to start, great reach. Friends of mine were starting a company with Peter Thiel. They flew me down to San Francisco to have dinner with Peter right when he wrote the $500,000 check to Facebook. He said, “Garry, what are you doing at Microsoft? You're wasting your time.” I said, “I wanted to work at a startup, but they weren't startups in 2003, 2004.”</p><p>He said, “I'm so sure this is the right thing. You need to quit your job.” He asked me how much a year I made. I told him it was $70,000. He said, “Well, how about this? I'll write you a personal check from my bank account to yours. This is your risk opportunity. Quit your job.” I said, “Thank you very much, Mr. Thiel, but I might make it to level 60 next year,” and I got on a plane and went back to Seattle. That company turned into Palantir. </p><p>David: Oh my gosh.</p><p>Garry: I ended up joining a year later, they had hired away some of my closest friends who were way smarter than me. Bob McGrew, who now runs stuff over at OpenAI. Just so many smart people you get to meet in Silicon Valley over time. Once they hired away people smarter than, I was like, I need to quit my job at Microsoft. At the moment, when your friends are starting a business and you don't know anything about startups, tech, or how these things are funded. You say, well, I have a real job, and you say no.</p><p>David: We're talking about this with Kevin and Julia Hartz on the Eventbrite episode. I imagine you're right out of college. Your parents were probably (if you even told them about this) like, no way would they let you do this.</p><p>Garry: They’re like don’t do it. That seems unsafe. Yeah. The immigrant mentality for sure.</p><p>Ben: Garry, that's an amazing story, and it is absolutely one of survivorship bias because I want to share my story. I had a friend who was starting a YC Company. I had just moved to Seattle, and I was about to start my job at Microsoft. This friend lobbied and lobbied and lobbied. They’d come and get me to co-found the company with them.</p><p>For years, it looked like a huge mistake that I said, are you kidding me? I'd have to give back my signing bonus. I just moved here. This person told me something very similar that they would help definitely pay back the signing bonus. [...] your risk thing, they’d pay for my move. But as years have gone by, that company sold for exactly the preference. It would have been completely awash.</p><p>Garry: Yeah. That's tough.</p><p>Ben: For a while I was like, wow, that's one of several examples in my life where I really blew it on joining something early. Sometimes, not that my choice was the right choice, but unless it's Peter Thiel calling, the story doesn't always end the way you’re—</p><p>Garry: Yeah. These things are crazy risky. I often think about, wow, he was willing to pay that much upfront for an engineer. The rest of my career—even as an investor today—is now actually the inversion of that. Which is now I realize actually, it's the software engineers, designers, product people, and the builders who create the future. That's why we're able to do early-stage at all is that we look a lot more like them at that stage and so they'll pick us.</p><p>On the flip side, because I can still code a little bit and I still do design. I'm probably better at marketing now than I was 10 years ago.</p><p>David: You've diversified your skillset.</p><p>Garry: Yeah. That's right. We look more like them. Then that's the cool thing. I think that fits with the overall series. It's like we're talking about the traitorous eight—sending real typewritten letters across the country to financiers who were totally different from them. Now, what we're talking about—what you guys do and what we're doing—is we're no different. I think that time was for venture capitalists to say we're set apart, we're different.</p><p>David: We're in this ivory tower.</p><p>Garry: Yeah. The ivory tower is different. There's actually a ritualistic aspect that I was talking with Geoff Lewis at Bedrock. He has this theory that is really interesting. There is something to be said for I'm walking up the steps of Sequoia, this is what legends before me did, and I can be a part of that.</p><p>Now it's Zoom. If anything, now it's flipped. Now it's about the one on one conversation that you can have right here. That's why you have the rise of influencer investing. I know, we'll talk about it later, but that's part of the reason why I think YouTube is so important, for me anyway. I'm investing very deeply into it because I think it's a very interesting innovation in the course of how ventures are created.</p><p>David: VCs historically take a long time to catch on. YouTube was founded right around this time that we're talking about, and here we are in 2020 and people only just were starting to catch on.</p><p>Garry: 2005, yeah.</p><p>Ben: Garry, you talked about your time as a builder, and we're going to put a pin in this influencer investing and definitely come back to it because I think it'll be a nice way to round out the full story. Take us through founding Posterous, leaving Palantir, how you raised money for that, and how you went about getting enough proof that there's a there, there to invest more of your time.</p><p>Garry: There's nothing quite like seeing a super early-stage startup for your own eyes. Actually, I've always been really thankful for my time working with Stephen Cohen, Joe Lonsdale, and Alex Karp—just the founders of Palantir. Being able to build software from scratch. The more subtle interesting thing that I feel like I learned was how important it is to basically continue to hire people way smarter than you actually.</p><p>The cult making and the mythmaking of the startup very early are really underplayed. I don't feel like people talk about it enough, and Palantir, I think remains very good at it. The only cult that was stronger than our cult was the Facebook cult (I think). But it's interesting to see. Years later, that's an order of magnitude bigger as a company, which is fascinating to me. I think that actually is directly proportional. How strong your cult will result in how big your company ends up being.</p><p>Assuming you're in the right market and 10 other things that you need to survive. You need to be one of those survivors. A lot of things have to break your way.</p><p>Ben: What's an example of something that was done at Palantir to help build that cult brand?</p><p>Garry: Honestly, I think the simplest thing was even just trying to get the smartest, most capable, and hardworking people, which sounds really stupid simple. It seems like everyone should do that, but honestly, people just don't. When you think about hiring, the mistake that a lot of founders make—and honestly, I made this mistake at some level too when I worked on Posterous—was who can I get? And that's the wrong question.</p><p>You should start with who is the smartest person I know, and it doesn't matter where they're at. Because if I get them, then our self-fulfilling prophecy becomes destiny. If I don't, then it doesn't. I'm not doing myself or them a favor by not going after them. We would just go. </p><p>People would pass out yellow legal pads, and we would force everyone to step away from their computers, and it'd be like, write down the names of 20 people who were the smartest people you've ever met in any walk of life. It didn't have to be engineering, design, or whatever. It was just, who are the smartest people you know in your life. We take it into a backroom and cross-reference it and then those are like our hit list. It's like let's go get those people. We're going to take them to dinner, we're going to take them to lunch, we're going to meet them, we're going to chop down the tree, and we're going to go get them.</p><p>David: That mindset leads to doing things like what Peter offered to you. I can't believe I've never thought of that before. Yeah, you were ungettable. You were at Microsoft, but what if you just offered to personally cover the gap in your salary? It didn't work for you then, but a certain number of people, that's going to work with. And if you're not in that mindset of okay, I don't even go to try to get this person. Well then, you don't know, but once you're like, no, I'm going to try and get them. Well, what can I do?</p><p>Garry: Yeah. It just compounds from there because smart people want to work with smart people. I think that is testament and credit to what they've been able to build over the years. That becomes a self-fulfilling …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn">https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn</a></em></p>]]>
            </description>
            <link>https://www.acquired.fm/episodes/how-yc-rewrote-the-seed-playbook-with-garry-tan-hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-24639871</guid>
            <pubDate>Wed, 30 Sep 2020 15:37:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Coinbase post was 100% right. Here's what you can do about it]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24638995">thread link</a>) | @ihm
<br/>
September 30, 2020 | https://parametricity.com/posts/2020-power/ | <a href="https://web.archive.org/web/*/https://parametricity.com/posts/2020-power/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<p>Recently there was a minor controversy over a blogpost from Coinbase’s <a href="https://blog.coinbase.com/coinbase-is-a-mission-focused-company-af882df8804">CEO</a>
discouraging employees from thinking about politics, and encouraging them to focus on profit-making.
I really appreciated this post, because I think it can be very clarifying for those employees who think their
company can be a pure force for good. The Coinbase blogpost gives the honest truth that in the final calculus,
corporations whose primary goal is profit-maximization can only incidentally and in small ways advance any other
aim.</p>
<p>At the same time, there is a lot of promise for good in the actual technology underlying the crypto industry. This post will argue</p>
<ol>
<li><strong>If work is organized through for-profit corporations, there will be strong pressure from management and capital to
distort positive aspects of the technology in favor of profit-making</strong></li>
<li>The <strong>naive utopianism</strong> which has sustained the industry so far and helped produce some good technology <strong>is no match for this pressure</strong></li>
<li><strong>The only realistic way to combat this</strong> distorting pressure and focus on the socially-beneficial aspects of the technology <strong>is for workers</strong>
(the other human factor of production apart from management and capitalists) <strong>to use their collective power to push back</strong>.</li>
</ol>
<p>This argument is not surprisingly somewhat controversial among executives and VCs, but happily that same group loves
free speech and hates cancel-culture, so I assume I will not be attacked for expressing it.</p>
<h2 id="what-is-the-web-and-what-should-it-be">What is the web and what should it be?</h2>
<p>What should the web be? It should be a medium for personal communication, organization for people in projects, art and creative expression, etc. that is oriented around the goals of individual and collective flourishing. That would be ideal.</p>
<p>In reality, it is not operated that way. It – like almost everything in our society – is operated as a profit-extraction machine in the service of a small group of people.
This machine started off performing the above functions (of course, already distorted by its origins as a technology for the military and academia).
Soon, an industry of for-profit companies sprung up around developing the internet. Many of these companies became large enough to have a decisive influence on what the web would be.</p>
<p>As the years went by, the parts of it that did not contribute much to profit generation (like weird personal websites) were scrapped under the influence of these companies,
and new pieces which enhanced profit generation were added. It happened this way because there is enormous pressure from management and investors to focus on profit-generation
to the exclusion of all else.</p>
<p>Of course, because these modifications were made primarily in pursuit of profit with other reasons being secondary, this resulted in a bunch of unintended (or un-“cared about”) negative consequences like</p>
<ul>
<li>creepy data-collection and surveillance which is used to manipulate us into buying things</li>
<li>right-wing radicalization which has resulted in widespread political violence</li>
<li>distribution of misinformation</li>
<li>an enormous carbon footprint</li>
<li>collective billions in wasted minutes spent scrolling because we were tricked by the algorithm to stay on just a bit longer</li>
</ul>
<p>All of these things are natural results of a system that cannot “see” the concept of social good and can only see profits.
These problems just do not matter to the system, and only could in the event that they interfered with profit-generation.</p>
<h2 id="how-to-push-back">How to push back</h2>
<p>Let’s say you are a worker at a crypto company who sees this history and want to make sure that in crypto, socially useful aspects of
the technology are prioritized rather than those that make the most profit. What should you do?</p>
<p>First, I think it is necessary to deal with the most popular response, which is a utopian faith that the decentralized
nature of some crypto technology will inherently stop bad things from happening.</p>
<h3 id="crypto-utopianism">Crypto-utopianism</h3>
<p>There’s a lot of utopianism in crypto. This ranges from the lowest Bitcoin-booster all the way up to people like Vitalik.</p>
<p>This quote is somewhat <a href="https://www.coindesk.com/this-political-conversation-with-vitalik-buterin-shows-how-ethereum-could-change-the-world">examplary</a>:</p>
<blockquote>
<p>These three white men talked about the protests erupting across the United States. To his credit, the Russian-Canadian Buterin spoke broadly instead of attempting to comment on inequality in American politics. He said the current generation is facing a global “crisis of legitimacy,” concerning both corporations and “many types of governments.”</p>
</blockquote>
<blockquote>
<p>“The challenge here is can we create systems that allow some groups of people to cooperate without that downside of a centralized or trusted actor having to be in the middle,” Buterin said.</p>
</blockquote>
<p>Crypto-utopianism usually includes a suggestion that crypto will replace existing systems (which are bad because they’re “centralized” or some other vague reason) and be better because they are “decentralized” (whatever that might mean).</p>
<p>These analyses give crypto projects a sense of grandeur and importance which is motivating to developers and investors alike. Unfortunately, if your goal is to push back against the existing profit-maximizing power structures, you need to have a better analysis than “centralized bad, decentralized good”. It sounds good, but it doesn’t actually constitute a plan to defeat the Facebooks of the world or even to resist pressure from investors.</p>
<p>The crypto-utopian prescription for all that ails the web is some new technology.
Technology is great and a necessary part of the puzzle, but unless Glenn Weyl has a way of forcing Facebook to use quadratic voting for corporate governance, it’s not useful yet.
<strong>The existing power structure can always pick-and-choose the new technology that best advances its goals.</strong>
If you have goals which are distinct from theirs, you need to build power that is capable of a serious challenge.</p>
<h3 id="what-kind-of-power-do-they-have">What kind of power do they have?</h3>
<p>What kind of power does the existing “power structure” consisting of senior management and capitalists have?
It has</p>
<ul>
<li><a href="https://www.theverge.com/2019/11/25/20983053/google-fires-four-employees-memo-rebecca-rivers-laurence-berland-union-busting-accusation-walkout">the ability to fire people</a> that won’t comply with its goals</li>
<li>the ability to withhold funding from companies that won’t comply with its goals (this is kind of another version of the previous)</li>
<li><a href="https://parametricity.com/posts/2020-power/news.ycombinator.com">public platforms</a> to spread <a href="https://blog.coinbase.com/coinbase-is-a-mission-focused-company-af882df8804">messaging that argues in favor of its goals</a></li>
<li>control of platforms which allows them to <a href="https://itsgoingdown.org/on-facebook-banning-anarchist-and-antifascist-pages-the-digital-censorship-to-come/">censor unfavorable currents in the culture at large</a></li>
<li>endless legal resources to neutralize those that won’t comply with its goals</li>
<li>in some cases, relationships with law enforcement that can be used against those that won’t comply with its goals
and surely much more.</li>
</ul>
<p>Profit-maximization is the “prime directive” of the existing power structure and as a result challenging it inevitably provokes a fight.
This fight is usually waged (on their end) by making use of all the above tools and more.</p>
<h3 id="what-kind-of-power-do-you-have">What kind of power do you have?</h3>
<p>What kind of power do you, a worker at a crypto company, have to resist all of the above? On your own, basically none. If you
make too much trouble, you will simply be fired and then you’re out of luck. Some small number of people can try starting a company (which I have done)
but you’re still subject to some of the same kinds of retaliation as funding can be withheld, which either “fires” the company or redirects
it toward whatever profit-maximization opportunity is available. And even if you have an independent revenue stream, you will be competing
against firms that are willing to do whatever it takes to maximize profits.</p>
<p>As a group however, workers have an enormous amount of power. It is sometimes considered controversial to say so, but VCs and senior
management cannot actually do anything without workers. As such, if workers organize together into a strong company or industry-wide union,
they can make demands of the existing power structure and refuse to participate in the production process (i.e., strike) if those demands are not met.
This power can be augmented with legal resources, platforms of their own, etc.</p>
<p>If you are interested in building power with your fellow workers to advance your own goals rather than those of the profit-maximizers,
both within your company and across your industry, I recommend reaching out to <a href="https://techworkerscoalition.org/">Tech Workers Coalition</a>
who can provide further guidance. You can also get in touch with or <a href="https://act.dsausa.org/donate/membership2020">join your local DSA</a> to
build your analysis, help push more broadly for a world beyond profit maximization, and get support.</p>

		</div></div>]]>
            </description>
            <link>https://parametricity.com/posts/2020-power/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24638995</guid>
            <pubDate>Wed, 30 Sep 2020 14:22:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Revisiting a 'Smaller Rust']]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24638129">thread link</a>) | @gbrown_
<br/>
September 30, 2020 | https://without.boats/blog/revisiting-a-smaller-rust/ | <a href="https://web.archive.org/web/*/https://without.boats/blog/revisiting-a-smaller-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
			<p>A bit over a year ago, I wrote some <a href="https://without.boats/blog/notes-on-a-smaller-rust">notes on a “smaller Rust”</a> - a higher level language
that would take inspiration from some of Rust’s type system innovations, but would be simpler by
virtue of targeting a domain with less stringent requirements for user control and performance.
During my time of unemployment this year, I worked on sketching out what a language like that would
look like in a bit more detail. I wanted to write a bit about what new conclusions I’ve come to
during that time.</p>
<h2 id="the-purpose-for-our-language">The purpose for our language</h2>
<p>Re-reading my previous post, I’m struck by how vague my statement of purpose for this language is.
My entire blog post is really focused on differentiating the language from <em>Rust</em>, and I frame the
discussion in terms of what I would remove from Rust, and how the language would not support certain
use cases of Rust. This isn’t really surprising: I was working on Rust, and I never had taken the
time to think of this hypothetical language in itself the way I have now.</p>
<p>The goal of this design was to create a language that could compete as an “application programming
language.” The design goals of this language were:</p>
<ol>
<li>It should try not to be notably hard to learn. To the extent possible, it should be familiar to
most programmers. Since I’m comitting by the exercise to trying to apply ownership and borrowing
to the application domain, it will necessarily contain some features most programmers find
pretty novel (like Rust “lifetimes”). But in general, we will try to reduce the onboarding ramp
and simplify things.</li>
<li>It should typecheck and compile quickly. It should not have bad batch compilation performance,
and it should be designed with incremental recompilation in mind, to enable a good experience for
users who integrate their compiler into their development environment (with a full IDE or even
just with a plugin for a text editor). I didn’t even mention this concern in the previous post.
As others have discussed elsewhere; Rust’s poor compile times are not the result of its advanced
type system, but of a combination of other factors. Some are essential, like the runtime
guarantees it makes (e.g. monomorphization) whereas others are accidental, like some aspects of
its module system. None of these factors would be essential for our language, so we would
carefully avoid these pitfalls.</li>
<li>It should have a runtime which suits it well to the major use cases for application programming
languages today. This means mainly being well suited to the developing for the web, both
front-end and back-end. (Being well-suited to the mobile platforms is unrealistic for a language
not sponsored by those platform developers, unfortunately.) Being well-suited to CLIs would also
be beneficial.</li>
</ol>
<p>I want to focus the rest of this post on my thoughts for evolving Rust’s ownership and borrowing
system, but before I do that I want to briefly touch on other design decisions that fell out of this
thought process:</p>
<ul>
<li>I would target WASM, and only WASM, for this language. WASM with reference types is suitable as
an environment for application programming (with shims for future extensions like properly
integrated garbage collection). This way the language designers can piggy back on the work being
done at many companies to establish WASM as a good shared VM platform, instead of being
responsible for things like platform compatibility or using the very slow LLVM. Targeting WASM
would also mean easier FFI integration into other languages that run on the same VM as WASM; that
is, other languages targeting WASM (like Rust) and JavaScript.</li>
<li>I would explore control-flow-capturing closures as a core language abstraction, similar to Kotlin.
As I wrote in <a href="https://without.boats/blog/the-problem-of-effects">an earlier blog post</a> inspired by the design on this hypothetical
language, I think these are a great way to integrate effects well with higher order function
abstractions.</li>
<li>I would provide syntactic sugar for <code>Result</code> and <code>Option</code> as the way to handle null and errors,
similar to Swift.</li>
<li>As I wrote in a previous blog post, I would provide green threads as the sole concurrency model,
with language or standard library provided channels and cells (discussed later) as the way of
sharing data between threads. How these green threads are mapped to CPUs is a matter for the
runtime you choose to run the compiled WASM in.</li>
<li>I didn’t get to the point of designing a polymorphism system; I would probably start with a
strenuous comparison of Rust’s traits and Go’s interfaces, and (knowing the other features of the
language) try to figure out what from Rust’s traits is unimportant.</li>
<li>I would be hope the language could avoid macros, which (in the case of pattern based macros) add a
second meta language to the language that advanced users need to understand, and in all cases
substantially complicate compilation.</li>
</ul>

<p>But now onto the meat of this post: the ownership and borrowing model. In my previous post I made
some points that I largely agree with still, but would probably reframe. Here’s what I wrote:</p>
<blockquote>
<p>Rust works because it enables users to write in an imperative programming style, which is the
mainstream style of programming that most users are familiar with, while avoiding to an impressive
degree the kinds of bugs that imperative programming is notorious for. As I said once, pure
functional programming is an ingenious trick to show you can code without mutation, but Rust is an
even cleverer trick to show you can just have mutation.</p>
<p>…</p>
<p><strong>Resource acquisition is initialization:</strong> Objects should manage conceptual resources like file
descriptors and sockets, and have destructors which clean up resource state when the object goes
out of scope. It should be trivial to be confident the destructor will run when the object goes
out of scope. This necesitates most of ownership, moving, and borrowing.</p>
<p><strong>Aliasable XOR mutable:</strong> The default should be that values can be mutated only if they are not
aliased, and there should be no way to introduce unsynchronized aliased mutation. However, the
language should support mutating values. The only way to get this is the rest of ownership and
borrowing, the distinction between borrows and mutable borrows and the aliasing rules between
them.</p>
<p>In other words, the core, commonly identified “hard part” of Rust - ownership and borrowing - is
essentially applicable for any attempt to make checking the correctness of an imperative program
tractable. So trying to get rid of it would be missing the real insight of Rust, and not building
on the foundations Rust has laid out.</p>
</blockquote>
<p>I still think this is Rust’s “secret sauce” and it does mean what I said: the language would have to
have ownership and borrowing. But what I’ve realized since is that there’s a very important
distinction between the cases in which users <em>want</em> these semantics and the cases where they largely
get in the way. This distinction is between types which represent <em>resources</em> and types which
represent <em>data</em>.</p>
<p>In this mental model, resources are types which represent “a thing” - something with an identity and
a state which can change with time as the program executes. In Rust, almost everything is a
resource: a String is a resource a HashMap is a resource, most user types are resources. In
contrast, data types are just “information” - a fact, which has no meaningful identity, contains no
state that evolves over time, etc. In Rust, types like integers, <code>&amp;str</code>, and so on - which all
implement <code>Copy</code> - are data types. (However, a mutable reference to those types is a resource: more
on this later.)</p>
<p>In Rust, only types which can be cloned by a mempcy can implement <code>Copy</code>. This is because Rust is
designed to encourage treating all heap memory as a <em>resource</em>, the management of which the end
user can control by selecting when the type representing that memory is dropped. This is very
valuable in the domains which Rust is intended to target. However, for higher level applications
that most programmers write, control over heap memory is not <em>usually</em> important. This is what users
mean when they want to “turn off the borrow checker” - they want to let a garbage collector figure
it out for them when this bit of data is freed, because to them it is “just data” and not a
resource.</p>
<p>This hypothetical language would lean into that distinction. Using persistent data structures (like
those from Clojure) and garbage collection, the set of types which could be treated as data types
would not be restricted in this language. The string type would be a data type, rather than
a resource; a dynamically sized array of data types would be a data type as well, as would a map
with keys and values that are data types.</p>
<p>Meanwhile, types representing IO objects would always be resource types. Collections containing
resource types would also be resource types. Composite types (like structs and enums) which contain
a resource type would also have to be a resource type. There would be an easy way to convert data
types to fully owned resource types as well; in the case of persistent data structures, converting
a data type to a resource type would be the point at which the “copy on write” operation occurs.
As a result users can use ownership semantics for things which impact global and external state
(like IO) and for cases where they know it will be an important performance optimization.</p>
<p>And the difference in how the language treats data and resources would be identical to the
difference between how Rust treats Copy and non-Copy types. Only resources would have affine
“ownership” semantics - in which moving them invalidates the previous binding. Data types would have
the standard non-linear semantics users are familiar with from most languages. This means that
writing algorithms using data types would be functionally the same as writing algorithms in other
imperative languages, easing the onboarding of users to the language and limiting their errors
related to linear types to areas where they are certain to care.</p>
<h2 id="borrowing-and-the-two-reference-types">Borrowing and the two reference types</h2>
<p>The previous discussion covers the ground of …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://without.boats/blog/revisiting-a-smaller-rust/">https://without.boats/blog/revisiting-a-smaller-rust/</a></em></p>]]>
            </description>
            <link>https://without.boats/blog/revisiting-a-smaller-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24638129</guid>
            <pubDate>Wed, 30 Sep 2020 12:38:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Ruby One-Liners Cookbook]]>
            </title>
            <description>
<![CDATA[
Score 188 | Comments 32 (<a href="https://news.ycombinator.com/item?id=24637797">thread link</a>) | @asicsp
<br/>
September 30, 2020 | https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html | <a href="https://web.archive.org/web/*/https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                
                

                
                
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        
<p>This chapter will give an overview of <code>ruby</code> syntax for command line usage and some examples to show what kind of problems are typically suited for one-liners.</p>
<h2><a href="#why-use-ruby-for-one-liners" id="why-use-ruby-for-one-liners">Why use Ruby for one-liners?</a></h2>
<p>I assume you are already familiar with use cases where command line is more productive compared to GUI. See also this series of articles titled <a href="https://sanctum.geek.nz/arabesque/series/unix-as-ide/">Unix as IDE</a>.</p>
<p>A shell utility like <code>bash</code> provides built-in commands and scripting features to make it easier to solve and automate various tasks. External *nix commands like <code>grep</code>, <code>sed</code>, <code>awk</code>, <code>sort</code>, <code>find</code>, <code>parallel</code> etc can be combined to work with each other. Depending upon your familiarity with those tools, you can either use <code>ruby</code> as a single replacement or complement them for specific use cases.</p>
<p>Here's some one-liners (options will be explained later):</p>
<ul>
<li><code>ruby -e 'puts readlines.uniq' *.txt</code> — retain only one copy if lines are duplicated from the given list of input file(s)</li>
<li><code>ruby -e 'puts readlines.uniq {|s| s.split[1]}' *.txt</code> — retain only first copy of duplicate lines using second field as duplicate criteria</li>
<li><code>ruby -rcommonregex -ne 'puts CommonRegex.get_links($_)' *.md</code> — extract only the URLs, using a third-party <a href="https://github.com/talyssonoc/CommonRegexRuby">CommonRegexRuby</a> library</li>
<li><a href="https://stackoverflow.com/questions/63954081/bash-remove-duplicate-of-key-values-with-preserving-order">stackoverflow: merge duplicate key values while preserving order</a> — a recent Q&amp;A that I answered with a simpler <code>ruby</code> solution compared to <code>awk</code></li>
</ul>
<p>The main advantage of <code>ruby</code> over tools like <code>grep</code>, <code>sed</code> and <code>awk</code> includes feature rich regular expression engine, standard library and third-party libraries. If you don't already know the syntax and idioms for <code>sed</code> and <code>awk</code>, learning command line options for <code>ruby</code> would be the easier option. The main disadvantage is that <code>ruby</code> is likely to be slower compared to those tools.</p>
<h2><a href="#command-line-options" id="command-line-options">Command line options</a></h2>
<table><thead><tr><th><strong>Option</strong></th><th><strong>Description</strong></th></tr></thead><tbody>
<tr><td><code>-0[octal]</code></td><td>specify record separator (<code>\0</code>, if no argument)</td></tr>
<tr><td><code>-a</code></td><td>autosplit mode with <code>-n</code> or <code>-p</code> (splits <code>$_</code> into <code>$F</code>)</td></tr>
<tr><td><code>-c</code></td><td>check syntax only</td></tr>
<tr><td><code>-Cdirectory</code></td><td>cd to directory before executing your script</td></tr>
<tr><td><code>-d</code></td><td>set debugging flags (set <code>$DEBUG</code> to true)</td></tr>
<tr><td><code>-e 'command'</code></td><td>one line of script. Several <code>-e</code>'s allowed. Omit [programfile]</td></tr>
<tr><td><code>-Eex[:in]</code></td><td>specify the default external and internal character encodings</td></tr>
<tr><td><code>-Fpattern</code></td><td><code>split()</code> pattern for autosplit (<code>-a</code>)</td></tr>
<tr><td><code>-i[extension]</code></td><td>edit <code>ARGV</code> files in place (make backup if extension supplied)</td></tr>
<tr><td><code>-Idirectory</code></td><td>specify <code>$LOAD_PATH</code> directory (may be used more than once)</td></tr>
<tr><td><code>-l</code></td><td>enable line ending processing</td></tr>
<tr><td><code>-n</code></td><td>assume <code>'while gets(); ... end'</code> loop around your script</td></tr>
<tr><td><code>-p</code></td><td>assume loop like <code>-n</code> but print line also like <code>sed</code></td></tr>
<tr><td><code>-rlibrary</code></td><td>require the library before executing your script</td></tr>
<tr><td><code>-s</code></td><td>enable some switch parsing for switches after script name</td></tr>
<tr><td><code>-S</code></td><td>look for the script using PATH environment variable</td></tr>
<tr><td><code>-v</code></td><td>print the version number, then turn on verbose mode</td></tr>
<tr><td><code>-w</code></td><td>turn warnings on for your script</td></tr>
<tr><td><code>-W[level=2|:category]</code></td><td>set warning level; 0=silence, 1=medium, 2=verbose</td></tr>
<tr><td><code>-x[directory]</code></td><td>strip off text before #!ruby line and perhaps cd to directory</td></tr>
<tr><td><code>--jit</code></td><td>enable JIT with default options (experimental)</td></tr>
<tr><td><code>--jit-[option]</code></td><td>enable JIT with an option (experimental)</td></tr>
<tr><td><code>-h</code></td><td>show this message, <code>--help</code> for more info</td></tr>
</tbody></table>
<p>This chapter will show examples with <code>-e</code>, <code>-n</code>, <code>-p</code> and <code>-a</code> options. Some more options will be covered in later chapters, but not all of them are discussed in this book.</p>
<h2><a href="#executing-ruby-code" id="executing-ruby-code">Executing Ruby code</a></h2>
<p>If you want to execute a <code>ruby</code> program file, one way is to pass the filename as argument to the <code>ruby</code> command.</p>
<pre><code>$ echo 'puts "Hello Ruby"' &gt; hello.rb
$ ruby hello.rb
Hello Ruby
</code></pre>
<p>For short programs, you can also directly pass the code as an argument to the <code>-e</code> option.</p>
<pre><code>$ ruby -e 'puts "Hello Ruby"'
Hello Ruby

$ # multiple statements can be issued separated by ;
$ ruby -e 'x=25; y=12; puts x**y'
59604644775390625
$ # or use -e option multiple times
$ ruby -e 'x=25' -e 'y=12' -e 'puts x**y'
59604644775390625
</code></pre>
<h2><a href="#filtering" id="filtering">Filtering</a></h2>
<p><code>ruby</code> one-liners can be used for filtering lines matched by a regexp, similar to <code>grep</code>, <code>sed</code> and <code>awk</code>. And similar to many command line utilities, <code>ruby</code> can accept input from both <code>stdin</code> and file arguments.</p>
<pre><code>$ # sample stdin data
$ printf 'gate\napple\nwhat\nkite\n'
gate
apple
what
kite

$ # print all lines containing 'at'
$ # same as: grep 'at' and sed -n '/at/p' and awk '/at/'
$ printf 'gate\napple\nwhat\nkite\n' | ruby -ne 'print if /at/'
gate
what

$ # print all lines NOT containing 'e'
$ # same as: grep -v 'e' and sed -n '/e/!p' and awk '!/e/'
$ printf 'gate\napple\nwhat\nkite\n' | ruby -ne 'print if !/e/'
what
</code></pre>
<p>By default, <code>grep</code>, <code>sed</code> and <code>awk</code> will automatically loop over input content line by line (with <code>\n</code> as the line distinguishing character). The <code>-n</code> or <code>-p</code> option will enable this feature for <code>ruby</code>. As seen before, the <code>-e</code> option accepts code as command line argument. Many shortcuts are available to reduce the amount of typing needed.</p>
<p>In the above examples, a regular expression (defined by the pattern between a pair of forward slashes) has been used to filter the input. When the input string isn't specified in a conditional context (for example: <code>if</code>), the test is performed against global variable <code>$_</code>, which has the contents of the input line (the correct term would be input <strong>record</strong>, see <a href="https://learnbyexample.github.io/learn_ruby_oneliners/record-separators.html#record-separators">Record separators</a> chapter). To summarize, in a conditional context:</p>
<ul>
<li><code>/regexp/</code> is a shortcut for <code>$_ =~ /regexp/</code></li>
<li><code>!/regexp/</code> is a shortcut for <code>$_ !~ /regexp/</code></li>
</ul>
<p><code>$_</code> is also the default argument for <code>print</code> method, which is why it is generally preferred in one-liners over <code>puts</code> method. More such defaults that apply to the <code>print</code> method will be discussed later.</p>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> See <a href="https://ruby-doc.org/core-2.7.1/doc/globals_rdoc.html">ruby-doc: Pre-defined global variables</a> for documentation on <code>$_</code>, <code>$&amp;</code>, etc.</p>
</blockquote>
<p>Here's an example with file input instead of <code>stdin</code>.</p>
<pre><code>$ cat table.txt
brown bread mat hair 42
blue cake mug shirt -7
yellow banana window shoes 3.14

$ # same as: grep -oE '[0-9]+$' table.txt
$ ruby -ne 'puts $&amp; if /\d+$/' table.txt
42
7
14
</code></pre>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> The <a href="https://github.com/learnbyexample/learn_ruby_oneliners/tree/master/example_files">learn_ruby_oneliners repo</a> has all the files used in examples.</p>
</blockquote>
<h2><a href="#substitution" id="substitution">Substitution</a></h2>
<p>Use <code>sub</code> and <code>gsub</code> methods for search and replace requirements. By default, these methods operate on <code>$_</code> when the input string isn't provided. For these examples, <code>-p</code> option is used instead of <code>-n</code> option, so that the value of <code>$_</code> is automatically printed after processing each input line.</p>
<pre><code>$ # for each input line, change only first ':' to '-'
$ # same as: sed 's/:/-/' and awk '{sub(/:/, "-")} 1'
$ printf '1:2:3:4\na:b:c:d\n' | ruby -pe 'sub(/:/, "-")'
1-2:3:4
a-b:c:d

$ # for each input line, change all ':' to '-'
$ # same as: sed 's/:/-/g' and awk '{gsub(/:/, "-")} 1'
$ printf '1:2:3:4\na:b:c:d\n' | ruby -pe 'gsub(/:/, "-")'
1-2-3-4
a-b-c-d
</code></pre>
<p>You might wonder how <code>$_</code> is modified without the use of <code>!</code> methods. The reason is that these methods are part of Kernel (see <a href="https://ruby-doc.org/core-2.7.1/Kernel.html">ruby-doc: Kernel</a> for details) and are available only when <code>-n</code> or <code>-p</code> options are used.</p>
<ul>
<li><code>sub(/regexp/, repl)</code> is a shortcut for <code>$_.sub(/regexp/, repl)</code> and <code>$_</code> will be updated if substitution succeeds</li>
<li><code>gsub(/regexp/, repl)</code> is a shortcut for <code>$_.gsub(/regexp/, repl)</code> and <code>$_</code> gets updated if substitution succeeds</li>
</ul>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> This book assumes you are already familiar with regular expressions. If not, you can check out my free <a href="https://learnbyexample.github.io/Ruby_Regexp/">Ruby Regexp</a> book.</p>
</blockquote>
<h2><a href="#field-processing" id="field-processing">Field processing</a></h2>
<p>Consider the sample input file shown below with fields separated by a single space character.</p>
<pre><code>$ cat table.txt
brown bread mat hair 42
blue cake mug shirt -7
yellow banana window shoes 3.14
</code></pre>
<p>Here's some examples that is based on specific field rather than the entire line. The <code>-a</code> option will cause the input line to be split based on whitespaces and the field contents can be accessed using <code>$F</code> global variable. Leading and trailing whitespaces will be suppressed and won't result in empty fields. More details is discussed in <a href="https://learnbyexample.github.io/learn_ruby_oneliners/field-separators.html#default-field-separation">Default field separation</a> section.</p>
<pre><code>$ # print the second field of each input line
$ # same as: awk '{print $2}' table.txt
$ ruby -ane 'puts $F[1]' table.txt
bread
cake
banana

$ # print lines only if last field is a negative number
$ # same as: awk '$NF&lt;0' table.txt
$ ruby -ane 'print if $F[-1].to_f &lt; 0' table.txt
blue cake mug shirt -7

$ # change 'b' to 'B' only for the first field
$ # same as: awk '{gsub(/b/, "B", $1)} 1' table.txt
$ ruby -ane '$F[0].gsub!(/b/, "B"); puts $F * " "' table.txt
Brown bread mat hair 42
Blue cake mug shirt -7
yellow banana window shoes 3.14
</code></pre>
<h2><a href="#begin-and-end" id="begin-and-end">BEGIN and END</a></h2>
<p>You can use a <code>BEGIN{}</code> block when you need to execute something before input is read and a <code>END{}</code> block to execute something after all of the input has been processed.</p>
<pre><code>$ # same as: awk 'BEGIN{print "---"} 1; END{print "%%%"}'
$ # note the use of ; after BEGIN block
$ seq 4 | ruby -pe 'BEGIN{puts "---"}; END{puts "%%%"}'
---
1
2
3
4
%%%
</code></pre>
<h2><a href="#env-hash" id="env-hash">ENV hash</a></h2>
<p>When it comes to automation and scripting, you'd often need to construct commands that can accept input from user, file, output of a shell command, etc. As mentioned before, this book assumes <code>bash</code> as the shell being used. To access environment variables of the shell, you can call the special hash variable <code>ENV</code> with the name of the environment variable as a string key.</p>
<pre><code>$ # existing environment variable
$ # output shown here is for my machine, would differ for you
$ ruby -e 'puts ENV["HOME"]'
/home/learnbyexample
$ ruby -e 'puts ENV["SHELL"]'
/bin/bash

$ # defined along with ruby command
$ # note that the variable is placed before the shell command
$ word='hello' ruby -e 'puts ENV["word"]'
hello
$ # the input characters are preserved as is
$ ip='hi\nbye' ruby -e 'puts ENV["ip"]'
hi\nbye
</code></pre>
<p>Here's another example when a regexp is passed as an environment variable content.</p>
<pre><code>$ cat word_anchors.txt
sub par
spar
apparent effort
two spare computers
cart part tart mart

$ # assume 'r' is a shell variable that has to be passed to the ruby command
$ r='\Bpar\B'
$ rgx="$r" ruby -ne 'print if /#{ENV["rgx"]}/' word_anchors.txt
apparent effort
two spare computers
</code></pre>
<blockquote>
<p><img src="https://learnbyexample.github.io/learn_ruby_oneliners/images/info.svg" alt="info"> As an example, see my repo <a href="https://github.com/learnbyexample/command_help/blob/master/ch">ch: command help</a> for a practical shell script, where commands are constructed …</p></blockquote></main></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html">https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html</a></em></p>]]>
            </description>
            <link>https://learnbyexample.github.io/learn_ruby_oneliners/one-liner-introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637797</guid>
            <pubDate>Wed, 30 Sep 2020 11:57:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spotify’s Failed Squad Goals]]>
            </title>
            <description>
<![CDATA[
Score 26 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24637656">thread link</a>) | @cocoflunchy
<br/>
September 30, 2020 | https://www.jeremiahlee.com/posts/failed-squad-goals/?repost | <a href="https://web.archive.org/web/*/https://www.jeremiahlee.com/posts/failed-squad-goals/?repost">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<!-- Spread 1 -->
<section>
<section>
<video autoplay="true" muted="true" loop="true">
<source srcset="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.png" media="(prefers-reduced-motion: reduce)">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.webm" type="video/webm">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.m4v" type="video/mp4">
<img src="https://www.jeremiahlee.com/posts/failed-squad-goals/cover.png" alt="Taylor Swift and her squad walking away from an explosion in front of the Spotify office at Birger Jarlsgatan.">
</video>
</section>
<section>
<div>

<p><b>Spotify</b> doesn’t use <em>“the Spotify model”</em><br>and neither should you.</p>
<p>By Jeremiah Lee</p>
<p>Sunday, April 19, 2020 • <a href="https://anchor.fm/jeremiah-oral-lee/episodes/Spotifys-Failed-SquadGoals-edia0p">Listen</a> •&nbsp;<a href="https://oyomy.fr/2020/04/l-echec-du-modele-spotify/" hreflang="fr">En français</a> •&nbsp;<a href="https://jp.quora.com/q/agile/Spotify%E3%81%AF-Spotify%E3%83%A2%E3%83%87%E3%83%AB-%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%84%E3%81%AA%E3%81%84" hreflang="ja">日本語で</a> • <a href="https://flavioclesio.com/2020/05/18/o-modelo-de-squadgoals-do-spotify-falhou/" hreflang="pt-br">Português (Brasil)</a></p>
</div>
</section>
</section>
<!-- Spread 2 -->
<section>
<section>
<div>
<p>Of all the allures of startup culture, few are more desireable than the speed and nimbleness of a small team. Maintaining that feeling as a company grows is a challenge. In 2012, Spotify shared its way of working and suggested it had figured it out.<sup><a href="#1">1</a></sup></p>
<p>I was excited to see <em>the Spotify model</em> in action when I interviewed for a product management role at its Stockholm headquarters in 2017. However, the recruiter surprised me before the first interview. She cautioned me to not expect Spotify to be an <a href="https://en.wikipedia.org/wiki/Agile_software_development">Agile</a> utopia.</p>
<p>I joined the company after it had tripled in size to 3,000 people over 18 months. I learned the famed squad model was only ever aspirational and never fully implemented. I witnessed organizational chaos as the company’s leaders incrementally transitioned to more traditional management structures.</p>
<p>When I asked my coworkers why the content was not removed or updated to reflect reality, I never got a good answer. Many people ironically thought the posts were great for recruiting. I no longer work at Spotify, so I am sharing my experience to set the record straight. The Spotify squad model failed Spotify and it will fail your company too.</p>
</div>
</section>
<section>
<div>
<h3>But you don’t have to take my word for it.</h3>
<p>The co-author of the Spotify model<sup><a href="#2">2</a></sup> and multiple agile coaches who worked at Spotify have been telling people to not copy it for years. Unfortunately, truth doesn’t spread as quickly or as widely as an idea people want to believe in.</p>
<blockquote>
<p>“Even at the time we wrote it, we weren’t doing it. It was part ambition, part approximation. People have really struggled to copy something that didn’t really exist.”</p>
</blockquote>
<p>—Joakim Sundén, agile coach at Spotify&nbsp;2011–2017<sup><a href="#4">4</a></sup></p>
<blockquote>
<p><em>“It worries me when people look at what we do and think it’s a framework they can just copy and implement.</em> … We are really trying hard now to emphasize we have problems as well. It’s not all ‘shiny and everything works well and all our squads are super amazing’.”</p>
</blockquote>
<p>—Anders Ivarsson, co-author of the Spotify&nbsp;whitepaper<sup><a href="#3">3</a></sup></p>
</div>
</section>
</section>
<!-- Spread 3 -->
<section>
<section>
<div>
<h2>Recap</h2>
<p>You can read and watch the <a href="#1">original content</a> in less than 30 minutes or <a href="#spread-4">skip to the next section</a> if you are familiar. Here is a brief summary.</p>
<p>Spotify had teams it called <em>squads</em> because it sounded cooler (not joking). A group of teams were organized into a department called a <em>tribe</em>. Each team was intended to be an autonomous mini-startup, with a product manager acting as mini-CEO for a feature area. The teams had designers and software engineers with a range of specializations. The intent was that a team should have every skill necessary without needing to rely on another team for success.</p>
<p>Product managers had a traditional management structure. A product manager for a team reported to their department’s product director (<em>“tribe lead”</em>). Same for designers. Software engineers, however, were managed outside of the team structure.</p>
</div>
</section>
<section>
<div>
<p><em>“Chapter leads”</em> managed software engineers specializing in a specific type of software development across the department. For example, all of the software engineers working on backend APIs across all the teams within the department would have one manager and all of the Android mobile engineers in the department would have a different manager. The intent was to allow engineers to be moved between teams within the department to best meet business requirements without them having to change managers.</p>
<figure>
<video autoplay="true" muted="true" loop="true">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/scaling-spotify-squads.webm" type="video/webm">
<source src="https://www.jeremiahlee.com/posts/failed-squad-goals/scaling-spotify-squads.m4v" type="video/mp4">
<img src="https://www.jeremiahlee.com/posts/failed-squad-goals/scaling-spotify-squads.png" alt="Spotify matrix organization diagram. A department is called a tribe and has multiple squads, another name for a team. Each squad has a product owner, another name for a product manager. Software engineers of the same discipline across multiple teams are called a chapter and managed by a chapter lead, another name for an engineering manager.">
</video>
<figcaption>Spotify tried a long-lived matrix team structure with unusual word choices.<sup><a href="#1">1</a></sup> It did not work well.</figcaption>
</figure>
</div>
</section>
</section>
<!-- Spread 4 -->
<section id="spread-4">
<section>

</section>
<section id="matrix-management-sucks">
<div>
<h3>Matrix management solved the wrong problem</h3>
<p>The “full stack” agile team worked well, but the matrix management of software engineers introduced more problems than it solved.</p>
<ul>
<li><p>Teams at Spotify were rather long-lived. The benefit of not having to change manager when moving to another team was limited.</p></li>
<li><p>Engineering managers in this model had little responsibility beyond the career development of the people they managed. Even then, their ability to help with interpersonal people skills development was limited. An engineering manager would not manage enough of the other people on the team or be involved enough in the daily context to independently assess conflict within the team.</p></li>
</ul>
</div>
</section>
</section>
<!-- Spread 5 -->
<section>
<section>
<div>
<ul>
<li><p>Without a single engineering manager responsible for the engineers on a team, the product manager lacked an equivalent peer—the mini-CTO to their mini-CEO role. There was no single person accountable for the engineering team’s delivery or who could negotiate prioritization of work at an equivalent level of responsibility.</p><p>When disagreements within the engineering team arose, the product manager needed to negotiate with all of the engineers on the team. If the engineers could not reach a consensus, the product manager needed to escalate to as many engineering managers as there were engineering specializations within the team. A team with backend, Web app, and mobile app engineers would have at least 3 engineering managers who might need to get involved. If those engineering managers could not reach a consensus, a single team’s issue would have to escalate to the department’s engineering director.</p></li>
</ul>
</div></section>
<section id="matrix-management-sucks">
<div>
<blockquote>
<p>“Chapter leads are servant-leaders who help you grow as an individual. They don’t really work with any team. They have direct reports on all the teams. They don’t have really any accountability for the delivery. They aren’t taking that responsibility. It’s easy to see the product owner as the manager for the team.”</p>
</blockquote>
<p>—Joakim Sundén, agile coach at Spotify<sup><a href="#4">4</a></sup></p>
<p><strong>Learn from Spotify’s mistakes:</strong></p>
<ul>
<li>A product—design—engineering team typically contains more engineers than designers or product managers. Having a single engineering manager for the engineers on the team creates an accountable escalation path for conflict within the team.</li>
<li>Product managers should have an equivalent peer for engineering. Product managers should be accountable for the prioritization of work. Engineering managers should be accountable for the engineers’ execution, which includes being able to negotiate speed and quality tradeoffs with the product manager.</li>
</ul>
</div>
</section>
</section>
<!-- Spread 6 -->
<section>
<section id="too-much-autonomy">
<div>
<h3>Spotify fixated on team autonomy</h3>
<p>When a company is small, teams have to do a wide range of work to deliver and have to shift initiatives frequently. As a company grows from startup to scale-up, duplicated functions across teams move to new teams dedicated to increasing organization efficiency by reducing duplication. With more teams, the need for a team to shift initiative decreases in frequency. Both of these changes allow for teams to think more deeply and long term about the problems they are scoped to solve. Faster iteration, however, is not guaranteed. Every responsibility a team cedes to increase its focus becomes a new cross-team dependency.</p>
<p>Spotify did not define a common process for cross-team collaboration. Allowing every team to have a unique way of working meant each team needed a unique way of engagement when collaborating. Overall organization productivity suffered.</p>
<p>The Spotify model was documented when Spotify was a much smaller company. It was supposed to be a multiple part series, according to Anders Ivarsson. Autonomy made the first cut, but the parts on alignment and accountability were never completed.</p>
</div>
</section>
<section>
<div>
<p><strong>Learn from Spotify’s mistakes:</strong></p>
<ul>
<li>Autonomy requires alignment. Company priorities must be defined by leadership. Autonomy does not mean teams get to do whatever they want.</li>
<li>Processes for cross-team collaboration must be defined. Autonomy does not mean leaving teams to self-organize every problem.</li>
<li>How success is measured must be defined by leadership so people can effectively negotiate cross-team dependency prioritization.</li>
<li>Autonomy requires accountability. Product management is accountable for value. The team is accountable for delivering ‘done’ increments. Mature teams can justify their independence with their ability to articulate business value, risk, learning, and the next optimal move.<sup><a href="#6">6</a></sup></li>
</ul>
</div>
</section>
</section>
<!-- Spread 7 -->
<section>
<section id="too-much-autonomy">
<div>
<blockquote>
<p>“If I were to do one thing differently, I would say we should not be focusing so much on autonomy.</p>
<p>“Every time you have a new team, they have to reinvent the wheel in how they should be working. Maybe, just maybe, we should have a ‘minimum viable agility’. You start with that. You are free to opt out, but people shouldn’t have to opt-in all the time.</p>
<p>“At what point do you start inserting this process? Probably when it’s too late.”</p>
</blockquote>
<p>—Joakim Sundén, agile coach at Spotify<sup><a href="#4">4</a></sup></p>
</div>
</section>
<section>
<div>
<blockquote>
<p>“Henrik Kniberg talked about how we're not that good at large initiatives and we’re still not that good at large initiatives.</p>
<p>“If you have inconsistent ways of working, it’s more difficult for people to move. If it’s more difficult for people to move, it’s more likely you have inconsistent ways of working. It will just reinforce until all of a sudden, you’re not really working for the same company anymore. You’re working for these kind of weird subcultures.”</p>
</blockquote>
<p>—Jason Yip, agile coach at Spotify<br>2015–time of writing<sup><a href="#5">5</a></sup></p>
</div>
</section>
</section>
<!-- Spread 8 -->
<section>
<section id="collaboration-is-a-competency">
<div>
<h3>Collaboration was an assumed competency</h3>
<p>While Spotify gave teams control over their way of working, many people did not have a basic understanding of Agile practices. This resulted in teams iterating through process tweaks in blind hope of finding the combination that would help them improve their delivery. People lacked a common language to effectively discuss the process problems, the education to solve them, and the experience to evaluate performance. It was not really <em>agile</em>. It was just <em>not-waterfall</em>.</p>
<p><em>“Agile coaches”</em> were internal consultants Spotify provided teams to teach and suggest process improvements. While well-intentioned, there were not enough coaches to help every team. A coach’s engagement with a team was rarely long enough to span a project’s completion to help a team evaluate performance. More so, they were not accountable for anything.</p>
</div>
</section>
<section>
<div>
<blockquote>
<p>“Control without competence is chaos.”</p>
</blockquote>
<p>—L. David Marquet, <a href="https://www.indiebound.org/book/9781591846406"><cite>Turn the Ship Around!</cite></a></p>
<p><strong>Learn from Spotify’s …</strong></p></div></section></section></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.jeremiahlee.com/posts/failed-squad-goals/?repost">https://www.jeremiahlee.com/posts/failed-squad-goals/?repost</a></em></p>]]>
            </description>
            <link>https://www.jeremiahlee.com/posts/failed-squad-goals/?repost</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637656</guid>
            <pubDate>Wed, 30 Sep 2020 11:35:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Six Figures in 6 days]]>
            </title>
            <description>
<![CDATA[
Score 622 | Comments 283 (<a href="https://news.ycombinator.com/item?id=24637405">thread link</a>) | @brunojppb
<br/>
September 30, 2020 | https://tr.af/6 | <a href="https://web.archive.org/web/*/https://tr.af/6">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-lax-preset="fadeIn">
        <p>This title is a little misleading.</p>

<p>Sure, it took me 6 days to amass 3,626 sales for a total of $101,528, but like any overnight success, it was years in the making. In my case, about 7 years.</p>

<p>In 2013, the jailbreaking days of iOS were in full effect. Inspired, I created and sold an icon set titled 'Greyish HD' on the Cydia store for $0.99. It was the first few dollars I ever made on the internet. I think I made a total of $17, and it was magical.</p>

<p>It wasn't about the $17, obviously; it was how I earned it. I spent time creating something that I thought was cool, then hit publish. After a few days, random strangers I've never met before decided to spend their hard-earned cents on my icon set, oftentimes while I was working on something else entirely. Without realizing it, I just discovered digital content leverage: <strong>create something once with sufficient effort, then sell it repeatedly with minimal effort.</strong></p>

<p>Fast forward 7 years later, minus 6 days. I saw some people sharing screenshots of their iPhones after discovering that iOS 14 now allows you to add custom icons to your home screen using the Siri Shortcuts app. This was the first time you can really customize iOS, and it was catching on. </p>

<p>This is something people have been doing for years on jailbroken iOS and Android devices, except this time, you can do it natively on iOS.</p>

<p>As soon as I noticed the hype, I put together some <a href="https://icons.tr.af/">icons</a> in my own style, downloaded some widgets, and tried it all out. I thought it looked cool, so I shared a screenshot of it on Twitter. Right away, people started asking about the icons in the screenshot. So I quickly packaged them, uploaded them to <a href="https://gumroad.com/">Gumroad</a>, and embedded them on a <a href="https://notion.so/">Notion</a> site using <a href="https://super.so/">Super</a>. All of this took about two hours.</p>

<p>The next day, the tweet had hundreds of retweets, thousands of likes, and over 100k impressions. The day after that, almost a million. The next thing I knew, it was everywhere. My icons got published on notable tech sites like <a href="https://www.cultofmac.com/723490/ios-14-iphone-home-screen-customization/">Cult of Mac</a>, <a href="https://www.imore.com/people-are-making-real-money-selling-icon-sets-ios-14-home-screens">iMore</a>, <a href="https://allthingst3ch.com/ios14homescreen">AllThingsTech</a>, and <a href="https://gridfiti.com/aesthetic-ios-home-screen-ideas">Gridfiti</a>. I think at this time I was around the $6k mark in sales.</p>

<p>Then, MKBHD happened. He shared a <a href="https://youtu.be/cH66LWWluVE">video</a> about all of this—using my icons for his setup, and linked them in the description. The next thing I knew, I was making $28 what felt like every 28 seconds. My phone turned into the ultimate dopamine dispenser (if it wasn't already). I had to disable notifications.</p>

<p>The day after, sales jumped from $6k to about $40k, and during the time of this writing, sales are at $116,147 from 4,188 customers.</p>

<p>All from one. Single. <a href="https://twitter.com/traf/status/1307707156788060160">Tweet</a>.</p>

<p>The right content, posted at the right time, can create unimaginable results. Although there's likely plenty of other variables that went into making this work, there are a few key insights that I think increased my odds.</p>

<h3>Publish often</h3>

<p>Continually putting things out into the world creates proof of work. The difference between where you are and where you want to be could be as little as sharing what you build, or even what you know.</p>

<p>There's only so much we can control once pushing something out into the world, but publishing often will increase your odds at finding something that sticks. They say that fortune favors the bold. In the internet age, the bold are those that aren't afraid to publish their work for the world to see. The internet is a never-ending stream of content, the idea of being annoying or over-sharing is only an idea that you invent to stop you from sharing. </p>

<h3>Act immediately</h3>

<p>I could have easily ignored the requests for the icons, or directed people to icon sets that already existed, but that would be a missed opportunity. Since I've done similar icon sets so many times already in the past, I was ready for it. Acting quickly is crucial if you plan on riding the wave. </p>

<h3>Be transparent</h3>

<p>I shared <a href="https://twitter.com/traf/status/1308158718975111175">this tweet</a> the day after posting my home screen with details of all the hype, and it created even more hype. It brought another 540,000 impressions and added fuel to an already growing fire. <strong>Share what works, and share what doesn't.</strong> Transparency is visibility.</p>

<h3>Have a clear schedule</h3>

<p>A lot of this was happening on a Monday morning. If I were working a more traditional job, or had time commitments that I had previously scheduled, I wouldn't have been able to act on my ideas as quickly as I did. I get that not everyone has the privilege of a flexible schedule, but almost anyone can start to move towards it if they want it badly enough. Freedom of a clear schedule allowed me to take action as soon as the inspiration hit, which was incredibly important to maximize exposure. Inspiration is a productivity-multiplier, but it's perishable—so act quickly.</p>

<h3>Charge more</h3>

<p>If I would have asked anyone what to price these at, most would have said $2, or maybe $5. One thing I knew for sure was that the people most likely to buy these were not the same people who were jailbreaking their iOS devices in the past. These are first time iOS customizers, so there's no notion of what an iOS icon set should be priced at.</p>

<h3>Do it for the art</h3>

<p>If I would have done this exclusively for the goal of making money, I'm convinced it wouldn't have worked nearly as well as it did. I see copycats showing up everywhere. They see success, and try to replicate exactly how it happened, but it won't work. At least not nearly as well. The reasons for this post is not to teach you how to sit on the edge of your seat, foaming at the mouth at the chance to exploit the next big internet trend, but it's to push you to keep working at the things you enjoy, share them with the world, and letting the internet do the rest.</p>

<h3>Aftermath</h3>

<p>So what came of all this? I suspect this is why most of you are here reading this, so here's a recap of everything that came from last week in numbers, from the time of this writing:</p>

<h4>Short-term metrics:</h4>

<ul><li>4,188 total icon sales</li><li>$116,147 in revenue</li><li>97% profit margins</li><li>6.2m impression</li><li>216,800 visitors</li></ul>

<h4>Long-term metrics:</h4>

<ul><li>5,216 email subscribers</li><li>4,620 Twitter followers</li><li>180 <a href="https://super.so/">Super</a> customers</li></ul>

<h3>What's next?</h3>

<p>In the same way that creating icon sets 7 years ago prepared me for last week, this whole experiment has likely prepared me for what's to come years from now.</p>

<p>The best part about this is the freedom its bought me, to keep building things that'll create even more freedom. <strong>Spending time on things that will buy you time is always a good use of it.</strong> When people buy into what ever it is you're selling, they're not only giving you money, but they're contributing to your future freedom.</p>

<p>The market will decide a huge part of what comes after sharing something, so continually increase your odds by building, publishing, then repeating. Try many things once to figure out what you want to do twice. Figure out what you've got stored in your brain that can be of value to others, then share it with the world. You might be surprised of what comes of it.</p>

<p>Thanks for reading.</p>

<p>PS. For proof that I did this in 2013, here's my <a href="https://deviantart.com/jtraf">DeviantArt profile</a>.<br>PPS. For details about the Notion/Super/Gumroad combo: <a href="https://www.makerpad.co/deep-dives/the-community-creator-how-to-run-a-low-cost-membership-community-or-creator-business-with-circle-memberspace-gumroad-notion-super-so">Makerpad deep-dive</a>.</p>

<p>— <a href="https://twitter.com/traf">@traf</a></p>
      </div></div>]]>
            </description>
            <link>https://tr.af/6</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637405</guid>
            <pubDate>Wed, 30 Sep 2020 10:59:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Commodore 64 Program Discovered on 35-Year Old Vinyl Album (2019)]]>
            </title>
            <description>
<![CDATA[
Score 204 | Comments 56 (<a href="https://news.ycombinator.com/item?id=24637146">thread link</a>) | @clockworksoul
<br/>
September 30, 2020 | https://popcultureretrorama.com/2019/11/25/35-years-later-a-commodore-64-program-was-found-on-this-1984-album/ | <a href="https://web.archive.org/web/*/https://popcultureretrorama.com/2019/11/25/35-years-later-a-commodore-64-program-was-found-on-this-1984-album/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1304">
	
	<div>
		
<p>Friends, this should give you a little boost at the end of your day – because 8-Bit Show and Tell has located a Commodore 64 program hidden within Prodigal’s 1984 album entitled Electric Eye. I was just goofing around on YouTube when I came across this video – which was originally uploaded back on October 19th of this year. Since I have been known to talk about my love of the Commdore 64 computer of my youth – the title of the video caught my attention pretty quickly. And seriously, how absolutely amazing is it that in this day and age we can still be surprised by little Easter eggs from 35 years ago? As Robin will demonstrate on the video itself the program was hidden in the runout groove on the B side of <em>Electric Eye</em>  – which you can plainly see in this article image header has a “C-64” scratched into said groove. Probably one of the reasons that not a lot of people know about the program is because it needed to be played on your turntable where you could record the hidden program on a cassette tape to upload it on your C64… that is a little bit of work. As the video will show – sometimes using older technology takes a couple of tries – or even totally different equipment in some cases.</p>







<p>I will have to in the near future share an article about the Mattel Electronics Aquarius computer that we obtained at the arcade – in fact I talked just a bit about it in the <em><a href="https://popcultureretrorama.com/2019/11/24/diary-of-an-arcade-employee-podcast-1up-night-stalker/">Night Stalker</a></em> podcast earlier today. Robin was able to make contact with one of the surviving members of <strong>Prodigal</strong> by the way – which was a Christian rock group, active from 1975 until 1986 – to ask how and why this 35 year-old computer program was included on the <em>Electric Eye</em> album. </p>



<div><figure><img data-attachment-id="1306" data-permalink="https://popcultureretrorama.com/2019/11/25/35-years-later-a-commodore-64-program-was-found-on-this-1984-album/35-years-later-a-commodore-64-program-electric-eye/" data-orig-file="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?fit=700%2C394&amp;ssl=1" data-orig-size="700,394" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="35-Years-Later-A-Commodore-64-Program-Electric-Eye" data-image-description="" data-medium-file="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?fit=300%2C169&amp;ssl=1" data-large-file="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?fit=640%2C360&amp;ssl=1" loading="lazy" width="640" height="360" src="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?resize=640%2C360&amp;ssl=1" alt="" srcset="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?w=700&amp;ssl=1 700w, https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?resize=150%2C84&amp;ssl=1 150w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?w=700&amp;ssl=1 700w, https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?resize=150%2C84&amp;ssl=1 150w" data-lazy-src="https://i0.wp.com/popcultureretrorama.com/wp-content/uploads/2019/11/35-Years-Later-A-Commodore-64-Program-Electric-Eye.jpg?resize=640%2C360&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></div>



<p>The discovery of what amounts to a simple message from <strong>Prodigal</strong> 35 years later might not be the most groundbreaking thing you’ll see today… but I thought it was special enough that I had to share it. Watching this particular 8-Bit Show and Tell video will in addition show you how to hack your turntable to disable the auto return if you need to!</p>



<figure><p><span><iframe width="640" height="360" src="https://www.youtube.com/embed/6_CZpFqvDQo?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span>
</p></figure>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

			<div>
	
	<p>
		An avid devotee to pretty much all things pop culture and retro related - I love to share my memories and passion for films, comics, gaming, podcasting... and curiously enough my overwhelming desire to never stop eating beef jerky.		<a href="https://popcultureretrorama.com/author/vicsagepopculture/" rel="author">
			View more posts		</a>
	</p><!-- .author-description -->
</div><!-- .author-bio -->
	
</article></div>]]>
            </description>
            <link>https://popcultureretrorama.com/2019/11/25/35-years-later-a-commodore-64-program-was-found-on-this-1984-album/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637146</guid>
            <pubDate>Wed, 30 Sep 2020 10:13:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Brief Guide to OTP in Elixir]]>
            </title>
            <description>
<![CDATA[
Score 229 | Comments 89 (<a href="https://news.ycombinator.com/item?id=24637121">thread link</a>) | @NaeosPsy
<br/>
September 30, 2020 | https://serokell.io/blog/elixir-otp-guide | <a href="https://web.archive.org/web/*/https://serokell.io/blog/elixir-otp-guide">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>One of the main advantages of Elixir is that it is awesome for server-side systems. Forget using a million different technologies for things like data persistence, background jobs, and service crash recovery, OTP can supply you with everything.</p><p><em>So what exactly is this magical thing?</em></p><p>In this article, I will introduce you to OTP, look at basic process loops, the GenServer and Supervisor behaviours, and see how they can be used to implement an elementary process that stores funds.</p><p>(This article assumes that you are already familiar with the basics of Elixir. If you’re not, you can check out the <a href="https://elixir-lang.org/getting-started/introduction.html">Getting Started guide</a> on Elixir’s website or use one of the other resources listed in our <a href="https://serokell.io/blog/learn-elixir">Elixir guide</a>.)</p><h2 id="what-is-otp%3F">What is OTP?</h2><p>OTP is an awesome set of tools and libraries that Elixir inherits from Erlang, <a href="https://serokell.io/blog/history-of-erlang-and-elixir">a programming language on whose VM it runs</a>.</p><p>OTP contains a lot of stuff, such as the Erlang compiler, databases, test framework, profiler, debugging tools. But, when we talk about OTP in the context of Elixir, we usually mean the Erlang actor model that is based on lightweight processes and is the basis of what makes Elixir so efficient.</p><h2 id="processes">Processes</h2><p><img src="https://serokell.io/files/dg/dg0wowcg.processes_(1).jpg" alt="Processes divider"></p><p>At the foundation of OTP, there are tiny things called processes.</p><p>Unlike OS processes, they are really, really lightweight. Creating them takes microseconds, and a single machine can easily run multiple thousands of them, simultaneously.</p><p>Processes loosely follow the <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a>. Every process is <em>basically</em> a mailbox that can receive messages, and in response to those messages it can:</p><ul>
<li>Create new processes.</li>
<li>Send messages to other processes.</li>
<li>Modify its private state.</li>
</ul><p><img src="https://serokell.io/files/j1/j1smm7kp.process.jpg" alt="Process graph"></p><h3 id="spawning-processes">Spawning processes</h3><p>The most basic way to spawn a process is with the spawn command. Let’s open IEx and launch one.</p><pre><code>iex(<span>1</span>)&gt; process = spawn(<span>fn</span> -&gt; IO.puts(<span>"hey there!"</span>) <span>end</span>)
</code></pre><p>The above function will return:</p><pre><code>hey there!

</code></pre><p>First is the result of the function, second is the output of spawn – PID, a unique process identification number.</p><p>Meanwhile, we have a problem with our process. While it did the task we asked it to do, it seems like it is now… dead? 😱</p><p>Let’s use its PID (stored in the variable <code>process</code>) to query for life signs.</p><pre><code>iex(<span>2</span>)&gt; Process.alive?(process)
<span>false</span>
</code></pre><p>If you think about it, it makes sense. The process did what we asked it to do, fulfilled its reason for existence, and closed itself. But there is a way to extend the life of the process to make it more worthwhile for us.</p><h3 id="receive-do-loop">Receive-do loop</h3><p>Turns out, we can extend the process function to a loop that can hold state and modify it.</p><p>For example, let’s imagine that we need to create a process that mimics the funds in a palace treasury. We’ll create a simple process to which you can store or withdraw funds, and ask for the current balance.</p><p>We’ll do that by creating a loop function that responds to certain messages while keeping the state in its argument.</p><pre><code><span><span>defmodule</span> <span>Palace</span></span>.SimpleTreasury <span>do</span>

  <span><span>def</span> <span>loop</span></span>(balance) <span>do</span>
      receive <span>do</span>
        {<span>:store</span>, amount} -&gt;
          loop(balance + amount)
        {<span>:withdraw</span>, amount} -&gt;
          loop(balance - amount)
        {<span>:balance</span>, pid} -&gt;
          send(pid, balance)
          loop(balance)
      <span>end</span>
    <span>end</span>
<span>end</span>
</code></pre><p>In the body of the function, we put the receive statement and pattern match all the messages we want our process to respond to. Every time the loop runs, it will check from the bottom of the mailbox (in order they were received) for messages that match what we need and process them.</p><p>If the process sees any messages with atoms <code>store</code>, <code>withdraw</code>, <code>balance</code>, those will trigger certain actions.</p><p>To make it a bit nicer, we can add an <code>open</code> function and also dump all the messages we don’t need to not pollute the mailbox.</p><pre><code><span><span>defmodule</span> <span>Palace</span></span>.SimpleTreasury <span>do</span>

  <span><span>def</span> <span>open</span></span>() <span>do</span>
    loop(<span>0</span>)
  <span>end</span>

  <span><span>def</span> <span>loop</span></span>(balance) <span>do</span>
    receive <span>do</span>
      {<span>:store</span>, amount} -&gt;
        loop(balance + amount)
      {<span>:withdraw</span>, amount} -&gt;
        loop(balance - amount)
      {<span>:balance</span>, pid} -&gt;
        send(pid, balance)
        loop(balance)
      <span>_</span> -&gt;
        loop(balance)
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre><p>While this seems quite concise, there’s already some boilerplate lurking, and we haven’t even covered corner cases, tracing, and reporting that would be necessary for production-level code.</p><p>In real life, we don’t need to write code with receive do loops. Instead, we use one of the behaviours created by people much smarter than us.</p><h2 id="behaviours">Behaviours</h2><p>Many processes follow certain similar  patterns. To abstract over these patterns, we use behaviours. Behaviours have two parts: abstract code that we don’t have to implement and a callback module that is implementation-specific.</p><p>In this article, I will introduce you to <a href="https://hexdocs.pm/elixir/GenServer.html">GenServer</a>, short for <em>generic server</em>, and <a href="https://hexdocs.pm/elixir/Supervisor.html">Supervisor</a>. Those are not the only behaviours out there, but they certainly are one of the most common ones.</p><h3 id="genserver">GenServer</h3><p><img src="https://serokell.io/files/za/za1kwa33.genserver.jpg" alt="GenServer divider"></p><p>To start off, let’s create a module called <code>Treasury</code>, and add the GenServer behaviour to it.</p><pre><code><span><span>defmodule</span> <span>Palace</span></span>.Treasury <span>do</span>
  <span>use</span> GenServer
<span>end</span>
</code></pre><p>This will pull in the necessary boilerplate for the behaviour. After that, we need to implement the callbacks for our specific use case.</p><p>Here’s what we will use for our basic implementation.</p><table>
  <tbody><tr>
   <td>Callback
   </td>
   <td>What it does
   </td>
   <td>What it <em>usually</em> returns
   </td>
  </tr>
  <tr>
   <td><code>init(state)</code>
   </td>
   <td>Initializes the server.
   </td>
   <td><code>{:ok, state}</code>
   </td>
  </tr>
  <tr>
   <td><code>handle_cast(pid, message)</code>
   </td>
   <td>An async call that doesn’t demand an answer from the server.
   </td>
   <td><code>{:noreply, state}</code>
   </td>
  </tr>
  <tr>
   <td><code>handle_call(pid, from, message)</code>
   </td>
   <td>A synchronous call that demands an answer from the server.
   </td>
   <td><code>{:reply, reply, state}</code>
   </td>
  </tr>
</tbody></table><p>Let’s start with the easy one – <code>init</code>. It takes a state and starts a process with that state.</p><pre><code> <span><span>def</span> <span>init</span></span>(balance) <span>do</span>
   {<span>:ok</span>, balance}
 <span>end</span> 
</code></pre><p>Now, if you look at the simple code we wrote with <code>receive</code>, there are two types of triggers. The first one (<code>store</code> and <code>withdraw</code>)  just asks for the treasury to update its state asynchronously, while the second one (<code>get_balance</code>) waits for an answer. <code>handle_cast</code> can handle the async ones, while <code>handle_call</code>can handle the synchronous one.</p><p>To handle adding and subtracting, we will need two casts. These take a message with the command and the transaction amount and update the state.</p><pre><code><span><span>def</span> <span>handle_cast</span></span>({<span>:store</span>, amount}, balance) <span>do</span>
  {<span>:noreply</span>, balance + amount}
<span>end</span>

<span><span>def</span> <span>handle_cast</span></span>({<span>:withdraw</span>, amount}, balance) <span>do</span>
  {<span>:noreply</span>, balance - amount}
<span>end</span>
</code></pre><p>Finally, <code>handle_call</code>takes the balance call, the caller, and state, and uses all that to reply to the caller and return the same state.</p><pre><code><span><span>def</span> <span>handle_call</span></span>(<span>:balance</span>, _from, balance) <span>do</span>
  {<span>:reply</span>, balance, balance}
<span>end</span>
</code></pre><p>These are all the callbacks we have:</p><pre><code><span><span>defmodule</span> <span>Palace</span></span>.Treasury <span>do</span>
  <span>use</span> GenServer

  <span><span>def</span> <span>init</span></span>(balance) <span>do</span>
    {<span>:ok</span>, balance}
  <span>end</span>

  <span><span>def</span> <span>handle_cast</span></span>({<span>:store</span>, amount}, balance) <span>do</span>
    {<span>:noreply</span>, balance + amount}
  <span>end</span>

  <span><span>def</span> <span>handle_cast</span></span>({<span>:withdraw</span>, amount}, balance) <span>do</span>
    {<span>:noreply</span>, balance - amount}
  <span>end</span>

  <span><span>def</span> <span>handle_call</span></span>(<span>:balance</span>, _from, balance) <span>do</span>
    {<span>:reply</span>, balance, balance}
  <span>end</span>
<span>end</span>
</code></pre><p>To hide the implementation details, we can add client commands in the same module. Since this will be the only treasury of the palace, let’s also give a name to the process equal to its module name when spawning it with <code>start_link.</code>This will make it easier to refer to it.</p><pre><code><span><span>defmodule</span> <span>Palace</span></span>.Treasury <span>do</span>
  <span>use</span> GenServer

  

  <span><span>def</span> <span>open</span></span>() <span>do</span>
    GenServer.start_link(__MODULE_<span>_</span>, <span>0</span>, <span>name:</span> __MODULE_<span>_</span>)
  <span>end</span>

  <span><span>def</span> <span>store</span></span>(amount) <span>do</span>
    GenServer.cast(__MODULE_<span>_</span>, {<span>:store</span>, amount})
  <span>end</span>

  <span><span>def</span> <span>withdraw</span></span>(amount) <span>do</span>
    GenServer.cast(__MODULE_<span>_</span>, {<span>:withdraw</span>, amount})
  <span>end</span>

  <span><span>def</span> <span>get_balance</span></span>() <span>do</span>
    GenServer.call(__MODULE_<span>_</span>, <span>:balance</span>)
  <span>end</span>

  

  <span><span>def</span> <span>init</span></span>(balance) <span>do</span>
    {<span>:ok</span>, balance}
  <span>end</span>

  <span><span>def</span> <span>handle_cast</span></span>({<span>:store</span>, amount}, balance) <span>do</span>
    {<span>:noreply</span>, balance + amount}
  <span>end</span>

  <span><span>def</span> <span>handle_cast</span></span>({<span>:withdraw</span>, amount}, balance) <span>do</span>
    {<span>:noreply</span>, balance - amount}
  <span>end</span>

  <span><span>def</span> <span>handle_call</span></span>(<span>:balance</span>, _from, balance) <span>do</span>
    {<span>:reply</span>, balance, balance}
  <span>end</span>
<span>end</span>
</code></pre><p>Let’s try it out:</p><pre><code><span><span>iex</span><span>(<span>1</span>)</span></span>&gt; Palace<span>.Treasury</span><span>.open</span>()
{:ok, #PID&lt;<span>0.138</span>.<span>0</span>&gt;}
<span><span>iex</span><span>(<span>2</span>)</span></span>&gt; Palace<span>.Treasury</span><span>.store</span>(<span>400</span>)
:ok
<span><span>iex</span><span>(<span>3</span>)</span></span>&gt; Palace<span>.Treasury</span><span>.withdraw</span>(<span>100</span>)
:ok
<span><span>iex</span><span>(<span>4</span>)</span></span>&gt; Palace<span>.Treasury</span><span>.get_balance</span>()
<span>300</span>
</code></pre><p>It works. 🥳</p><p>Here’s <a href="https://elixir-lang.org/cheatsheets/gen-server.pdf">a cheatsheet on GenServer</a> to help you remember where to put what.</p><h3 id="supervisor">Supervisor</h3><p><img src="https://serokell.io/files/7j/7jpa2q4j.supervisor_(1).jpg" alt="Supervisor divider"></p><p>However, just letting a treasury run without supervision is a bit irresponsible, and a good way to lose your funds or your head. 😅</p><p>Thankfully, OTP provides us with the <a href="https://hexdocs.pm/elixir/Supervisor.html">supervisor behaviour</a>. Supervisors can:</p><ul>
<li>start and shutdown applications,</li>
<li>provide fault tolerance by restarting crashed processes,</li>
<li>be used to make a hierarchical supervision structure, called a <em>supervision tree</em>.</li>
</ul><p>Let’s equip our treasury with a simple supervisor.</p><pre><code><span><span>defmodule</span> <span>Palace</span></span>.Treasury.Supervisor <span>do</span>
  <span>use</span> Supervisor

  <span><span>def</span> <span>start_link</span></span>(init_arg) <span>do</span>
    Supervisor.start_link(__MODULE_<span>_</span>, init_arg,  <span>name:</span> __MODULE_<span>_</span>)
  <span>end</span>

  <span><span>def</span> <span>init</span></span>(_init_arg) <span>do</span>
    children = [
      %{
       <span>id:</span> Palace.Treasury,
       <span>start:</span> {Palace.Treasury, <span>:open</span>, []}
      }
    ]   


    Supervisor.init(children, <span>strategy:</span> <span>:one_for_one</span>)
  <span>end</span>
<span>end</span>
</code></pre><p>In its most basic, a supervisor has two functions: <code>start_link()</code>, which runs the supervisor as a process, and <code>init</code>, which provides the arguments necessary for the supervisor to initialize.</p><p>Things we need to pay attention to are:</p><ul>
<li><strong>The list of children.</strong> Here, we list all the processes that we want the supervisor to start, together with their init functions and starting arguments. Each of the processes is a map, with at least the <code>id</code>and <code>start</code> keys in it.</li>
<li><strong>Supervisor’s <code>init</code> function.</strong> To it, we supply the list of children processes and a supervision strategy. Here, we use <code>:one_for_one</code>– if a child process will crash, only that process will be restarted. There are <a href="https://hexdocs.pm/elixir/Supervisor.html#module-strategies">a few more</a>.</li>
</ul><p>Running the <code>Palace.Treasury.Supervisor.start_link()</code> function will open a treasury, which will be supervised by the process. If the treasury crashes, it will get restarted with the initial state – 0.</p><p>If we wanted, we could add several other processes to this supervisor that are relevant to the treasury function, such as a process that can exchange looted items for their monetary value.</p><p>Additionally, we could also duplicate or persist the state of the treasury …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://serokell.io/blog/elixir-otp-guide">https://serokell.io/blog/elixir-otp-guide</a></em></p>]]>
            </description>
            <link>https://serokell.io/blog/elixir-otp-guide</link>
            <guid isPermaLink="false">hacker-news-small-sites-24637121</guid>
            <pubDate>Wed, 30 Sep 2020 10:08:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It Is Never a Compiler Bug Until It Is]]>
            </title>
            <description>
<![CDATA[
Score 244 | Comments 130 (<a href="https://news.ycombinator.com/item?id=24636326">thread link</a>) | @nullc
<br/>
September 30, 2020 | http://r6.ca/blog/20200929T023701Z.html | <a href="https://web.archive.org/web/*/http://r6.ca/blog/20200929T023701Z.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>Last week I was trying to add some <a href="https://github.com/bitcoin-core/secp256k1/pull/822/commits/aa833603a6b4c947c21da04aeac40d80444ebcc1#diff-b04459e37839cd223176618536295715R425">testing code to libsecp256k1</a> and I was pulling out my hair trying to get it to work.
No amount of <code>printf</code> was working to illuminate what I was doing wrong.
Finally, out of desperation, I thought I would do a quick check to see if there are any compiler bugs related to <code>memcmp</code>, and lo and behold, I found <a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95189">GCC bug #95189: memcmp being wrongly stripped like strcmp</a>.</p><p>Honestly this was a pretty horrifying bug to read about.
Under some circumstances GCC 9 and 10 will cause <code>memcmp</code> to return an incorrect value when one of the inputs is statically known array that contains <code>NULL</code> bytes.
As I rushed to <a href="https://gist.githubusercontent.com/roconnor/2b8e22e829ed80088ed6690cc3c7f3a8/raw/455571a6d9053c597c1585debe6f9dbd6af85071/gistfile1.txt">recompile my computer system using GCC 8</a>, I contemplated the vast consequences of such a bug could be, and pondered how it was possible that computers could function at all.</p><p>However over the week, with the help of my colleagues, we managed to get a better understanding of the scope of the bug.
The bug can only convert non-zero values to zero values.
The static array needs to have a <code>NULL</code> byte within the first 4 bytes.
Most importantly, the <code>memcmp</code> result must not immediately be compared to <code>0</code> for equality or inequality, or any equivalent test.
A different code path is taken in the compiler in that case.
That explained why computers were still functioning.
I expect the vast majority of the uses of <code>memcmp</code> does an immediate test for equality with <code>0</code>.</p><p>I still wondered though, how much code was being affected. My colleague Tim suggested that it would be possible to instrument GCC to emit a message when it was about to miscompile a program.
Together we came up with <a href="https://gcc.gnu.org/bugzilla/attachment.cgi?id=49276&amp;action=diff">a patch</a> to GCC 9 and 10 that would print a debugging message.
Once again, I recompiled my entire system, to see what GCC was miscompiling.
This is what I found:</p><ul>
<li><a href="https://github.com/unicode-org/icu/blob/4fb47b12a70737ee12326220e71c2d73c5ec658f/icu4c/source/common/uniset_props.cpp#L709">https://github.com/unicode-org/icu/blob/4fb47b12a70737ee12326220e71c2d73c5ec658f/icu4c/source/common/uniset_props.cpp#L709</a></li>
<li><a href="https://github.com/xiph/flac/blob/ce6dd6b5732e319ef60716d9cc9af6a836a4011a/src/flac/decode.c#L1310">https://github.com/xiph/flac/blob/ce6dd6b5732e319ef60716d9cc9af6a836a4011a/src/flac/decode.c#L1310</a></li>
<li><a href="https://github.com/torvalds/linux/blob/fb0155a09b0224a7147cb07a4ce6034c8d29667f/drivers/atm/zatm.c#L1172">https://github.com/torvalds/linux/blob/fb0155a09b0224a7147cb07a4ce6034c8d29667f/drivers/atm/zatm.c#L1172</a></li>
<li><a href="https://github.com/nss-dev/nss/blob/1f3746f5107535a47bb4e3969f561e1bd1314bab/gtests/pk11_gtest/pk11_chacha20poly1305_unittest.cc#L425">https://github.com/nss-dev/nss/blob/1f3746f5107535a47bb4e3969f561e1bd1314bab/gtests/pk11_gtest/pk11_chacha20poly1305_unittest.cc#L425</a></li>
<li><a href="https://github.com/GNOME/glib/blob/010569b3734f864fcf584f771915b78bd391eb5f/glib/tests/refstring.c#L70">https://github.com/GNOME/glib/blob/010569b3734f864fcf584f771915b78bd391eb5f/glib/tests/refstring.c#L70</a></li>
<li><a href="https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L390">https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L390</a>, <a href="https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L661">https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L661</a>, <a href="https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L1279">https://github.com/heimdal/heimdal/blob/7ae2dfd853c87f9cbecb6f399de4dad09bad4606/lib/gssapi/krb5/arcfour.c#L1279</a></li>
<li><a href="https://github.com/zeromq/libzmq/blob/22d218a182855f28038e865cb75bf5897ff0c786/tests/test_mock_pub_sub.cpp#L203">https://github.com/zeromq/libzmq/blob/22d218a182855f28038e865cb75bf5897ff0c786/tests/test_mock_pub_sub.cpp#L203</a></li>
<li><a href="https://github.com/pigoz/mplayer-svn/blob/8d651873a9eb193f5155ffb51ece206f187cf00f/sub/sub_cc.c#L391-L412">https://github.com/pigoz/mplayer-svn/blob/8d651873a9eb193f5155ffb51ece206f187cf00f/sub/sub_cc.c#L391-L412</a></li>
</ul>
<p>On my entire system I only found 10 lines of code that were miscompiled.
Three lines are tests.
All of the lines could be rewritten as a comparison to 0.
None of the lines looked that serious.
I am not sure which one is the worse: the reduced message integrity code(?) from some ARCFOUR implementation or the something something from an ATM driver?</p><p>The mplayer miscompilation is the most mysterious.
The code surrounding that function all appears to be immediately compare <code>memcmp</code> with <code>0</code>.
And given that my debug message refused to point to exactly what line is being miscompiled in that function, I fear some set of optimizations has happened to allow this code to be miscompiled in some way.</p><p>With more hardware I could do <a href="https://hydra.nixos.org/jobset/nixpkgs/trunk#tabs-jobs">a more thorough investigation</a> of the consequences of this GCC bug.
Until then I am going to stick with GCC 8 until GCC 9 and 10 have a new point releases.

</p></div></div>]]>
            </description>
            <link>http://r6.ca/blog/20200929T023701Z.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24636326</guid>
            <pubDate>Wed, 30 Sep 2020 07:01:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Understanding How UUIDs Are Generated]]>
            </title>
            <description>
<![CDATA[
Score 160 | Comments 58 (<a href="https://news.ycombinator.com/item?id=24636204">thread link</a>) | @aryamansharda
<br/>
September 29, 2020 | https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/ | <a href="https://web.archive.org/web/*/https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="mainEntityOfPage">
<h2><strong>Introduction</strong></h2>
<p>You’ve likely used UUIDs in projects before and assumed them to be unique. Today, we’ll take a look at the main aspects of the implementation and understand why UUIDs are <em>practically </em>unique, though an incredibly small potential for duplication exists.&nbsp;</p>
<p>The modern day implementation of UUIDs can be tied back to <a href="https://tools.ietf.org/html/rfc4122" target="_blank" rel="noreferrer noopener">RFC 4122 </a>which introduced 5 different approaches for generating these identifiers. We’ll take a look at each one and we’ll step through the implementation details of Version 1 &amp; Version 4 in a moment.&nbsp;</p>
<h2><strong>Background</strong></h2>
<p>UUIDs, or universally unique IDentifiers, are simply a 128 bit number used to uniquely identify items in software development. Their canonical textual representation is as a series of 32 hexadecimal characters which are separated into five groups by hyphens in the form 8-4-4-4-12.&nbsp;</p>
<p>For example,&nbsp;</p>
<p><code>3422b448-2460-4fd2-9183-8000de6f8343</code></p>
<p>Embedded inside this seemingly random series of hexadecimal characters is information about the UUID implementation.&nbsp;</p>
<p><code>xxxxxxxx-xxxx-<strong>M</strong>xxx-<strong>N</strong>xxx-xxxxxxxxxxxx</code></p>
<p>The values in the M and N locations uniquely identify the UUID version and variant respectively.</p>
<h4><strong>Version</strong></h4>
<p>The version number is identified by looking at the most significant 4 bits of the value in the M position.&nbsp;</p>
<p>The following table lists the currently defined versions:</p>
<div><figure><img src="https://lh3.googleusercontent.com/1ePxhp-OP2JdmdHVLd3rbfDc9dcNP5J9QQncT0bYPdJ5W3_2pAHfGZ9a7Q7m3mav6VA1syO22nTIavepVicTMW9YxqiuGcSePUOanJeyz67K_bZwDbd_KO25oLgPirr53qj1wZq9" alt="" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></div>
<h4><strong>&nbsp;Variant</strong></h4>
<p>The variant field determines the layout of the information embedded in the UUID. The interpretation of all other bits in the UUID depends on the value of the variant.&nbsp;</p>
<p>We identify the variant by considering the first 1-3 most significant bits of N.&nbsp;</p>
<div><figure><img src="https://lh3.googleusercontent.com/FeXxFJcHdVVJA7IorpHtQx67F3eiSkMJVAr2Q5E4t3IierXggi-DR1uthMBOgRBXRP4JzcdRMuHbVqMMkCWZ47fo7sUO8sDBQcwgALwr45qLdpC9bSUBoXKQF9Bhdnnc9kN-g8p9" alt="" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></div>
<p>The most common implementation nowadays is Variant 1 where the MSB0&nbsp; is fixed as 1 and MSB1 is fixed as 0. This means that considering our wildcard values – the bits marked with an <code>x</code> above – the only possible values are 8, 9, A, or B.</p>
<blockquote><p><strong>Quick Reminder:&nbsp;</strong></p><p>1 0 0 0 = 8&nbsp;</p><p>1 0 0 1 = 9</p><p>1 0 1 0 = A</p><p>1 0 1 1 = B</p></blockquote>
<p>So, if you ever see a UUID with these values in the N position, you’ll know it’s the common Variant 1 type.&nbsp;</p>
<h4><strong>Version 1 (Time Based + Unique or Random Host Identifier)&nbsp;</strong></h4>
<p>In this version, the UUID is generated by taking the current timestamp and some identifying property of the device generating the UUID – most commonly, the MAC address (also called the node ID).&nbsp;</p>
<p>This UUID is generated by concatenating the 48 bit MAC address, a 60 bit-timestamp, and a 14 bit “uniquifying” clock sequence, along with the 6 reserved bits for version and variant to generate a unique UUID.&nbsp;</p>
<blockquote><p>The clock sequence is simply a value incremented every time the clock is modified.&nbsp;</p></blockquote>
<p>The timestamp used in this version is the number of 100 nanosecond time intervals since October 15, 1582 – the date of Gregorian reform to the Christian calendar.&nbsp;</p>
<blockquote><p>You may be familiar with Unix systems and time since epoch. This is simply just a different Day 0. There are several algorithms online that allow you to convert one time representation to the other, so we won’t go over this here.</p></blockquote>
<p>Though this implementation seems fairly straightforward and robust, because it reveals the MAC address of the machine it was generated on, this approach is not suitable for all use cases.&nbsp; Especially, when security is a major concern. Instead, some implementations will use 6 random bytes sourced from a cryptographically secure random number generator as a replacement for the node ID.&nbsp;</p>
<p>&nbsp;To assemble a Version 1 UUID, we’ll do the following steps:&nbsp;</p>
<ol><li>Take the low 32 bits of the current UTC timestamp. This will be the first 4 bytes / 8 hex characters of our UUID [TimeLow]</li><li>Take the middle 16 bits of the current UTC timestamp. These will be the following 2 bytes / 4 hex characters. [TimeMid]</li><li>The next 2 bytes / 4 hex characters will concat the 4 bit UUID version with the remaining high 12 bits of the current UTC timestamp (which is 60 bits in total). [TimeHighAndVersion]</li><li>Now, the next 1-3 bits will specify the variant of the UUID version. The remaining bits will contain the clock sequence which is meant to contribute some small amount of randomness to this implementation. In doing so, it helps avoid collisions in the event multiple UUID generators are running on the same system, a system clock for a UUID generator is set backwards, or the system clock doesn’t advance fast enough. [ClockSequenceHiAndRes &amp;&amp; ClockSequenceLow]</li><li>The final 6 bytes / 12 hex characters / 48 bits are the “node id” which is most commonly the MAC address of the issuing device. [NodeID]</li></ol>
<p>Putting everything together, our Version 1 UUID is generated by concatenating:</p>
<p><code>TimeLow + TimeMid + TimeHighAndVersion + (ClockSequenceHiAndRes &amp;&amp; ClockSequenceLow) + NodeID&nbsp;</code></p>
<p>Due to this implementation’s reliance on the clock, there are some edge cases we’ll need to handle. Firstly, to minimize correlation across systems, the clock sequence is defaulted to a random number – this will only happen once in the lifetime of a system. This has the added benefit of allowing us to support node identifiers that may move from system to system as the initial value of the clock sequence is completely agnostic of the node identifier.&nbsp;</p>
<p>Remember that the main purpose of the clock sequence is to introduce some amount of randomness into the equation. The bits allocated for the clock sequence help us extend the timestamp and account for situations where multiple UUIDs are generated before the processor clock advances. This helps us avoid the potential creation of duplicates when the clock is set backwards in time (device is powered off) or the node ID changes. If the clock is set backwards, or might have been set backwards (e.g., while the system was powered off), and the UUID generator can not be sure that no UUIDs were generated with timestamps larger than the value to which the clock was set, then the clock sequence has to be changed. If the previous value of the clock sequence is known, it can just be incremented; otherwise it should be set to a random or high-quality pseudo-random value.</p>
<h4><strong>Version 2 (Distributed Computing Environment Security)</strong></h4>
<p>The main difference between this version and Version 1 is that this implementation uses some identifier specific to the system in place of the “randomness” generated by using the least significant bits of the clock sequence. This value is often just the current user’s ID. This version is less common and only a small deviation from Version 1, so we won’t explore it any further.&nbsp;</p>
<h4><strong>Version 3 (Name-based + MD5 Hash)</strong></h4>
<p>In the event that you want to have unique identifiers for information within a namespace or for “nameable” information more generally, UUID version 3 and version 5 are your preferred options.&nbsp;</p>
<p>They’ll encode any “nameable” entity (website, DNS information, plain text, etc) into the UUID value. The main takeaway here is that for the same namespace and text, the generated UUID will be the same.&nbsp;</p>
<p>Take note that the namespace itself is a UUID.</p>
<pre><code>let namespace = “digitalbunker.dev”
let namespaceUUID = UUID3(.DNS, namespace)

// Ex: 
UUID3(namespaceUUID, “/category/things-you-should-know-1/”) 
4896c91b-9e61-3129-87b6-8aa299028058

UUID3(namespaceUUID, “/category/things-you-should-know-2/”) 
29be0ee3-fe77-331e-a1bf-9494ec18c0ba

UUID3(namespaceUUID, “/category/things-you-should-know-3/”) 
33b06619-1ee7-3db5-827d-0dc85df1f759
</code></pre>
<p>In this implementation, the UUID of the namespace is transformed to a string of bytes, concatenated with the input name, then hashed with MD5, yielding the 128 bits for the UUID. Then, we’ll overwrite some of those bits to accurately reflect the version and variant information leaving the rest unchanged.&nbsp;</p>
<p>It’s also important to understand that neither the namespace nor the inputted name can be generated from the UUID. This is a one-way operation. The only exception here would be through a brute force approach if one of the values (namespace or text) were known by the attacker.</p>
<p>For Version 3 and Version 5, as long as you use the same inputs, the generated UUID will be deterministic.&nbsp;</p>
<h4><strong>Version 4 (PRNG)</strong></h4>
<p>This is probably the simplest implementation of the bunch.</p>
<p>As we’ve now seen, 6 bits are reserved for the version and variant information leaving us 122 bits free to decide. This version simply generates all 128 random bits and then fills in the values for the version and variant information as a secondary step.&nbsp;</p>
<p>UUIDs of this variety rely heavily on the quality of the PRNG in use (pseudo-random number generator). If the PRNG is lacking a sophisticated algorithm or the correct seed and initialization values, the likelihood of a duplicate can increase. To better understand how computers generate random numbers, <a href="https://digitalbunker.dev/2020/09/08/how-do-computers-generate-random-numbers/" target="_blank" rel="noreferrer noopener">check out my previous article.</a>&nbsp;</p>
<p>Version 4 is what you’ll find most commonly implemented in modern programming languages.</p>
<p>It’s implementation is reasonably simple.&nbsp;</p>
<ol><li>Generate 128 random bits</li><li>Now, we’ll need to overwrite some of these bits with the correct version and variant information&nbsp;<ol><li>Take the 7th byte and perform an AND operation with <code>0x0F</code> to clear out the high nibble. Then, OR it with <code>0x40</code> to set the version number to 4.&nbsp;</li><li>Next, take the 9th byte and perform an AND operation with <code>0x3F</code> and then OR it with <code>0x80</code>.&nbsp;</li></ol></li><li>Convert the 128 bits to hexadecimal representation and insert the hyphens to achieve the canonical text representation.&nbsp;</li></ol>
<h4><strong>Version 5 (Name-based + SHA-1 Hash)</strong></h4>
<p>This version is no different than Version 3 with the exception that the <code>SHA-1</code> hashing algorithm is used here in place of <code>MD5</code>. This version is preferred to Version 3 (SHA-1 &gt; MD5).&nbsp;</p>
<h2><strong>In Practice</strong></h2>
<p>One of the notable benefits of UUIDs is that their uniqueness does not depend on a central authority or coordination between different systems. Anyone can create UUIDs with reasonable assurance that a duplicate value does not exist and will conceivably not be made in the future.&nbsp;</p>
<p>This has the added benefit of allowing UUIDs generated by independent parties to be combined into a single database or moved across databases with a trivial probability of duplication / collision.&nbsp;</p>
<p>Due to this uniqueness, you can use UUIDs as primary keys in databases, unique filenames for uploaded files, unique names for any web resource, or …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/">https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/</a></em></p>]]>
            </description>
            <link>https://digitalbunker.dev/2020/09/30/understanding-how-uuids-are-generated/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24636204</guid>
            <pubDate>Wed, 30 Sep 2020 06:31:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple’s T2 security chip jailbreak]]>
            </title>
            <description>
<![CDATA[
Score 726 | Comments 374 (<a href="https://news.ycombinator.com/item?id=24636166">thread link</a>) | @Yeri
<br/>
September 29, 2020 | https://reportcybercrime.com/hackers-jailbreak-apples-t2-security-chip-powered-by-bridgeos/ | <a href="https://web.archive.org/web/*/https://reportcybercrime.com/hackers-jailbreak-apples-t2-security-chip-powered-by-bridgeos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>The Apple T2 security chip has finally been jailbroken! Here’s all you need to know about it.&nbsp; &nbsp; &nbsp;</p>
<h2><span id="The_Apple_T2_Security_chip_now_has_a_jailbreak"><strong>The Apple T2 Security chip now has a jailbreak</strong><span></span></span></h2>
<p>The <a href="https://yalujailbreak.net/checkra1n-jailbreak-ios-14/">latest update of checkra1n</a> adds support for bridgeOS – the operating system that powers the Apple T2 security chip.</p>
<p>For what it’s worth, the T2 chip is not A10 per se but it is derived from the Apple A10 Fusion architecture.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>
<p><strong>bridgeOS</strong> is a proprietary operating system created by Apple for its hardware. It is responsible for operating the Touch Bar and managing secure data.&nbsp;&nbsp;&nbsp;</p>
<p>Here’s what hacker Jamie Bishop had to say about this development.&nbsp;&nbsp;</p>
<blockquote data-width="550" data-dnt="true">
<p lang="en" dir="ltr">With <a href="https://twitter.com/checkra1n?ref_src=twsrc%5Etfw" target="_blank" rel="nofollow noopener noreferrer">@checkra1n</a> 0.11.0, you can now jailbreak the T2 chip in your Mac. An incredible amount of work went into this and it required changes at multiple levels.</p>
<p>There’s too many people to tag, but shoutout to everyone who worked on getting this incredible feature shipped.</p>
<p>— Jamie Bishop (@jamiebishop123) <a href="https://twitter.com/jamiebishop123/status/1308355178307948545?ref_src=twsrc%5Etfw" target="_blank" rel="nofollow noopener noreferrer">September 22, 2020</a></p>
</blockquote>
<p>Since checkra1n is still in beta, there are a few issues you need to be aware of. Firstly, you might have to reconnect your device after jailbreaking for bootstrap upload.</p>
<p>Secondly, macOS takes over the USB connection and blocks communication after bootup.&nbsp;</p>
<p>If you are interested in jailbreaking the T2 chip, download checkra1n jailbreak v0.11 from this <a href="https://yalujailbreak.net/checkra1n-jailbreak-ios-14/">link</a>.&nbsp;</p>
<h2><span id="What_can_a_bridgeOS_jailbreak_be_used_for"><strong>What can a bridgeOS jailbreak be used for?&nbsp;</strong><span></span></span></h2>
<p>The T2 security processor and the Touch Bar can run while the operating system is shutdown.</p>
<p>Apparently, jailbreak tweak developers could develop tweaks for the Touch Bar if it gets Substrate support in the future.</p>
<p>At present, there are no publicly dumped headers available for bridgeOS. It lacks a MobileSubstrate port too. However, that could change in the future because it shares some of the components of watchOS and iOS frameworks.</p>
<p>Once we get Substrate working, <strong>tweaking and theming could become possible.</strong></p>

<p>The ability to exploit the T2 processor could also allow you to bypass the anti-repair mechanism built into the Touch Bar. Further, it may allow hackers to get rid of the password or unlock MDM-locked systems.&nbsp;</p>
<p>As far as the OS goes, we could also add secure boot certificates like Microsoft’s secure boot signing or a self-signed Linux certificate.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</p>
<p>It will definitely be interesting to see what the future holds for the <em>T2 security chip</em> following the release of checkra1n.&nbsp; &nbsp; &nbsp;</p>
<p>Don’t forget to follow us on Twitter and Facebook for the latest jailbreak news and updates.&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p>
</div></div>]]>
            </description>
            <link>https://reportcybercrime.com/hackers-jailbreak-apples-t2-security-chip-powered-by-bridgeos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24636166</guid>
            <pubDate>Wed, 30 Sep 2020 06:19:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elements of Programmnig]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24635947">thread link</a>) | @todsacerdoti
<br/>
September 29, 2020 | http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html | <a href="https://web.archive.org/web/*/http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>

<p>The C++ STL may be the most impressive achievement in language standard libraries. Where most programmers are stuck complaining that their language’s default strings aren’t performant enough, about every standard C++ function for strings actually runs on arbitrary character sequences. Design your own container? <span>std::find_if</span> works just as well as for the built-ins. And it does this while often being more performant than the code you’d write yourself.</p>

<p>Alex Stepanov is the man who made this happen, and Elements of Programming (EOP) is his 200-page paean to his method for writing generic code. He’s a believer that programming can be turned from an art to a rigorous discipline based on mathematics, and I’ve long admired him for his deep knowledge and impact. Indeed, he’s spent years at  #1 on my list of people I’d like to have lunch with. (Hey readers — can anyone help?)</p>

<p>And now, I’m about to ruin all that by writing a negative review.</p>

<p>EOP has a strong beginning and a strong finish. The first chapter explains core programming language concepts such as types and state — using non-standard terminology, but probably intentionally, judging by the explanation’s lucidity. This culminates in his big idea of being able to write programs on any type that offers a bundle of operations and properties called a concept, explained in this book over a decade before they entered the C++ standard. The afterword is a few pages of reflection on the power of this approach.</p>

<p>Between them is 11 chapters where he plays the same game: define a new abstraction and then show a bunch of functions that can be written using it. And unfortunately, these functions and abstractions are largely not very interesting.</p>

<p>There’s a famous site called Project Euler, where users write code to solve mathy problems such as “In a modified version of the board game Monopoly, on what three squares is a player most likely to land?” My former programming-contest coach advocated against using it to practice, because “It’s not really programming, and it’s not really math.”</p>

<p>I think this is an apt description of EOP, particularly the first half. This starts from Chapter 2, which is about cool algorithms that involve applying some function to itself repeatedly (iteration). One of my <a href="https://www.cs.cmu.edu/~cdm/pdf/lect-05.pdf">favorite lectures</a> in undergrad was on this topic, and yet I still couldn’t enjoy this chapter, as I know no application of these algorithms outside of niche mathematical topics. This gets taken up another notch in Chapter 5, where, in about 5 pages, he goes from explaining commutativity to defining the algebraic structures of monoids, groups, and rings, all the way up to algebraic modules (no relation to software modules). I cannot fathom these explanations being useful to someone who does not already know these concepts, and certainly not to someone who already does. And while I do know many uses of groups in software engineering — as an abstraction of the idea of an invertible operation — he actually spends the remainder of this chapter considering generalizations of the greatest common divisor function.</p>

<p>I slogged through these chapters, excited for the second half of the book, which focused largely on iterators and containers, things more relevant to typical software engineering. Yet after encountering endless listings of variations of list-copy functions, I found myself no more fulfilled, and soon regressed to skimming through the pages.</p>

<p>Aside from my long-term goal to find all the good writing on software design, I had a short-term goal when reading this book: a student wanted me to teach a lesson based on it. But, halfway through the book, my deadline was fast approaching, and I hadn’t found any material useful enough for a software design lesson for experienced engineers.</p>

<p>I then noticed all the chapters were generated by the deeper principle of coming up with a good abstraction to write generic functions against. I got the idea that maybe I could use EOP as a problem book, telling them to look at the descriptions of generic functions, and then come up with both the code and the abstractions they can be written against. Alas, the topic selection is not suitable for this purpose. One section of the book, for example, deals with computing integer sequences by matrix exponentiation. Asking students to come up with this themselves would be too familiar for many who have taken a linear algebra course, and impossible for those who haven’t. I did design a lesson where students come up with their own abstractions for generic programming problems, but I used examples completely unrelated to the book.</p>

<p>I asked the friend who recommended EOP what he got out of the book, and his first answer was a technique for elegantly expressing state machines using goto’s. I similarly loved that part, but, alas, that was the only concrete thing I got out of this book. I’ll explain it at the end of this review and spare you the other 200 pages.</p>

<p>I have an undergraduate degree in mathematics and have authored several papers on generic programming, so I knew I was reading it for others’ benefit. Still, I don’t think my opinion would be changed were this not the case, and I’d really like help understanding the viewpoint of the many readers who did thoroughly enjoy it. Instead, I find myself agreeing with this Amazon reviewer, although I have too much admiration for Stepanov to contemplate a 1-star rating:</p>

<blockquote>If you've ever written a generic function, you already know that the type parameters must obey a set of preconditions. This book lays out a big pile of definitions for types, numbers, algebraic structures, iterators, and such, so as to bamboozle people easily impressed by mathematical notation. It does so sloppily and writes some trivial generic algorithms in C++. To whatever extent one might accomplish something interesting with this topic, this book doesn't. Avoid.</blockquote>

<p>As I read other material by Stepanov, I mourn for the book that could have been. Stepanov clearly cares about these abstractions and algorithms, to the point where he wrote a <a href="https://www.amazon.com/Mathematics-Generic-Programming-Alexander-Stepanov/dp/0321942043">second book</a> on largely the same material, with a chattier exposition and chapters more explicitly focusing on pure math. How different would it be had he managed to transmit this appreciation to me? The day after finishing, I watched <a href="https://www.youtube.com/watch?v=W2tWOdzgXHA&amp;t=1344s">this talk</a> by a close colleague of Stepanov. “In this menu, you can select a bunch of rows and drag them somewhere else,” he explained over animated slides. “How many of you could implement this in one line?” It made me want to open section 10.4 on “rotation algorithms” again.</p>

<p>I’ve started watching <a href="https://www.youtube.com/playlist?list=PLHxtyCq_WDLXryyw91lahwdtpZsmo4BGD">a seminar</a> he gave at Amazon. I’m only a few lectures in, but I’m already enthralled by his high teaching ability. I feel like I’m there with him working through problems. I feel like I’ve learned a great secret as he tells the story of how he invented “regular types,” something used throughout EOP but never motivated. To be honest, I still don’t know what this lecture series is about, but nonetheless expect to recommend it when I’m done.</p>

<p>In short, Stepanov has given many gifts to the world of programming, and EOP is not one of them.</p>

<p><b>Overall rating</b>: <span>Not recommended</span></p>

<p>With a smattering of exceptions, EOP neither teaches abstractions useful in everyday programming, nor teaches you the skills to invent your own.</p>

<h2><b>Addendum</b>: State machines by goto’s</h2>

<blockquote>“The fastest way to go from one place in code to another is goto.”</blockquote><p>

— <a href="https://www.youtube.com/watch?v=sp_IBYVqMeQ&amp;list=PLHxtyCq_WDLXryyw91lahwdtpZsmo4BGD">Alexander Stepanov</a></p><p>Many iterative algorithms can be described as state machines: first it looks for an X, then it does Y with it, then it looks for another X, and so on. Rather than trying to massage the cycles in the state machine into structured loops, Stepanov advocates a style using goto’s, with one goto-label per machine state.</p>

<p>In searching for an example to best illustrate this, I wanted something where the code was under 40 lines (which ruled out Stepanov’s examples), understandable with little context or knowledge of C++, and which was not equivalent to a regular expression. And so:</p>

<p>In my defunctionalization talk, I showed that many state machines are derived from recursive functions, being turned into iterative traversals by creating a state for each point in the program between recursive calls. For that talk, I demonstrated this in full for the example of printing a binary tree. It turns out that adding parentheses makes this derivation substantially harder, as an arbitrary number of close-parentheses may need to be printed after processing a node. And that difficulty comes from trying to massage a state machine into a loop.</p>

<p>In this case, seeing as I came up with this example by starring with a recursive function, the recursive version is quite simple:</p>

<div><pre><span>void</span> <span>print_tree_rec</span><span>(tree</span> <span>*</span><span>t)</span> <span>{</span>
  <span>if</span> <span>(t</span> <span>!=</span> <span>NULL)</span> <span>{</span>
    <span>printf(</span><span>"("</span><span>);</span>
    <span>print_tree_rec(t</span><span>-&gt;</span><span>left);</span>
    <span>printf(</span><span>" %d "</span><span>,</span> <span>t</span><span>-&gt;</span><span>element);</span>
    <span>print_tree_rec(t</span><span>-&gt;</span><span>right);</span>
    <span>printf(</span><span>")"</span><span>);</span>
  <span>}</span>
<span>}</span>
</pre></div>


<p>But, for other state machines, the recursive version is not so easy. For example, Dijkstra created the “shunting yard” algorithm for parsing an arithmetic expression all the way back in 1961, yet I’m not aware of the recursive equivalent being discovered <a href="https://www.brics.dk/RS/07/7/BRICS-RS-07-7.pdf">until 2007</a>, using the technique of refunctionalization.</p>

<p>Here’s the state machine:</p><p><span id="docs-internal-guid-068c5e82-7fff-aeae-61d2-74453ca23aeb"><span><span><img height="566" src="https://lh4.googleusercontent.com/bfYT7CeU0eo8KAoMt8gB5Y4FDgD7Y1UlUJYSNI3ndYZ6jQpZv5Nwpyqyqoqffi_tgSRBYslU93EZy_nCil9ub6FZraaRGOBoABGPk0i9DoNbgr1fWAnkQzXMxPvLvEYFSDRWZ1RL" width="572"></span></span></span></p>

<p>A confession: the first time I thought about how to make this recursive function function iterative, I didn’t get it, and had to look it up. The solution is to merge the “Next from stack” state in the diagram with its successors, resulting in a solution with two nested while-loops, at the cost of some duplicated code.</p>

<p>However, the version based on goto’s reads off this diagram rather nicely. One C++-ism in this code to note: while the stack s is initialized to NULL, the  <span>push()</span> and  <span>pop()</span> calls can actually change it.</p>

<!--HTML generated using hilite.me--><div><pre><span>void</span> <span>print_tree</span><span>(tree</span> <span>*</span><span>t)</span> <span>{</span>
  <span>tree</span> <span>*</span><span>cur</span> <span>=</span> <span>t;</span>
  <span>stack</span> <span>*</span><span>s</span> <span>=</span> <span>NULL;</span>
  
<span>begin_print_node:</span>
  <span>if</span> <span>(cur</span> <span>==</span> <span>NULL)</span> <span>{</span>
    <span>goto</span> <span>dispatch_stack_frame;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>printf(</span><span>"("</span><span>);</span>
    <span>push(LEFT_CHILD,</span> <span>cur,</span> <span>s);</span>
    <span>cur</span> <span>=</span> <span>cur</span><span>-&gt;</span><span>left;</span>
    <span>goto</span> <span>begin_print_n…</span></pre></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html">http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html</a></em></p>]]>
            </description>
            <link>http://www.pathsensitive.com/2020/09/book-review-elements-of-programmnig.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24635947</guid>
            <pubDate>Wed, 30 Sep 2020 05:28:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[From zero to main(): How to write a bootloader from scratch]]>
            </title>
            <description>
<![CDATA[
Score 330 | Comments 44 (<a href="https://news.ycombinator.com/item?id=24635383">thread link</a>) | @tigerlily
<br/>
September 29, 2020 | https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch | <a href="https://web.archive.org/web/*/https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        <p>This is the third post in our <a href="https://interrupt.memfault.com/blog/tag/zero-to-main">Zero to main() series</a>,
where we bootstrap a working firmware from zero code on a
cortex-M series microcontroller.</p>

<p>Previously, <a href="https://interrupt.memfault.com/blog/zero-to-main-1">we wrote a startup file to bootstrap our C environment</a>, and <a href="https://interrupt.memfault.com/blog/how-to-write-linker-scripts-for-firmware">a linker
script to get the right data at the right addresses</a>. These two will allow us to
write a monolithic firmware which we can load and run on our microcontrollers.</p>

<p>In practice, this is not how most firmware is structured. Digging through vendor
SDKs, you’ll notice that they all recommend using a <em>bootloader</em> to load your
applications. A bootloader is a small program which is responsible for loading
and starting your application.</p>

<!-- excerpt start -->
<p>In this post, we will explain why you may want a
bootloader, how to implement one, and cover a few advanced techniques you may
use to make your bootloader more useful.
<!-- excerpt end --></p>

<p>Like Interrupt? <a href="http://eepurl.com/gpRedv" target="_blank">Subscribe</a> to get our latest posts straight to your mailbox.</p>



<h2 id="why-you-may-need-a-bootloader">Why you may need a bootloader</h2>

<p>Bootloaders serve many purposes, ranging from security to software architecture.</p>

<p>Most commonly, you may need a bootloader to load your software. Some
microcontrollers like Dialog’s
<a href="https://www.dialog-semiconductor.com/products/connectivity/bluetooth-low-energy/smartbond-da14580-and-da14583">DA14580</a>
have little to no onboard flash and instead rely on an external device to store
firmware code. In that case, it is the bootloader’s job to copy code from
non-executable storage, such as a SPI flash, to an area of memory that can be
executed from, such as RAM.</p>

<p>Bootloaders also allow you to decouple parts of the program that are mission
critical, or that have security implications, from application code which
changes regularly.  For example, your bootloader may contain firmware update
logic so your device can recover no matter how bad a bug ships in your
application firmware.</p>

<p>Last but certainly not least, bootloaders are an essential component of a
trusted boot architecture.  Your bootloader can, for example, verify a
cryptographic signature to make sure the application has not been replaced or
tampered with.</p>

<h2 id="a-minimal-bootloader">A minimal bootloader</h2>
<p>Let’s build a simple bootloader together. To start, our bootloader must do two
things:</p>

<ol>
  <li>Execute on MCU boot</li>
  <li>Jump to our application code</li>
</ol>

<p>We’ll need to decide on a memory map, write some bootloader code, and update our
application to make it bootload-able.</p>

<h3 id="setting-the-stage">Setting the stage</h3>

<p>For this example, we’ll be using the same setup as we did in our previous Zero
to Main posts:</p>
<ul>
  <li>Adafruit’s <a href="https://www.adafruit.com/product/3505">Metro M0 Express</a> as our
development board,</li>
  <li>a simple <a href="https://www.adafruit.com/product/2764">CMSIS-DAP Adapter</a></li>
  <li>OpenOCD (the <a href="https://github.com/arduino/OpenOCD">Arduino fork</a>) for
programming</li>
</ul>

<h3 id="deciding-on-a-memory-map">Deciding on a memory map</h3>

<p>We must first decide on how much space we want to dedicate to our bootloader.
Code space is precious - your application may come to need more of it - and you
will not be able to change this without updating your bootloader, so make
this as small as you possibly can.</p>

<p>Another important factor is your flash sector size: you want to make sure you
can erase app sectors without erasing bootloader data, or vice versa.
Consequently, your bootloader region must end on a flash sector boundary
(typically 4kB).</p>

<p>I decided to go with a 16kB region, leading to the following memory map:</p>

<div><div><pre><code>        0x0 +---------------------+
            |                     |
            |     Bootloader      |
            |                     |
     0x4000 +---------------------+
            |                     |
            |                     |
            |     Application     |
            |                     |
            |                     |
    0x30000 +---------------------+
</code></pre></div></div>

<p>We can transcribe that memory into a linker script:</p>

<div><div><pre><code>/* memory_map.ld */
MEMORY
{
  bootrom  (rx)  : ORIGIN = 0x00000000, LENGTH = 0x00004000
  approm   (rx)  : ORIGIN = 0x00004000, LENGTH = 0x0003C000
  ram      (rwx) : ORIGIN = 0x20000000, LENGTH = 0x00008000
}

__bootrom_start__ = ORIGIN(bootrom);
__bootrom_size__ = LENGTH(bootrom);
__approm_start__ = ORIGIN(approm);
__approm_size__ = LENGTH(approm);
</code></pre></div></div>

<p>Since linker scripts are composable, we will be able to <code>include</code> that memory
map into the linker scripts we write for our bootloader and our application.</p>

<p>You’ll notice that the linker script above declares some variables. We’ll need
those for our bootloader to know where to find the application. To make them
accessible in C code, we declare them in a header file:</p>

<div><div><pre><code><span>/* memory_map.h */</span>
<span>#pragma once
</span>
<span>extern</span> <span>int</span> <span>__bootrom_start__</span><span>;</span>
<span>extern</span> <span>int</span> <span>__bootrom_size__</span><span>;</span>
<span>extern</span> <span>int</span> <span>__approm_start__</span><span>;</span>
<span>extern</span> <span>int</span> <span>__approm_size__</span><span>;</span>
</code></pre></div></div>

<h3 id="implementing-the-bootloader-itself">Implementing the bootloader itself</h3>
<p>Next up, let’s write some bootloader code. Our bootloader needs to start
executing on boot and then jump to our app.</p>

<p>We know how to do the first part from our previous post: we need a valid stack
pointer at address <code>0x0</code> , and a valid <code>Reset_Handler</code>  function setting up our
environment at address <code>0x4</code>. We can reuse our previous startup file and linker
script, with one change: we use  <code>memory_map.ld</code> rather than define our own
<code>MEMORY</code> section.</p>

<p>We also need to put our code in the <code>bootrom</code> region from our memory rather than
the <code>rom</code> region in our previous post.</p>

<p>Our linker script therefore looks like this:</p>

<div><div><pre><code>/* bootloader.ld */
INCLUDE memory_map.ld

/* Section Definitions */
SECTIONS
{
    .text :
    {
        KEEP(*(.vectors .vectors.*))
        *(.text*)
        *(.rodata*)
        _etext = .;
    } &gt; bootrom
  ...
}
</code></pre></div></div>

<p>To jump into our application, we need to know where the <code>Reset_Handler</code> of the
app is, and what stack pointer to load.  Again, we know from our previous post
that those should be the first two 32-bit words in our binary, so we just need
to dereference those addresses using the <code>__approm_start__</code> variable from our
memory map.</p>

<div><div><pre><code><span>/* bootloader.c */</span>
<span>#include &lt;inttypes.h&gt;
#include "memory_map.h"
</span>
<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
  <span>uint32_t</span> <span>*</span><span>app_code</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span><span>__approm_start__</span><span>;</span>
  <span>uint32_t</span> <span>app_sp</span> <span>=</span> <span>app_code</span><span>[</span><span>0</span><span>];</span>
  <span>uint32_t</span> <span>app_start</span> <span>=</span> <span>app_code</span><span>[</span><span>1</span><span>];</span>
  <span>/* TODO: Start app */</span>
  <span>/* Not Reached */</span>
  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{}</span>
<span>}</span>
</code></pre></div></div>

<p>Next we must load that stack pointer and jump to the code. This will require a
bit of assembly code.</p>

<p>ARM MCUs use the <a href="http://www.keil.com/support/man/docs/armasm/armasm_dom1361289882044.htm"><code>msr</code> instruction
</a> to
load immediate or register data into system registers, in this case the MSP
register or “Main Stack Pointer”.</p>

<p>Jumping to an address is done with a branch, in our case with a <a href="http://www.keil.com/support/man/docs/armasm/armasm_dom1361289866466.htm"><code>bx</code>
instruction</a>.</p>

<p>We wrap those two into a <code>start_app</code> function which accepts our <code>pc</code> and <code>sp</code> as
arguments, and get our minimal bootloader:</p>

<div><div><pre><code><span>/* app.c */</span>
<span>#include &lt;inttypes.h&gt;
#include "memory_map.h"
</span>
<span>static</span> <span>void</span> <span>start_app</span><span>(</span><span>uint32_t</span> <span>pc</span><span>,</span> <span>uint32_t</span> <span>sp</span><span>)</span> <span>__attribute__</span><span>((</span><span>naked</span><span>))</span> <span>{</span>
    <span>__asm</span><span>(</span><span>"           </span><span>\n</span><span>\
          msr msp, r1 /* load r1 into MSP */</span><span>\n</span><span>\
          bx r0       /* branch to the address at r0 */</span><span>\n</span><span>\
    "</span><span>);</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
  <span>uint32_t</span> <span>*</span><span>app_code</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span><span>__approm_start__</span><span>;</span>
  <span>uint32_t</span> <span>app_sp</span> <span>=</span> <span>app_code</span><span>[</span><span>0</span><span>];</span>
  <span>uint32_t</span> <span>app_start</span> <span>=</span> <span>app_code</span><span>[</span><span>1</span><span>];</span>
  <span>start_app</span><span>(</span><span>app_start</span><span>,</span> <span>app_sp</span><span>);</span>
  <span>/* Not Reached */</span>
  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{}</span>
<span>}</span>
</code></pre></div></div>

<blockquote>
  <p>Note: hardware resources initialized in the bootloader must be de-initialized
before control is transferred to the app. Otherwise, you risk breaking
assumptions the app code is making about the state of the system</p>
</blockquote>

<h3 id="making-our-app-bootloadable">Making our app bootloadable</h3>

<p>We must update our app to take advantage of our new memory map. This is again
done by updating our linker script to include <code>memory_map.ld</code> and changing our
sections to go to the <code>approm</code> region rather than <code>rom</code>.</p>

<div><div><pre><code>/* app.ld */
INCLUDE memory_map.ld

/* Section Definitions */
SECTIONS
{
    .text :
    {
        KEEP(*(.vectors .vectors.*))
        *(.text*)
        *(.rodata*)
        _etext = .;
    } &gt; approm
  ...
}
</code></pre></div></div>

<p>We also need to update the <a href="https://developer.arm.com/docs/dui0552/latest/the-cortex-m3-processor/exception-model/vector-table"><em>vector
table</em></a>
used by the microcontroller. The vector table contains the address of every
exception and interrupt handler in our system. When an interrupt signal comes
in, the ARM core will call the address at the corresponding offset in the vector
table.</p>

<p>For example, the offset for the Hard fault handler is <code>0xc</code>, so when a hard
fault is hit, the ARM core will jump to the address contained in the table at
that offset.</p>

<p>By default, the vector table is at address <code>0x0</code>, which means that when our chip
powers up, only the bootloader can handle exceptions or interrupts! Fortunately, ARM
provides the <a href="https://developer.arm.com/docs/dui0552/latest/cortex-m3-peripherals/system-control-block/vector-table-offset-register">Vector Table Offset
Register</a>
to dynamically change the address of the vector table. The register is at
address <code>0xE000ED08</code> and has a simple layout:</p>

<div><div><pre><code>31                                  7              0
+-----------------------------------+--------------+
|                                   |              |
|              TBLOFF               |   Reserved   |
|                                   |              |
+-----------------------------------+--------------+
</code></pre></div></div>

<p>Where <code>TBLOFF</code> is the address of the vector table. In our case, that’s the start
of our text section, or <code>_stext</code>. To set it in our app, we add the following to
our <code>Reset_Handler</code>:</p>

<div><div><pre><code><span>/* startup_samd21.c */</span>
<span>/* Set the vector table base address */</span>
<span>uint32_t</span> <span>*</span><span>vector_table</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span> <span>&amp;</span><span>_stext</span><span>;</span>
<span>uint32_t</span> <span>*</span><span>vtor</span> <span>=</span> <span>(</span><span>uint32_t</span> <span>*</span><span>)</span><span>0xE000ED08</span><span>;</span>
<span>*</span><span>vtor</span> <span>=</span> <span>((</span><span>uint32_t</span><span>)</span> <span>vector_table</span> <span>&amp;</span> <span>0xFFFFFFF8</span><span>);</span>
</code></pre></div></div>

<p>One quirk of the ARMv7-m architecture is the alignment requirement for the
vector table, as specified in section B1.5.3 of the <a href="https://static.docs.arm.com/ddi0403/eb/DDI0403E_B_armv7m_arm.pdf">reference
manual</a>:</p>

<blockquote>
  <p>The Vector table must be naturally aligned to a power of two whose alignment value is greater than or equal
to (Number of Exceptions supported x 4), with a minimum alignment of 128 bytes.The entry at offset 0 is
used to initialize the value for SP_main, see The SP registers on page B1-8. All other entries must have bit
[0] set, as the bit is used to define the EPSR T-bit on exception entry (see Reset behavior on page B1-20 and
Exception entry behavior on page B1-21 for details).</p>
</blockquote>

<p>Our SAMD21 MCU has 28 interrupts on top of the 16 system reserved exceptions,
for a total of 44 entries in the table. Multiply that by 4 and you get 176. The
next power of 2 is 256, so our vector table must be 256-byte aligned.</p>

<h3 id="putting-it-all-together">Putting it all together</h3>

<p>Because it is hard to witness the bootloader execute, we add a print line to
each of our programs:</p>

<div><div><pre><code><span>/* boootloader.c */</span>
<span>#include &lt;inttypes.h&gt;
#include "memory_map.h"
</span>
<span>static</span> <span>void</span> <span>s…</span></code></pre></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch">https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch</a></em></p>]]>
            </description>
            <link>https://interrupt.memfault.com/blog/how-to-write-a-bootloader-from-scratch</link>
            <guid isPermaLink="false">hacker-news-small-sites-24635383</guid>
            <pubDate>Wed, 30 Sep 2020 03:19:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nobody seems to give a crap about Google’s monopoly on search, not even Google]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24635322">thread link</a>) | @pcr910303
<br/>
September 29, 2020 | https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google | <a href="https://web.archive.org/web/*/https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><a href="https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google">29 September 2020</a></p>
<p>Quick summary of the situation, from the <a href="https://www.android.com/choicescreen/">Choice Screen webpage on the Android website</a>:</p>
<blockquote>
<p>On August 2, 2019, following the European Commission’s July 2018 Android decision, Google announced that it would implement a choice screen for general search providers on all new Android phones and tablets shipped into the European Economic Area (<span>EEA</span>) where the Google Search app is pre-installed. This updated Help Center article describes a modified choice screen design that was developed in consultation with the European Commission.</p>
<p>The choice screen will appear during initial device setup and will feature multiple search providers, including Google. An illustrative version of the choice screen follows. Providers may vary by country. […]</p>
<p>Eligible search providers will need to fill out an application form and can bid for inclusion based on an auction. The auction process is explained in greater detail below.</p>
</blockquote>
<p>Considering how much this farce has been <a href="https://www.neowin.net/news/duckduckgo-blasts-googles-android-choice-screen-says-it-incentivizes-ads">covered</a> <a href="https://techcrunch.com/2020/07/30/googles-no-choice-screen-on-android-isnt-working-says-ecosia-querying-the-eus-approach-to-antitrust-enforcement/">already</a>, I will not repeat what <a href="https://www.businessinsider.com/google-android-choice-screen-eu-duckduckgo-2020-9?IR=T">everybody knows</a>. Instead I will just point out how Google announced the result of the auctions <a href="https://www.android.com/choicescreen-winners/">on the Android website</a>: four sentences, two footnotes, a table listing the<span></span> <span>“</span>winners” per country, a title that reads do-not-give-a-shit, and that’s it.</p>
<p>Also, I don’t think I have ever heard of — <em>check notes</em> — PrivacyWall and info.com.<span></span> <span>’</span>What are they?’, you may ask: they are search engines apparently, and they are the only two search engine options offered to Android users in France that are not Google or Bing. And yes, in case you’re wondering, <a href="https://www.youtube.com/watch?v=nfHuZ5qrYX4">Bing is still around</a>. Ecosia, DuckDuckGo, barely appear on the list.</p>
<p>If anything, this whole thing will reinforce Google’s monopoly on search. Users will see this<span></span> <span>’</span>choice screen’ and think:<span></span> <span>“</span><span>OK</span>, so two shady things I don’t know, Bing (laughs), and yep, Google. Why would they even ask me to choose?”</p>
<p>It’s like asking kids who have only ever watched the movie <em>Ratatouille</em> if they want to watch Ratatouille again, or another movie from this list: <a href="https://www.imdb.com/title/tt2120120/?ref_=fn_al_tt_1"><em>Pixels</em></a> (even the kids would know it sucks), <em><a href="https://www.imdb.com/title/tt0257778/?ref_=ttls_li_tt">The Hunchback of Notre Dame <span>II</span>: The Secret of the Bell</a></em> (you’re lucky if the kids even know about the first one), and <em><a href="https://www.imdb.com/title/tt0083630/?ref_=nv_sr_srsg_0">The Beastmaster</a></em>.</p>

          
          <p>
            <a href="https://thejollyteapot.com/tagged/technology">Technology</a>
          </p>

          
          


      

          <a href="https://thejollyteapot.com/2020/08/18/samsung-commits-to-deliver-three-years-of-android-updates-for-its-phones">
            <h5>Previous post</h5>
            <span>Samsung commits to deliver three years of Android updates for its phones</span>
            
          </a>

          <a href="https://thejollyteapot.com/2020/10/1/looking-back-at-my-short-list-of-apple-wishes-from-april">
            <h5>Next post</h5>
            <span>Looking back at my short list of Apple wishes from April</span>
          </a>

        </div></div>]]>
            </description>
            <link>https://thejollyteapot.com/2020/09/29/nobody-seems-to-give-a-crap-about-google-s-monopoly-on-search-not-even-google</link>
            <guid isPermaLink="false">hacker-news-small-sites-24635322</guid>
            <pubDate>Wed, 30 Sep 2020 03:07:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I tracked and analyzed 13,159 Hacker News posts. Here's what I learned]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24634991">thread link</a>) | @mellosouls
<br/>
September 29, 2020 | https://www.ankle.io/posts/hacker-news-analysis | <a href="https://web.archive.org/web/*/https://www.ankle.io/posts/hacker-news-analysis">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2 id="motivation"><a href="#motivation" aria-label="motivation permalink"></a>Motivation</h2>
<p>Hitting the front page of Hacker News is an incredible source of short-term traffic to your website or article.</p>
<p>According to this <a href="https://thehftguy.com/2017/09/26/hitting-hacker-news-front-page-how-much-traffic-do-you-get" rel="nofollow">article</a>, in 2017, being on the front page brings a sudden influx of between 10k and 30k visitors to your website/article, something many content marketers, product people, and entrepreneurs undoubtedly salivate over, including me.</p>
<p>I occasionally post to Hacker News (with random things and personal stuff) but have never reached the front page before. Therefore, I decided to dig a little deeper into the mechanics of Hacker News and the best approach to take if the sole goal is to hit the front page.</p>
<p>Obviously, a critical part of hitting the front page is submitting an interesting article which resonates with the HN audience. But beyond this, there are other factors at play—timing, title, upvotes, etc.</p>
<h2 id="methodology-a-unique-approach"><a href="#methodology-a-unique-approach" aria-label="methodology a unique approach permalink"></a>Methodology: A unique approach</h2>
<p>I am not the only person to want to know the best approach to hitting the front page. A few people have analyzed data from <a href="https://console.cloud.google.com/marketplace/details/y-combinator/hacker-news" target="_blank" rel="nofollow noopener noreferrer">Google’s Hacker News Dataset</a> on BigQuery to find the best time to submit to Hacker News.</p>
<p>The problem with this approach is that BigQuery doesn’t know how long a post spent on the front page, nor how long it was there for or what position it reached. It can only provide the total number of upvotes.</p>
<p>With this in mind, I wrote a small NodeJS script to poll Hacker News every 2 minutes. Using a Postgres database, I stored every post, user, and upvote over two weeks.</p>
<p>With this information, I compiled a list of useful information and insights below.</p>
<p>Note: To filter out posts that are either flagged or transiently featured, I set an arbitrary duration of 60+ minutes as a time requirement in the top for a post to be considered a “front page post.”</p>
<h2 id="limitations"><a href="#limitations" aria-label="limitations permalink"></a>Limitations</h2>
<p>Obviously, the lack of data (13,159 posts) and time spent collecting data (2 weeks) is the most significant limitation of my results. Nonetheless, I believe the results are still relatively useful, at least for the next few weeks or months.</p>
<h2 id="results"><a href="#results" aria-label="results permalink"></a>Results</h2>
<h3 id="basic-stats"><a href="#basic-stats" aria-label="basic stats permalink"></a>Basic stats</h3>
<ul>
<li>I tracked a total of 13,159 posts over two weeks.</li>
<li>Of these, 1,073 made the front page for more than 60 minutes.</li>
<li>That’s a hit rate of 8.15%</li>
<li>Frontpage posts stayed at the top for an average of 483.3 minutes (8 hours)</li>
<li>30% of posts reaching the front page were posted by users who posted more than once over the two weeks.</li>
</ul>
<p><strong>Takeaway:</strong> In my opinion, more posts reach the front page than expected and for longer than expected.</p>
<h3 id="types-of-posts-breakdown"><a href="#types-of-posts-breakdown" aria-label="types of posts breakdown permalink"></a>Types of posts breakdown</h3>

<h3 id="hit-rate--of-different-post-types-making-the-front-page"><a href="#hit-rate--of-different-post-types-making-the-front-page" aria-label="hit rate  of different post types making the front page permalink"></a>Hit rate % of different post types making the front page</h3>

<p><strong>Takeaway:</strong> Interestingly, “Show HN” posts make the front page by proportion very slightly less than “Links,” which goes against previous thought that “Show HN” posts perform better due to an algorithm bias. That being said, there may well be an algorithm bias, just many “Show HN” posts are simply not worth upvoting.</p>
<h3 id="when-is-the-best-day-to-post-to-hacker-news-in-2020"><a href="#when-is-the-best-day-to-post-to-hacker-news-in-2020" aria-label="when is the best day to post to hacker news in 2020 permalink"></a>When is the best day to post to Hacker News in 2020?</h3>

<p>The chart above shows that you should post on the weekend to stand the best chance of reaching the front page of Hacker News. The difference from the worst to the best day is only 3.2%, so there’s not a lot in it.</p>
<p>The weekend likely gives you a higher chance to reach the front page because fewer posts are submitted, probably due to lower traffic.</p>
<p><strong>Takeaway</strong>: If your goal is to reach the front page regardless of traffic, posting on the weekend is your best bet. However, because the difference in chance is relatively low throughout the week, posting on a more popular day might be a better risk-reward ratio based on the considerable difference in the traffic you’d likely receive.</p>
<h3 id="when-is-the-best-time-to-post-to-hacker-news-in-2020"><a href="#when-is-the-best-time-to-post-to-hacker-news-in-2020" aria-label="when is the best time to post to hacker news in 2020 permalink"></a>When is the best time to post to Hacker News in 2020?</h3>

<p>Taking a more granular look at the combined hours of all the posts submitted, the chart above highlights a much more significant difference (10.9%) between the chance of reaching the front page of Hacker News.</p>
<p>The best time to post to Hacker News is between 6 am—12 pm UTC (11 pm—5 am PDT). Again, the reason is likely because this is when the website traffic is at it’s lowest, and therefore the competition is relatively low.</p>
<p>I imagine the US is responsible for most of Hacker News’s web traffic, which lines up with the results above. Therefore, if you are posting a link for a US-specific audience to HN, it’s probably best to pick a different time like 4 pm UTC (9 am PDT), or midnight UTC (5 pm PDT).</p>
<p><strong>Takeaway:</strong> If your link requires a US-specific audience, post at 4 pm UTC (9 am PDT / 12 pm EDT), or midnight UTC (5 pm PDT). Otherwise, post between 6 am—12 pm UTC (11 pm—5 am PDT) for the best chance of reaching the front page of Hacker News.</p>
<h3 id="when-in-the-best-day-and-time-to-post-to-hacker-news-in-2020"><a href="#when-in-the-best-day-and-time-to-post-to-hacker-news-in-2020" aria-label="when in the best day and time to post to hacker news in 2020 permalink"></a>When in the best day and time to post to Hacker News in 2020?</h3>
<p>Combining both days and hours into a total weekly picture is even more evident when it is best to post. However, please note, the lack of data makes this chart vulnerable to skewed results.</p>

<p>The chart above illustrates wild fluctuations between the different hours of the week. The lowest chance of reaching the front page is 1-2%, and the highest is 18%-27%. That’s a massive difference.</p>
<p>Looking at the chart, below are generally the best days &amp; times to post to Hacker News:</p>
<ul>
<li><strong>Monday:</strong> 1 am—4 pm UTC (6 pm—9 am PDT)</li>
<li><strong>Tuesday, Wednesday &amp; Thursday:</strong> 12 am—6 am &amp; 9 am—12 pm UTC (5 pm—11 pm &amp; 2 am—5 am PDT)</li>
<li><strong>Friday:</strong> 5 am—12 pm UTC (10 pm—5 am PDT)</li>
<li><strong>Saturday:</strong> 4 am—9 am UTC (9 pm—2 am PDT)</li>
<li><strong>Sunday:</strong> 3 am—8 am &amp; 10 am—6 pm UTC (8 pm—1 am &amp; 3 am—11 am PDT)</li>
</ul>
<p>And here are the best times specifically (bear in mind these may not be the case week-in-week-out because the amount of data limits the results):</p>
<ul>
<li><strong>Monday:</strong> 12 pm UTC (5 am PDT)</li>
<li><strong>Tuesday:</strong> 9 am UTC (2 am PDT)</li>
<li><strong>Wednesday:</strong> 10 am UTC (3 am PDT)</li>
<li><strong>Thursday:</strong> 12 pm UTC (5 pm PDT)</li>
<li><strong>Friday:</strong> 8 am UTC (1 am PDT)</li>
<li><strong>Saturday:</strong> 9 am UTC (2 am PDT)</li>
<li><strong>Sunday:</strong> 5 am OR 1 pm (10 pm OR 6 am PDT)</li>
</ul>
<p><strong>Takeaway:</strong> It’s pretty clear that the best times to post on Hacker News to reach the front page are usually around 6 am UTC (11 pm PDT), 9 am UTC (2 am PDT), or 12 pm UTC (5 am PDT), when the US is asleep.</p>
<p>However, given that 8 hours is the average time spent on the front page, it’s probably best to post at 12 pm UTC on Monday, Thursday, Saturday or Sunday to maximize traffic for when the US wakes up.</p>
<h3 id="how-many-votes-does-a-post-need-to-reach-the-front-page-of-hacker-news-in-2020"><a href="#how-many-votes-does-a-post-need-to-reach-the-front-page-of-hacker-news-in-2020" aria-label="how many votes does a post need to reach the front page of hacker news in 2020 permalink"></a>How many votes does a post need to reach the front page of Hacker News in 2020?</h3>
<p>5.22 is the average number of votes a post needed within an average time of 27.1 minutes to reach the front page for the first time.</p>
<p>Below is a chart of box plots to illustrate the variation of votes required over 24 hours.</p>

<p>Interestingly, this graph clearly shows every post, regardless of time, required at least three upvotes, and for 75% of posts, they never needed more than eight votes to reach the front page of Hacker News.</p>
<p>Expectedly, the range of upvotes required is lower for the first half of the day than the second half, roughly matching up to the timings above.</p>
<p>In some cases, posts required a lot more votes to reach the front page. Presumably, because either the time it took to get these votes was over more time, or there was a lot of competition.</p>
<p><strong>Takeaway:</strong> The votes required to be seen on the front page is relatively small. For the best chance of getting on the front page, you’ll need between 4-6 votes in about 30 minutes, posting in the first half of the day (12 am—12pm UTC).</p>
<h3 id="how-much-karma-do-you-need-to-reach-the-front-page-of-hacker-news-in-2020"><a href="#how-much-karma-do-you-need-to-reach-the-front-page-of-hacker-news-in-2020" aria-label="how much karma do you need to reach the front page of hacker news in 2020 permalink"></a>How much karma do you need to reach the front page of Hacker News in 2020?</h3>
<p>Finally, I thought it would be interesting to see the relationship between karma and reaching the front page of Hacker News.</p>
<p>This first pie chart demonstrates the proportion of posts being posted by users with varying levels of karma.</p>

<p>Surprisingly, over 50% of posts are submitted by users with over 1,000 karma points. However, the largest single segment is by users with little to no karma.</p>

<p>The chart above shows that newer users (0-50 karma) make the front page proportionally less than more experienced users. Interestingly, really experienced users (over 10,000 karma) reached the front page significantly more than experienced users.</p>
<p><strong>Takeaway:</strong> It’s hard to say whether karma has anything to do with rankings. Likely, users with more karma are just better at posting content that resonates with the Hacker News audience.</p></div></div>]]>
            </description>
            <link>https://www.ankle.io/posts/hacker-news-analysis</link>
            <guid isPermaLink="false">hacker-news-small-sites-24634991</guid>
            <pubDate>Wed, 30 Sep 2020 01:43:17 GMT</pubDate>
        </item>
    </channel>
</rss>
