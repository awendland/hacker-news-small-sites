<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sun, 17 Jan 2021 09:05:08 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sun, 17 Jan 2021 09:05:08 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Beating Up on Qsort (2019)]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25783617">thread link</a>) | @tjalfi
<br/>
January 14, 2021 | https://travisdowns.github.io/blog/2019/05/22/sorting.html | <a href="https://web.archive.org/web/*/https://travisdowns.github.io/blog/2019/05/22/sorting.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Recently, Daniel Lemire <a href="https://lemire.me/blog/2019/05/07/almost-picking-n-distinct-numbers-at-random/">tackled the topic</a> of selecting N <em>distinct</em> numbers at random. In the case we want sorted output, an obvious solution presents itself: sorting randomly chosen values and de-duplicating the list, which is easy since identical values are now adjacent.<sup id="fnref:distinct" role="doc-noteref"><a href="#fn:distinct">1</a></sup></p>

<p>While Daniel suggests a clever method of avoiding a sort entirely<sup id="fnref:danmethod" role="doc-noteref"><a href="#fn:danmethod">2</a></sup>, I’m also interested in they <em>why</em> for the underlying performace of the sort method: it takes more than 100 ns per element, which means 100s of CPU clock cycles and usually even more instructions than that (on a superscalar processor)! As a sanity check, a quick benchmark (<code>perf record ./bench &amp;&amp; perf report</code>) shows that more than 90% of the time spent in this approach is in the sorting routine, <a href="https://devdocs.io/c/algorithm/qsort">qsort</a> - so we are right to focus on this step, rather than say the de-duplication step or the initial random number generation. This naturally, this raises the question: how fast is qsort when it comes to sorting integers and can we do better?</p>

<p>All of the code for this post <a href="https://github.com/travisdowns/sort-bench">is available on GitHub</a>, so if you’d like to follow along with the code open in an editor, go right ahead (warning: there are obviously some spoilers if you dig through the code first).</p>

<h2 id="benchmarking-qsort">Benchmarking Qsort</h2>

<p>First, let’s take a look at what <code>qsort</code> is doing, to see if there is any delicous low-hanging performance fruit. We use <code>perf record ./bench qsort</code> to capture profiling data, and <code>perf report --stdio</code> to print a summary<sup id="fnref:long-tail" role="doc-noteref"><a href="#fn:long-tail">3</a></sup>:</p>

<div><div><pre><code># Samples: 101K of event 'cycles:ppp'
# Event count (approx.): 65312285835
#
# Overhead  Command  Shared Object      Symbol
# ........  .......  .................  ..............................................
#
    64.90%  bench    libc-2.23.so       [.] msort_with_tmp.part.0
    21.45%  bench    bench              [.] compare_uint64_t
     8.65%  bench    libc-2.23.so       [.] __memcpy_sse2
     0.87%  bench    libc-2.23.so       [.] __memcpy_avx_unaligned
     0.83%  bench    bench              [.] main
     0.41%  bench    [kernel.kallsyms]  [k] clear_page_erms
     0.34%  bench    [kernel.kallsyms]  [k] native_irq_return_iret
     0.31%  bench    bench              [.] bench_one
</code></pre></div></div>

<p>The assembly for the biggest offender, <code>msort_with_tmp</code> looks like this<sup id="fnref:annotate-command" role="doc-noteref"><a href="#fn:annotate-command">4</a></sup> :</p>

<div><div><pre><code> Percent | Address      | Disassembly
--------------------------------------------------
   30.55 :   39200:       mov    rax,QWORD PTR [r15]
    0.61 :   39203:       sub    rbp,0x1
    0.52 :   39207:       add    r15,0x8
    7.30 :   3920b:       mov    QWORD PTR [rbx],rax
    0.39 :   3920e:       add    rbx,0x8
    0.07 :   39212:       test   r12,r12
    0.09 :   39215:       je     390e0   ; merge finished
    1.11 :   3921b:       test   rbp,rbp
    0.01 :   3921e:       je     390e0   ; merge finished
    5.24 :   39224:       mov    rdx,QWORD PTR [rsp+0x8]
    0.42 :   39229:       mov    rsi,r15
    0.19 :   3922c:       mov    rdi,r13
    6.08 :   3922f:       call   r14
    0.59 :   39232:       test   eax,eax
    3.52 :   39234:       jg     39200
   32.69 :   39236:       mov    rax,QWORD PTR [r13+0x0]
    1.31 :   3923a:       sub    r12,0x1
    1.01 :   3923e:       add    r13,0x8
    1.09 :   39242:       jmp    3920b &lt;bsearch@@GLIBC_2.2.5+0x205b&gt;
</code></pre></div></div>

<p>Depending on your level of assembly reading skill, it may not be obvious, but this is basically a classic merge routine: it is merging two lists by comparing the top elements of each list (pointed to by <code>r13</code> and <code>r15</code>), and then storing the smaller element (the line <code>QWORD PTR [rbx],rax</code>) and loading the next element from that list. There are also two checks for termination (<code>test   r12,r12</code> and <code>test   rbp,rbp</code>). This hot loop corresponds directly to this code from <code>glibc</code> (from the file<code>msort.c</code><sup id="fnref:msort-note" role="doc-noteref"><a href="#fn:msort-note">5</a></sup>) :</p>

<div><div><pre><code><span>while</span> <span>(</span><span>n1</span> <span>&gt;</span> <span>0</span> <span>&amp;&amp;</span> <span>n2</span> <span>&gt;</span> <span>0</span><span>)</span>
<span>{</span>
    <span>if</span> <span>((</span><span>*</span><span>cmp</span><span>)</span> <span>(</span><span>b1</span><span>,</span> <span>b2</span><span>,</span> <span>arg</span><span>)</span> <span>&lt;=</span> <span>0</span><span>)</span>
    <span>{</span>
        <span>*</span><span>(</span><span>uint64_t</span> <span>*</span><span>)</span> <span>tmp</span> <span>=</span> <span>*</span><span>(</span><span>uint64_t</span> <span>*</span><span>)</span> <span>b1</span><span>;</span>
        <span>b1</span> <span>+=</span> <span>sizeof</span> <span>(</span><span>uint64_t</span><span>);</span>
        <span>--</span><span>n1</span><span>;</span>
    <span>}</span>
    <span>else</span>
    <span>{</span>
        <span>*</span><span>(</span><span>uint64_t</span> <span>*</span><span>)</span> <span>tmp</span> <span>=</span> <span>*</span><span>(</span><span>uint64_t</span> <span>*</span><span>)</span> <span>b2</span><span>;</span>
        <span>b2</span> <span>+=</span> <span>sizeof</span> <span>(</span><span>uint64_t</span><span>);</span>
        <span>--</span><span>n2</span><span>;</span>
    <span>}</span>
    <span>tmp</span> <span>+=</span> <span>sizeof</span> <span>(</span><span>uint64_t</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>This loop suffers heavily from branch mispredictions, since the “which element is larger” branch is highly unpredictable (at least for random-looking input data). Indeed, we see roughly 128 million mispredicts while sorting ~11 million elements: close to 12 mispredicts per element.</p>

<p>We also note the presence of the indirect call at the <code>call r14</code> line. This corresponds to the <code>(*cmp) (b1, b2, arg)</code> expression in the source: it is calling the user provided comparator function through a function pointer. Since the <code>qsort()</code> code is compiled ahead of time and is found inside the shared libc binary, there is no chance that the comparator, passed as a function pointer, can be inlined.</p>

<p>The comparator function I provide looks like:</p>

<div><div><pre><code><span>int</span> <span>compare_uint64_t</span><span>(</span><span>const</span> <span>void</span> <span>*</span><span>l_</span><span>,</span> <span>const</span> <span>void</span> <span>*</span><span>r_</span><span>)</span> <span>{</span>
    <span>uint64_t</span> <span>l</span> <span>=</span> <span>*</span><span>(</span><span>const</span> <span>uint64_t</span> <span>*</span><span>)</span><span>l_</span><span>;</span>
    <span>uint64_t</span> <span>r</span> <span>=</span> <span>*</span><span>(</span><span>const</span> <span>uint64_t</span> <span>*</span><span>)</span><span>r_</span><span>;</span>
    <span>if</span> <span>(</span><span>l</span> <span>&lt;</span> <span>r</span><span>)</span> <span>return</span> <span>-</span><span>1</span><span>;</span>
    <span>if</span> <span>(</span><span>l</span> <span>&gt;</span> <span>r</span><span>)</span> <span>return</span>  <span>1</span><span>;</span>
    <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>which on gcc compiles to branch-free code:</p>

<div><div><pre><code>mov    rax,QWORD PTR [rsi]
mov    edx,0xffffffff
cmp    QWORD PTR [rdi],rax
seta   al
movzx  eax,al
cmovb  eax,edx
ret
</code></pre></div></div>

<p>Note that the comparator has to redundantly load from memory the two locations to compare, something the merge loop already did (the merge loop reads them because it is responsible for moving the elements).</p>

<p>How much better could things get if we inline the comparator into the merge loop? That’s what we do in <code>qsort-inlined</code><sup id="fnref:inline-hard" role="doc-noteref"><a href="#fn:inline-hard">6</a></sup>, and here’s the main loop which now includes the comparator function<sup id="fnref:cmdline1" role="doc-noteref"><a href="#fn:cmdline1">7</a></sup> :</p>

<pre><code> 0.07 :   401dc8:       test   rbp,rbp
 0.66 :   401dcb:       je     401e0c &lt;void msort_with_tmp&lt;CompareU64&gt;(msort_param const*, void*, unsigned long, CompareU64)+0xbc&gt;
 3.51 :   401dcd:       mov    rax,QWORD PTR [r9]
 5.00 :   401dd0:       lea    rdx,[rbx+0x8]
 1.62 :   401dd4:       mov    rcx,QWORD PTR [rbx]
 0.24 :   401dd7:       lea    r8,[r9+0x8]
 6.96 :   401ddb:       cmp    rax,rcx
20.83 :   401dde:       cmovbe r9,r8
 8.88 :   401de2:       cmova  rbx,rdx
 0.27 :   401de6:       cmp    rcx,rax
 6.23 :   401de9:       sbb    r8,r8
 0.74 :   401dec:       cmp    rcx,rax
 4.93 :   401def:       sbb    rdx,rdx
 0.24 :   401df2:       not    r8
 6.69 :   401df5:       add    rbp,rdx
 0.44 :   401df8:       cmp    rax,rcx
 5.34 :   401dfb:       cmova  rax,rcx
 5.96 :   401dff:       add    rdi,0x8
 7.48 :   401e03:       mov    QWORD PTR [rdi-0x8],rax
 0.00 :   401e07:       add    r15,r8
 0.71 :   401e0a:       jne    401dc8 &lt;void msort_with_tmp&lt;CompareU64&gt;(msort_param const*, void*, unsigned long, CompareU64)+0x78&gt;
</code></pre>

<p>A key difference is that the core of the loop is now branch free. Yes, there are still two conditional jumps, but they are both just checking for the termination condition (that one of the lists to merge is exhausted), so we expect this loop to be free of branch mispredictions other than the final iteration. Indeed, we measure with <code>perf stat</code> that the misprediction rate has dropped from to close to 12 mispredicts per element to around 0.75 per element. The loop has only two loads and one store, so the memory access redundancy between the merge code and the comparator has been eliminated<sup id="fnref:load-redundancy" role="doc-noteref"><a href="#fn:load-redundancy">8</a></sup>. Finally, the comparator does a three-way compare (returning distrinct results for <code>&lt;</code>, <code>&gt;</code> and <code>==</code>), but the merge code only needs a two-way compare (<code>&lt;=</code> or <code>&gt;</code>) - inlining the comparator manages to remove extra code associated with distinguishing the <code>&lt;</code> and <code>==</code> cases.</p>

<p>What’s the payoff? It’s pretty big:</p>

<p><img src="https://travisdowns.github.io/assets/2019-05-22/fig2.svg" alt="Effect of comparator inlining"></p>

<p>The speedup hovers right around 1.77x. Note that this is much larger than simply eliminating all the time spent in the separate comparator function in the original version (about 17% of the time implying a speedup of 1.2x if all the function time disapeared). This is a good example of how inlining isn’t just about removing function call overhead but enabling further <em>knock on</em> optimizations which can have a much larger effect than just removing the overhead associated with function calls.</p>

<h2 id="what-about-c">What about C++?</h2>

<p>Short of copying the existing glibc (note: LGPL licenced) sorting code to allow inlining, what else can we do to speed things up? I’m writing in C++, so how about the C++ sort functions available in the <code>&lt;algorithm&gt;</code> header? Unlike C’s <code>qsort</code> which is generic by virtue of taking a function pointer and information about the object size, the C++ sort functions use templates to achieve genericity and so are implemented directly in header files. Since the sort code and the comparator are being compiler together, we expect the comparator to be easily inlined, and perhaps other optimizations may occur.</p>

<p>Without further ado, let’s just throw <code>std::sort</code>, <code>std::stable_sort</code> and <code>std::partial_sort</code> into the mix:</p>

<p><img src="https://travisdowns.github.io/assets/2019-05-22/fig3.svg" alt="C vs C++ sort functions"></p>

<p>The C++ sort functions, other than perhaps <code>std::partial_sort</code><sup id="fnref:partial-sort" role="doc-noteref"><a href="#fn:partial-sort">9</a></sup>, put in a good showing. It is interesting that <code>std::stable_sort</code> which has <em>stricly more requirements</em> on its implementation than <code>std::sort</code> (i.e., any stable sort is also suitable for <code>std::sort</code>) ends up faster. I re-wrote this paragaph several times, since sometimes after a reboot <code>stable_sort</code> was slower and sometimes it was faster (as shown above). When it was “fast” it had less than 2% branch mispredictions, and when it was slow it was at 15%. So perhaps there was some type of aliasing issue in the branch predictor which depends on the physical addresses assigned, which can vary from run to run, I’m not sure. See <sup id="fnref:stablesort" role="doc-noteref"><a href="#fn:stablesort">10</a></sup> for an old note from when <code>std::stable_sort</code> was slower.</p>

<h2 id="can-we-do-better">Can we do better?</h2>

<p>So that’s as fast as it gets, right? We aren’t going to beat <code>std::sort</code> or <code>std::stable_sort</code> without a huge amount of effort, I think? After all, these are presumably highly optimized sorting routines written by the standard library implementors. Sure, we might expect to be able to beat <code>qsort()</code>, but that’s mostly because of built-in disadvantages that <code>qsort</code> has, lacking the ability to inline the comparator, etc.</p>

<h3 id="radix-sort-attempt-1">Radix Sort Attempt 1</h3>

<p>Well, one thing we can try is a non-comparison sort. We know we have integer keys, so why stick to comparing …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://travisdowns.github.io/blog/2019/05/22/sorting.html">https://travisdowns.github.io/blog/2019/05/22/sorting.html</a></em></p>]]>
            </description>
            <link>https://travisdowns.github.io/blog/2019/05/22/sorting.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25783617</guid>
            <pubDate>Thu, 14 Jan 2021 22:25:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elasticsearch and Kibana are now business risks]]>
            </title>
            <description>
<![CDATA[
Score 81 | Comments 21 (<a href="https://news.ycombinator.com/item?id=25781695">thread link</a>) | @vmbrasseur
<br/>
January 14, 2021 | https://anonymoushash.vmbrasseur.com/2021/01/14/elasticsearch-and-kibana-are-now-business-risks | <a href="https://web.archive.org/web/*/https://anonymoushash.vmbrasseur.com/2021/01/14/elasticsearch-and-kibana-are-now-business-risks">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
        <header>
          
          
          <p><strong> </strong> <time datetime="2021-01-14T00:00:00-08:00">January 14, 2021</time></p>
          
          
            <p> 




  4 minute read

</p>
          
        </header>
      

      <section itemprop="text">
        
        <p>In a play to convert users of their open source projects into paying customers, today Elastic announced that they are <a href="https://www.elastic.co/blog/licensing-change">changing the license</a> of both Elasticsearch and Kibana from the open source Apache v2 license to <a href="https://www.mongodb.com/licensing/server-side-public-license">Server Side Public License</a> (SSPL). If your organisation uses the open source versions of either Elasticsearch or Kibana in its products or projects, it is now at risk of being forced to release its intellectual property under terms dictated by another.</p>

<p>If you’re not yet aware of the SSPL, you can catch up <a href="https://mjg59.dreamwidth.org/51230.html">here</a>. As licenses go, it’s pretty problematic from a business perspective. Every <a href="https://en.wikipedia.org/wiki/Intellectual_property">IP lawyer</a> to whom I’ve showed the text of the SSPL has been rather alarmed before they even reach the end of it. Basically, it’s a hostile proprietary license masquerading in open source clothing. By using an SSPL project in your code, you are agreeing that if you provide an online service using that code then you will release not only that code but also the code for every supporting piece of software, all under the SSPL. It’s not a stretch to interpret the wording of the license as requiring users of the SSPL’d software therefore to release the code for everything straight down to the bare metal. There are those who will point to <a href="https://www.mongodb.com/licensing/server-side-public-license/faq">the FAQ</a> for the SSPL and claim that the license isn’t interpreted in that way because the FAQ says so. Unfortunately, when you agree to a license you are agreeing to the <em>text of that license document</em> and not to a FAQ. If the text of that license document is ambiguous, then so are your rights and responsibilities under that license. Should your compliance to that license come before a judge, it’s <em>their</em> interpretation of those rights and responsibilities that will hold sway. This ambiguity puts your organisation at risk.</p>

<p>In <a href="https://www.elastic.co/blog/licensing-change">its announcement</a>, Elastic claims that this is simply a change of open source license. In one way they’re correct: they’re changing the license away from the open source Apache v2 license. However they are changing to what can best be described as a proprietary source available license, <em>not</em> to an open source one. MongoDB, the originators of SSPL, requested that <a href="https://opensource.org/">Open Source Initiative</a> (OSI) (the standards body that maintains the <a href="https://opensource.org/osd-annotated">Open Source Definition</a> and certifies licenses as open source) certify the SSPL as such. After a great deal of discussion among the panel of legal, licensing, and open source experts, MongoDB withdrew the SSPL from consideration as an open source license, as it appeared highly unlikely it would be certified as open source. That SSPL is not an open source license is no longer in dispute. That ship has sailed. If you have a problem with this, I suggest you <a href="https://lists.opensource.org/pipermail/license-discuss_lists.opensource.org/2019-May/020483.html">take it up</a> with OSI. As for Elastic’s public and verifiably false claim that SSPL is an open source license, it’s my hope that OSI will have a conversation with them and make a public statement of their own shortly.</p>

<p>No, this is a business decision, not an ideological one. Elastic made a business decision to change to this hostile proprietary license to give them a way to <del>extort</del>influence users to become customers. Without a great deal more strategic information about Elastic’s business and operations none of us are qualified to judge whether it’s the correct decision, but the decision itself is valid. They are allowed to make this sort of strategic move for their company.</p>

<p>However, you and your organisation have now also been forced into a business decision. If your organisation uses the Apache v2 licensed Elasticsearch or Kibana in its projects or products, it must now assume that it is at risk one way or another. It can upgrade to version 7.11 of these projects, thereby accepting the terms of SSPL and potentially also being required to release the code for its entire stack (a great deal of which it will not have the copyright over and will be unable to release, thereby potentially being in violation of SSPL). It can remain on version 7.10, but then it will no longer receive future updates, including important security fixes, thereby taking on another sort of risk. It could choose to pay for a Gold+ license for the software, but it’s unlikely that the budget is prepared for this sort of unexpected expense. And finally it can rearchitect its project or product, replacing Elasticsearch and/or Kibana with alternatives. Frankly, considering today’s unfriendly move by Elastic, putting some space between it and your organisation may be the safest alternative in the long run, but it will come with its own considerable price tag in time and other potential opportunity and switching costs.</p>

<p>The one thing your organisation cannot afford to do is ignore this. It’s time to call a meeting with your legal, software development, product, finance, and strategy teams to start to figure out the best option for you.</p>

<blockquote>
  <p>For more information on relicensing moves like this, please see <a href="https://anonymoushash.vmbrasseur.com/2019/06/07/the-problem-with-amazon-and-open-source-isnt-amazon/">The problem with Amazon and Open Source isn’t Amazon</a>.</p>
</blockquote>

<hr>

<blockquote>
  <p>Judging from the bandwidth usage stats on my hosting service, people seem to appreciate this post. Thank you for that. If you’d like me to provide corporate open source strategy for your company, please <a href="https://www.vmbrasseur.com/about/#contact">drop me an email</a>. I’ll soon be kicking my job search into high gear after <a href="https://anonymoushash.vmbrasseur.com/2020/06/01/farewell-juniper">Juniper laid off its open source team</a> last year. Contacting me now makes it more likely your company will be in consideration for my next role.</p>
</blockquote>

        
      </section>

      

      


      
  

    </div></div>]]>
            </description>
            <link>https://anonymoushash.vmbrasseur.com/2021/01/14/elasticsearch-and-kibana-are-now-business-risks</link>
            <guid isPermaLink="false">hacker-news-small-sites-25781695</guid>
            <pubDate>Thu, 14 Jan 2021 20:02:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[U.S. jobless numbers surge as worsening Covid-19 pandemic hurts businesses]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 85 (<a href="https://news.ycombinator.com/item?id=25778289">thread link</a>) | @heyheyheysome
<br/>
January 14, 2021 | https://www.cbc.ca/news/business/us-jobless-numbers-1.5872817 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/business/us-jobless-numbers-1.5872817">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The number of Americans filing first-time applications for unemployment benefits surged last week, confirming a weakening in labour market conditions as a worsening COVID-19 pandemic disrupts operations at restaurants and other businesses.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5872837.1610635730!/fileImage/httpImage/image.JPG_gen/derivatives/16x9_780/usa-smallbusiness-ppp.JPG"></p></div><figcaption>A woman walks past a business that is closing in New York City in August. Initial claims for state unemployment benefits totalled a seasonally adjusted 965,000 for the week ended Jan. 9, compared to 784,000 in the prior week, the U.S. Labour Department said on Thursday.<!-- --> <!-- -->(Carlo Allegri/Reuters)</figcaption></figure><p><span><p>The number of Americans filing first-time applications for unemployment benefits surged last week, confirming a weakening in labour market conditions as a worsening COVID-19 pandemic disrupts operations at restaurants and other businesses.</p>  <p>Initial claims for state unemployment benefits totalled a seasonally adjusted 965,000 for the week ended Jan. 9, compared to 784,000 in the prior week, the U.S. Labour Department said on Thursday. Economists polled by Reuters had forecast 795,000 applications in the latest week.</p>  <p>It's the highest number since late August.&nbsp;Applications declined over the summer but have been stuck above 700,000 since September.</p>  <p>Claims were also likely lifted by re-applications for benefits following the government's renewal of a $300 US unemployment supplement until March 14 as part of nearly $900 billion in additional relief approved at the end of December.</p>    <p>Government-funded programs for the self-employed, gig workers and others who do not qualify for the state unemployment programs as well as those who have exhausted their benefits were also extended.</p>  <p>Authorities in many states have banned indoor dining to slow the spread of the coronavirus. The economy shed jobs in December for the first time in eight months.</p>  <p>The Federal Reserve's Beige Book report of anecdotal information on business activity collected from contacts nationwide in early January showed on Wednesday that "contacts in the leisure and hospitality sectors reported renewed employment cuts due to stricter containment measures."</p>  <p>The central bank also noted that the resurgence in the coronavirus was causing staff shortages in the manufacturing, construction and transportation&nbsp;sectors.</p>  <h2>Most infections of any country</h2>  <p>The virus has infected more than 22.5 million people in the United States and killed over 376,188, the most of any country.&nbsp;More than 4,300 deaths were reported Tuesday, a&nbsp;record high.</p>  <p>Though jobless claims have dropped from a record 6.867 million in March, they remain above their 665,000 peak during the 2007-09 Great Recession. Economists say it could take several years for the labour market to recover from the pandemic.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5859227.1609502755!/fileImage/httpImage/image.JPG_gen/derivatives/original_300/health-coronavirus-usa-florida.JPG 300w,https://i.cbc.ca/1.5859227.1609502755!/fileImage/httpImage/image.JPG_gen/derivatives/original_460/health-coronavirus-usa-florida.JPG 460w,https://i.cbc.ca/1.5859227.1609502755!/fileImage/httpImage/image.JPG_gen/derivatives/original_620/health-coronavirus-usa-florida.JPG 620w,https://i.cbc.ca/1.5859227.1609502755!/fileImage/httpImage/image.JPG_gen/derivatives/original_780/health-coronavirus-usa-florida.JPG 780w,https://i.cbc.ca/1.5859227.1609502755!/fileImage/httpImage/image.JPG_gen/derivatives/original_1180/health-coronavirus-usa-florida.JPG 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5859227.1609502755!/fileImage/httpImage/image.JPG_gen/derivatives/original_780/health-coronavirus-usa-florida.JPG"></p></div><figcaption>Hundreds wait in line to receive the COVID-19 vaccine in Fort Myers, Fla., in late December. Economists are hopeful the economy will turn around in late 2021.<!-- --> <!-- -->(Andrew West/The News-Press/USA Today Network/Reuters)</figcaption></figure></span></p>  <p>"While prospects for the economy later in 2021 are upbeat, the labour market recovery has taken a step backward," said Nancy Vanden Houten, an economist at Oxford Economics, "and we expect claims to remain elevated, with the risk that they rise from last week's levels."</p>  <p>Last week's applications for aid might have been elevated in part because state employment offices had been closed over the holidays, requiring some jobless people to wait until last week to apply.&nbsp;</p>  <h2>5.3 million Americans receiving jobless benefits</h2>  <p>In addition to last week's first-time applications for unemployment aid, the government said Thursday that 5.3 million Americans are continuing to receive state jobless benefits, up from 5.1 million in the previous week. It suggests that fewer people who are out of work are finding jobs.</p>  <p>About 11.6 million people received jobless aid from two federal programs in the week that ended Dec. 26, the latest period for which data is available. One of those programs provides extended benefits to people who have exhausted their state aid. The other supplies benefits to self-employed and contract workers.</p>  <p>Those two programs had expired near the end of December. They were belatedly renewed, through mid-March, in the $900-billion rescue aid package that Congress approved and President Donald Trump signed into law. That legislation also included $600 relief cheques&nbsp;for most adults and a supplemental unemployment benefit payment of $300 a week. Congressional Democrats favour boosting the cheques&nbsp;to $2,000 and extending federal aid beyond March, as does president-elect Joe Biden.</p>    <p>The U.S. job market's weakness was made painfully clear in the December employment report that the government issued last week. Employers shed jobs for the first time since April as the pandemic tightened its grip on consumers and businesses.</p>  <p>The figures also depicted a sharply uneven job market: The losses last month were concentrated among restaurants, bars, hotels and entertainment venues. Educational services, mostly colleges and universities, also cut workers in December. So did film and music studios.</p>  <p>Most other large industries, though, reported job gains. Many economists had expected last spring that job losses would spread to more industries. Though all sectors of the economy initially laid off workers, most of them have avoided deep job cuts. Manufacturing, construction, and professional services like engineering and architecture, for example, all added jobs in December.</p>  <p>At the same time, many companies seem reluctant to sharply ramp up hiring. A government report Tuesday showed that employers advertised fewer open jobs in November than in October. The decline, while small, was widespread across most industries.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/business/us-jobless-numbers-1.5872817</link>
            <guid isPermaLink="false">hacker-news-small-sites-25778289</guid>
            <pubDate>Thu, 14 Jan 2021 16:28:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Licensing changes to Elasticsearch and Kibana]]>
            </title>
            <description>
<![CDATA[
Score 266 | Comments 317 (<a href="https://news.ycombinator.com/item?id=25776657">thread link</a>) | @sl_
<br/>
January 14, 2021 | https://www.elastic.co/blog/licensing-change | <a href="https://web.archive.org/web/*/https://www.elastic.co/blog/licensing-change">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><div><h2>Upcoming licensing changes to Elasticsearch and Kibana</h2><p>We are moving our Apache 2.0-licensed source code in Elasticsearch and Kibana to be dual licensed under Server Side Public License (SSPL) and the Elastic License, giving users the choice of which license to apply. This license change ensures our community and customers have free and open access to use, modify, redistribute, and collaborate on the code. It also protects our continued investment in developing products that we distribute for free and in the open by restricting cloud service providers from offering Elasticsearch and Kibana as a service without contributing back. This will apply to all maintained branches of these two products and will take place before our upcoming 7.11 release. Our releases will continue to be under the Elastic License as they have been for the last three years.
</p><p>This change in source code licensing has <b>no impact on the overwhelming majority of our user community</b> who use our default distribution for free. It also has <b>no impact on our cloud customers or self-managed software customers</b>.</p><p>In recent years, the market has evolved, and the community has come to appreciate that open source companies need to better protect their software to continue to innovate and make the investments required. As companies continue the shift to SaaS offerings, some cloud service providers have taken open source products and provided them as a service without investing back into the community. Moving to the dual license strategy with <a href="https://www.mongodb.com/licensing/server-side-public-license">SSPL</a> or the Elastic License is a natural next step for us after opening our commercial code and creating a free tier, all under the Elastic License, nearly 3 years ago. It is similar to those made by many other open source companies over these years, including <a href="https://www.mongodb.com/licensing/server-side-public-license/faq">MongoDB</a>, which developed the SSPL. The SSPL allows free and unrestricted use, as well as modification, with the simple requirement that if you provide the product as a service, you must also publicly release any modifications as well as the source code of your management layers  under SSPL.
</p><h2>Our open origins</h2><p>My personal journey with open source goes a long way back. In 2005, I open sourced my first project, Compass, to provide a Java framework on top of Apache Lucene while I was building a recipe app for my wife. In the following five years, I invested many weekends and nights working on it, from writing code to helping users with bugs, features, and questions.
</p><p>I had no idea what I was signing up for, especially with a day job “on the side,” but I fell in love with the opportunity to make such a positive impact — trying to build a great product, but more importantly, a great community around it, through the power of open source.
</p><p>In 2009, I decided to do it again, and started to write a brand new project called Elasticsearch. I spent many nights and weekends building it, and in 2010 open sourced it. I even quit my job and decided to dedicate my full attention to it. To be there for the users, through writing code, and engaging on GitHub, mailing lists, and IRC.
</p><p>And when we founded Elastic as a company in 2012, we brought the same spirit to our company. We invested heavily in our free and open products, and supported the rapid growth of our community of users. We expanded from just Elasticsearch to Kibana, Logstash, Beats, and now a complete set of solutions built into the Elastic Stack: Elastic Enterprise Search, Observability, and Security.
</p><p>We have matured the products, fostered vibrant communities around them, and focused on providing the greatest amount of value to our users. Today, we have hundreds of engineers who wake up every day and work to make our products even better. And we have hundreds of thousands of community members who engage with us and contribute to our shared success.
</p><p>I am proud of the company we built, and humbled by the level of trust that we have earned with our user base. This starts by being open and transparent, and continues with being true to our community and user base in our choices.
</p><h2>Free and open FTW</h2><p>Back in 2018, we <a href="https://www.elastic.co/blog/doubling-down-on-open">opened the code of our free and paid proprietary features</a> under the Elastic License, a source-available license, and we changed our default distribution to include all of our features, with all free features enabled by default.
</p><p>We did this for a few reasons. It allowed us to engage with our paying customers in the same way we engage with our community: in the open. It also allowed us to build free features that empower our users without providing those capabilities to companies that take our products and provide them as a service, like Amazon Elasticsearch Service, and profit from our open source software without contributing back.
</p><p>This approach was well received — today, over 90% of new downloads choose this distribution — and has allowed us to make so much of our work available for free while also building a successful company.
</p><p>The list of improvements under this new free and open, yet proprietary, license, is overwhelming. I am humbled by the amazing progress our team and community has made across all our products, so much so that I would love to share some of them:
</p><p>We've dramatically improved the speed, scalability, and reliability of Elasticsearch, with a new distributed consensus algorithm and significantly reduced memory usage, in addition to new data storage and compression approaches that have reduced the typical index size by nearly 40% while improving indexing and query throughput. We added new field types for geospatial analysis, and more efficient ways to store and search logs and perform fast, case-insensitive search on security data. In Kibana, we cut load time by 80% and eliminated whole-page refreshes thanks to a multiyear replatforming project, while at the same time introducing an intuitive drag-and-drop data visualization experience with Kibana Lens, key capabilities like dashboard drill-downs, and so much more.
</p><p>Over the last three years, we also built first-class experiences around our most common use cases. In the security area, we created a free and open SIEM right inside Kibana, with a powerful detection engine that supports simple rules as well as complex correlations via a new query language called EQL in Elasticsearch. We include hundreds of detection rules, which we develop publicly, in collaboration with our community. And we joined forces with Endgame, a leading endpoint security company, and have released powerful malware protection for free as part of the Elastic Agent, our unified, centrally managed observability and security agent for servers and endpoints, with more to come.
</p><p>In observability, the story is similar. We've built an entire observability suite right inside Kibana — from a live-tail logging UI to an intuitive infrastructure-level view of the key metrics and alerts across your hosts, pods, and containers. And we now have a fully featured APM product with open source data collectors and agents, supporting OpenTelemetry, real user monitoring (RUM), synthetic monitoring, and the recent addition of user experience monitoring.
</p><p>With Elastic Enterprise Search, we introduced App Search, a layer on top of Elasticsearch that simplifies building rich applications and provides powerful management interfaces for relevance tuning, as well as analytics on how it's being used. We also provide a free Workplace Search product that makes it easy to integrate and search the content sources that you use to run your life or company, like Google Workplace, Microsoft 365, Atlassian Jira and Confluence, and Salesforce.
</p><p>It is simply amazing that we've been able to build all of these capabilities and provide them for free to our community. It has been humbling to see the level of engagement and adoption around our products and how these new features have helped so many people and businesses succeed. And this was possible because the overwhelming majority of our community chose our default distribution under the Elastic License, where all these features are free and open.
</p><h2>Why change?</h2><p>As previously mentioned, over the last three years, the market has evolved and the community has come to appreciate that open source companies need to better protect their software in order to maintain a high level of investment and innovation. With the shift to SaaS as a delivery model, some cloud service providers have taken advantage of open source products by providing them as a service, without contributing back. This diverts funds that would have been reinvested into the product and hurts users and the community.
</p><p>Similar to our open source peers, we have lived this experience firsthand, from our trademarks being misused to outright attempts to splinter our community with “open” repackaging of our OSS products or even taking “inspiration” from our proprietary code. While each open source company has taken a slightly different approach to address this issue, they have generally modified their open source license in order to protect their investment in free software, while trying to preserve the principles of openness, transparency, and collaboration. Similarly, we are taking the natural next step of making a targeted change to how we license our source code. This change won't affect the vast majority of our users, but it will restrict cloud service providers from offering our software as a service.
</p><p>We expect that a few of our competitors will attempt to spread all kinds of FUD around this change. Let me be clear to any naysayers. We believe deeply in the principles of free and open products, and of transparency with the community. Our track record speaks to this commitment, and we will continue to build upon it.
</p><h2>The change</h2><p>Starting with the upcoming Elastic 7.11 release, we will be moving the Apache 2.0-licensed code of Elasticsearch and Kibana to be dual licensed under SSPL and the Elastic License, giving users the choice of which license to apply. SSPL is a source-available license created by MongoDB to embody the principles …</p></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.elastic.co/blog/licensing-change">https://www.elastic.co/blog/licensing-change</a></em></p>]]>
            </description>
            <link>https://www.elastic.co/blog/licensing-change</link>
            <guid isPermaLink="false">hacker-news-small-sites-25776657</guid>
            <pubDate>Thu, 14 Jan 2021 14:29:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[10 years-ish of Elixir]]>
            </title>
            <description>
<![CDATA[
Score 395 | Comments 113 (<a href="https://news.ycombinator.com/item?id=25776525">thread link</a>) | @1_player
<br/>
January 14, 2021 | https://dashbit.co/blog/ten-years-ish-of-elixir | <a href="https://web.archive.org/web/*/https://dashbit.co/blog/ten-years-ish-of-elixir">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <article>
    
<ul>
  <li>
    <i></i> José Valim
  </li>
  <li>
    <i></i> January 11th, 2021
  </li>
  <li>
    <i></i><a href="https://dashbit.co/blog/tags/elixir">elixir</a>, <a href="https://dashbit.co/blog/tags/phoenix">phoenix</a>, <a href="https://dashbit.co/blog/tags/broadway">broadway</a>, <a href="https://dashbit.co/blog/tags/nerves">nerves</a>, <a href="https://dashbit.co/blog/tags/liveview">liveview</a>, <a href="https://dashbit.co/blog/tags/membrane">membrane</a>
  </li>
</ul>
<p>
This past weekend, on January 9th, we celebrated 10 years since <a href="https://github.com/elixir-lang/elixir/commit/337c3f2d569a42ebd5fcab6fef18c5e012f9be5b">the first commit to the Elixir repository</a>. While I personally don’t consider <a href="https://elixir-lang.org/">Elixir</a> to be 10 years old yet - the language that became what Elixir is today <a href="https://github.com/elixir-lang/elixir/commit/6052352b6281752b905b30eb5b08fac0f51f68cd">surfaced only 14 months later</a> - a decade is a mark to celebrate!</p>
<p>
The goal of this post is to focus on the current state of some projects in the ecosystem and then briefly highlight a few of the exciting efforts coming over the next months.</p>
<h2>
Recap: The language goals</h2>
<p>
When I started working on Elixir, I personally had the ambition of using it for building scalable and robust web applications. However, I didn’t want Elixir to be tied to the web. My goal was to design an <em>extensible</em> language with a diverse ecosystem. Elixir aims to be a general purpose language and allows developers to extend it to new domains.</p>
<p>
Given Elixir is built on top of Erlang and Erlang is used for networking and distributed systems, Elixir would naturally be a good fit in those domains too, as long as I didn’t screw things up. The Erlang VM is essential to everything we do in Elixir, which is why <em>compatibility</em> has become a language goal too.</p>
<p>
I also wanted the language to be <em>productive</em>, especially by focusing on the tooling. Learning a functional programming language is a new endeavor for most developers. Consequently their first experiences getting started with the language, setting up a new project, searching for documentation, and debugging should go as smoothly as possible.</p>
<p>
Extensibility, compatibility, and productivity are the goals we built the language upon.</p>
<h2>
Recap: Elixir in production</h2>
<p>
Last year we started <a href="https://elixir-lang.org/cases.html">a series of articles on companies using Elixir in production on the official website</a>. As of today, we have 7 production cases listed with more coming this year! Overall it is very exciting to see many different companies using a variety of business models and industries running Elixir in production.</p>
<p>
Companies like <a href="https://www.brex.com/">Brex</a> (<a href="https://elixir-lang.org/blog/2020/06/23/growing-with-elixir-at-brex/">case</a>), <a href="https://discord.com/">Discord</a> (<a href="https://elixir-lang.org/blog/2020/10/08/real-time-communication-at-scale-with-elixir-at-discord/">case</a>), <a href="https://getdivvy.com/">Divvy</a>, <a href="https://www.podium.com/">Podium</a>, and <a href="https://salesloft.com/">SalesLoft</a> have reached <a href="https://en.wikipedia.org/wiki/Unicorn_(finance)">“unicorn status”</a> and rely heavily on Elixir. Startups like <a href="https://www.joinblvd.com/">Boulevard</a> (<a href="https://soundcloud.com/elixirtalk/episode-166-feat-sean-stavropoulos">podcast</a>), <a href="https://community.com/">Community</a>, <a href="https://duffel.com/">Duffel</a> (<a href="https://elixir-lang.org/blog/2020/12/10/integrating-travel-with-elixir-at-duffel/">case</a>), <a href="https://www.ockam.io/">Ockam</a>, <a href="https://www.mux.com/">Mux</a>, <a href="https://ramp.com/">Ramp</a>, <a href="https://remote.com/">Remote</a>, and <a href="https://www.v7labs.com/">V7</a> (<a href="https://elixir-lang.org/blog/2021/01/13/orchestrating-computer-vision-with-elixir/">case</a>) also use Elixir and have received funding in the last year or two. Elixir is also used within known brands and enterprises such as <a href="https://bleacherreport.com/">Bleacher Report</a>, <a href="https://www.change.org/">Change.org</a> (<a href="https://elixir-lang.org/blog/2020/10/27/delivering-social-change-with-elixir-at-change.org/">case</a>), <a href="https://heroku.com/">Heroku</a> (<a href="https://elixir-lang.org/blog/2020/09/24/paas-with-elixir-at-Heroku/">case</a>), <a href="https://www.pagerduty.com/">PagerDuty</a>, <a href="https://www.pepsico.com/">PepsiCo</a>, and <a href="https://www.therealreal.com/">TheRealReal</a>.</p>
<p>
There is also a special category of startups that run Elixir alonside an open source model, such as <a href="https://plausible.io/">Plausible Analytics</a>, <a href="https://app.supabase.io/">Supabase</a>, <a href="https://logflare.app/">Logflare</a> (<a href="https://runninginproduction.com/podcast/11-logflare-is-a-log-management-and-event-analytics-platform">podcast</a>), and <a href="https://hex.pm/">Hex.pm</a> (<a href="https://runninginproduction.com/podcast/19-hexpm-is-elixirs-official-package-manager">podcast</a>) itself. Still on the open source front, you will find projects like <a href="https://pleroma.social/">Pleroma</a> and <a href="https://changelog.com/">Changelog</a>. There also many small scale and hobby projects that use Elixir for a productive and joyful development experience.</p>
<h2>
Recap: Diverse ecosystem</h2>
<p>
Today, Elixir has a diverse ecosystem that works on a wide range of domains and industries. Let’s take a look at some examples.</p>
<h3>
Web</h3>
<p>
Most developers are familiar with using Elixir for web development thanks to <a href="https://phoenixframework.org/">the Phoenix web framework</a>. Phoenix gained traction in the ecosystem because it was the first to fully leverage the language and the platform for building real-time applications besides the usual MVC (Model-View-Controller) offering.</p>
<p>
It all started with Phoenix Channels, as a bi-directional communication between clients and servers, and Phoenix PubSub, which uses Erlang’s distributed compatibilities to broadcast messages across nodes. As far as I know, Phoenix was the first major web framework to provide a multi-node web real-time solution completely out-of-the-box. Regardless if you are using one node or ten nodes, everything just works, with minimal configuration and dependencies.</p>
<p>
Phoenix has matured a lot since its first stable release. Phoenix v1.2 included <a href="https://hexdocs.pm/phoenix/Phoenix.Presence.html">Phoenix Presence</a>, that allows developers to track which users, IoT devices, etc are connected to your cluster right now. No databases or external dependencies required! This is one of the problems that look deceptively simple at first, but once you outline all scalability, performance, and fault-tolerance requirements, it <a href="https://elixir-lang.org/blog/2020/11/17/real-time-collaboration-with-elixir-at-slab/">becomes quite complex</a>. Luckily, Phoenix is running on a platform that excels at these problems, and I am not aware of any other framework that provides such a lean and elegant solution as part of its default stack.</p>
<p>
Most recently, <a href="https://github.com/phoenixframework/phoenix_live_view">Phoenix LiveView</a> was released and brought new ways to build rich, real-time user experiences with server-rendered HTML, inspiring developers to attempt similar solutions for other languages and frameworks. You can read the <a href="https://dockyard.com/blog/2018/12/12/phoenix-liveview-interactive-real-time-apps-no-need-to-write-javascript">original announcement</a> or <a href="https://www.phoenixframework.org/blog/build-a-real-time-twitter-clone-in-15-minutes-with-live-view-and-phoenix-1-5">learn how to build a real-time Twitter clone in 15 minutes</a>. As part of the <em>Live</em> family, we have also announced <a href="https://twitter.com/josevalim/status/1250846714665357315">Phoenix LiveDashboard</a>, making monitoring and instrumentation a first-class citizen for Phoenix applications.</p>
<h3>
Embedded and IoT</h3>
<p>
While I always expected Elixir to shine for building web applications, I was taken by surprise when I heard about the <a href="https://www.nerves-project.org/">Nerves platform</a> for creating high-end embedded applications. However, once I learned their premise, it all made sense: writing embedded systems <em>is</em> complicated. Reasoning about failures is hard. So what if we could leverage the decades of lessons learnt by Erlang/OTP to design embedded applications? What if a fault on the Wi-Fi driver could be fixed by having a supervisor simply restart it? After all, the first major use of Erlang/OTP was in an embedded system, the Ericsson AXD301 ATM switch.</p>
<p>
Nerves brings the Elixir ecosystem and the battle-tested Erlang VM to edge computing, providing a rich developer experience using proven technology. Nerves started as a one step process for turning an Elixir project into a complete software image for common hardware devices. Today, Nerves is being used in production in industrial automation, machine learning, consumer electronics and more, with <a href="https://farm.bot/">Farmbot</a> (<a href="https://elixir-lang.org/blog/2020/08/20/embedded-elixir-at-farmbot/">case</a>) and <a href="https://www.rosepoint.com/">Rose Point Navigation</a> being two of the most notable examples.</p>
<p>
The Nerves team also created <a href="https://www.nerves-hub.org/">NervesHub</a>, a fully open-source device management system. Combining all these technologies makes Elixir a comprehensive language for building end-to-end IoT platforms.</p>
<h3>
Data ingestion and pipelines</h3>
<p>
Shortly after Elixir v1.0 was released, the Elixir Core Team and I started looking into abstractions for tackling data ingestions and data pipelines in Elixir. We ran through a couple designs until we eventually <a href="https://elixir-lang.org/blog/2016/07/14/announcing-genstage/">landed on GenStage</a>: a behaviour for exchanging data with back-pressure between Elixir processes and external systems. For an introduction, make sure to check out <a href="https://youtu.be/srtMWzyqdp8?t=242">my keynote introducing both GenStage and Flow</a>.</p>
<p>
Today, almost 5 years later, GenStage has been used by many industries and has become one of the factors driving Elixir adoption. For example, you can read how both <a href="https://elixir-lang.org/blog/2020/10/08/real-time-communication-at-scale-with-elixir-at-discord/">Discord</a> and <a href="https://elixir-lang.org/blog/2020/10/27/delivering-social-change-with-elixir-at-change.org/">Change.org</a> have built systems on Elixir and GenStage that handle spikes and run at massive scale.</p>
<p>
However, GenStage was just the beginning. In 2019, <a href="https://www.youtube.com/watch?v=ZOExnT1PYjs">we announced Broadway</a>, which is a higher-level abstraction on top of GenStage that makes building data ingestion pipelines a breeze. We originally released with Amazon SQS support. Nowadays, RabbitMQ, Google Cloud PubSub, Apache Kafka, and other sources (known as producers in Broadway terms) are also available.</p>
<h3>
Audio/Video streaming</h3>
<p>
Since the Erlang VM was designed for scalable network processing, one can expect to also be an excellent platform for audio and video streaming. However, if you also wanted to process and transform those streams on the fly, the situation becomes much more complicated as you likely have to integrate with native code.</p>
<p>
Luckily, the tables have turned when Erlang/OTP 20 was released a couple years ago with the so-called Dirty NIFs. The Erlang VM always had the ability to invoke native code, but this native code could not run for long, as to not interfere with the preemptive features of the Erlang runtime. Dirty NIFs allow developers to tag native code either as IO or CPU bound, which runs on specific threads. Between ports (I/O based), NIFs, Dirty NIFs, and remote nodes, developers now have many options to interface with native code with different performance and reliability guarantees. That’s exactly the foundation the <a href="http://www.membraneframework.org/">Membrane Framework</a> builds on top of.</p>
<p>
Membrane was extracted from RadioKit, a startup aiming at disrupting the radio broadcasting industry. Originally it focused on processing and mixing audio. Later, <a href="https://www.swmansion.com/">Software Mansion</a> acquired the framework and provided stable funding and a solid team to help it grow into a full-scale framework. Currently, it allows developers to process, transmit, broadcast, and transform audio and videos streams on the fly. Whether you are building a Twitch clone, a VOD application or a video conferencing system, Membrane provides a growing set of high-level abstractions and pre-made modules so you don’t have to dive into idiosyncrasies of particular codecs, protocols, and formats. </p>
<h2>
Looking ahead: what is coming in 2021</h2>
<p>
The year of 2021 looks very exciting for the Erlang Ecosystem and the Elixir community. In this section, we are going to mention some of the things we expect to see in 2021.</p>
<h3>
Erlang/OTP 24 with JIT</h3>
<p>
In September 2020, <a href="https://github.com/erlang/otp/pull/2745">Lukas Larsson and the Erlang/OTP team</a> announced a JIT compiler for the Erlang VM called BeamAsm. How faster the JIT will be in practice depends on your application but the results posted in the announcement are promising. To quote Lukas:</p>
<blockquote>
  <p>
If we run the JSON benchmarks found in the <a href="https://github.com/devinus/poison/tree/master/bench">Poison</a> or <a href="https://github.com/michalmuskala/jason/tree/master/bench">Jason</a>, BeamAsm achieves anything from 30% to 130% increase (average at about 70%) in the number of iterations per second for all Erlang/Elixir implementations. For some benchmarks, BeamAsm is even faster than the pure C implementation <a href="https://github.com/davisp/jiffy">jiffy</a>.  </p>
</blockquote>
<blockquote>
  <p>
More complex applications tend to see a more moderate performance increase, for instance, RabbitMQ is able to handle 30% to 50% more messages per second depending on the scenario.  </p>
</blockquote>
<p>
I have been running …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dashbit.co/blog/ten-years-ish-of-elixir">https://dashbit.co/blog/ten-years-ish-of-elixir</a></em></p>]]>
            </description>
            <link>https://dashbit.co/blog/ten-years-ish-of-elixir</link>
            <guid isPermaLink="false">hacker-news-small-sites-25776525</guid>
            <pubDate>Thu, 14 Jan 2021 14:21:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We don't need data scientists, we need data engineers]]>
            </title>
            <description>
<![CDATA[
Score 640 | Comments 319 (<a href="https://news.ycombinator.com/item?id=25775872">thread link</a>) | @winkywooster
<br/>
January 14, 2021 | https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/ | <a href="https://web.archive.org/web/*/https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-reactid="11"><figure>
    
  <a href="https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-85f3f.jpg" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="world in data" title="" src="https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-c6823.jpg" srcset="https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-dad4f.jpg 240w,
https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-1808a.jpg 480w,
https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-c6823.jpg 960w,
https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-e8e5f.jpg 1440w,
https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-54793.jpg 1920w,
https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-c4df6.jpg 2880w,
https://www.mihaileric.com/static/nasa-Q1p7bh3SHj8-unsplash-1d801dd5a6a5e44b7503e433b3a540b5-85f3f.jpg 4256w" sizes="(max-width: 960px) 100vw, 960px">
    </span>
  </span>
  
  </a>
    
</figure>
<p>Data. It’s everywhere and we’re <a href="https://techjury.net/blog/how-much-data-is-created-every-day/#gref">only getting more of it</a>. For the last 5-10 years, <em>data science</em> has attracted newcomers near and far trying to get a taste of that forbidden fruit. </p>
<p>But what does the state of <em>data science</em> hiring look like today?</p>
<p>Here’s the gist of the article in two-sentences for the busy reader.</p>
<p><strong>TLDR</strong>: There are <strong>70% more open roles</strong> at companies in <em>data engineering</em> as compared to <em>data science</em>. As we train the next generation of data and machine learning practitioners, let’s place more emphasis on engineering skills.</p>
<hr>
<p>As part of my work developing an <a href="https://www.confetti.ai/">educational platform</a> for data professionals, I think a lot about how the market for data-driven (machine learning and data science) roles is evolving. </p>
<p>In talking to dozens of prospective entrants to data fields including students at top institutions around the world, I’ve seen a tremendous amount of confusion around what skills are most important to help candidates stand out in the crowd and prepare for their careers. </p>
<p>When you think about it, a <em>data scientist</em> can be responsible for any subset of the following: machine learning modelling, visualization, data cleaning and processing (i.e. SQL wrangling), engineering, and production deployment. </p>
<p>How do you even begin to recommend a study curriculum for newcomers?</p>
<p>Data speaks louder than words. So I decided to do an analysis of the data roles being hired for at every company coming out of <a href="https://www.ycombinator.com/">Y-Combinator</a> since 2012. The questions that guided my research:</p>
<ul>
<li>What data roles are companies most frequently hiring for?</li>
<li>How in-demand is the conventional <em>data scientist</em> that we talk about so much?</li>
<li>Are the same skills that started the data revolution relevant today?</li>
</ul>
<p>If you want the full details and analysis, read on. </p>
<h2>Methodology</h2>
<p>I chose to do an analysis of YC portfolio companies that claim to make some sort of data work part of their value proposition. </p>
<p>Why focus on YC? Well, for starters, they do a good job of providing an easily searchable (and scrapable) <a href="https://www.ycombinator.com/companies/">directory of their companies</a>. </p>
<p>In addition, as a particularly forward-thinking incubator that has funded companies from around the world across domains for over a decade, I felt they provided a representative sample of the market with which to conduct my analyses. That being said, take what I say wit a grain of salt, as I didn’t analyze super-large tech companies.</p>
<p>I scraped the homepage URLs of every YC company since 2012, producing an initial pool of ~1400 companies. </p>
<p>Why stop at 2012? Well, 2012 was the year that <a href="https://en.wikipedia.org/wiki/AlexNet">AlexNet</a> won the ImageNet competition, effectively kickstarting the machine learning and data-modelling wave we are now living through. It’s fair to say that this birthed some of the earliest generations of data-first companies.</p>
<p>From this initial pool, I performed keyword filtering to reduce the number of relevant companies I would have to look through. In particular, I only considered companies whose websites included at least one of the following terms: AI, CV, NLP, natural language processing, computer vision, artificial intelligence, machine, ML, data. I also disregarded companies whose website links were broken. </p>
<p>Did this generate a ton of false positives? Absolutely! But here I was trying to prioritize high recall as much as possible, recognizing that I would do a more fine-grained manual inspection of the individual websites for relevant roles.</p>
<p>With this reduced pool, I went through every site, found where they were advertising jobs (typically a <em>Careers</em>, <em>Jobs</em>, or <em>We’re Hiring</em> page), and took note of every role that included data, machine learning, NLP, or CV in the title. This gave me a pool of about 70 distinct companies hiring for data roles. </p>
<p>One note here: it’s conceivable that I missed some companies as there were certain websites with very little information (typically those in stealth) that might actually be hiring. In addition, there were companies that didn’t have a formal <em>Careers</em> page but asked that prospective candidates reach out directly via email. </p>
<p>I disregarded both of these types of companies rather than reach out to them, so they are not part of this analysis.</p>
<p>Another thing: the bulk of this research was done towards the final weeks of 2020. Open roles may have changed as companies update their pages periodically. However, I don’t believe this will drastically impact the conclusions drawn. </p>
<h2>What Are Data Practitioners Responsible For?</h2>
<p>Before diving into the results, it’s worth spending some time clarifying what responsibilities each data role is typically responsible for. Here are the four roles we will spend our time looking at with a short description of what they do:</p>
<ul>
<li><em>Data scientist</em>: Use various techniques in statistics and machine learning to process and analyse data. Often responsible for building models to probe what can be learned from some data source, though often at a prototype rather than production level. </li>
<li><em>Data engineer</em>: Develops a robust and scalable set of data processing tools/platforms. Must be comfortable with SQL/NoSQL database wrangling and building/maintaining ETL pipelines.</li>
<li><em>Machine Learning (ML) Engineer</em>: Often responsible for both training models and productionizing them. Requires familiarity with some high-level ML framework and also must be comfortable building scalable training, inference, and deployment pipelines for models.</li>
<li><em>Machine Learning (ML) Scientist</em>: Works on cutting-edge research. Typically responsible for exploring new ideas that can be published at academic conferences. Often only needs to prototype new state-of-the-art models before handing off to ML engineers for productionization.</li>
</ul>
<h2>How Many Data Roles Are There?</h2>
<p>So what happens when we plot the frequency of each data role that companies are hiring for? The plot looks  like this:</p>
<figure>
    
  <a href="https://www.mihaileric.com/static/all_roles-76753bdb67cdaac40a0ea69ffbe76267-7931f.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="all machine learning, data science, data engineering roles at Y-Combinator companies" title="" src="https://www.mihaileric.com/static/all_roles-76753bdb67cdaac40a0ea69ffbe76267-3d61e.png" srcset="https://www.mihaileric.com/static/all_roles-76753bdb67cdaac40a0ea69ffbe76267-d182c.png 240w,
https://www.mihaileric.com/static/all_roles-76753bdb67cdaac40a0ea69ffbe76267-5220f.png 480w,
https://www.mihaileric.com/static/all_roles-76753bdb67cdaac40a0ea69ffbe76267-3d61e.png 960w,
https://www.mihaileric.com/static/all_roles-76753bdb67cdaac40a0ea69ffbe76267-7931f.png 985w" sizes="(max-width: 960px) 100vw, 960px">
    </span>
  </span>
  
  </a>
    
</figure>
<p>What immediately stands out is how many more open <em>data engineer</em> roles there are compared to traditional <em>data scientists</em>. In this case, the raw counts correspond to companies hiring <strong>roughly 55% more</strong> for data engineers than data scientists, and roughly the same number of machine learning engineers as data scientists.</p>
<p>But we can do more. If you look at the titles of the various roles, there seems to be some repetition. </p>
<p>Let’s only provide coarse-grained categorization through role consolidation. In other words, I took roles whose descriptions were roughly equivalent and consolidated them under a single title. </p>
<p>That included the following set of equivalence relations: </p>
<ul>
<li><em>NLP engineer</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>CV engineer</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>ML engineer</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>Deep Learning engineer</em> (while the domains might be different, the responsiblities are roughly the same)</li>
<li><em>ML scientist</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>Deep Learning researcher</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>ML intern</em> (the internship description very much seemed research-focused)</li>
<li><em>Data engineer</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>Data architect</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>Head of data</em> <span><span><math><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span></span> <em>Data platform engineer</em></li>
</ul>
<figure>
    
  <a href="https://www.mihaileric.com/static/consolidated_roles-d0609bd70c768ce428b0873ea5ff1bd3-7931f.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="all machine learning, data science, data engineering roles at Y-Combinator companies consolidated into coarse categories" title="" src="https://www.mihaileric.com/static/consolidated_roles-d0609bd70c768ce428b0873ea5ff1bd3-3d61e.png" srcset="https://www.mihaileric.com/static/consolidated_roles-d0609bd70c768ce428b0873ea5ff1bd3-d182c.png 240w,
https://www.mihaileric.com/static/consolidated_roles-d0609bd70c768ce428b0873ea5ff1bd3-5220f.png 480w,
https://www.mihaileric.com/static/consolidated_roles-d0609bd70c768ce428b0873ea5ff1bd3-3d61e.png 960w,
https://www.mihaileric.com/static/consolidated_roles-d0609bd70c768ce428b0873ea5ff1bd3-7931f.png 985w" sizes="(max-width: 960px) 100vw, 960px">
    </span>
  </span>
  
  </a>
    
</figure>
<p>If we don’t like dealing with raw counts, here are some percentages to put us at ease:</p>
<figure>
    
  <a href="https://www.mihaileric.com/static/normalized_consolidated_roles-48138e6d849e501e2823381e49ba06e1-7931f.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="all machine learning, data science, data engineering roles at Y-Combinator companies normalized frequencies" title="" src="https://www.mihaileric.com/static/normalized_consolidated_roles-48138e6d849e501e2823381e49ba06e1-3d61e.png" srcset="https://www.mihaileric.com/static/normalized_consolidated_roles-48138e6d849e501e2823381e49ba06e1-d182c.png 240w,
https://www.mihaileric.com/static/normalized_consolidated_roles-48138e6d849e501e2823381e49ba06e1-5220f.png 480w,
https://www.mihaileric.com/static/normalized_consolidated_roles-48138e6d849e501e2823381e49ba06e1-3d61e.png 960w,
https://www.mihaileric.com/static/normalized_consolidated_roles-48138e6d849e501e2823381e49ba06e1-7931f.png 985w" sizes="(max-width: 960px) 100vw, 960px">
    </span>
  </span>
  
  </a>
    
</figure>
<p>I probably could have lumped <em>ML research engineer</em> into one of the <em>ML scientist</em> or <em>ML engineer</em> bins, but given that it was a bit of a hybrid role, I left it as is. </p>
<p>Overall the consolidation made the differences even more pronounced! There are <strong>~70%</strong> more open <em>data engineer</em> than <em>data scientist</em> positions. In addition, there are <strong>~40%</strong> more open <em>ML engineer</em> than <em>data scientist</em> positions. There are also only <strong>~30%</strong> as many <em>ML scientist</em> as <em>data scientist</em> positions. </p>
<h2>Takeaways</h2>
<p><em>Data engineers</em> are in increasingly high demand compared to other data-driven professions. In a sense, this represents an evolution for the broader field. </p>
<p>When machine learning become hot 🔥 5-8 years ago, companies decided they need people that can make classifiers on data. But then frameworks like <a href="https://www.tensorflow.org/">Tensorflow</a> and <a href="https://pytorch.org/">PyTorch</a> became really good, democratizing the ability to get started with deep learning and machine learning. </p>
<p>This commoditized the data modelling skillset. </p>
<p>Today, the bottleneck in helping companies get machine learning and modelling insights to production center on data problems. </p>
<p>How do you annotate data? How do you process and clean data? How do you move it from A to B? How do you do this every day as quickly as possible?</p>
<figure>
    
  <a href="https://www.mihaileric.com/static/patrick-951fdc9920aa2a6cc7b75e0959379665-321c9.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="patrick star moving data" title="" src="https://www.mihaileric.com/static/patrick-951fdc9920aa2a6cc7b75e0959379665-321c9.png" srcset="https://www.mihaileric.com/static/patrick-951fdc9920aa2a6cc7b75e0959379665-8c52f.png 240w,
https://www.mihaileric.com/static/patrick-951fdc9920aa2a6cc7b75e0959379665-0f208.png 480w,
https://www.mihaileric.com/static/patrick-951fdc9920aa2a6cc7b75e0959379665-321c9.png 720w" sizes="(max-width: 720px) 100vw, 720px">
    </span>
  </span>
  
  </a>
    
</figure>
<p>All that amounts to having good engineering skills. </p>
<p>This may sound boring and unsexy, but old-school software engineering with a bend toward data may be what we really need right now. </p>
<p>For years, we’ve become enamored with the idea of data professionals that breathe life into raw data thanks to cool demos and media hype. After all, when was the last time you saw a <a href="https://techcrunch.com/">TechCrunch</a> article about an ETL pipeline? </p>
<p>If nothing else, I believe solid engineering is something we don’t emphasize enough in data science job training or educational programs. In addition to learning how to use <em>linear_regression.fit()</em>, learn how to write a unit test too!</p>
<p>So does that mean you shouldn’t study data science? No. </p>
<p>What it means is that competition is going to be tougher. There are going to be fewer positions available for what is looking to be an abundance of newcomers to the market trained to do data science. </p>
<p>There will always be a need for people that can effectively analyze and extract actionable insights from data. But they have to be good. </p>
<p>Downloading a pretrained model off the Tensorflow website on the <a href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html">Iris dataset</a> probably is no longer enough to get that data science job. </p>
<p>It’s clear, however, with the large number of <em>ML engineer</em> openings that companies often want a hybrid data practitioner: someone that can build and deploy models. Or said more succinctly, someone that can use Tensorflow but can also build it from source.</p>
<p>Another takeaway here is that there just aren’t that many ML research positions. </p>
<p>Machine learning research tends to get its fair share of hype because that’s where all the cutting-edge stuff happens, all the <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> and <a href="https://openai.com/blog/openai-api/">GPT-3</a> and what-not. </p>
<p>But for …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/">https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/</a></em></p>]]>
            </description>
            <link>https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25775872</guid>
            <pubDate>Thu, 14 Jan 2021 13:09:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Parisian Accent in 1912]]>
            </title>
            <description>
<![CDATA[
Score 297 | Comments 154 (<a href="https://news.ycombinator.com/item?id=25775091">thread link</a>) | @paganel
<br/>
January 14, 2021 | https://www.franceculture.fr/sciences-du-langage/archive-exceptionnelle-ecoutez-laccent-parisien-en-1912 | <a href="https://web.archive.org/web/*/https://www.franceculture.fr/sciences-du-langage/archive-exceptionnelle-ecoutez-laccent-parisien-en-1912">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>"C’est extraordinaire que j’aie une voix aussi traînarde, jamais je l’aurais cru ! On ne s’entend pas, absolument !" Dans ce document unique en son genre, un Parisien réagit à l'écoute de son propre accent, celui du 14e arrondissement, en 1912. Il est interviewé par le linguiste Ferdinand Brunot. </p><div>
            <p>Cette archive exceptionnelle est l’une des premières interviews sonores, le premier enregistrement d'un échange spontané, non lu. C’est aussi l’une des rares traces de l’accent parisien d’avant-guerre. En 1912, le linguiste Ferdinand Brunot veut enregistrer les dialectes des artisans. Ici, c’est le parler parisien qui l’intéresse, l’accent populaire des différents quartiers de la capitale. Louis Ligabue, tapissier dans le 14e arrondissement, a alors 37 ans, et note déjà l'embourgeoisement de son quartier.</p>

<h2>Le parler d'un pur "Parisien de Paris"</h2>
<p>Le linguiste Ferdinand Brunot, fondateur des "Archives de la parole"&nbsp;en 1911, est l'un des rares universitaires de son temps à s'intéresser à l'enregistrement du français parlé&nbsp;"commun". Pour lui, le "parler parisien" est une forme de dialecte dont il faut garder la trace. Chaque quartier de la capitale est censé présenter ses spécificités linguistiques&nbsp;: on ne parle pas à Montrouge comme à Montmartre. Or pour <a target="_blank" rel="noopener" href="https://gallica.bnf.fr/blog/11052020/quand-un-parisien-entend-pour-la-premiere-fois-le-son-de-sa-voix?mode=desktop">Pascal Cordereix</a>, responsable du service des documents sonores à la BnF, "<em>ce 'dialecte' parisien renvoie lui-même à l’un des plus grands mythes de la linguistique romane parisienne de la fin du XIXe siècle, à savoir le "françien", un supposé dialecte d’Île-de-France dans lequel le français trouverait sa seule origine. On est là au cœur de la construction jacobine de la langue française mise en œuvre après 1870, opposant définitivement français et autres idiomes parlés sur le territoire.</em>"</p>
<p><em>Ferdinand Brunot interviewe Louis Ligabue en 1912&nbsp;:&nbsp;</em></p>


<p>- Vous êtes vraiment, vous monsieur, un Parisien de Paris&nbsp;?&nbsp;</p>
<p><em>-</em> Je suis né à Paris même, boulevard Sébastopol et j’ai monté du côté de Montrouge, rue Daguerre. Ensuite, j’ai été avenue d’Orléans. J’ai travaillé dans le quartier constamment. Je ne l’ai jamais quitté du reste.&nbsp;</p>
<p>- Et vous êtes exclusivement tapissier, alors&nbsp;?&nbsp;</p>
<p>- Absolument. (...) et je travaille pour le client personnel (...)</p>
<p>- Vous êtes exclusivement dans la clientèle bourgeoise&nbsp;?&nbsp;</p>
<p>- Oui, monsieur.</p>
<p>- Il y a eu beaucoup d’installations de ce côté-là, ce doit être un bon métier&nbsp;?</p>
<p><em>-</em> Ah le quartier a beaucoup gagné. Nous avons depuis quelques années travaillé admirablement.</p>
<p>- J’ai entendu dire que vous n’étiez pas payé très régulièrement...</p>
<p>- Oh vous savez, le 14e est tout à fait spécial. Nous avons de bons clients, de bons bourgeois, et puis régulièrement, on hésite à donner une affaire... Mais quant au règlement, jamais nous ne perdons quoi que ce soit.&nbsp;</p>
<p>- On m’avait dit au contraire que rue Alphonse Daudet, il y avait une clientèle peu recommandable...</p>
<p>- Ah dame ! Ça, je m’en réjouis bien !</p>
<h2>Entendre sa propre voix</h2>
<p>Ce document est unique à plus d'un titre&nbsp;: c’est aussi la première fois qu’on entend quelqu’un réagir à ses propos, à sa voix. &nbsp;Dans la foulée de l’interview, Louis Ligabue s’écoute, et s’étonne, toujours en dialoguant avec le linguiste Ferdinand Brunot. <a target="_blank" rel="noopener" href="https://gallica.bnf.fr/blog/11052020/quand-un-parisien-entend-pour-la-premiere-fois-le-son-de-sa-voix?mode=desktop">Pascal Cordereix</a>, spécialiste du fonds sonore ancien à la Bibliothèque nationale de France (BnF)&nbsp;: "<em>On connaît beaucoup de récits écrits d’un enregistrement et de la surprise du locuteur s’écoutant parler. Mais à notre connaissance, Ferdinand Brunot est le seul, avant longtemps, à avoir l’idée de graver ainsi sur disque les réactions du témoin à l’écoute de sa propre voix.</em>"</p>
<p><strong>Ferdinand Brunot interviewe Louis Ligabue en 1912&nbsp;:&nbsp;</strong></p>
<p>- Eh bien, vous avez entendu ce que vous avez dit. Vous êtes-vous reconnu&nbsp;?&nbsp;</p>
<p>- Oui, parfaitement monsieur !</p>
<p>- N’est-ce pas que c’est bien votre voix&nbsp;?&nbsp;</p>
<p>- C’est parfait, parfait, c’est même très très curieux ! C’est très drôle, il me semble même que c’est extraordinaire que j’aie une voix si traînarde, jamais je ne l’aurais cru ! On ne s’entend pas, absolument !&nbsp;</p>
<p>- Vous n’avez pas la voix traînarde, vous avez tout simplement l’accent de Paris, c’est justement ça que je veux enregistrer.&nbsp;</p>
<p>- Ah ça aujourd'hui j’en suis convaincu, bien des gens m’ont dit des fois&nbsp;: “Comme il traîne ce garçon dans sa conversation”. Ben, je disais non, pourtant, il me semble que c’est tout naturel&nbsp;; mais alors là, vous savez, j’ai un accent presque d’La Villette on dirait…</p>
<p>- La Villette&nbsp;? Ne croyez-vous pas qu’il y a une grande différence justement entre l’accent de La Villette et le vôtre&nbsp;?</p>
<p>- Ah peut-être, je ne sais pas…</p>
<p>- Vous qui êtes de Paris, est-ce que vous ne reconnaissez pas justement quelqu’un qui est de nos arrondissements&nbsp;?</p>
<p>- Ah absolument, si, il y a réellement des différences.</p>
<p>- Quelqu’un qui s’est beaucoup occupé de ça me disait par exemple qu’il reconnaissait du premier coup un habitant du 14e et un habitant de Montmartre.</p>
<p>- Ah peut-être, mais enfin, il me semble que c’est une étude assez sérieuse.</p>
<p>- Oui, il y a des gars qui imitent ça étonnamment et à volonté vous savez, ils se transforment en gens de Montparnasse, ou en gens de Montmartre comme ils veulent.</p>
<p>- Ah oui, on voit ça dans les revues, là, dans les concerts, là&nbsp;; on a des types spéciaux là-dessus.</p>
<p>- Oui. Est-ce que dans la rue de la Gaîté, là, il y a des gens qui imitent justement l’accent du quartier&nbsp;?</p>
<p>- Ah y en a, y en a. Mais alors, ça devient peut-être un peu en exagération. Tandis que là, moi, je cause naturellement, et quand j’écoute, il me semble que j’exagère… Je suis bien content, vous savez, d’avoir jugé et ça m’a bien intéressé !</p>
<p>- Et bien je vous remercie de vous être prêté à l’expérience.</p>
<p>- De rien.</p>
<h2>Les Archives de la parole à découvrir sur Gallica</h2>
<p>Archive conservée à la Bibliothèque nationale de France. Merci au service Son du département de l’Audiovisuel, BnF et au Service de la coopération numérique et de Gallica, BnF. Archives de la Parole, conservation&nbsp;: BnF, Département de l’Audiovisuel, service Son.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://gallica.bnf.fr/html/und/enregistrements-sonores/archives-de-la-parole-ferdinand-brunot-1911-1914">Les Archives de la parole, 1911-1914</a></li>
<li><a target="_blank" rel="noopener" href="https://gallica.bnf.fr/html/und/enregistrements-sonores/archives-de-la-parole-jean-poirot-enregistrements-la-sorbonne">Les Archives de la parole, 1920-1924</a></li>
<li><a target="_blank" rel="noopener" href="https://gallica.bnf.fr/html/und/enregistrements-sonores/archives-de-la-parole-musee-de-la-parole-et-du-geste-hubert-pernot-1924-1930">Les Archives de la parole, 1924-1930</a></li>
</ul>



    </div></div>]]>
            </description>
            <link>https://www.franceculture.fr/sciences-du-langage/archive-exceptionnelle-ecoutez-laccent-parisien-en-1912</link>
            <guid isPermaLink="false">hacker-news-small-sites-25775091</guid>
            <pubDate>Thu, 14 Jan 2021 11:26:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tracing and visualizing the Python GIL with perf and VizTracer]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25774945">thread link</a>) | @maartenbreddels
<br/>
January 14, 2021 | https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/14/Tracing-the-Python-GIL.html | <a href="https://web.archive.org/web/*/https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/14/Tracing-the-Python-GIL.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div>

<p>There are plenty of articles explaining why the Python GIL (The Global Interpreter Lock) exists<sup id="fnref-1"><a href="#fn-1">1</a></sup>, and why it is there. The TLDR version is: the GIL prevents multithreaded pure Python code from using multiple CPU cores.</p>
<p>However, in <a href="https://vaex.io/">Vaex</a> we execute most of the CPU intensive parts in C (C++) code, where we release the GIL. This is a common practice in high-performance Python libraries, where Python acts merely as a high-level glue.</p>
<p>However, the GIL needs to be released explicitly, and this is the responsibility of the programmer and might be forgotten, leading to suboptimal use of your machine.</p>
<p>I recently had this issue in <a href="https://github.com/vaexio/vaex/pull/1114">Vaex</a> where I simply forgot to release the GIL and found a similar issue in <a href="https://github.com/apache/arrow/pull/7756">Apache Arrow</a><sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<p>Also, when running on 64 cores, I sometimes see a performance in Vaex that I am not happy with. It might be using 4000% CPU, instead of 6400% CPU, which is something I am not happy with. Instead of blindly pulling some levers to inspect the effect, I want to understand what is happening, and if the GIL is the problem, why, and where is it holding Vaex down.</p>


</div>
</div>
</div><div>
<div>
<div>

<p>I'm planning to write a series of articles explaining some tools and techniques available for profiling/tracing Python together with native extensions, and how these tools can be glued together, to analyze and visualize what Python is doing, and when the GIL it taken or dropped.</p>
<p>I hope this leads to improvement of tracing, profiling, and other performance tooling in the Python ecosystem, and the performance of the whole Python ecosystem.</p>

<h2 id="Linux">
Linux<a href="#Linux"> </a>
</h2>
<p>Get access to a Linux machine, and make sure you have root privileges (sudo is fine), or ask your sysadmin to execute some of these commands for you. For the rest of the document, we only run as user.</p>
<h2 id="Perf">
Perf<a href="#Perf"> </a>
</h2>
<p>Make sure you have perf installed, e.g. on Ubuntu:</p>

<pre><code>$ sudo yum install perf</code></pre>
<h2 id="Kernel-configuration">
Kernel configuration<a href="#Kernel-configuration"> </a>
</h2>
<p>To enable running it as a user:</p>

<pre><code># Enable users to run perf (use at own risk)
$ sudo sysctl kernel.perf_event_paranoid=-1

# Enable users to see schedule trace events:
$ sudo mount -o remount,mode=755 /sys/kernel/debug
$ sudo mount -o remount,mode=755 /sys/kernel/debug/tracing</code></pre>
<h2 id="Python-packages">
Python packages<a href="#Python-packages"> </a>
</h2>
<p>We will make use of <a href="https://github.com/gaogaotiantian/viztracer/">VizTracer</a> and <a href="https://github.com/maartenbreddels/per4m">per4m</a></p>

<pre><code>$ pip install "viztracer&gt;=0.11.2" "per4m&gt;=0.1,&lt;0.2"</code></pre>

</div>
</div>
</div><div>
<div>
<div>

<p>There is no way to get the GIL state in Python <sup id="fnref-3"><a href="#fn-3">1</a></sup> since there is no API for this. We can track it from the kernel, and the right tool for this under Linux is <strong>perf</strong>.</p>
<p>Using the linux perf tool (aka perf_events), we can listen to the state changes for processes/threads (we only care about sleeping and running), and log them. Although perf may look scary, it is a powerful tool. If you want to know a bit more about perf, I recommend reading <a href="https://jvns.ca/blog/2018/04/16/new-perf-zine/">Julia Evans' zine on perf</a> or <a href="http://www.brendangregg.com/perf.html">go through Brendan Gregg's website</a>.</p>
<p>To build our intuition, we will first run perf on a <a href="https://github.com/maartenbreddels/per4m/blob/master/per4m/example0.py">very trivial program</a>:</p>


</div>
</div>
</div><div>
<div>
<div>
<p>We listen to just a few events to keep the noise down (note the use of wildcards):</p>

<pre><code>$ perf record -e sched:sched_switch  -e sched:sched_process_fork \
        -e 'sched:sched_wak*' -- python -m per4m.example0
[ perf record: Woken up 2 times to write data ]
[ perf record: Captured and wrote 0,032 MB perf.data (33 samples) ]</code></pre>
<p>And use the <code>perf script</code> command to write human/parsable output.</p>

<pre><code>$ perf script
        :3040108 3040108 [032] 5563910.979408:                sched:sched_waking: comm=perf pid=3040114 prio=120 target_cpu=031
        :3040108 3040108 [032] 5563910.979431:                sched:sched_wakeup: comm=perf pid=3040114 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995616:                sched:sched_waking: comm=kworker/31:1 pid=2502104 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995618:                sched:sched_wakeup: comm=kworker/31:1 pid=2502104 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995621:                sched:sched_waking: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995622:                sched:sched_wakeup: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995624:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=R+ ==&gt; next_comm=kworker/31:1 next_pid=2502104 next_prio=120
          python 3040114 [031] 5563911.003612:                sched:sched_waking: comm=kworker/32:1 pid=2467833 prio=120 target_cpu=032
          python 3040114 [031] 5563911.003614:                sched:sched_wakeup: comm=kworker/32:1 pid=2467833 prio=120 target_cpu=032
          python 3040114 [031] 5563911.083609:                sched:sched_waking: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563911.083612:                sched:sched_wakeup: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563911.083613:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=R ==&gt; next_comm=ksoftirqd/31 next_pid=198 next_prio=120
          python 3040114 [031] 5563911.108984:                sched:sched_waking: comm=node pid=2446812 prio=120 target_cpu=045
          python 3040114 [031] 5563911.109059:                sched:sched_waking: comm=node pid=2446812 prio=120 target_cpu=045
          python 3040114 [031] 5563911.112250:          sched:sched_process_fork: comm=python pid=3040114 child_comm=python child_pid=3040116
          python 3040114 [031] 5563911.112260:            sched:sched_wakeup_new: comm=python pid=3040116 prio=120 target_cpu=037
          python 3040114 [031] 5563911.112262:            sched:sched_wakeup_new: comm=python pid=3040116 prio=120 target_cpu=037
          python 3040114 [031] 5563911.112273:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120
          python 3040116 [037] 5563911.112418:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112450:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112473: sched:sched_wake_idle_without_ipi: cpu=31
         swapper     0 [031] 5563911.112476:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040114 [031] 5563911.112485:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120
          python 3040116 [037] 5563911.112485:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112489:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112496:                sched:sched_switch: prev_comm=python prev_pid=3040116 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/37 next_pid=0 next_prio=120
         swapper     0 [031] 5563911.112497:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040114 [031] 5563911.112513:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120
         swapper     0 [037] 5563912.113490:                sched:sched_waking: comm=python pid=3040116 prio=120 target_cpu=037
         swapper     0 [037] 5563912.113529:                sched:sched_wakeup: comm=python pid=3040116 prio=120 target_cpu=037
          python 3040116 [037] 5563912.113595:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563912.113620:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
         swapper     0 [031] 5563912.113697:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031</code></pre>

</div>
</div>
</div><div>
<div>
<div>
<p>Take a moment to digest the output. I can see a few things. Looking at the 4th column (time in seconds), we can see where the program slept (it skips 1 second). Here we see that we enter the sleeping state with a line like:</p>
<p><code>python 3040114 [031] 5563911.112513:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120</code></p>
<p>This means the kernel changed the state of the Python thread to <code>S</code> (=sleeping) state.</p>
<p>A full second later, we see it being woken up:</p>
<p><code>swapper     0 [031] 5563912.113697:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031</code></p>
<p>Of course, you need to build some tooling around this, to really see what is happening. But one can imagine this output can be easily parsed and this is what <a href="https://github.com/maartenbreddels/per4m/">per4m</a> does. However, before we go there, I'd first like to visualize the flow of a slightly more advanced program using <a href="https://github.com/gaogaotiantian/viztracer/">VizTracer</a>.</p>

</div>
</div>
</div><div>
<div>
<div>

<p><a href="https://github.com/gaogaotiantian/viztracer/">VizTracer</a> is a Python tracer that can visualize what your program does in the browser. Let us run it on a slightly <a href="https://github.com/maartenbreddels/per4m/blob/master/per4m/example1.py">more advanced example</a> to see what it looks like.</p>

</div>
</div>
</div><div>
<div>
<div>
<p>Running viztracer gives output like:</p>

<pre><code>$ viztracer -o example1.html --ignore_frozen -m per4m.example1
Loading finish                                        
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...
Dumping trace data to json, total entries: 94, estimated json file size: 11.0KiB
Generating HTML report
Report saved.</code></pre>
<p>And the HTML should render as:
<img src="https://www.maartenbreddels.com/images/copied_from_nb/per4m/example1.png" alt="image.png"></p>
<p>From this, it seems that <code>some_computation</code> seem to be executed in parallel (twice), while in fact, we know the GIL is preventing that. So what is really going on?</p>

</div>
</div>
</div><div>
<div>
<div>

<p>Let us run <code>perf</code> on this, similarly to what we did to example0.py. However, we add the argument <code>-k CLOCK_MONOTONIC</code> so that we use <a href="https://github.com/gaogaotiantian/viztracer/blob/3321ba4024afe5623f938a601d7f7db3b08f534d/src/viztracer/modules/snaptrace.c#L91">the same clock as VizTracer</a> and ask VizTracer to generate a JSON, instead of an HTML file:</p>

<pre><code>$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*' \
   -k CLOCK_MONOTONIC  -- viztracer -o viztracer1.json --ignore_frozen -m per4m.example1</code></pre>
<p>Then …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/14/Tracing-the-Python-GIL.html">https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/14/Tracing-the-Python-GIL.html</a></em></p>]]>
            </description>
            <link>https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/14/Tracing-the-Python-GIL.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25774945</guid>
            <pubDate>Thu, 14 Jan 2021 11:06:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Analyzing the performance of Tensorflow training on M1 Mac Mini and Nvidia V100]]>
            </title>
            <description>
<![CDATA[
Score 222 | Comments 81 (<a href="https://news.ycombinator.com/item?id=25773109">thread link</a>) | @briggers
<br/>
January 13, 2021 | https://wandb.ai/vanpelt/m1-benchmark/reports/Can-Apple-s-M1-help-you-train-models-faster-cheaper-than-NVIDIA-s-V100---VmlldzozNTkyMzg | <a href="https://web.archive.org/web/*/https://wandb.ai/vanpelt/m1-benchmark/reports/Can-Apple-s-M1-help-you-train-models-faster-cheaper-than-NVIDIA-s-V100---VmlldzozNTkyMzg">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://wandb.ai/vanpelt/m1-benchmark/reports/Can-Apple-s-M1-help-you-train-models-faster-cheaper-than-NVIDIA-s-V100---VmlldzozNTkyMzg</link>
            <guid isPermaLink="false">hacker-news-small-sites-25773109</guid>
            <pubDate>Thu, 14 Jan 2021 07:04:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[If you want peace, study war]]>
            </title>
            <description>
<![CDATA[
Score 196 | Comments 192 (<a href="https://news.ycombinator.com/item?id=25772365">thread link</a>) | @ascertain
<br/>
January 13, 2021 | https://www.persuasion.community/p/if-you-want-peace-study-war-533 | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/if-you-want-peace-study-war-533">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fee9f8cb1-eee9-4e69-bce8-d7b52537900a_3896x2568.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fee9f8cb1-eee9-4e69-bce8-d7b52537900a_3896x2568.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/ee9f8cb1-eee9-4e69-bce8-d7b52537900a_3896x2568.jpeg&quot;,&quot;height&quot;:960,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1194587,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></figure></div><p><strong>“War…What is it good for?” </strong>the classic song asks. Many universities agree on the answer: “Absolutely nothing.”</p><p>Although anthropologists and archeologists still wonder why human beings have for so long organized themselves to fight, the study of war in history and political science departments is fading. Senior scholars are retiring and not being replaced, or their posts are allocated to other fields of history. Each year, fewer courses are offered on great conflicts such as the Napoleonic wars, the total wars of the 20th century, and the Cold War. The Second World War, you may hear on campus, “has been done.”</p><p>Yet war remains one of the events—along with revolution, famine, financial collapse and, as we are learning again, pandemics—that change the course of history. In privileged countries, we forget the importance of military conflicts because we have enjoyed the “Long Peace” that followed the Second World War. Other places have not been so fortunate, with wars around the world almost every year since 1945, bringing millions of deaths and creating millions more refugees. Meanwhile, the world’s great powers maintain large military establishments, and still prepare for battle. </p><p>It is as important as ever to understand war—its causes, nature and consequences—and the many ways that conflicts have shaped our societies. The conquering armies that came from the Arabian Peninsula in the 7th century after the Prophet Mohammad’s death created new empires and spread the new religion of Islam across the Middle East, North Africa and the Iberian Peninsula. The Seven Years’ War (1756-1763), a global conflict, laid the foundations for the dominance of the British Empire and the bankruptcy of France, which helped to fuel the French Revolution.&nbsp; </p><p>War can also speed up advances in science and technology that have benefits in peacetime. Think of the development of penicillin, blood transfusions, radar, or the transistor. And war can bring about significant social change, often for the better. The need for mass armies in the 19th and 20th centuries meant that governments had to treat the lower classes better, whether by educating them or by improving public health. In many countries that fought in the two world wars, ruling classes recognized the contributions of women and the working classes, granting them the franchise and introducing social benefits. </p><p>So why do history faculties, which accept the need to study other great forces in history, such as changes in the means of production or systems of belief, shy from war? I suspect that horror at the phenomenon itself has affected universities’ willingness to treat it as a subject for scholarship. Years ago, when I proposed a new course on war and society, an education consultant asked me, “Why don’t you call it peace studies?” </p><p>I have since met with incomprehension, even hostility, when I have pointed out that wars can bring unintended benefits. However much I say that we would not <em>choose</em> to make war in order to improve our societies, I am charged with loving war. Yet nobody would say that the study of imperialism, racism or famine means that we think those are good things.</p><p><strong>The history of war is neglected for other reasons, too. </strong>First, separate histories—of women, emotions, food and the environment, for example—have come along. Quite rightly, room has been made for them in the big and eclectic house that is the study of the past. However, part of the shift away from war studies owes to the quest for “social justice”—intended as a drive for radical change in society at large—that has taken root in many history faculties. </p><p>Here, for example, is how the chair of Historical and Cultural Studies at the Scarborough campus of the University of Toronto, Natalie Rothman, <a href="https://www.utsc.utoronto.ca/hcs/">welcomed students</a> this autumn: “As a department, we have strengthened our resolve to confront racism, colonialism and Islamophobia throughout our curriculum and in our co-curricular initiatives.” The University of Berkeley in California <a href="https://history.berkeley.edu/graduate/prospective-students/admissions">asks prospective graduate students</a> to provide “evidence of how you have come to understand the barriers faced by others, evidence of your academic service to advance equitable access to higher education for women, racial minorities, and individuals from other groups that have been historically underrepresented in higher education, evidence of your research focusing on underserved populations or related issues of inequality, or evidence of your leadership among such groups.” It is hard to quarrel with such goals, but their impact on curriculum has been to <a href="http://www.nytimes.com/2016/08/29/opinion/why-did-we-stop-teaching-political-history.html?smid=em-share">downgrade subjects such as political and military history,</a> which are seen as too focused on elites and complicit with hierarchy and oppression. </p><p>Another factor is that history overall is worryingly in decline as an academic subject. While it remains popular among publishers and readers, enrollments in history majors are significantly down. They have <a href="https://www.historians.org/publications-and-directories/perspectives-on-history/december-2018/the-history-ba-since-the-great-recession-the-2018-aha-majors-report">dropped more than any other major</a> in the humanities—perhaps by as much as <a href="http://www.chronicle.com/article/why-are-students-ditching-the-history-major">one-third</a> in <a href="https://www.insidehighered.com/news/2018/11/27/new-analysis-history-major-data-says-field-new-low-can-it-be-saved">American universities</a> in the past decade. At the University of Toronto, where I am a professor, colleagues fear that history enrollment may be down as much as 50% over the same period. Even in the United Kingdom, where history remains popular among undergraduates, the number of those majoring in history has dropped by about <a href="https://www.economist.com/britain/2019/07/18/the-study-of-history-is-in-decline-in-britain">one-tenth</a> in the past decade. </p><p>Part of the reason is that, given the lingering effects of the 2008 financial crisis and uncertainty over the economy, students and their parents want university courses to lead to jobs. The decline in history students in turn affects university hiring, and the fewer the tenured faculty, the fewer places for the doctoral students who are the future of the profession.&nbsp; </p><p>Faculties and administrators are not necessarily helping matters, reluctant to include popular courses on war in the curriculum, or to support well-established centers for the study of conflict, some of which are being remodeled, such as the Laurier Centre for Military, Strategic and Disarmament Studies, in Waterloo, Ontario, which will focus more on Canadian history; another at the University of Calgary is fading as those who retire are not replaced. This seems to be particularly true of elite universities. War studies remains in better health at military colleges or some second-tier public universities, while schools of public policy are also still teaching military history, strategic studies and diplomatic history. </p><p>But those in other fields stereotype war studies, characterizing it as too narrowly focused on tactics, battles, or “toys for boys,” meaning armaments. If that caricature were ever true, it has not been for decades. The great historian Sir Michael Howard, who pioneered the modern study of war and trained generations of historians, always insisted that what he was doing was to consider wars within their social and political contexts as a part of the great sweep of history, not somehow separate from it. &nbsp;</p><p><strong>My own evidence of the distaste for military and diplomatic history </strong>at North American universities comes from tales exchanged privately among fellow academics. One retired Canadian military historian recounted that his old faculty had asked him how to reverse the collapse in enrollment. He suggested a course in military history, but the response was a flat no. When a university in the Maritime provinces of eastern Canada was offered a fully funded post in naval history a few years ago—a good fit in a port town that had been deeply affected by conflict on the Atlantic—members of the department rejected it. </p><p>Yet, despite the overall downturn in university history programs, we know from course enrollments that students are interested in war, as indeed they are in international relations, when they get the chance to study them: At Yale, Paul Kennedy’s “Military History of the West” attracted large crowds; at Toronto’s Ryerson University, international relations courses are the most popular choices among students.</p><p>I would not suggest that student preference should determine what departments offer. But they should at least be listened to. Much more important is what we, as societies, want our future leaders to know. Political history, diplomatic history and the study of war—they all offer critical warnings and instructive analogies to our times. Social and cultural histories, and history from the bottom up, add to our understanding too. But we need balance, and a sense of how the micro- and macro-histories mesh with each other. </p><p>Do we really want citizens who have so little knowledge of how war helped to shape our values and societies and our world? Do we ever want another president asking, as Donald Trump <a href="https://www.businessinsider.com/trump-pearl-harbor-memorial-tour-john-kelly-stable-genius-2020-1?r=US&amp;IR=T">did</a> during a visit to the Pearl Harbor memorial: “What’s this all about? What’s this a tour of?” </p><p>If we aren’t aware of how wars happen, we may fail to recognize warning signs when the next conflict brews, as it will. </p><p><strong>Margaret MacMillan is a professor of history at the University of Toronto and emeritus professor at the University of Oxford. Her latest book, </strong><em><strong><a href="https://www.penguinrandomhouse.com/books/609692/war-how-conflict-shaped-us-by-margaret-macmillan/">War: How Conflict Shaped Us</a></strong></em><strong>, was among the </strong><em><strong>New York Times</strong></em><strong> 10 Best Books of 2020. </strong></p></div></div>]]>
            </description>
            <link>https://www.persuasion.community/p/if-you-want-peace-study-war-533</link>
            <guid isPermaLink="false">hacker-news-small-sites-25772365</guid>
            <pubDate>Thu, 14 Jan 2021 05:10:46 GMT</pubDate>
        </item>
    </channel>
</rss>
