<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 18 Feb 2021 12:39:43 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 18 Feb 2021 12:39:43 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[US Presidential Election $25k Database Bounty Review]]>
            </title>
            <description>
<![CDATA[
Score 139 | Comments 31 (<a href="https://news.ycombinator.com/item?id=26149511">thread link</a>) | @mjangle1985
<br/>
February 15, 2021 | https://www.dolthub.com/blog/2021-02-15-election-bounty-review/ | <a href="https://web.archive.org/web/*/https://www.dolthub.com/blog/2021-02-15-election-bounty-review/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-cy="blog-post-text"><p>On December 14, we launched <a href="https://www.dolthub.com/blog/2020-12-16-data-bounties/">our first data bounty</a> to <a href="https://www.dolthub.com/blog/2020-12-14-make-money-data-wrangling/">earn a share of $25,000 by wrangling US Presidential Precinct-level data</a>. The bounty ended yesterday. How did it go? This blog entry will answer that question.</p>
<p><a href="https://www.doltdb.com/">Dolt</a> is a SQL database with Git-style versioning. It's the first SQL database you can branch and merge. <a href="https://www.dolthub.com/">DoltHub</a> is a place on the internet to share and collaborate on Dolt databases. Without both, data bounties would not be possible.</p>

<p>We built the <a href="https://www.dolthub.com/repositories/dolthub/us-president-precinct-results">best open database of US Precinct-level Election results</a> on the internet. </p>
<p>Here's some statistics:</p>
<ul>
<li>15.5M cells edited. 1.7GB of data collected</li>
<li><a href="https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+COUNT%28distinct%28state%29%29%0AFROM+%60vote_tallies%60+where+election_year%3D2016%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&amp;active=Tables">All 51 "states" covered for 2016</a>. <a href="https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+COUNT%28distinct%28state%29%29%0AFROM+%60vote_tallies%60+where+election_year%3D2020%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&amp;active=Tables">38 states covered for 2020</a>. </li>
<li><a href="https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+sum%28votes%29%2F136669276%0AFROM+%60vote_tallies%60+where+election_year%3D2016%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&amp;active=Tables">100% of the vote covered for 2016</a>. <a href="https://www.dolthub.com/repositories/dolthub/us-president-precinct-results/query/master?q=SELECT+sum%28votes%29%2F159633396%0AFROM+%60vote_tallies%60+where+election_year%3D2020%0ALIMIT+200%3B%0A%0A%0A%0A%0A%0A%0A&amp;active=Tables">78% for 2020</a>.</li>
<li>75 Pull Requests (PRs) accepted across 6 bounty participants. </li>
<li>Top bounty participant earned over $10,000.</li>
</ul>
<p><span>
      <a href="https://www.dolthub.com/blog/static/4e02e3f4498d3a6b5bc56bd9f85cad81/0fcea/first-bounty-final-scoreboard.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Final Scoreboard" title="Final Scoreboard" src="https://www.dolthub.com/blog/static/4e02e3f4498d3a6b5bc56bd9f85cad81/0fcea/first-bounty-final-scoreboard.png" srcset="https://www.dolthub.com/blog/static/4e02e3f4498d3a6b5bc56bd9f85cad81/a48b3/first-bounty-final-scoreboard.png 214w,
https://www.dolthub.com/blog/static/4e02e3f4498d3a6b5bc56bd9f85cad81/47730/first-bounty-final-scoreboard.png 428w,
https://www.dolthub.com/blog/static/4e02e3f4498d3a6b5bc56bd9f85cad81/0fcea/first-bounty-final-scoreboard.png 851w" sizes="(max-width: 851px) 100vw, 851px" loading="lazy">
  </a>
    </span></p>
<p>We're very excited to mail the checks to all the folks who worked hard to make this bounty a success.</p>
<p><span>
      <a href="https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/2ad33/fat-check.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="Fat Check" title="Fat Check" src="https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/79a4e/fat-check.jpg" srcset="https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/606a2/fat-check.jpg 214w,
https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/65a3f/fat-check.jpg 428w,
https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/79a4e/fat-check.jpg 856w,
https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/99aeb/fat-check.jpg 1284w,
https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/e88f6/fat-check.jpg 1712w,
https://www.dolthub.com/blog/static/9791bedbcbd3ed06e15435f17a7ff195/2ad33/fat-check.jpg 3851w" sizes="(max-width: 856px) 100vw, 856px" loading="lazy">
  </a>
    </span></p>

<p>Now that the bounty is complete, we must figure out what to do with this data. How do we get the most out of our investment in the data?</p>
<h2>Clean up the data we got</h2>
<p>Given the heavily keyed nature of this data, we decided early on to not accept PRs for standardization or normalization of things like party names. Because most columns are part of the primary key, any change looks like a deletion and corresponding addition to Dolt. Thus, changes like that give the corrector credit for the whole row in the bounty instead of just the cell he or she changed. We queued up a number of these types of changes and we'll execute them now that the bounty is over.</p>
<h2>Find some users</h2>
<p>We collected the best US presidential Precinct level results on the internet. Now, we want people to use it. We think potentially the data is good enough that an open data community could be bootstrapped around it. The people who use it would have incentive to finish it as new state level data is released. We will reach out to the people at <a href="http://openelections.net/">Open Elections</a> and <a href="https://electionlab.mit.edu/">MIT Elections Lab</a> and start a conversation. If you know anyone else who could use this data, <a href="https://discord.com/invite/RFwfYpu">come let us know in our Discord</a>.</p>
<h2>Run another bounty?</h2>
<p>We're still missing 12 states and about 22% of the vote from 2020. If we want to get complete data and an open data community can't be bootstrapped, we may run another bounty in a couple months to finish the dataset. Let us know if you'd be interested in another bounty or the complete dataset.</p>

<p>We think <a href="https://www.dolthub.com/bounties">DoltHub Bounties</a> may be the fastest, cheapest way to build databases from open data. For $25,000 and 8 weeks, we were able to assemble a 1.7GB database of election results. </p>
<p>Our plan is to run at least one bounty per month for the rest of the year across a number of distinct data disciplines. We're running a <a href="https://www.dolthub.com/blog/2021-01-14-hopsital-prices-bounty/">hospital price transparency bounty</a> right now. We'll be launching two more over the next month or so. Hang out in <a href="https://discord.com/invite/RFwfYpu">our Discord</a> to keep up to date. Start wrangling data as your new side hustle.</p></div></div>]]>
            </description>
            <link>https://www.dolthub.com/blog/2021-02-15-election-bounty-review/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26149511</guid>
            <pubDate>Tue, 16 Feb 2021 00:20:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building Rich Terminal Dashboards]]>
            </title>
            <description>
<![CDATA[
Score 332 | Comments 71 (<a href="https://news.ycombinator.com/item?id=26149488">thread link</a>) | @lumpa
<br/>
February 15, 2021 | https://www.willmcgugan.com/blog/tech/post/building-rich-terminal-dashboards/ | <a href="https://web.archive.org/web/*/https://www.willmcgugan.com/blog/tech/post/building-rich-terminal-dashboards/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    

    
<p><a href="https://github.com/willmcgugan/rich">Rich</a> has become a popular (20K stars on GH) way of beautifying CLIs, and I'm pleased to see a number of projects using it.</p>
<p>Since Rich is mature and battle-tested now, I had considered winding-down development. Until, I saw this tweet:</p>
<h2>The Tweet</h2>
<blockquote><div lang="en" dir="ltr"><p>Do you want to see something really cool you can do with <a href="https://twitter.com/willmcgugan?ref_src=twsrc%5Etfw">@willmcgugan</a> 's rich library? </p><p>Checkout ghtop <a href="https://t.co/mn7oLbpw8e">https://t.co/mn7oLbpw8e</a>. It's a really fun CLI tool that demonstrates the power of rich (and other things).</p><p>Examples below üßµüëá (1/8)</p></div>‚Äî Hamel Husain (@HamelHusain) <a href="https://twitter.com/HamelHusain/status/1357771218095546368?ref_src=twsrc%5Etfw">February 5, 2021</a></blockquote> 
<p><a href="https://twitter.com/HamelHusain">@HamelHusain</a> and <a href="https://twitter.com/jeremyphoward">@jeremyphoward</a> used Rich to enhance <a href="https://github.com/nat/ghtop">ghtop</a> (a repo owned by the CEO of Github, no less). Ghtop shows a realtime a stream of events from the Github platform. And it looks good! So good that I realised how much potential Rich has for these type of htop-like applications.</p>
<p>Hamel and Jeremy had to overcome a few technical hurdles to make that work. Fortunately this will no longer be required as the latest version of Rich has first-class support for full-screen interfaces via a new Layout system.</p>
<h2>Full-screen terminal interface</h2>
<p>Here's a video demonstration of a terminal interface built with Layout:</p>
<p>
<iframe width="auto" src="https://www.youtube.com/embed/slrdSojCvlk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p>
<h2>Layout API</h2>
<p>The API to create a flexible layout is surprisingly simple. You construct a <code>Layout()</code> object, then call <code>split()</code> to create sub-layouts. These sub-layouts may then be further divided. Layouts have a small number of settings which define their size relative to the terminal window. It's a simple system that can create terminal interfaces that almost resemble modern web apps.</p>
<div><figure> <img width="1140" height="724" title="layout_vscode.png" name="layout_vscode" data-md="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.md.3.jpeg" data-square="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.square.7.jpeg" data-xlg2x="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.xlg2x.2.jpeg" data-lg="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.lg.3.jpeg" data-xlg="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.xlg.8.jpeg" data-author="¬© 2021 Will McGugan<br>all rights reserved" data-og_preview="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.og.preview.2.jpeg" data-height="2100" data-width="3308" data-toggle="tooltip" data-details=" " data-sm="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.sm.3.jpeg" data-blur="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.blur.6.jpeg" data-slug="layout_vscode" sizes="(min-width: 1200px) 1170px, (min-width: 992px) 970px, (min-width: 768px) 750px" srcset="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.lg.3.jpeg 1170w, https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.md.3.jpeg 970w, https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.sm.3.jpeg 750w" src="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44.png/a7ab7ab6-6fb0-11eb-bc8d-f23c91845b44png.lg.3.jpeg"> 
<figcaption>
<p>¬© 2021 Will McGugan</p>


<p>A Rich layout in VSCode</p>


</figcaption>

</figure>
</div>
<h2>Finally, some code</h2>
<p>Here's how you would create a basic layout with a header, a footer, and two side-panels.</p>
<pre><code>from rich.console import Console
from rich.layout import Layout

console = Console()
layout = Layout()

# Divide the "screen" in to three parts
layout.split(
    Layout(name="header", size=3),
    Layout(ratio=1, name="main"),
    Layout(size=10, name="footer"),
)
# Divide the "main" layout in to "side" and "body"
layout["main"].split(
    Layout(name="side"),
    Layout(name="body", ratio=2),
    direction="horizontal"
)
# Divide the "side" layout in to two
layout["side"].split(Layout(), Layout())

console.print(layout)
</code></pre>
<p>Running the code above produces the following output:</p>
<div><figure> <img width="1140" height="739" title="layout.png" name="layout" data-md="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.md.3.jpeg" data-square="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.square.7.jpeg" data-xlg2x="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.xlg2x.2.jpeg" data-lg="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.lg.3.jpeg" data-xlg="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.xlg.8.jpeg" data-author="¬© 2021 Will McGugan<br>all rights reserved" data-og_preview="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.og.preview.2.jpeg" data-height="1464" data-width="2258" data-toggle="tooltip" data-details=" " data-sm="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.sm.3.jpeg" data-blur="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.blur.6.jpeg" data-slug="layout" sizes="(min-width: 1200px) 1170px, (min-width: 992px) 970px, (min-width: 768px) 750px" srcset="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.lg.3.jpeg 1170w, https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.md.3.jpeg 970w, https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.sm.3.jpeg 750w" src="https://media.moyaproject.com/willmcgugan/uploads/thumbnails/techblog/98f8e5f8-6fab-11eb-bc8d-f23c91845b44.png/98f8e5f8-6fab-11eb-bc8d-f23c91845b44png.lg.3.jpeg"> 
<figcaption>
<p>¬© 2021 Will McGugan</p>


<p>Terminal split in to 5 sub-layouts. The boxes are placeholders, you can insert any content in their place.</p>


</figcaption>

</figure>
</div>
<p>Any renderable (text, table, progress bars etc) may be placed inside those sub-layouts.</p>
<p>We can now use the <a href="https://rich.readthedocs.io/en/latest/live.html">Live</a> class to create an application that adapts itself to the terminal window:</p>
<pre><code>from rich.live import Live
from time import sleep

with Live(layout, screen=True):
    while True:
        sleep(1)
</code></pre>
<p>In a real app, that do-nothing loop will be doing something useful like pulling data from the network to update contents.</p>
<p>See the <a href="https://rich.readthedocs.io/en/latest/layout.html">layout docs</a> for an in-depth tutorial.</p>
<h2>What's next?</h2>
<p>I think it's clear that Rich is acquiring more TUI (text user interface) features, and I've decided not to fight it. Rich's core purpose is still to beautify CLI output, but I think there is an opportunity here for a new way to create terminal apps. Ultimately it will be less like curses and more like HTML in a browser.</p>
<p>There are a number of things to do before Rich could replace a full TUI library (keyboard and mouse input for one) but the potential is there. Stay tuned for progress.</p>
<p>Follow <a href="https://twitter.com/willmcgugan">@willmcgugan</a> for more Rich related news.</p>



</article></div>]]>
            </description>
            <link>https://www.willmcgugan.com/blog/tech/post/building-rich-terminal-dashboards/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26149488</guid>
            <pubDate>Tue, 16 Feb 2021 00:16:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What security does a default OpenBSD installation offer?]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 67 (<a href="https://news.ycombinator.com/item?id=26148667">thread link</a>) | @zdw
<br/>
February 15, 2021 | https://dataswamp.org/~solene/2021-02-14-openbsd-default-security.html | <a href="https://web.archive.org/web/*/https://dataswamp.org/~solene/2021-02-14-openbsd-default-security.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<article id="20210214">
  <header>
  
    
    <p>Written by <em>Sol√®ne</em>, on 14 February 2021.<br>Tags: 
<span><a href="https://dataswamp.org/~solene/tag-openbsd69.html">#openbsd69</a></span>


<span><a href="https://dataswamp.org/~solene/tag-openbsd.html">#openbsd</a></span>


<span><a href="https://dataswamp.org/~solene/tag-security.html">#security</a></span>

</p>
    
  </header>
  
<p>In this text I will explain what makes OpenBSD secure by default when you install it.  Do not take this for a security analysis, but more like a guide to help you understand what is done by OpenBSD to have a secure environment.  The purpose of this text is not to compare OpenBSD to others OS but to say what you can honestly expects from OpenBSD.
</p>
<p>There are no security without a threat model, I always consider the following cases: computer stolen at home by a thief, remote attacks trying to exploit running services, exploit of user network clients.
</p>

<p>Here is a list of features that I consider important for an operating system security.  While not every item from the following list are strictly security features, they help having a strict system that prevent software to misbehave and lead to unknown lands.
</p>
<p>In my opinion security is not only about preventing remote attackers to penetrate the system, but also to prevent programs or users to make the system unusable.
</p>
<h2> Pledge / unveil on userland</h2>
<p>Pledge and unveil are often referred together although they can be used independently.  Pledge is a system call to restrict the permissions of a program at some point in its source code, permissions can't be get back once pledge has been called.  Unveil is a system call that will hide all the file system to the process except the paths that are unveiled, it is possible to choose what permissions is allowed for the paths.
</p>
<p>Both a very effective and powerful surgical security tools but they require some modification within the source code of a software, but adding them requires a deep understanding on what the software is doing.  It is not always possible to forbid some system calls to a software that requires to do almost anything, software designed with privilege separation are better candidate for a proper pledge addition because each part has its own job.
</p>
<p>Some software in packages have received pledge or/and unveil support, like Chromium or Firefox for the most well known.
</p>
<p><a href="https://www.openbsd.org/papers/bsdcan2019-unveil/index.html">OpenBSD presentation about Unveil (BSDCan2019)</a></p>
<p><a href="https://www.openbsd.org/papers/BeckPledgeUnveilBSDCan2018.pdf">OpenBSD presentation of Pledge and Unveil (BSDCan2018)</a></p>
<h2> Privilege separation</h2>
<p>Most of the base system services used within OpenBSD runs using a privilege separation pattern.  Each part of a daemon is restricted to the minimum required.  A monolithic daemon would have to read/write files, accept network connections, send messages to the log, in case of security breach this allows a huge attack surface.  By separating a daemon in multiple parts, this allow a more fine grained control of each workers, and using pledge and unveil system calls, it's possible to set limits and highly reduce damage in case a worker is hacked.
</p>
<h2> Clock synchronization</h2>
<p>The daemon server is started by default to keep the clock synchronized with time servers.  A reference TLS server is used to challenge the time servers.  Keeping a computer with its clock synchronized is very important.  This is not really a security feature but you can't be serious if you use a computer on a network without its time synchronized.
</p>
<h2> X display not as root</h2>
<p>If you use the X, it drops privileges to _x11 user, it runs as unpriviliged user instead of root, so in case of security issue this prevent an attacker of accessing through a X11 bug more than what it should.
</p>
<h2> Resources limits</h2>
<p>Default resources limits prevent a program to use too much memory, too many open files or too many processes.  While this can prevent some huge programs to run with the default settings, this also helps finding file descriptor leaks, prevent a fork bomb or a simple daemon to steal all the memory leading to a crash.
</p>
<h2> Genuine full disk encryption</h2>
<p>When you install OpenBSD using a full disk encryption setup, everything will be locked down by the passphrase at the bootloader step, you can't access the kernel or anything of the system without the passphrase.
</p>
<h2> W^X</h2>
<p>Most programs on OpenBSD aren't allowed to map memory with Write AND Execution bit at the same time (W^X means Write XOR Exec), this can prevents an interpreter to have its memory modified and executed.  Some packages aren't compliant to this and must be linked with a specific library to bypass this restriction AND must be run from a partition with the "wxallowed" option.
</p>
<p><a href="https://www.openbsd.org/papers/hackfest2015-w-xor-x.pdf">OpenBSD presentation ¬´ Kernel W^X Improvements In OpenBSD ¬ª</a></p>
<h2> Only one reliable randomness source</h2>
<p>When your system requires a random number (and it does very often), OpenBSD only provides one API to get a random number and they are really random and can't be exhausted.  A good random number generator (RNG) is important for many cryptography requirements.
</p>
<p><a href="https://www.openbsd.org/papers/hackfest2014-arc4random/index.html">OpenBSD presentation about arc4random</a></p>
<h2> Accurate documentation</h2>
<p>OpenBSD comes with a full documentation in its man pages.  One should be able to fully configure their system using only the man pages.  Man pages comes with CAVEATS or BUGS sections sometimes, it's important to take care about those sections.  It is better to read the documentation and understand what has to be done in order to configure a system instead of following an outdated and anonymous text available on the Internet.
</p>
<p><a href="https://man.openbsd.org/">OpenBSD man pages online</a></p>
<p><a href="https://www.openbsd.org/papers/eurobsdcon2018-mandoc.pdf">EuroBSDcon 2018 about ¬´ Better documentation ¬ª</a></p>
<h2> IPSec and Wireguard out of the box</h2>
<p>If you need to setup a VPN, you can use IPSec or Wireguard protocols only using the base system, no package required.
</p>
<h2> Memory safeties</h2>
<p>OpenBSD has many safeties in regards to memory allocation and will prevent use after free or unsafe memory usage very aggressively, this is often a source of crash for some software from packages because OpenBSD is very strict when you want to use the memory.  This helps finding memory misuses and will kill software misbehaving.
</p>
<h2> Dedicated root account</h2>
<p>When you install the system, a root account is created and its password is asked, then you create an user that will be member of "wheel" group, allowing it to switch user to root with root's password.  doas (OpenBSD base system equivalent of sudo) isn't configured by default.  With the default installation, the root password is required to do any root action.  I think a dedicated root account that can be logged in without use of doas/sudo is better than a misconfigured doas/sudo allowing every thing only if you know the user password.
</p>
<h2> Small network attack surface</h2>
<p>The only services that could be enabled at installation time listening on the network are OpenSSH (asked at install time with default = yes), dhclient (if you choose dhcp) and slaacd (if you use ipv6 in automatic configuration).
</p>
<h2> Encrypted swap</h2>
<p>By default the OpenBSD swap is encrypted, meaning if programs memory are sent to the swap nobody can recover it later.
</p>
<h2> SMT disabled</h2>
<p>Due to a heavy number of security breaches due to SMT (like hyperthreading), the default installation disable half the logical cores to prevent any data leak.
</p>
<p><a href="https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)">Meltdown: one of the first security issue related to speculative execution in the CPU</a></p>
<h2> Micro and Webcam disabled</h2>
<p>With the default installation, both microphone and webcam won't actually record anything except blank video/sound until you set a sysctl for this.
</p>
<h3> Maintainability, release often, update often</h3>
<p>The OpenBSD team publish a new release a new version every six months and only last two releases receives security updates.  This allows to upgrade often but without pain, the upgrade process are small steps twice a year that help keep the whole system up to date.  This avoids the fear of a huge upgrade and never doing it and I consider it a huge security bonus.  Most OpenBSD around are running latest versions.
</p>
<h3> Signify chain of trust</h3>
<p>Installer, archives and packages are signed using signify public/private keys.  OpenBSD installations comes with the release and release n+1 keys to check the packages authenticity.  A key is used only six months and new keys are received in each new release allowing to build a chain of trust.  Signify keys are very small and are published on many medias to double check when you need to bootstrap this chain of trust.
</p>
<p><a href="https://www.openbsd.org/papers/bsdcan-signify.html">Signify at BSDCan 2015</a></p>
<h2> Packages</h2>
<p>While most of the previous items were about the base system or the kernel, the packages also have a few tricks to offer.
</p>
<h3> Chroot by default when available</h3>
<p>Most daemons that are available offering a chroot feature will have it enabled by default.  In some circumstances like for Nginx web server, the software is patched by the OpenBSD team to enable chroot which is not an official feature.
</p>
<h3> Dedicated users for services</h3>
<p>Most packages that provide a server also create a new dedicated user for this exact service, allowing more privilege separation in case of security issue in one service.
</p>
<h3> Installing a service doesn't enable it</h3>
<p>When you install a service, it doesn't get enabled by default.  You will have to configure the system to enable it at boot.  There is a single /etc/rc.conf.local files that can be used to see what is enabled at boot, this can be manipulated using rcctl command.  Forcing the user to enable services makes the system administrator fully aware of what is running on the system, which is good point for security.
</p>
<p><a href="https://man.openbsd.org/rcctl">rcctl man page</a></p>

<p>Most of the previous "security features" should be considered good practices and not features.  Limiting users resources, reducing daemon privileges, being strict about memory usage, provide a good documentation, start the least required amount of services and provide the user with a clean default installation that can be customized later.
</p>
<p>There are also many other features that have been added and which I don't fully understand, and that I prefer letting the reader take notice. 
</p>
<p><a href="https://www.openbsd.org/papers/bsdtw.pdf">¬´ Mitigations and other real security features ¬ª by Theo De Raadt</a></p>
<p><a href="https://www.openbsd.org/innovations.html">OpenBSD innovations</a></p>
<p><a href="https://www.openbsd.org/events.html">OpenBSD events, often including slides or videos</a></p>

</article>
</div></div>]]>
            </description>
            <link>https://dataswamp.org/~solene/2021-02-14-openbsd-default-security.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26148667</guid>
            <pubDate>Mon, 15 Feb 2021 22:51:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are the New M1 Macbooks Any Good for Deep Learning?]]>
            </title>
            <description>
<![CDATA[
Score 132 | Comments 118 (<a href="https://news.ycombinator.com/item?id=26148333">thread link</a>) | @syntaxing
<br/>
February 15, 2021 | https://www.betterdatascience.com/m1-deep-learning/ | <a href="https://web.archive.org/web/*/https://www.betterdatascience.com/m1-deep-learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                                                                        
<!-- WP QUADS Content Ad Plugin v. 2.0.19 -->

<p><span data-preserver-spaces="true">There‚Äôs a lot of hype behind the new Apple M1 chip. So far, it‚Äôs proven to be superior to anything Intel has offered. But what does this mean for deep learning? That‚Äôs what you‚Äôll find out today.</span></p>
<p><span data-preserver-spaces="true">The new M1 chip isn‚Äôt just a CPU. On the MacBook Pro, it consists of 8 core CPU, 8 core GPU, and 16 core neural engine, among other things. Both the processor and the GPU are far superior to the previous-generation Intel configurations.</span></p>
<p><span data-preserver-spaces="true">I‚Äôve already demonstrated how fast the M1 chip is for&nbsp;</span><a href="https://www.betterdatascience.com/mac-m1-python/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">regular data science tasks</span></a><span data-preserver-spaces="true">, but what about deep learning?&nbsp;</span></p>
<p><span data-preserver-spaces="true">Short answer ‚Äì yes, there are some improvements in this department, but are Macs now better than, let‚Äôs say,&nbsp;</span><em><span data-preserver-spaces="true">Google Colab</span></em><span data-preserver-spaces="true">? Keep in mind, Colab is an entirely free option.</span></p>
<p><span data-preserver-spaces="true">The article is structured as follows:</span></p>
<ul>
<li><a href="#cpu">CPU and GPU benchmark</a></li>
<li><a href="#test">Performance test ‚Äì MNIST</a></li>
<li><a href="#fashion">Performance test ‚Äì Fashion MNIST</a></li>
<li><a href="#cifar">Performance test ‚Äì CIFAR-10</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2><span data-preserver-spaces="true">Important notes</span></h2>
<p><span data-preserver-spaces="true">Not all data science libraries are compatible with the new M1 chip yet. Getting TensorFlow (version 2.4) to work properly is easier said than done.</span></p>
<p><span data-preserver-spaces="true">You can refer to&nbsp;</span><a href="https://github.com/apple/tensorflow_macos" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">this link</span></a><span data-preserver-spaces="true">&nbsp;to download the .</span><em><span data-preserver-spaces="true">whl</span></em><span data-preserver-spaces="true">&nbsp;files for TensorFlow and it‚Äôs dependencies. This is only for macOS 11.0 and above, so keep that in mind.</span></p>
<p><span data-preserver-spaces="true">The test you‚Äôll see aren‚Äôt ‚Äúscientific‚Äù in any way, shape or form. They only compare the average training time per epoch.</span></p>
<h2 id="cpu"><span data-preserver-spaces="true">CPU and GPU benchmark</span></h2>
<p><span data-preserver-spaces="true">Let‚Äôs start with the basic CPU and GPU benchmarks first. The comparison is made between the new MacBook Pro with the M1 chip and the base model (Intel) from 2019.&nbsp;</span><a href="https://www.geekbench.com/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Geekbench 5</span></a><span data-preserver-spaces="true">&nbsp;was used for the tests, and you can see the results below:</span></p>
<div id="attachment_3868"><p><img aria-describedby="caption-attachment-3868" loading="lazy" src="https://www.betterdatascience.com/wp-content/uploads/2021/01/1-7.png" alt="Image 1 - Geekbench 5 results (Intel MBP vs. M1 MBP) (image by author)" width="1514" height="324" srcset="https://www.betterdatascience.com/wp-content/uploads/2021/01/1-7.png 1514w, https://www.betterdatascience.com/wp-content/uploads/2021/01/1-7-768x164.png 768w" sizes="(max-width: 1514px) 100vw, 1514px"></p><p id="caption-attachment-3868">Image 1 ‚Äì Geekbench 5 results (Intel MBP vs. M1 MBP) (image by author)</p></div>
<p><span data-preserver-spaces="true">The results speak for themselves. M1 chip demolished Intel chip in my 2019 Mac. So far, things look promising.</span></p>
<h2 id="test"><span data-preserver-spaces="true">Performance test ‚Äì MNIST</span></h2>
<p><span data-preserver-spaces="true">The MNIST dataset is something like a ‚Äúhello world‚Äù of deep learning. It comes built-in with TensorFlow, making it that much easier to test.&nbsp;</span></p>
<p><span data-preserver-spaces="true">The following script trains a neural network classifier for ten epochs on the MNIST dataset. If you‚Äôre on an M1 Mac, uncomment the <code>mlcompute</code> lines, as these will make things run a bit faster:</span></p>

<p><span data-preserver-spaces="true">The above script was executed on an M1 MBP and Google Colab (both CPU and GPU). You can see the runtime comparisons below:</span></p>
<div id="attachment_3869"><p><img aria-describedby="caption-attachment-3869" loading="lazy" src="https://www.betterdatascience.com/wp-content/uploads/2021/01/2-6.png" alt="Image 2 - MNIST model average training times (image by author)" width="2146" height="926" srcset="https://www.betterdatascience.com/wp-content/uploads/2021/01/2-6.png 2146w, https://www.betterdatascience.com/wp-content/uploads/2021/01/2-6-768x331.png 768w, https://www.betterdatascience.com/wp-content/uploads/2021/01/2-6-1536x663.png 1536w, https://www.betterdatascience.com/wp-content/uploads/2021/01/2-6-2048x884.png 2048w, https://www.betterdatascience.com/wp-content/uploads/2021/01/2-6-20x10.png 20w" sizes="(max-width: 2146px) 100vw, 2146px"></p><p id="caption-attachment-3869">Image 2 ‚Äì MNIST model average training times (image by author)</p></div>
<p><span data-preserver-spaces="true">The results are somewhat disappointing for a new Mac. Colab outperformed it in both CPU and GPU runtimes. Keep in mind that results may vary, as there‚Äôs no guarantee of the runtime environment in Colab.</span></p>
<h2 id="fashion"><span data-preserver-spaces="true">Performance test ‚Äì Fashion MNIST</span></h2>
<p><span data-preserver-spaces="true">This dataset is quite similar to the regular MNIST, but is contains pieces of clothing instead of handwritten digits. Because of that, you can use the identical neural network architecture for the training:</span></p>
<!-- WP QUADS Content Ad Plugin v. 2.0.19 -->



<p><span data-preserver-spaces="true">As you can see, the only thing that‚Äôs changed here is the function used to load the dataset. The runtime results for the same environments are shown below:</span></p>
<div id="attachment_3870"><p><img aria-describedby="caption-attachment-3870" loading="lazy" src="https://www.betterdatascience.com/wp-content/uploads/2021/01/3-5.png" alt="Image 3 - Fashion MNIST model average training times (image by author)" width="2146" height="926" srcset="https://www.betterdatascience.com/wp-content/uploads/2021/01/3-5.png 2146w, https://www.betterdatascience.com/wp-content/uploads/2021/01/3-5-768x331.png 768w, https://www.betterdatascience.com/wp-content/uploads/2021/01/3-5-1536x663.png 1536w, https://www.betterdatascience.com/wp-content/uploads/2021/01/3-5-2048x884.png 2048w, https://www.betterdatascience.com/wp-content/uploads/2021/01/3-5-20x10.png 20w" sizes="(max-width: 2146px) 100vw, 2146px"></p><p id="caption-attachment-3870">Image 3 ‚Äì Fashion MNIST model average training times (image by author)</p></div>
<p><span data-preserver-spaces="true">Once again, we get similar results. It‚Äôs expected, as this dataset is quite similar to MNIST.&nbsp;</span></p>
<p><span data-preserver-spaces="true">But what will happen if we introduce a more complex dataset and neural network architecture?&nbsp;</span></p>
<h2 id="cifar"><span data-preserver-spaces="true">Performance test ‚Äì CIFAR-10</span></h2>
<p><span data-preserver-spaces="true">CIFAR-10 also falls into the category of ‚Äúhello world‚Äù deep learning datasets. It contains 60K images from ten different categories, such as airplanes, birds, cats, dogs, ships, trucks, etc.</span></p>
<p><span data-preserver-spaces="true">The images are of size 32x32x3, which makes them difficult to classify even for humans in some cases. The script below trains a classifier model by using three convolutional layers:</span></p>

<p><span data-preserver-spaces="true">Let‚Äôs see how convolutional layers and more complex architecture affects the runtime:</span></p>
<div id="attachment_3871"><p><img aria-describedby="caption-attachment-3871" loading="lazy" src="https://www.betterdatascience.com/wp-content/uploads/2021/01/4-5.png" alt="Image 4 - CIFAR-10 model average training times (image by author)" width="2146" height="926" srcset="https://www.betterdatascience.com/wp-content/uploads/2021/01/4-5.png 2146w, https://www.betterdatascience.com/wp-content/uploads/2021/01/4-5-768x331.png 768w, https://www.betterdatascience.com/wp-content/uploads/2021/01/4-5-1536x663.png 1536w, https://www.betterdatascience.com/wp-content/uploads/2021/01/4-5-2048x884.png 2048w, https://www.betterdatascience.com/wp-content/uploads/2021/01/4-5-20x10.png 20w" sizes="(max-width: 2146px) 100vw, 2146px"></p><p id="caption-attachment-3871">Image 4 ‚Äì CIFAR-10 model average training times (image by author)</p></div>
<p><span data-preserver-spaces="true">As you can see, the CPU environment in Colab comes nowhere close to the GPU and M1 environments. The Colab GPU environment is still around 2x faster than Apple‚Äôs M1, similar to the previous two tests.</span></p>
<h2 id="conclusion"><span data-preserver-spaces="true">Conclusion</span></h2>
<p><span data-preserver-spaces="true">I love every bit of the new M1 chip and everything that comes with it ‚Äì better performance, no overheating, and better battery life. Still, it‚Äôs a difficult laptop to recommend if you‚Äôre into deep learning.</span></p>
<p><span data-preserver-spaces="true">Sure, there‚Äôs around 2x improvement in M1 than my other Intel-based Mac, but these still aren‚Äôt machines made for deep learning. Don‚Äôt get me wrong, you can use the MBP for any basic deep learning tasks, but there are better machines in the same price range if you‚Äôll do deep learning daily.</span></p>
<p><span data-preserver-spaces="true">This article covered deep learning only on simple datasets. The next one will compare the M1 chip with Colab on more demanding tasks ‚Äì such as transfer learning.&nbsp;</span></p>
<p><span data-preserver-spaces="true">Thanks for reading.</span></p>
<p><a href="https://www.linkedin.com/in/darioradecic/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Connect on LinkedIn.</span></a></p>
<h4><a href="https://mailchi.mp/46a3d2989d9b/bdssubscribe" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Join my private email list for more helpful insights.</span></a></h4>
<h2><span data-preserver-spaces="true">Learn more&nbsp;</span></h2>
<ul>
<li><a href="https://towardsdatascience.com/top-5-books-to-learn-data-science-in-2020-f43153851f14" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Top 5 Books to Learn Data Science in 2021</span></a></li>
<li><a href="https://www.betterdatascience.com/mac-m1-python/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Are The New M1 Macbooks Any Good for Data Science? Let‚Äôs Find Out</span></a></li>
<li><a href="https://www.betterdatascience.com/python-pdf-reports/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">How to Create PDF Reports with Python ‚Äì The Essential Guide</span></a></li>
<li><a href="https://www.betterdatascience.com/python-multiprocessing/" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">Python Parallelism: Essential Guide to Speeding up Your Python Code in Minutes</span></a></li>
<li><a href="https://towardsdatascience.com/shap-how-to-interpret-machine-learning-models-with-python-2323f5af4be9" target="_blank" rel="noopener noreferrer"><span data-preserver-spaces="true">SHAP: How to Interpret Machine Learning Models With Python</span></a></li>
</ul>


<!-- WP QUADS Content Ad Plugin v. 2.0.19 -->


            					</div></div>]]>
            </description>
            <link>https://www.betterdatascience.com/m1-deep-learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26148333</guid>
            <pubDate>Mon, 15 Feb 2021 22:23:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Improved Thread with C++20]]>
            </title>
            <description>
<![CDATA[
Score 65 | Comments 56 (<a href="https://news.ycombinator.com/item?id=26142257">thread link</a>) | @ibobev
<br/>
February 15, 2021 | http://modernescpp.com/index.php/an-improved-thread-with-c-20 | <a href="https://web.archive.org/web/*/http://modernescpp.com/index.php/an-improved-thread-with-c-20">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="blogContent">
				<p><code>std::jthread</code> stands for joining thread. In addition to <code>std::thread</code> (C++11),<code> std::jthread</code> automatically joins in its destructor and can cooperatively be interrupted. Read in this post to know why <code>std::jthread</code> should be your first choice.</p>

<p>&nbsp;<img src="http://modernescpp.com/images/blog/Cpp20/jthread/TimelineCpp20.png" alt="TimelineCpp20" width="650" height="227"></p>
<p>The following table gives you a concise overview of the functionality of <code>std::jthread</code>.</p>
<p><img src="http://modernescpp.com/images/blog/Cpp20/jthread/jthread.png" alt="jthread" width="650" height="572"></p>

<p>For additional details, please refer to <a href="https://en.cppreference.com/w/cpp/thread/jthread">cppreference.com</a>. When you want to read more post about <code>std::thread</code>, here are they: <a href="http://modernescpp.com/index.php/der-einstieg-in-modernes-c#h1-2-the-threading-interface">my post about std::thread.</a></p>
<p>First, why do we need an improved thread in C++20? Here is the first reason.</p>
<h2 id="h1-automatically-joining">Automatically Joining</h2>
<p>This is the <strong>non-intuitive</strong> behavior of <code>std::thread</code>. If a <code>std::thread</code> is still joinable, <a href="https://en.cppreference.com/w/cpp/error/terminate">std::terminate</a> is called in its destructor. A thread<code> thr</code> is joinable if neither <code>thr.join()</code> nor <code>thr.detach()</code> was called. Let me show, what that means.</p>
<!-- HTML generated using hilite.me -->
<div>
<pre><span>// threadJoinable.cpp</span>

<span>#include &lt;iostream&gt;</span>
<span>#include &lt;thread&gt;</span>

<span>int</span> <span>main</span>() {
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
    std<span>::</span>cout <span>&lt;&lt;</span> std<span>::</span>boolalpha;
    
    std<span>::</span><span>thread</span> thr{[]{ std<span>::</span>cout <span>&lt;&lt;</span> <span>"Joinable std::thread"</span> <span>&lt;&lt;</span> <span>'\n'</span>; }};
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"thr.joinable(): "</span> <span>&lt;&lt;</span> thr.joinable() <span>&lt;&lt;</span> <span>'\n'</span>;
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
    
}
</pre>
</div>

<p>When executed, the program terminates when the local object <code>thr</code> goes out of scope.</p>
<p><img src="http://modernescpp.com/images/blog/Cpp20/jthread/threadJoinable.png" alt="threadJoinable" width="500" height="279"></p>
<p>Both executions of<code> std::thread</code> terminate. In the second run, the thread <code>thr</code> has enough time to display its message:&nbsp;<code>Joinable std::thread</code>.</p>
<p>In the next example, I use <code>std::jthread</code> from the C++20 standard.</p>
<!-- HTML generated using hilite.me -->
<div>
<pre><span>// jthreadJoinable.cpp</span>

<span>#include &lt;iostream&gt;</span>
<span>#include &lt;thread&gt;</span>

<span>int</span> <span>main</span>() {
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
    std<span>::</span>cout <span>&lt;&lt;</span> std<span>::</span>boolalpha;
    
    std<span>::</span>jthread thr{[]{ std<span>::</span>cout <span>&lt;&lt;</span> <span>"Joinable std::thread"</span> <span>&lt;&lt;</span> <span>'\n'</span>; }};
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"thr.joinable(): "</span> <span>&lt;&lt;</span> thr.joinable() <span>&lt;&lt;</span> <span>'\n'</span>;
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
    
}
</pre>
</div>

<p>Now, the thread<code> thr</code> automatically joins in its destructor if it's still joinable such as in this case.</p>
<p><img src="http://modernescpp.com/images/blog/Cpp20/jthread/jthreadJoinable.png" alt="jthreadJoinable" width="350" height="177"></p>
<p>But this is not all that<code> std::jthread</code>&nbsp; provides additionally to <code>std::thread</code>. A <code>std::jthread</code> can be cooperatively interrupted. I already presented the general ideas of cooperative interruption in my last post: <a href="http://modernescpp.com/index.php/cooperative-interruption-of-a-thread-in-c-20">Cooperative Interruption of a Thread in C++20</a>.</p>
<h2 id="h2-cooperative-interruption-of-a-std-jthread">Cooperative Interruption of a<code> std::jthread</code></h2>
<p>To get a general idea, let me present a simple example.</p>
<!-- HTML generated using hilite.me -->
<div>
<pre><span>// interruptJthread.cpp</span>

<span>#include &lt;chrono&gt;</span>
<span>#include &lt;iostream&gt;</span>
<span>#include &lt;thread&gt;</span>

<span>using</span> <span>namespace</span><span>::</span>std<span>::</span>literals;

<span>int</span> <span>main</span>() {
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
    
    std<span>::</span>jthread nonInterruptable([]{                           <span>// (1)</span>
        <span>int</span> counter{<span>0</span>};
        <span>while</span> (counter <span>&lt;</span> <span>10</span>){
            std<span>::</span>this_thread<span>::</span>sleep_for(<span>0.2</span>s);
            std<span>::</span>cerr <span>&lt;&lt;</span> <span>"nonInterruptable: "</span> <span>&lt;&lt;</span> counter <span>&lt;&lt;</span> <span>'\n'</span>; 
            <span>++</span>counter;
        }
    });
    
    std<span>::</span>jthread interruptable([](std<span>::</span>stop_token stoken){     <span>// (2)</span>
        <span>int</span> counter{<span>0</span>};
        <span>while</span> (counter <span>&lt;</span> <span>10</span>){
            std<span>::</span>this_thread<span>::</span>sleep_for(<span>0.2</span>s);
            <span>if</span> (stoken.stop_requested()) <span>return</span>;               <span>// (3)</span>
            std<span>::</span>cerr <span>&lt;&lt;</span> <span>"interruptable: "</span> <span>&lt;&lt;</span> counter <span>&lt;&lt;</span> <span>'\n'</span>; 
            <span>++</span>counter;
        }
    });
    
    std<span>::</span>this_thread<span>::</span>sleep_for(<span>1</span>s);
    
    std<span>::</span>cerr <span>&lt;&lt;</span> <span>'\n'</span>;
    std<span>::</span>cerr <span>&lt;&lt;</span> <span>"Main thread interrupts both jthreads"</span> <span>&lt;&lt;</span> <span>'\n'</span>;
    nonInterruptable.request_stop();
    interruptable.request_stop();                              <span>// (4)</span>
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
    
}
</pre>
</div>

<p>In the main program, I start the two threads <code>nonInterruptable</code> and interruptable (lines 1)and 2). Unlike in the thread <code>nonInterruptable</code> , the thread <code>interruptable</code> gets a <code>std::stop_token</code> and uses it in line (3) to check if it was interrupted: <code>stoken.stop_requested()</code>. In case of a stop request, the lambda function returns, and, therefore, the thread ends. The call <code>interruptable.request_stop()</code> (line 4) triggers the stop request. This does not hold for the previous call <code>nonInterruptable.request_stop()</code> . The call has no effect.</p>
<p><img src="http://modernescpp.com/images/blog/Cpp20/jthread/interruptJthread.png" alt="interruptJthread" width="400" height="382"></p>
<p>To make my post complete, with C++20, you can also cooperatively interrupt a condition variable.</p>
<h2 id="h3-new-wait-overloads-for-std-condition-variable-any">New wait Overloads for <code>std::condition_variable_any</code></h2>
<p>Before I write about <code>std::condition_variable_any</code>, here are my post about <a href="http://modernescpp.com/index.php/tag/condition-variable">condition variables</a>.&nbsp;</p>
<p>The three wait variations <code>wait, wait_for</code>, and<code> wait_until</code> of the std::condition_variable_any get new overloads. These overloads take a <code>std::stop_token</code>.</p>
<!-- HTML generated using hilite.me -->
<div>
<pre><span>template</span> <span>&lt;</span><span>class</span> <span>Predicate</span><span>&gt;</span>
<span>bool</span> wait(Lock<span>&amp;</span> lock,  
          stop_token stoken,
          Predicate pred);

<span>template</span> <span>&lt;</span><span>class</span> <span>Rep</span>, <span>class</span> <span>Period</span>, <span>class</span> <span>Predicate</span><span>&gt;</span>
<span>bool</span> wait_for(Lock<span>&amp;</span> lock, 
              stop_token stoken, 
              <span>const</span> chrono<span>::</span>duration<span>&lt;</span>Rep, Period<span>&gt;&amp;</span> rel_time, 
              Predicate pred);
                
<span>template</span> <span>&lt;</span><span>class</span> <span>Clock</span>, <span>class</span> <span>Duration</span>, <span>class</span> <span>Predicate</span><span>&gt;</span>
<span>bool</span> wait_until(Lock<span>&amp;</span> lock, 
                stop_token stoken,
                <span>const</span> chrono<span>::</span>time_point<span>&lt;</span>Clock, Duration<span>&gt;&amp;</span> abs_time, 
                Predicate pred);
</pre>
</div>

<p>These new overloads need a predicate. The presented versions ensure to get notified if a stop request for the passed <code>std::stop_token stoken</code> is signaled. They return a boolean that indicates whether the predicate evaluates to <code>true</code>. This returned boolean is independent of whether a stop was requested or of whether the timeout was triggered.</p>
<p>After the wait calls, you can check if a stop request occurred.</p>
<!-- HTML generated using hilite.me -->
<div>
<pre>cv.wait(lock, stoken, predicate);
<span>if</span> (stoken.stop_requested()){
    <span>// interrupt occurred</span>
}
</pre>
</div>

<p>The following example shows the usage of a condition variable with a stop request.</p>
<!-- HTML generated using hilite.me -->
<div>
<pre><span>// conditionVariableAny.cpp</span>

<span>#include &lt;condition_variable&gt;</span>
<span>#include &lt;thread&gt;</span>
<span>#include &lt;iostream&gt;</span>
<span>#include &lt;chrono&gt;</span>
<span>#include &lt;mutex&gt;</span>
<span>#include &lt;thread&gt;</span>

<span>using</span> <span>namespace</span> std<span>::</span>literals;

std<span>::</span>mutex mutex_;
std<span>::</span>condition_variable_any condVar;

<span>bool</span> dataReady;

<span>void</span> <span>receiver</span>(std<span>::</span>stop_token stopToken) {                 <span>// (1)</span>

    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Waiting"</span> <span>&lt;&lt;</span> <span>'\n'</span>;

    std<span>::</span>unique_lock<span>&lt;</span>std<span>::</span>mutex<span>&gt;</span> lck(mutex_);
    <span>bool</span> ret <span>=</span> condVar.wait(lck, stopToken, []{<span>return</span> dataReady;});
    <span>if</span> (ret){
        std<span>::</span>cout <span>&lt;&lt;</span> <span>"Notification received: "</span> <span>&lt;&lt;</span> <span>'\n'</span>;
    }
    <span>else</span>{
         std<span>::</span>cout <span>&lt;&lt;</span> <span>"Stop request received"</span> <span>&lt;&lt;</span> <span>'\n'</span>;
    }
}

<span>void</span> <span>sender</span>() {                                            <span>// (2)</span>

    std<span>::</span>this_thread<span>::</span>sleep_for(<span>5</span>ms);
    {
        std<span>::</span>lock_guard<span>&lt;</span>std<span>::</span>mutex<span>&gt;</span> lck(mutex_);
        dataReady <span>=</span> <span>true</span>;
        std<span>::</span>cout <span>&lt;&lt;</span> <span>"Send notification"</span>  <span>&lt;&lt;</span> <span>'\n'</span>;
    }
    condVar.notify_one();                                  <span>// (3)</span>

}

<span>int</span> <span>main</span>(){

  std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;

  std<span>::</span>jthread t1(receiver);
  std<span>::</span>jthread t2(sender);
  
  t1.request_stop();                                       <span>// (4)</span>

  t1.join();
  t2.join();

  std<span>::</span>cout <span>&lt;&lt;</span> <span>'\n'</span>;
  
}
</pre>
</div>

<p>The receiver thread (line 1) is waiting for the notification of the sender thread (line 2). Before the sender thread sends its notification (line 3), the main thread triggered a stop request in<br>line (4). The output of the program shows that the stop request happened before the notification.</p>
<h2><img src="http://modernescpp.com/images/blog/Cpp20/jthread/conditionVariableAny.png" alt="conditionVariableAny" width="281" height="113"></h2>
<h2 id="h4-what-s-next">What's next?</h2>
<p>What happens when your write without synchronization to <code>std::cout</code>? You get a mess. Thanks to C++20, we have synchronized output streams.</p>
<div>
	<p><strong>Thanks a lot to my <a href="https://www.patreon.com/rainer_grimm">Patreon Supporters</a></strong><strong>: Matt Braun, Roman Postanciuc, Tobias Zindl, Marko, </strong><span title="Emyr Williams"><strong>G Prvulovic, Reinhold Dr√∂ge, Abernitzke,</strong> </span><strong><span title="Emyr Williams">Frank Grimm</span></strong><span title="Emyr Williams"><strong>, Sakib, Broeserl, </strong></span><strong><span title="Emyr Williams">Ant√≥nio Pina, Darshan Mody, Sergey Agafyin, <span data-tag="user-details-full-name">–ê–Ω–¥—Ä–µ–π –ë—É—Ä–º–∏—Å—Ç—Ä–æ–≤, Jake, GS, Lawton Shoemake, Animus24, Jozo Leko, John Breland, espkk, Wolfgang G√§rtner</span></span><span title="Emyr Williams"><span><span></span></span></span>,&nbsp; Louis St-Amour, Stephan Roslen, Venkat Nandam, Jose Francisco, Douglas Tinkham, Kuchlong Kuchlong, Avi Kohn, Robert Blanch, Truels Wissneth, Kris Kafka, Mario Luoni, Neil Wang, Friedrich Huber, lennonli, Pramod Tikare Muralidhara, Peter Ware, and Tobi Heideman.<br></strong></p>

<p><strong>Thanks in particular to Jon Hess, Lakshman,</strong> <strong>Christian Wittenhorst, Sherhy Pyton, Dendi Suhubdy, Sudhakar Belagurusamy, and Richard Sargeant.<br></strong></p>
<p>My special thanks to <a href="https://www.embarcadero.com/de/products/cbuilder">Embarcadero</a> <a href="https://www.embarcadero.com/products/cbuilder"><img src="http://modernescpp.com/images/Embarcadero/CBUIDER_STUDIO_FINAL_ICONS_1024_Small.png" alt="CBUIDER STUDIO FINAL ICONS 1024 Small" width="100" height="100"></a></p>

<h2>Seminars</h2>
<p>I'm happy to give online-seminars or face-to-face seminars world-wide. Please call me if you have any questions.</p>
<h3>Bookable (Online)</h3>
<h4>Deutsch</h4>
<ul>
<li><a href="https://www.modernescpp.de/index.php/c/2-c/30-embedded-programmierung-mit-modernem-c20210126195655">Embedded Programmierung mit modernem C++: </a>12.04.2021 - 14.04.2021 (9:00 - 17:00 CEST)</li>
<li><a href="https://www.modernescpp.de/index.php/c/2-c/31-clean-code-mit-modernem-c">Clean Code mit modernem C++: </a>22.06.2021 - 24.06.2021 (9:00 - 17:00 CEST)</li>
</ul>
<h4>English</h4>
<ul>
<li>Workshop: <a href="https://www.modernescpp.net/index.php/c/2-c/32-workshop-concepts-in-c-20">Concepts in C++20</a>: 22.03.2021 (17:00 - 21:00 CET)</li>
<li>Workshop: <a href="https://www.modernescpp.net/index.php/c/2-c/33-workshop-coroutines-in-c-20">Coroutines in C++20</a>: 06.04.2021 (17:00 - 21:00 CEST)</li>
</ul>
<h3>Standard Seminars&nbsp;</h3>
<p>Here is a compilation of my standard seminars. These seminars are only meant to give you a first orientation.</p>
<ul>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/22">C++ - The Core Language</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/23">C++ - The Standard Library</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/23">C++ - Compact</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/18">C++11 and C++14</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/19">Concurrency with Modern C++</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/21">Design Patterns and Architecture Patterns with C++</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/17">Embedded Programming with Modern C++</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/17">Generic Programming (Templates) with C++</a></li>
</ul>
<h4>New</h4>
<ul>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/16">Clean Code with Modern C++</a></li>
<li><a href="https://www.modernescpp.net/index.php/c/plan/2-c/25">C++20</a></li>
</ul>
<h3>Contact Me</h3>
<ul>
<li>Tel.: +49 7472 917441</li>
<li>Mobil: +49 152 31965939</li>
<li>Mail: <a href="http://modernescpp.com/%3Ca%20href="><span id="cloak074c3d1e132371bc9560ca558c89afb5">This email address is being protected from spambots. You need JavaScript enabled to view it.</span></a></li>
<li>German Seminar Page: <a href="https://www.modernescpp.de/">www.ModernesCpp.de</a></li>
<li>English Seminar Page: <a href="http://www.modernescpp.net/">www.ModernesCpp.net</a></li>
</ul>
<h3>Modernes C++,</h3>
<p><img src="http://modernescpp.com/images/signatur/RainerGrimmSmall.png" alt="RainerGrimmSmall"></p>
</div>


			</div></div>]]>
            </description>
            <link>http://modernescpp.com/index.php/an-improved-thread-with-c-20</link>
            <guid isPermaLink="false">hacker-news-small-sites-26142257</guid>
            <pubDate>Mon, 15 Feb 2021 12:55:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Running Nomad for a Home Server]]>
            </title>
            <description>
<![CDATA[
Score 264 | Comments 135 (<a href="https://news.ycombinator.com/item?id=26142005">thread link</a>) | @elliebike
<br/>
February 15, 2021 | https://mrkaran.dev/posts/home-server-nomad/ | <a href="https://web.archive.org/web/*/https://mrkaran.dev/posts/home-server-nomad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<section>
    
    <article>
        <p>It's been a long time since I've written a post on Hydra (my home server). I use Hydra as a testbed to learn new tools, workflows and it just gives me joy to self-host applications while learning something in return.</p>
<h2 id="history">History</h2>
<p>A brief history of how <a href="https://github.com/mr-karan/hydra">Hydra's</a> setup evolved over time:</p>
<p><a href="https://mrkaran.dev/posts/home-server-setup/">2019</a>: </p>
<ul>
<li>A pretty minimal K3s setup deployed on 2 RPi4 nodes. I couldn't continue with this setup because:
<ul>
<li>Some of the apps didn't have ARM-based image (this was 2019, pre M1 hype era).</li>
<li>Didn't want to risk deploying persistent workloads on RPi.</li>
<li>A lot of tooling to deploy workloads was missing (storing env variables for eg.).</li>
<li>It was so boring to write YAML (that I also did at work). Didn't give me joy.</li>
</ul>
</li>
</ul>
<p><a href="https://mrkaran.dev/posts/home-server-updates/">2020 First Half</a>:</p>
<ul>
<li>RPi 2x Nodes + K3s + DO Droplet. Tailscale for networking.
<ul>
<li>This was a considerable step up from the previous setup. I deployed a DO node and added <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/">Node Labels</a> to deploy persistent workloads on DO Node only.</li>
<li>I used my own tooling <a href="https://github.com/mr-karan/kubekutr/">Kubekutr</a> + Kustomize which helped with version control of my configs.</li>
<li>Took quite a bit of time to onboard new services. Got lazy, didn't host much apart from initial 3-4 applications.</li>
<li>Writing long YAMLs. No joy.</li>
</ul>
</li>
</ul>
<p>2020 Second Half:</p>
<ul>
<li>Single node on DO. Terraform for deploying Docker containers.
<ul>
<li>I believe the third iteration nailed it for me. I kept the setup super simple, used Terraform for deploying workloads as Docker containers.</li>
<li>Used Terraform extensively for setting up the node, Cloudflare records, DO firewall rules.</li>
<li>Time to onboard new services reduced from a couple of hours to a few minutes. This was a huge win for me. I deployed around 10-15 new services to try it out on the server directly.</li>
<li>Writing HCL is actually a much better experience than YAML.</li>
</ul>
</li>
</ul>
<h2 id="why-nomad">Why Nomad</h2>
<p><img src="https://mrkaran.dev/images/nomad-hydra.png" alt="image"></p>
<p>Around a month back, <a href="https://nadh.in/">Kailash</a> had asked about feedback on <a href="https://www.nomadproject.io/">Nomad</a>. We at <a href="https://zerodha.com/">Zerodha</a> (India's largest stock broker) are evaluating it to migrate our services to Nomad from Kubernetes (more on this later). It was almost 2 years since I last saw Nomad so it was definitely worth re-evaluating (esp since it hit 1.0 recently). I wanted to try out Nomad to answer a personal curiosity: <em>What does it do differently than Kubernetes?</em> No better way than actually getting hands dirty, right?!</p>
<p>After following the brief tutorials from the <a href="https://learn.hashicorp.com/nomad">official website</a> I felt confident to try it for actual workloads. In my previous setup, I was hosting quite a few applications (Pihole, Gitea, Grafana etc) and thought it'll be a nice way to learn how Nomad works by deploying the same services in the Nomad cluster. And I came in with zero expectations, I already had a nice setup which was reliable and running for me. My experience with a local Nomad cluster was joyful, I was able to quickly go from 0-&gt;1 in less than 30 minutes. This BTW is a strong sign of how easy Nomad is to get started with as compared to K8s. The sheer amount of different concepts you've to register in your mind before you can even deploy a single container in a K8s cluster is bizarre. Nomad takes the easy way out here and simplified the concepts for developers into just three things:</p>
<pre><code><span>job
  \_ group
        \_ task
</span></code></pre>
<ul>
<li>Job: Job is a collection of different groups. Job is where the constraints for type of scheduler, update strategies and ACL is placed. </li>
<li>Group: Group is a collection of different tasks. A group is always executed on the same Nomad client node. You'll want to use Groups for use-cases like a logging sidecar, reverse proxies etc.</li>
<li>Task: Atomic unit of work. A task in Nomad can be running a container/binary/Java VM etc, defining the mount points, env variables, ports to be exposed etc.</li>
</ul>
<p>If you're coming from K8s you can think of Task as a Pod and Group as a Replicaset. There's no equivalent to Job in K8s. BUT! The coolest part? You don't have to familiarise yourself with all different types of Replicasets (Deployments, Daemonsets, Statefulsets) and different ways of configuring them.</p>
<p>Want to make a normal job as a periodic job in Nomad? Simply add the following block to your existing Job:</p>
<pre><code><span>periodic {
  cron = "@daily"
}
</span></code></pre>
<p>You want to make a service run as a batch job (on all Nomad nodes -- the equivalent of Daemonset in K8s)? Simply make the following change to your existing job:</p>
<pre><code><span>-type="service"
</span><span>+type="batch"
</span></code></pre>
<p>You see <strong>this</strong> is what I mean by the focus on UX. There are many many such examples which will leave a nice smile on your face if you're coming from K8s background.</p>
<p>I'd recommend reading <a href="https://www.nomadproject.io/docs/internals/architecture">Internal Architecture</a> of Nomad if you want to understand this in-depth.</p>
<h2 id="architecture">Architecture</h2>
<p>Tech stack for Hydra:</p>
<ul>
<li>Tailscale VPN: Serves as a mesh layer between my laptop/mobile and DO server. Useful for exposing internal services.</li>
<li>Caddy for reverse proxying and automatic SSL setup for all services. I run 2 instances of Caddy:
<ul>
<li>Internal: Listens on Tailscale Network Interface. Reverse proxies all private services.</li>
<li>Public: Listens on DO's Public IPv4 network interface. Reverse proxies all public-facing services.</li>
</ul>
</li>
<li>Terraform: Primary component to have IaC (Infra as Code). Modules to manage:
<ul>
<li>Cloudflare DNS Zone and Records</li>
<li>DO Droplet, Firewall rules, SSH Keys, Floating IPs etc.</li>
<li>Nomad Jobs. Used for running workloads after templating env variables, config files in Nomad job files.</li>
</ul>
</li>
</ul>
<h2 id="complexity-of-nomad-vs-kubernetes">Complexity of Nomad vs Kubernetes</h2>
<p><a href="https://twitter.com/mrkaran_/status/1268762357355823104"><img src="https://mrkaran.dev/images/k8s-meme.jpeg" alt="image"></a></p>
<p>Nomad shines because it follows the UNIX philosophy of "Make each program do one thing well". To put simply, Nomad is <em>just</em> a workload orchestrator. It only is concerned about things like Bin Packing, scheduling decisions.</p>
<p>If you're running heterogeneous workloads, running a server (or a set of servers) quickly becomes expensive. Hence orchestrators tend to make sense in this context. They tend to save costs by making it efficient to run a vast variety of workloads. This is all an orchestrator has to do really. </p>
<p>Nomad doesn't interfere in your DNS setup, Service Discovery, secrets management mechanisms and pretty much anything else. If you read some of the posts at <a href="https://k8s.af/">Kubernetes Failure Stories</a>, the most common reason for outages is Networking (DNS, ndots etc). A lot of marketing around K8s never talks about these things.</p>
<p>I always maintain "Day 0 is easy, Day N is the real test of your skills". Anyone can deploy a workload to a K8s cluster, it's always the Day N operations which involve debugging networking drops, mysterious container restarts, proper resource allocations and other such complex issues that require real skills <strong>and</strong> effort. It's not as easy as <code>kubectl apply -f</code> and my primary gripe is with people who miss out on this in their "marketing" pitches (obvious!).</p>
<h2 id="when-to-use-nomad">When to use Nomad</h2>
<p>Nomad hits the sweet spot of being operationally easy and functional. Nomad is a great choice if you want to:</p>
<ul>
<li>Run not just containers but other forms of workloads.</li>
<li>Increase developer productivity by making it easier to deploy/onboard new services.</li>
<li>Consistent experience of deployment by testing the deployments locally.</li>
<li>(Not joking) You are tired of running Helm charts or writing large YAML manifests. The config syntax for Nomad jobs is human friendly and easy to grasp.</li>
</ul>
<p>Nomad is available as a single binary. If you want to try it locally, all you need is <code>sudo nomad agent -dev</code> and you'll have a Nomad Server, Client running in dev mode along with a UI. This makes it easy for the developers to test out the deployments locally because there's very little configuration difference between this and production deployment. Not to forget it's super easy to self-host Nomad clusters. I'm yet to meet anyone who self hosts K8s clusters in production without a dedicated team babysitting it always.</p>
<p>Once you eliminate the "blackbox" components from your stack, life becomes easier for everyone.</p>
<h2 id="when-to-not-use-nomad">When to not use Nomad</h2>
<ul>
<li>If you're relying on custom controllers and operators. <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Operator Pattern</a> is a new way of managing large complex distributed systems (like databases, job queues etc). There are a lot of community built operators which help in reducing the effort to run these services. However, all of these are tied deeply into the "Kubernetes" ecosystem. If you find yourself running any of such operators, it'll be tough (not impossible) to translate the same in Nomad ecosystem.</li>
</ul>
<p>I <em>genuinely</em> cannot think of any other reason to not use Nomad!</p>
<h2 id="practical-scenarios">Practical Scenarios</h2>
<p>Since I migrated a couple of workloads from my DO docker containers setup to Nomad, I'd demonstrate a few use cases which might be helpful if you want to start migrating your services to Nomad</p>
<h3 id="accessing-a-web-service-with-reverse-proxy">Accessing a Web service with Reverse Proxy</h3>
<p>Context: I'm running Caddy as a reverse proxy for all the services. Since we discussed earlier, Nomad <strong>only</strong> is concerned about scheduling, so how exactly do you do Service Discovery? You need Consul (or something like Consul, Nomad has no hard restrictions) to register a service name with it's IP Address. Here's how you can do that:</p>
<p>In the <code>.task</code> section of your Nomad job spec, you need to register the service name with the port you're registering and additional tags as metadata (optional):</p>
<pre><code><span>service {
  name = "gitea-web"
  tags = ["gitea", "web"]
  port = "http"
}
</span></code></pre>
<p>Nomad's <a href="https://www.nomadproject.io/docs/job-specification/template">template</a> uses <code>consul-template</code> behind the scenes. This is a small utility which continuously watches for Consul/Vault keys and provides the ability to reload/restart your workloads if any of those keys change. It can also be used to <em>discover</em> the address of the service registered in Consul. So here's an example of <code>Caddyfile</code> using Consul Template functions to pull the IP address of the upstream <code>gitea-web</code> service:</p>
<pre><code><span>git.mrkaran.dev {
    {{ range service "gitea-web" }}
    reverse_proxy {{ .Address }}:{{ .Port }}
    {{ end }}
}
</span></code></pre>
<p>When a job is submitted to Nomad, a rendered template is mounted inside the container. You can define actions on what to do when the values change. For eg on a redeployment of Gitea container, the address will most likely change. We'd like Caddy to automatically restart with the new address configured in the Caddyfile in that case:</p>
<pre><code><span>template {
  data = &lt;&lt;EOF
${caddyfile_public}
EOF

  destination = "configs/Caddyfile" # Rendered template.

  change_mode = "restart"
}
</span></code></pre>
<p>Using <a href="https://www.nomadproject.io/docs/job-specification/template#change_mode"><code>change_m‚Ä¶</code></a></p></article></section></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mrkaran.dev/posts/home-server-nomad/">https://mrkaran.dev/posts/home-server-nomad/</a></em></p>]]>
            </description>
            <link>https://mrkaran.dev/posts/home-server-nomad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26142005</guid>
            <pubDate>Mon, 15 Feb 2021 12:15:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is This a Branch?]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 46 (<a href="https://news.ycombinator.com/item?id=26141047">thread link</a>) | @wheresvic4
<br/>
February 15, 2021 | https://bartwronski.com/2021/01/18/is-this-a-branch/ | <a href="https://web.archive.org/web/*/https://bartwronski.com/2021/01/18/is-this-a-branch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						
<p>Let‚Äôs try a new format ‚Äì <strong>‚Äúshorts‚Äù</strong>; small blog posts where I elaborate on ideas that I‚Äôd discuss at my twitter, but they either come back over and over, or the short form doesn‚Äôt convey all the nuances.</p>



<div><figure><img src="https://lh5.googleusercontent.com/lc0z89hwcBtH59QnEvHrU__d1Be5uxjtjrjaBYi3updGObpyYlmpYvhlhXobMLWwyeRB2Sh--pEO7_oupCuDWdQXMtQnjcYfBjrO0TLDB_Zn5CJQLmDQm27enOR1C-tUEvmGGDL_" alt="" width="370" height="333"></figure></div>



<p>I often see advice about avoiding ‚Äúif‚Äù statements in the code (especially GPU shaders) at all costs. The worst of all is when this happens in glsl and programmers replace some simple if statements with (subjectively) horrible <a href="https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/step.xhtml">step instruction</a> which IMO completely destroys all readability, or even worse ‚Äì sequences of mixes, maxes, clips and extra ALU.</p>



<p>After all, <strong>those ifs are branches, and branches are bad, right?</strong></p>



<p><strong>TLDR: No, definitely not ‚Äì on all accounts.</strong></p>



<h2>Why do programmers want to avoid branches?</h2>



<p>Let‚Äôs first address why programmers would like to avoid branches. There are some good reasons for that.</p>



<h3>Scalar CPU</h3>



<p>On the CPU, ‚Äúbranches‚Äù in a non-SIMD code might seem ‚Äúinnocent‚Äù, it‚Äôs just a conditional jump to some other code. Instead of executing an <em>instruction a</em> next address, we jump to <em>instruction b</em> at some other address. Unfortunately, there are many complicated things going on under the hood: All modern CPUs are heavily pipelined, superscalar, and out-of order.</p>



<div><figure><img src="https://lh5.googleusercontent.com/aJOrhsi_sQ7C4gG6z0cov73NA_qt-UckHrz10Uo5OGX22TuwRRf_n1ogyaRUSBsTsCU3wigOkYwIZMclAGslXKSruRk9P7ieNjYvyukZ9erHFaCr-QMuKvwPjMbLNuBfyMZGTqjc" alt="" width="344" height="330"><figcaption>All modern CPUs are superscalar and deeply pipelined. Many instructions are processed and executed at the same time. Source: <a href="https://en.wikipedia.org/wiki/Branch_predictor">wikipedia</a></figcaption></figure></div>



<p>To extract maximum efficiency, they use a model of <a href="https://en.wikipedia.org/wiki/Branch_predictor">speculative execution</a> ‚Äì a CPU ‚Äúgambles‚Äù and ‚Äúguesses‚Äù results of a branch ahead of time, not knowing if it will be taken or not, and starts executing those instructions. If it‚Äôs right ‚Äì great! However if it‚Äôs not, then the CPU needs to stall, cancel all the guesses, go back to the branch, and execute the other instructions. Oops.</p>



<p>Luckily, CPUs are usually very good at this ‚Äúguessing‚Äù (they actually analyze past branch patterns and take them into account!). On the other hand, without going into too much detail ‚Äì each branch consumes some of the branch predictor unit resources. One extra branch on some short, innocent code can make results of the branch prediction worse, and slow down other code (where branch predictor might be necessary for good performance!).</p>



<h3>SIMD and the GPU</h3>



<p>When considering SIMD or the GPU, the situation is even worse.</p>



<p>Simplifying ‚Äì single program and same instruction stream executes on multiple ‚Äúlanes‚Äù of the same data, processing 4/8/16/32/64 elements at the same time.</p>



<p>If you want to take a branch when some of the elements take one path, some another one, you have two main options: <strong>either abandon vectorization and scalarize code</strong> (very bad option; usually means that it‚Äôs not worth vectorizing given piece of code), or <strong>execute both code paths</strong> ‚Äì first the first one, store the results somewhere, execute the second one, and ‚Äúcombine‚Äù the results.</p>



<div><figure><img src="https://lh3.googleusercontent.com/GmopPKjJ_t4QpsGIhaIhjJn6cTfUjVaMSl9lDj74skwpNRUVIYtWQ8-V0j4mMLIyZk1CZEgn-9z5C1kExAlVjIw8Oikdx0L-gJnyrB-qUc5oi3_4aQmmcGwCcP51YutgGoAlRHdT" alt="" width="448" height="194"></figure></div>



<p>This is a general (over)simplification, and example GPUs have dedicated hardware to help do it efficiently and without wasting resources (hardware execution masks etc), but generally means extra cost of managing those masks, storing results somewhere, double execution costs and finally ‚Äì branches can serve as barriers, preventing some <a href="https://bartwronski.com/2014/03/27/gcn-two-ways-of-latency-hiding-and-wave-occupancy/">latency hiding (see my old blog post on it)</a>.</p>



<h3>Combined</h3>



<p>Given those two, it might seem that avoiding branches seems reasonable, and the common advice of avoiding ifs makes sense. But whether it makes sense to avoid branch or not is more complicated (more on it later), and the second one (whether an if is a branch) is not the case.</p>



<h2>Are if statements branches?</h2>



<p>Let‚Äôs clear the worst misconception.</p>



<p><strong>When you write an ‚Äúif‚Äù in your code, the compiler won‚Äôt necessarily generate a branch with a jump</strong>.</p>



<p>I will focus now on CPUs, mostly because how easy it is to test it with <a href="https://godbolt.org/">Matt Godbolt‚Äôs amazing compiler explorer</a>. üôÇ (Plus how there are two mainstream architectures, unlike many different shader ISAs)</p>



<p>Let‚Äôs have a look at the following simple integer function.</p>


<pre title="">int is_this_a_branch(int v, int w) {
    if (v &lt; 10) {
        return v + w; 
    } 
    else {
        return 2 * v - 2 * w;
    }
}
</pre>


<p>I tested it with 3 x64 compilers (GCC x64 trunk, Clang x64 trunk, MSVC 19.28) and one ARM (armv8-a clang), and only MSVC generates an actual branch there.</p>



<pre><code><strong>clang</strong>              
        mov     eax, edi
        sub     eax, esi
        add     esi, edi
        add     eax, eax
        cmp     edi, 10
        cmovl   eax, esi
        ret
<strong>gcc</strong>
        mov     eax, edi
        lea     edx, [rdi+rsi]
        sub     eax, esi
        add     eax, eax
        cmp     edi, 9
        cmovle  eax, edx
        ret
<strong>msvc </strong>       
        cmp     ecx, 10
        <strong>jge     SHORT $LN2@is_this_a_ BRANCH!!!
</strong>   BRANCH!
        lea     eax, DWORD PTR [rcx+rdx]
        ret     0
$LN2@is_this_a_:
        sub     ecx, edx
        lea     eax, DWORD PTR [rcx+rcx]
        ret     0

<strong>arm clang
</strong>        sub     w9, w0, w1
        add     w8, w1, w0
        lsl     w9, w9, #1
        cmp     w0, #10                         // =10
        csel    w0, w8, w9, lt</code></pre>



<p>When using floats:</p>


<pre title="">float is_this_a_branch2(float v, float w) {
    if (v &lt; 10.0f) {
        return v + w; 
    } 
    else {
        return 2.0f * v - 2.0f * w;
    }
}
</pre>


<p>Now 2 out of 4 compilers generate a branch:</p>



<pre><code><strong>clang</strong>           
        movaps  xmm2, xmm0
        addss   xmm2, xmm1
        movaps  xmm3, xmm0
        addss   xmm3, xmm0
        addss   xmm1, xmm1
        cmpltss xmm0, dword ptr [rip + .LCPI1_0]
        subss   xmm3, xmm1
        andps   xmm2, xmm0
        andnps  xmm0, xmm3
        orps    xmm0, xmm2
        ret

<strong>gcc
</strong>        movss   xmm2, DWORD PTR .LC0[rip]
        comiss  xmm2, xmm0
        <strong>jbe     .L10  BRANCH!!!
</strong>
        addss   xmm0, xmm1
        ret
.L10:
        addss   xmm1, xmm1
        addss   xmm0, xmm0
        subss   xmm0, xmm1
        ret

<strong>msvc
</strong>        movss   xmm2, DWORD PTR __real@41200000
        comiss  xmm2, xmm0
        <strong>jbe     SHORT $LN2@is_this_a_  BRANCH!!!</strong>
        addss   xmm0, xmm1
        ret     0
$LN2@is_this_a_:
        addss   xmm0, xmm0
        addss   xmm1, xmm1
        subss   xmm0, xmm1
        ret     0

<strong>arm clang
</strong>        fadd    s2, s0, s1
        fadd    s3, s0, s0
        fadd    s1, s1, s1
        fmov    s4, #10.00000000
        fsub    s1, s3, s1
        fcmp    s0, s4
        fcsel   s0, s2, s1, mi
        ret</code></pre>



<h3>What are those conditionals and selects?</h3>



<p>This is basically CPUs‚Äô and compilers‚Äô way of ‚Äúserializing‚Äù both code paths. If it considers overhead of a branch larger than the amount of work it can save, it makes sense to compute both code paths, and use a single instruction to select which one is the actual result.</p>



<p>This obviously ‚Äúdepends‚Äù on the use case and whether it is even ‚Äúlegal‚Äù for a compiler to do such a transformation (like ‚Äúside effects‚Äù; or if a compiler executed some reads of memory, it could be unsafe and lead to segmentation fault).&nbsp;</p>



<h2>Does writing ternary conditional help avoid branches?</h2>



<p><strong>No.</strong></p>



<p>This is a common advice, but it‚Äôs generally not true. If you look at the above example rewritten slightly:</p>


<pre title="">float is_this_a_branch3(float v, float w) {
    return v &lt; 10.0f ? (v + w) : (2.0f * v - 2.0f * w);
}
</pre>


<p>The generated code is <strong>identical </strong>(!).</p>



<p>It might be <em>possible</em> (?) that some compilers do generate branchless code for like this, but giving this as a general advice makes no sense. Whether you write an actual if, or a ternary expression, it‚Äôs still a high level if statement, just expressed differently.</p>



<h2>What about the GPU?</h2>



<p><strong>On the GPU, it‚Äôs the same; the compiler will generate conditional selects when appropriate.</strong></p>



<p>I used <a href="http://shader-playground.timjones.io/">Tim Jones Shader Playground</a> and AMD ISA (mostly because I know it relatively better than the others), and the unfortunate step function:</p>


<pre title="">#version 460

layout (location = 0) out vec4 fragColor;
layout (location = 0) in float inColor;

void main()
{
#if 1
	fragColor = vec4(step(1.0, inColor));
#else
    float x;
    if (inColor &lt; 1.0)
        x = 1.0;
   	else
        x = 0.0;
   	fragColor = vec4(x);
#endif
}
</pre>


<p>See for yourself, both versions generate identical assembly:</p>



<pre><code>  s_mov_b32     m0, s3
  s_nop         0x0000
  v_interp_p1_f32  v0, v0, attr0.x
  v_interp_p2_f32  v0, v1, attr0.x
  v_cmp_gt_f32  vcc, 1.0, v0
  v_cndmask_b32  v0, 1.0, 0, vcc
  v_cvt_pkrtz_f16_f32  v0, v0, v0
  exp           mrt0, v0, v0, v0, v0 done compr vm</code></pre>



<p>When using fastmath and not obeying strict IEEE float specification (which is standard for compiling shaders for real time graphics), compilers can even generate other instructions like min or max from your if statements. But YMMV and be sure to check the assembly.</p>



<p>So generally ‚Äì using ‚Äústep‚Äù just to avoid if statements is <strong>pointless</strong>.</p>



<p>If you like it for your coding style, then it‚Äôs obviously up to you, use as much as you like just for this reason. But please consider programmers who don‚Äôt have as much shader or GLSL experience and will be always puzzled by it. üôÇ </p>



<h2>Are branches always to be avoided?</h2>



<p><strong>No.</strong></p>



<p>This is way beyond my aimed ‚Äúshort‚Äù format, but <strong>branches are not necessarily ‚Äúbad‚Äù</strong>.</p>



<p>They can be actually an awesome optimization, even on the GPU!</p>



<p>If you start to use a lot of conditionals</p>



<p>Lots of advice on how branches are bad and have to be avoided comes from a prehistoric era of old CPUs and GPUs or old compilers; an advice that is passed without re(verifying) ‚Äì which is understandable, I am not re-checking my whole knowledge or intuition every few months either. üôÇ </p>



<p>But the CPU/GPU/compiler architects improved designs significantly and adapted them to general, highly branchy code, and whether there is a penalty or not depends on way too many details to list or give advice.</p>



<p><strong>Rule of thumb</strong>: if a branch is generally coherent, it makes sense to use it when it allows to save on memory bandwidth or cache utilization. On GPUs it is usually worth adding branches if you can avoid texture fetches this way with a high probability and coherence. And it‚Äôs usually better to have some conditional masks and selects around ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bartwronski.com/2021/01/18/is-this-a-branch/">https://bartwronski.com/2021/01/18/is-this-a-branch/</a></em></p>]]>
            </description>
            <link>https://bartwronski.com/2021/01/18/is-this-a-branch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26141047</guid>
            <pubDate>Mon, 15 Feb 2021 09:52:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Interactive Data Visualisation with Rust]]>
            </title>
            <description>
<![CDATA[
Score 81 | Comments 12 (<a href="https://news.ycombinator.com/item?id=26141018">thread link</a>) | @batterylow
<br/>
February 15, 2021 | https://shahinrostami.com/posts/programming/rust-notebooks/visualisation-of-co-occurring-types/ | <a href="https://web.archive.org/web/*/https://shahinrostami.com/posts/programming/rust-notebooks/visualisation-of-co-occurring-types/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<pre>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]</pre>
</div></div>]]>
            </description>
            <link>https://shahinrostami.com/posts/programming/rust-notebooks/visualisation-of-co-occurring-types/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26141018</guid>
            <pubDate>Mon, 15 Feb 2021 09:49:20 GMT</pubDate>
        </item>
    </channel>
</rss>
