<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 03 Mar 2021 12:45:39 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 03 Mar 2021 12:45:39 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[The Last Message Sent on Aim]]>
            </title>
            <description>
<![CDATA[
Score 109 | Comments 27 (<a href="https://news.ycombinator.com/item?id=26300266">thread link</a>) | @luu
<br/>
February 28, 2021 | https://justanman.org/posts/the-last-message-sent-on-aim/ | <a href="https://web.archive.org/web/*/https://justanman.org/posts/the-last-message-sent-on-aim/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p><img src="https://justanman.org/images/aol-instant-messenger-shuts-down.png" alt="AOL Instant Messenger ‚ÄòRunning Man‚Äô waving goodbye"></p>
<p>In the early 2000s social media sites like Facebook and Twitter weren‚Äôt commonplace yet. Even text messages, at ten cents each, were something to be rationed. For many teenagers the primary means of communication outside school was AOL Instant Messenger (AIM). So when it was announced in October that they were shutting down AIM after 20 years, I felt a wave of nostalgia for the messaging app of my youth.</p>
<p>It wasn‚Äôt clear when they would be pulling the plug for good, but a high school friend and I planned to be online for the occasion. Their website indicated that AIM would continue to work until the morning of December 15, 2017. Assuming this meant UTC time, that put it going offline sometime after 7:00 PM EST on the 14th.</p>
<p>I wondered: Could I be the last person to sign off? The last person to send a message? Would I have anything profound to say?</p>
<p><strong>Research</strong></p>
<p>I decided to research famous last words. First stop: the Book of Revelation. Final chapter, final verse.</p>
<blockquote>
<p>‚ÄúThe grace of our Lord Jesus Christ be with you all. Amen.‚Äù</p>
</blockquote>
<p>Not quite what I was looking for.</p>
<blockquote>
<p>‚ÄúLast words are for fools who haven‚Äôt said enough!‚Äù</p>
</blockquote>
<p>Too cantankerous. And as a somewhat libertarian leaning person, I wasn‚Äôt going to let Karl Marx have the last word.</p>
<p>I‚Äôd figure out what to say later and in the meantime started reading about the technology behind AIM. I learned about OSCAR, the proprietary protocol used by AIM and ICQ. Parts of the protocol were documented by AOL in an official SDK. This is what made third-party AIM clients possible. I had used Gaim (now Pidgin) on Linux machines before and indeed, there was even BSFlite, an AIM client for the command line.</p>
<p>AOL‚Äôs desktop client no longer worked for me. They had already pulled the iOS app from the App Store. Ditto for Android. Fortunately browser sign on still worked. I was greeted by a familiar interface that supported new features like embedded media and SMS.</p>
<p>I sent a couple test messages from the browser to my phone. Monitoring outbound activity in the Chrome network panel revealed the structure of a request: a simple HTTP POST to a url, with what appeared to be a session ID in the query string and a message body. I tried to send a message using <code>curl</code> and it worked. As long as I was signed on in the browser the request was accepted.</p>
<p>There were several paths to automating this but given the time constraint, something quick and easy would probably suffice. All I had to do was keep sending messages until the server stopped responding.</p>
<p><strong>December 14, 2017: The final countdown</strong></p>
<p>It was almost midnight UTC time. It would be the 15th soon and I didn‚Äôt want to miss the shutdown, so I set up a Bash script to run the <code>curl</code> command at one second intervals. Every now and then I had to manually reauthenticate in the browser. It wasn‚Äôt elegant but it worked. Now I had to wait‚Ä¶</p>
<p>To pass time I searched Twitter for mentions of AIM‚Äôs last day. Plenty of people were reminiscing about their old screen names, but only a handful were actually signing on one last time. I added them to my buddy list and we talked for a bit. One lived in DC, one in Toledo. Another somewhere in Maryland. Two were engineers, one was a wrestling announcer! I made dinner and watched <em>The Office</em> while occasionally checking on the script.</p>
<p>It ran for another six hours until 1:21 AM EST. I witnessed the drama play out in HTTP status codes:</p>
<pre><code>200 OK
200 OK
200 OK
408 Request Timeout
408 Request Timeout
401 Unauthorized
</code></pre><p>And like that, AIM was gone. Requests to aim.com returned an Invalid URL status page. <code>curl</code> returned nothing but 401s.</p>
<p><strong>Epitaph</strong></p>
<p>I examined the script logs and found it ‚Äì the last message sent on AIM. [1] It was timestamped Fri Dec 15 01:21:42 EST 2017. From me, to me. (I didn‚Äôt want to spam anyone.)</p>
<p>I had borrowed the words from Leonard Nimoy‚Äôs final tweet:</p>
<blockquote>
<p>‚ÄúA life is like a garden. Perfect moments can be had, but not preserved, except in memory. LLAP‚Äù</p>
</blockquote>
<p>üññ</p>
<p><strong>Notes</strong></p>
<p>[1] At least in my server region.</p>
<p><strong>Thanks</strong> to behind2greeneyes, dalilmoo, croftonworldwide, nuklermuleburger, and sirmatthew84.</p>
<p>If you enjoyed reading this, please consider a donation to the <a href="https://archive.org/donate/">Internet Archive</a>.</p>
<p>You can also read Kat Timpf‚Äôs brilliant <a href="https://www.nationalreview.com/2017/10/aol-instant-messenger-eulogy-aim-social-media-millennials/">eulogy for AIM</a>.</p>

      <hr>
      <p>
        For more frequent updates, follow me on Twitter.
      </p>
      <p>
        <a href="https://twitter.com/jtangofx?ref_src=twsrc%5Etfw" data-show-count="false">Follow @jtangofx</a>
      </p>
    </div></div>]]>
            </description>
            <link>https://justanman.org/posts/the-last-message-sent-on-aim/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26300266</guid>
            <pubDate>Mon, 01 Mar 2021 06:40:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OMU ‚Äì ‚ÄúOne Man Unix‚Äù]]>
            </title>
            <description>
<![CDATA[
Score 125 | Comments 25 (<a href="https://news.ycombinator.com/item?id=26298022">thread link</a>) | @marcodiego
<br/>
February 28, 2021 | http://www.pix.net/mirrored/discordia.org.uk/~steve/omu.html | <a href="https://web.archive.org/web/*/http://www.pix.net/mirrored/discordia.org.uk/~steve/omu.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
<center>

</center>
<h2>
History:
</h2>
In the late 1970s and early 1980s (the good old days of "hobby computing") before
the IBM PC and its clones took over the world, various microprocessor based kits
appeared on the hobbyist market. You could build a Z80-based board with 8K of
RAM, and with its hexadecimal keypad and LED display you could run simple
programs laboriously keyed in hex.
<p>
The first generation commercial machines also appeared - usually based on
the 8080 or the Z80 or 6502. The Commodore PET was one of the early sucesses
and it, along with the Commodore 64, Sinclair's three machines (ZX80, ZX81 and
Spectrum) and the BBC Micro probably accounted for most of the infant home-computer
market in Britain.
</p><p>
I decided on a different approach. Unhappy with the concept of having to re-live
the glory-days of the
<a href="http://www.computer50.org/mark1/MM1.html">
Manchester MkI
</a>
with the hobby-boards fraternity (!), and unhappy with the straitjacketed
architectures and joke operating systems of the 1st generation home computers,
I yearned for more. Specifically, what I wanted was something
more like a Unix clone at home. I was used to V7 Unix on the PDP-11 at university
and wasn't keen to step backwards 10 years to the technology of CP/M and BASIC
programming.
<br>
<i>
I wasn't to know that eight years later a guy called Linus
Torvalds was going to think the same thoughts and do much the same things. The
big difference was that he was in the right place and the right time and 
had internet connectivity - I didn't have any of these advantages!
</i>
</p><p>
I went off and built myself a 6809 based computer with 64K of RAM, a 360K
floppy disk drive and an RS232 interface which could drive a glass-TTY.
Please remember that this was 1982. Hard disks, megabytes of RAM and other
familiar modern things were a long way off.
</p><p>
After going through the usual hard work of writing a BIOS complete with
an S-records downloader, I was in a position to write myself an operating
system. Preliminary work on OMU started in late 1983. My earliest surviving
printouts of the sources on yellowed, faded lineprinter paper show that the
bulk of the work on OMU dates from March/April/May 1984.
</p><p>
Work was not helped by the fact that I was having to write a 'C' compiler
in parallel, but certainly by mid 1984 I had a perfectly usable O/S with a
primitive shell, a port of Unix 'ed' for an editor, and with 'fsck' and other
such tools available to repair the filestore disks when they got damaged by
miswrites from the rather dodgy floppy-disk hardware. I remember taking
the machine home over Christmas 1984 and typing up several reports using it
on the kitchen table.
</p><p>
<table>
<tbody><tr>
<td>
I just found this photo of the 1984 machine amongst a box of slides taken
over that Christmas. It appears to be the only photo of the machine I ever
took at the time.
<p>
At this point in its evolution, the hardware was mostly on one double
eurocard, with a floppy-disk interface piggybacked onto it. The floppy-disk
drive is in the metal box at front, and the bigger metal box at the RHS
of the PCBs is the power-supply.
</p></td>
<td>
<a href="http://www.pix.net/mirrored/discordia.org.uk/~steve/omu-big.jpg">
<img src="http://www.pix.net/mirrored/discordia.org.uk/~steve/omu.jpg">
</a>
</td>
</tr>
</tbody></table>
</p><h2>
Features and misfeatures list:
</h2>
<ul>
<li>
Tiny (24K) Kernel.
</li><li>
V7 Unix-compatable filestore (doesn't support triple-indirection blocks).
Not exactly critical on a floppy-disk-only system!
</li><li>
Normal Unix-style use of /dev/* files to interface to devices.
</li><li>
Mountable/dismountable filestores as expected.
</li><li>
Shell is built-in to save space, rather than run as a process.
</li><li>
tty driver is exceedingly minimal, but functional.
</li><li>
No true multitasking (see below).
</li><li>
UIDs and GIDs not implemented. They're ignored on the filestore, and the
process table doesn't bother to hold them. Any files created would be given
UID = GID = 0.
</li></ul>
<p>
The lack of an MMU and the small amount of RAM available meant that I
decided against trying to implement true multitasking. Instead (like DOS
as it happens) I came up with a scheme whereby a process could launch another
process, but would have to wait for it to complete. Clunky, but it worked
OK.
</p><p>
For your entertainment (or possibly if you're interested in the possibility
of porting my old O/S to more modern hardware like that old XT that's
gathering dust under your table) here's a downloadable copy of my
final version of the 6809 OMU as it stood in 1987:
</p><center>

</center>
<p>
In order to make any use of OMU, I had compiled up a set of utilities like
'ed' and 'mount' and 'fsck'. Sadly I don't think I can make their sources
downloadable here as they were based on the Unix V7 sources for which we had
a Bell Labs "academic use" licence at the university.
</p><p>
<strong>
<span color="#FF0000">
NEW!
</span>
</strong>
The above packages now include some utilities which I did write from scratch:
</p><ul>
<li><strong>adb</strong>
An interactive disassembler. (Doesn't debug core-dumps though, just lets
you look at code in files).
</li><li><strong>aka</strong>
("Also Known As") - finds other files hardlinked to the one mentioned on the
command-line.
</li><li><strong>chbase</strong>
Prints the number(s) given on the command-line in octal, decimal &amp; hex.
</li><li><strong>hex</strong>
Makes Motorola S-record files for sending to the 6809. Usually runs on a
host machine (a PDP-11 originally).
</li><li><strong>ida</strong>
An intelligent disassembler - shares source-code with 'adb' above.
</li><li><strong>sh</strong>
The tiny shell needed by the OMU kernel. OMU runs this as if it was the
'init' process.
</li><li><strong>sirius</strong>
An interactive disk-examination program.
</li><li><strong>unhex</strong>
A program to convert Motorola S-records into an OMU-loadable file. Usually
runs on the 6809 itself of course.
</li></ul>
Sorry, no documentation, but the source-code should be documentation enough!
<p>
I
<strong>
don't
</strong>
suggest any modern user of OMU on a 6809 tries getting GNU utilities running
with it! Bear in mind that V7 sources would compile into a smaller executable
than any modern GNU equivalents. With the 64K memory-address limitation of my
6809, this was important! The BIOS and O/S accounted for about 24K of this, so
user-programs were limited to about 40K total. I never did complete my
page-switched 256K memory system.....
</p><p>
Mind you, if you're contemplating porting to a 68000 or IBM-PC, the size
constraints will be much less of a problem.
</p><hr>
<p>
My work on OMU was pretty much unnoticed by anyone else in the world (we didn't
have an internet connection in those days), but others in the Electrical
Engineering dept at Swansea University saw the potential to port OMU to one of
several 68000 single-board-computers which began to appear in the department
from about 1984/85 onwards.
</p><p>
My colleage Terry Barnaby and I were variously involved with porting OMU
to several of these 68000 based SBCs. Terry did most of the work!
The first stage was merely to take OMU and get it to run on the new
hardware much as my 6809 version was doing already. This was accomplishe
reasonably quickly, complete with the addition of the OMU's first driver
for a hard-disk.
The much greater memory-addressing ability of the 68000 coupled with the
fact that the SBCs typically had 256K of RAM on board meant that this version
of OMU soon featured some multi-tasking capabilities (though with no MMU
available the processes had to be well-behaved).
</p><p>
For convenience, the details of the filestore layout and the system-call
interface of 68000 OMU was designed to duplicate those of the commercial 68000
V7 Unix system made by "Codata" (we had just taken delivery of two of these
units to augment the aging PDP-11).
It is I think a credit to OMU's potential that
it soon became possible to compile programs on the Codata, and run them on
OMU with no problems. Indeed, it was also possible to take system binaries
shipped with the Codata and run them on OMU too - an easy way to get 'vi'
and other state-of-the-art (!) software on OMU without having to dig out the
sources and recompile!
</p><center>

</center>
<p>
Terry Barnaby and another of his colleages Tim Ingersoll then made some
fundamental changes to the message-handling abilities of the 68000 OMU port to
make it suitable for Real-Time signal processing and control applications.
This was after all what the two of them really
wanted as part of their Ph.D projects...! In 1988 they finished their
projects, wrote up and left. Part of their legacy was the RTOS version of OMU,
and I managed to salvage a set of sources for that too:
</p><center>

</center>
<p>
If any reader of these pages feels suitably daring they may be able to
beat me to the target of porting OMU (probably working from Terry's and my
initial 68000 version above) to the IBM PC. I consider that there really is no
point in aiming for the 386 processor machines onwards as Linux already does
everything you could hope for on that class of machines.
</p><p>
It does however seem that there is mileage in starting with the boot-loader
that comes with
<a href="http://metalab.unc.edu/pub/micro/pc-stuff/freedos/">
FreeDOS
</a>,
and getting OMU to run on 8086/80286 machines of which there must be
<strong>
loads
</strong>
lying around virtually unwanted these days.
</p><p>
If you do decide to try such a thing, then I wish you the best of luck and
I'll offer any email help I can. My email address is in the README file
included with the any of the sets of downloadable sources.
</p><h2>
Acknowledgements:
</h2>
My thanks go to Terry and Tim whose work in enhancing OMU is credited here
and really should have been reflected in a name-change from OMU back in the
Eighties! (I never did get it straight in my mind as to whether the "one man"
mentioned in the title were there to signify the number of authors or
the number of simultaneous users. Either way, neither was very relevant once
as the software started its modest spread in the department.)
<p>
My thanks also go to Alan Cox (he of Linux networking and kernel hacking fame)
without whose prodding this page might never have been written.
</p><center>
<a href="http://www.pix.net/mirrored/discordia.org.uk/~steve/steve.html">
Home
</a>
</center>
<hr>
<p>
Copyright (c) 1999, Steve Hosgood


</p></div>]]>
            </description>
            <link>http://www.pix.net/mirrored/discordia.org.uk/~steve/omu.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26298022</guid>
            <pubDate>Sun, 28 Feb 2021 23:15:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Little League wants all your information]]>
            </title>
            <description>
<![CDATA[
Score 335 | Comments 160 (<a href="https://news.ycombinator.com/item?id=26296845">thread link</a>) | @ColinWright
<br/>
February 28, 2021 | https://honeypot.net/post/little-league-wants-all-your-information/ | <a href="https://web.archive.org/web/*/https://honeypot.net/post/little-league-wants-all-your-information/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
            <div>
                <h3>Little League wants all your information</h3>
                
                    <p><em></em>
                        <span> Sun, Feb 28, 2021 
                                           </span>
                        <em></em>
                        <span>2-minute read</span>
                    </p>
                
            </div>

            <p>To sign kids up for our city‚Äôs Little League baseball program, you have to prove that they‚Äôre residents, which is reasonable. What‚Äôs not reasonable is the amount of information you have to provide on the registration website. You have to upload scans of a document in each of 3 categories:</p>
<blockquote>
<p><strong>Proof of Residency 1</strong>
Choose one of the following: Driver‚Äôs license, School records, Vehicle records, Employment records, Insurance documents</p>
<p><strong>Proof of Residency 2</strong>
Choose one of the following: Welfare/child care records, Federal records, State records, Local records, Support payment records, Homeowner or tenant records, Military records</p>
<p><strong>Proof of Residency 3</strong>
Choose one of the following: Voter‚Äôs registration, Utility bills, Financial records, Medical records, Internet, cable, or satellite bills</p>
</blockquote>
<p>That alone is ripe for identity theft, but couple it with their privacy policy which includes this (emphasis mine):</p>
<blockquote>
<p>Without limitation, this typically requires the use of certain personal information, including registration data, event data, and other personal information, to provide program information, <strong>special offers or services through Little League and/or its trusted sponsors, partners, or licensees</strong>, to fulfill your requests for information or products/services, to maintain a list of verified and eligible participants, to maintain a list of volunteers and provide them with the operating tools to manage leagues, or to respond to your inquiries about our programs.</p>
</blockquote>
<p>In other words, you have to upload your most private information and agree to allow them to do as they like with it, including sharing it with whomever they like for any reason they choose.</p>
<p>This is unacceptable.</p>
</div></div>]]>
            </description>
            <link>https://honeypot.net/post/little-league-wants-all-your-information/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26296845</guid>
            <pubDate>Sun, 28 Feb 2021 20:38:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I cut GTA Online loading times by 70%]]>
            </title>
            <description>
<![CDATA[
Score 2818 | Comments 481 (<a href="https://news.ycombinator.com/item?id=26296339">thread link</a>) | @kuroguro
<br/>
February 28, 2021 | https://nee.lv/2021/02/28/How-I-cut-GTA-Online-loading-times-by-70/ | <a href="https://web.archive.org/web/*/https://nee.lv/2021/02/28/How-I-cut-GTA-Online-loading-times-by-70/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>GTA Online. <a target="_blank" rel="noopener" href="https://www.reddit.com/r/gtaonline/comments/9vgo0g/how_the_fuck_are_20_minute_load_times_acceptable/">Infamous</a> for its slow loading times. Having picked up the game again to finish some of the newer heists I was <em>shocked</em> (/s) to discover that it still loads just as slow as the day it was released 7 years ago.</p>
<p>It was time. Time to get to the bottom of this.</p>
<h2 id="Recon"><a href="#Recon" title="Recon"></a>Recon</h2><p>First I wanted to check if someone had already solved this problem. Most of the results I found pointed towards anecdata about <a target="_blank" rel="noopener" href="https://metro.co.uk/2017/11/01/why-does-gta-v-take-so-long-to-load-7041927/">how the game is so sophisticated</a> that it needs to load so long, stories on how the <a target="_blank" rel="noopener" href="https://steamcommunity.com/app/271590/discussions/0/217690940938819317/">p2p network architecture</a> is rubbish (not saying that it isn‚Äôt), some elaborate ways of <a target="_blank" rel="noopener" href="https://gtaforums.com/topic/908000-fastest-way-to-load-into-gtao-single-player-first-or-straight-in/">loading into story mode and a solo session after that</a> and a couple of mods that allowed skipping the startup R* logo video. Some more reading told me we could save a whopping 10-30 seconds with these combined!</p>
<p>Meanwhile on my PC‚Ä¶</p>
<h2 id="Benchmark"><a href="#Benchmark" title="Benchmark"></a>Benchmark</h2><figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br></pre></td><td><pre><span>Story mode load time:  ~1m 10s</span><br><span>Online mode load time: ~6m flat</span><br><span>Startup menu disabled, time from R* logo until in-game (social club login time isn't counted).</span><br><span></span><br><span>Old but decent CPU:   AMD FX-8350</span><br><span>Cheap-o SSD:          KINGSTON SA400S37120G</span><br><span>We have to have RAM:  2x Kingston 8192 MB (DDR3-1337) 99U5471</span><br><span>Good-ish GPU:         NVIDIA GeForce GTX 1070</span><br></pre></td></tr></tbody></table></figure>

<p>I know my setup is dated but what on <em>earth</em> could take 6x longer to load into online mode? I couldn‚Äôt measure any difference using the story-to-online loading technique <a target="_blank" rel="noopener" href="https://www.reddit.com/r/gtaonline/comments/kycy7a/gtao_loading_times_using_different_methods/">as others have found before me</a>. Even if it did work the results would be down in the noise.</p>
<h2 id="I-Am-Not-Alone"><a href="#I-Am-Not-Alone" title="I Am (Not) Alone"></a>I Am (Not) Alone</h2><p>If <a target="_blank" rel="noopener" href="https://www.reddit.com/r/gtaonline/comments/ht4i56/your_average_online_loading_time/">this poll</a> is to be trusted then the issue is widespread enough to mildly annoy more than 80% of the player base. It‚Äôs been 7 years R*!</p>
<p><img src="https://nee.lv/images/pasted-0.png" alt="üéµWhat does the poll say?üéµ"></p>
<p>Looking around a bit to find who are the lucky ~20% that get sub 3 minute load times I came across <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=RdCqDdjp6iU">a</a> <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=pJzr3qfyCyg">few</a> <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=RK7BUFx_NGk">benchmarks</a> with high-end gaming PCs and an online mode load time of about 2 minutes. I would <del>kill</del> <em>hack</em> for a 2 minute load time! It does seem to be hardware-dependent but something doesn‚Äôt add up here‚Ä¶</p>
<p>How come their story mode still takes near a minute to load? (The M.2 one didn‚Äôt count the startup logos btw.) Also, loading story to online takes them only a minute more while I‚Äôm getting about five more. I know that their hardware specs are a lot better but surely not 5x better.</p>
<h2 id="Highly-accurate-measurements"><a href="#Highly-accurate-measurements" title="Highly accurate measurements"></a>Highly accurate measurements</h2><p>Armed with such powerful tools as <em>the Task Manager</em> I began to investigate what resources could be the bottleneck.</p>
<p><img src="https://nee.lv/images/pasted-1.png" alt="Can you smell it?"></p>
<p>After taking a minute to load the common resources used for both story and online modes (which is near on par with high-end PCs) GTA decides to max out a single core on my machine for four minutes and do nothing else.</p>
<p>Disk usage? None! Network usage? There‚Äôs a bit, but it drops basically to zero after a few seconds (apart from loading the rotating info banners). GPU usage? Zero. Memory usage? Completely flat‚Ä¶</p>
<p>What, is it mining crypto or something? I smell code. <em>Really bad code</em>.</p>
<h2 id="Single-thread-bound"><a href="#Single-thread-bound" title="Single thread-bound"></a>Single thread-bound</h2><p>While my old AMD CPU has 8 cores and it does pack a punch, it was made in the olden days. Back when AMD‚Äôs <a target="_blank" rel="noopener" href="https://valid.x86.fr/bench/6u7sdy/1">single-thread performance</a> was <em>way</em> behind Intel‚Äôs. This might not explain all of the load time differences but it should explain most of it.</p>
<p>What‚Äôs odd is that it‚Äôs using up <em>just</em> the CPU. I was expecting vast amounts of disk reads loading up resources or loads of network requests trying to negotiate a session in the p2p network. But this? This is probably a bug.</p>
<h2 id="Profiling"><a href="#Profiling" title="Profiling"></a>Profiling</h2><p>Profilers are a great way of finding CPU bottlenecks. There‚Äôs only one problem - most of them rely on instrumenting the source code to get a perfect picture of what‚Äôs happening in the process. And I don‚Äôt have the source code. Nor do I need microsecond-perfect readings - I have 4 minutes‚Äô worth of a bottleneck.</p>
<p>Enter stack sampling: for closed source applications there‚Äôs only one option. Dump the running process‚Äô stack and current instruction pointer‚Äôs location to build a calling tree in set intervals. Then add them up to get statistics on what‚Äôs going on. There‚Äôs only one profiler that I know of (might be ignorant here) that can do this on Windows. And it hasn‚Äôt been updated in over 10 years. It‚Äôs <a target="_blank" rel="noopener" href="http://lukestackwalker.sourceforge.net/">Luke Stackwalker</a>! Someone, please give this project some love :)</p>
<p><img src="https://nee.lv/images/pasted-2.png" alt="The power of statistics compels you!"></p>
<p>Normally Luke would group the same functions together but since I don‚Äôt have debugging symbols I had to eyeball nearby addresses to guess if it‚Äôs the same place. And what do we see? Not one bottleneck but two of them!</p>
<h2 id="Down-the-rabbit-hole"><a href="#Down-the-rabbit-hole" title="Down the rabbit hole"></a>Down the rabbit hole</h2><p>Having borrowed <em>my friend‚Äôs</em> completely legitimate copy of <em>the industry-standard disassembler</em> (no, I really can‚Äôt afford the thing‚Ä¶ gonna learn to <a target="_blank" rel="noopener" href="https://ghidra-sre.org/">ghidra</a> one of these days) I went to take GTA apart.</p>
<p><img src="https://nee.lv/images/pasted-3.png" alt="Gibberish Galore"></p>
<p>That doesn‚Äôt look right at all. Most high-profile games come with built-in protection against reverse engineering to keep away pirates, cheaters, and modders. Not that it has ever stopped them.</p>
<p>There seems to be some sort of an obfuscation/encryption at play here that has replaced most instructions with gibberish. Not to worry, we simply need to dump the game‚Äôs memory while it‚Äôs executing the part we want to look at. The instructions have to be de-obfuscated before running one way or another. I had <a target="_blank" rel="noopener" href="https://github.com/glmcdona/Process-Dump">Process Dump</a> lying around, so I used that, but there are plenty of other tools available to do this sort of thing.</p>
<h2 id="Problem-one-It‚Äôs‚Ä¶-strlen"><a href="#Problem-one-It‚Äôs‚Ä¶-strlen" title="Problem one: It‚Äôs‚Ä¶ strlen?!"></a>Problem one: It‚Äôs‚Ä¶ strlen?!</h2><p>Disassembling the now-less-obfuscated dump reveals that one of the addresses has a label pulled out of somewhere! It‚Äôs <code>strlen</code>? Going down the call stack the next one is labeled <code>vscan_fn</code> and after that the labels end, tho I‚Äôm fairly confident it‚Äôs <a target="_blank" rel="noopener" href="https://github.com/chakra-core/ChakraCore/blob/master/pal/src/safecrt/sscanf.c#L47"><code>sscanf</code></a>.</p>
<p><img src="https://nee.lv/images/pasted-4.png" alt="A graph a day keeps the skeptics away"></p>
<p>It‚Äôs parsing something. Parsing what? Untangling the disassembly would take forever so I decided to dump some samples from the running process using <a target="_blank" rel="noopener" href="https://x64dbg.com/">x64dbg</a>. Some debug-stepping later it turns out it‚Äôs‚Ä¶ JSON! They‚Äôre parsing JSON. A whopping <strong>10 megabytes</strong> worth of JSON with some <strong>63k item entries</strong>.</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br></pre></td><td><pre><span>...,</span><br><span>{</span><br><span>    <span>"key"</span>: <span>"WP_WCT_TINT_21_t2_v9_n2"</span>,</span><br><span>    <span>"price"</span>: <span>45000</span>,</span><br><span>    <span>"statName"</span>: <span>"CHAR_KIT_FM_PURCHASE20"</span>,</span><br><span>    <span>"storageType"</span>: <span>"BITFIELD"</span>,</span><br><span>    <span>"bitShift"</span>: <span>7</span>,</span><br><span>    <span>"bitSize"</span>: <span>1</span>,</span><br><span>    <span>"category"</span>: [<span>"CATEGORY_WEAPON_MOD"</span>]</span><br><span>},</span><br><span>...</span><br></pre></td></tr></tbody></table></figure>

<p>What is it? It appears to be data for a ‚Äúnet shop catalog‚Äù according to some references. I assume it contains a list of all the possible items and upgrades you can buy in GTA Online.</p>
<p><strong>Clearing up some confusion: I beleive these are in-game money purchasable items, not directly linked with <a target="_blank" rel="noopener" href="https://gta.fandom.com/wiki/Cash_Cards">microtransactions</a>.</strong></p>
<p>But 10 megs? That‚Äôs nothing! And using <code>sscanf</code> may not be optimal but surely it‚Äôs not that bad? Well‚Ä¶</p>
<p><img src="https://nee.lv/images/pasted-5.png" alt="Ouch!"></p>
<p>Yeah, that‚Äôs gonna take a while‚Ä¶ To be fair I had no idea most <code>sscanf</code> implementations called <code>strlen</code> so I can‚Äôt blame the developer who wrote this. I would assume it just scanned byte by byte and could stop on a <code>NULL</code>.</p>
<h2 id="Problem-two-Let‚Äôs-use-a-Hash-‚Ä¶-Array"><a href="#Problem-two-Let‚Äôs-use-a-Hash-‚Ä¶-Array" title="Problem two: Let‚Äôs use a Hash- ‚Ä¶ Array?"></a>Problem two: Let‚Äôs use a Hash- ‚Ä¶ Array?</h2><p>Turns out the second offender is called right next to the first one. They‚Äôre both even called in the same <code>if</code> statement as seen in this ugly decompilation:</p>
<p><img src="https://nee.lv/images/pasted-6.png" alt="Beggar thy neighbour"></p>
<p>All labels are mine, no idea what the functions/parameters are actually called.</p>
<p>The second problem? Right after parsing an item, it‚Äôs stored in an array (or an inlined C++ list? not sure). Each entry looks something like this:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br></pre></td><td><pre><span><span><span>struct</span> {</span></span><br><span>    <span>uint64_t</span> *hash;</span><br><span>    <span>item_t</span>   *item;</span><br><span>} entry;</span><br></pre></td></tr></tbody></table></figure>

<p>But before it‚Äôs stored? It checks the <em>entire</em> array, one by one, comparing the hash of the item to see if it‚Äôs in the list or not. With ~63k entries that‚Äôs <code>(n^2+n)/2 = (63000^2+63000)/2 = 1984531500</code> checks if my math is right. Most of them useless. You have unique <em>hashes</em> why not use a <em>hash map</em>.</p>
<p><img src="https://nee.lv/images/pasted-7.png" alt="Oof!"></p>
<p>I named it <code>hashmap</code> while reversing but it‚Äôs clearly <code>not_a_hashmap</code>. And it gets even better. The hash-array-list-thing is empty before loading the JSON. And all of the items in the JSON are unique! They don‚Äôt even <em>need</em> to check if it‚Äôs in the list or not! They even have a function to directly insert the items! Just use that! Srsly, WAT!?</p>
<h2 id="PoC"><a href="#PoC" title="PoC"></a>PoC</h2><p>Now that‚Äôs nice and all, but no one is going to take me seriously unless I test this so I can write a clickbait title for the post.</p>
<p>The plan? Write a <code>.dll</code>, inject it in GTA, <a target="_blank" rel="noopener" href="https://github.com/TsudaKageyu/minhook">hook</a> some functions, ???, profit.</p>
<p>The JSON problem is hairy, I can‚Äôt realistically replace their parser. Replacing <code>sscanf</code> with one that doesn‚Äôt depend on <code>strlen</code> would be more realistic. But there‚Äôs an even easier way.</p>
<ul>
<li>hook strlen</li>
<li>wait for a long string</li>
<li>‚Äúcache‚Äù the start and length of it</li>
<li>if it‚Äôs called again within the string‚Äôs range, return cached value</li>
</ul>
<p>Something like:</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br><span>22</span><br><span>23</span><br><span>24</span><br><span>25</span><br><span>26</span><br><span>27</span><br><span>28</span><br><span>29</span><br><span>30</span><br><span>31</span><br><span>32</span><br><span>33</span><br><span>34</span><br><span>35</span><br><span>36</span><br></pre></td><td><pre><span><span><span>size_t</span> <span>strlen_cacher</span><span>(<span>char</span>* str)</span></span></span><br><span><span></span>{</span><br><span>  <span>static</span> <span>char</span>* start;</span><br><span>  <span>static</span> <span>char</span>* end;</span><br><span>  <span>size_t</span> len;</span><br><span>  <span>const</span> <span>size_t</span> cap = <span>20000</span>;</span><br><span></span><br><span>  </span><br><span>  <span>if</span> (start &amp;&amp; str &gt;= start &amp;&amp; str &lt;= end) {</span><br><span>    </span><br><span>    len = end - str;</span><br><span></span><br><span>    </span><br><span>    </span><br><span>    <span>if</span> (len &lt; cap / <span>2</span>)</span><br><span>      MH_DisableHook((LPVOID)strlen_addr);</span><br><span></span><br><span>    </span><br><span>    <span>return</span> len;</span><br><span>  }</span><br><span></span><br><span>  </span><br><span>  </span><br><span>  </span><br><span>  len = builtin_strlen(str);</span><br><span></span><br><span>  </span><br><span>  </span><br><span>  <span>if</span> (len &gt; cap) {</span><br><span>    start = str;</span><br><span>    end = str + len;</span><br><span>  }</span><br><span></span><br><span>  </span><br><span>  <span>return</span> len;</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>

<p>And as for the hash-array problem, it‚Äôs more straightforward - just skip the duplicate checks entirely and insert the items directly since we know the values are unique.</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br><span>7</span><br><span>8</span><br><span>9</span><br><span>10</span><br><span>11</span><br><span>12</span><br><span>13</span><br><span>14</span><br><span>15</span><br><span>16</span><br><span>17</span><br><span>18</span><br><span>19</span><br><span>20</span><br><span>21</span><br></pre></td><td><pre><span><span><span>char</span> __fastcall <span>netcat_insert_dedupe_hooked</span><span>(<span>uint64_t</span> catalog, <span>uint64_t</span>* key, <span>uint64_t</span>* item)</span></span></span><br><span><span></span>{</span><br><span>  </span><br><span>  <span>uint64_t</span> not_a_hashmap = catalog + <span>88</span>;</span><br><span></span><br><span>  </span><br><span>  <span>if</span> (!(*(<span>uint8_t</span>(__fastcall**)(<span>uint64_t</span>*))(*item + <span>48</span>))(item))</span><br><span>    <span>return</span> <span>0</span>;</span><br><span></span><br><span>  </span><br><span>  netcat_insert_direct(not_a_hashmap, key, &amp;item);</span><br><span></span><br><span>  </span><br><span>  </span><br><span>  <span>if</span> (*key == <span>0x7FFFD6BE</span>) {</span><br><span>    MH_DisableHook((LPVOID)netcat_insert_dedupe_addr);</span><br><span>    unload();</span><br><span>  }</span><br><span></span><br><span>  <span>return</span> <span>1</span>;</span><br><span>}</span><br></pre></td></tr></tbody></table></figure>

<p>Full source of PoC <a target="_blank" rel="noopener" href="https://github.com/tostercx/GTAO_Booster_PoC">here</a>.</p>
<h2 id="Results"><a href="#Results" title="Results"></a>Results</h2><p>Well, did it work then?</p>
<figure><table><tbody><tr><td><pre><span>1</span><br><span>2</span><br><span>3</span><br><span>4</span><br><span>5</span><br><span>6</span><br></pre></td><td><pre><span>Original online mode load time:        ~6m flat</span><br><span>Time with only duplication check patch: 4m 30s</span><br><span>Time with only JSON parser patch:       2m 50s</span><br><span>Time with both issues patched:          1m 50s</span><br><span></span><br><span>(6*60 - (1*60+50)) / (6*60) = 69.4% load time improvement (nice!)</span><br></pre></td></tr></tbody></table></figure>

<p>Hell yes, it did! :))</p>
<p>Most likely, this won‚Äôt solve everyone‚Äôs load times - there might be other bottlenecks on different systems, but it‚Äôs such a gaping hole that I have no idea how R* has missed it all these years.</p>
<h2 id="tl-dr"><a href="#tl-dr" title="tl;dr"></a>tl;dr</h2><ul>
<li>There‚Äôs a single thread CPU bottleneck while starting up GTA Online</li>
<li>It turns out GTA struggles to parse a 10MB JSON file</li>
<li>The JSON parser itself is poorly built / naive and</li>
<li>After parsing there‚Äôs a ‚Ä¶</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nee.lv/2021/02/28/How-I-cut-GTA-Online-loading-times-by-70/">https://nee.lv/2021/02/28/How-I-cut-GTA-Online-loading-times-by-70/</a></em></p>]]>
            </description>
            <link>https://nee.lv/2021/02/28/How-I-cut-GTA-Online-loading-times-by-70/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26296339</guid>
            <pubDate>Sun, 28 Feb 2021 19:38:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Create animated GIF and WebP from videos using FFmpeg]]>
            </title>
            <description>
<![CDATA[
Score 150 | Comments 47 (<a href="https://news.ycombinator.com/item?id=26296315">thread link</a>) | @Audiolite
<br/>
February 28, 2021 | https://mattj.io/posts/2021-02-27-create-animated-gif-and-webp-from-videos-using-ffmpeg/ | <a href="https://web.archive.org/web/*/https://mattj.io/posts/2021-02-27-create-animated-gif-and-webp-from-videos-using-ffmpeg/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article class="page">
    <div>
      
      <div>

        

<p>Saturday, February 27, 2021</p>

<p><a href="https://mattj.io/posts/">Click here to go to all posts</a>. <em>Also published on <a href="https://mattjoseph.medium.com/create-animated-gif-and-webp-from-videos-using-ffmpeg-f1012267935a" target="_blank" rel="noopener">Medium</a></em></p>

<p><em>A guide to using FFmpeg to create all the animated content you want.</em></p>

<p>Whether it‚Äôs for a website, a presentation, or sharing a fun clip with a friend on chat, you might want to convert a video to an animated GIF or animated WebP. Unfortunately, the visual tools for doing this vary by your operating system. Additionally, most conversion tools don‚Äôt support <a href="https://en.wikipedia.org/wiki/WebP" target="_blank" rel="noopener">the WebP format</a>, even in 2021. WebP is based on VP8, a relatively recent video codec standard compared to the <a href="https://en.wikipedia.org/wiki/GIF" target="_blank" rel="noopener">GIF image format</a>.</p>

<p>So, this guide is for those who are willing to learn a bit of terminal in order to convert any video to the animated format of their choosing. The best part: this will work on all major operating systems and gives you all the control of the output you could want!</p>


<figcaption>Example GIF of typing "GIF" on a mechanical keyboard</figcaption>

<p>Let‚Äôs get started!</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>To use this guide, you will need the following:</p>

<ul>
  <li>Basic knowledge of how to open and use the terminal on your operating system. If you need a cheat sheet or introductory guides, check out <a href="https://terminalcheatsheet.com/" target="_blank" rel="noopener">Terminal Cheat Sheet</a>.</li>
  <li>FFmpeg v4+ installed on your operating system and executable from your path. Here are some suggested places to learn down to do this:
    <ul>
      <li>macOS: <a href="https://superuser.com/a/624562" target="_blank" rel="noopener">https://superuser.com/a/624562</a></li>
      <li>Windows: <a href="https://video.stackexchange.com/a/20496" target="_blank" rel="noopener">https://video.stackexchange.com/a/20496</a></li>
      <li>Linux: Use your preferred package manager (e.g., <code>sudo apt install ffmpeg</code> on Ubuntu)</li>
    </ul>
  </li>
</ul>

<h2 id="what-isffmpeg">What is&nbsp;FFmpeg?</h2>

<p><a href="https://en.wikipedia.org/wiki/FFmpeg" target="_blank" rel="noopener">From Wikipedia</a>:</p>

<blockquote>
  <p>FFmpeg is a free and open-source software project consisting of a large suite of libraries and programs for handling video, audio, and other multimedia files and streams.</p>
</blockquote>

<p>For our purposes, we will use it to convert between formats, such as videos to GIFs or animated WebP. It has many uses, so I recommend checking it out for all your video processing needs!</p>

<h2 id="before-you-start-make-sure-you-can-run-ffmpeg-from-yourterminal">Before you start: make sure you can run FFmpeg from your&nbsp;terminal</h2>

<p>Since all of these commands require FFmpeg, we need to make sure it‚Äôs available.</p>

<p>Open your terminal, and run this:</p>



<p>If FFmpeg is available, you will note output similar to this:</p>

<div><div><pre><code>FFmpeg version 4.3.1 Copyright ¬© 2000‚Äì2020 the FFmpeg developers
...
</code></pre></div></div>


<figcaption>Checking the FFmpeg version on Linux</figcaption>

<p>Version 4 or higher of FFmpeg is recommended for this guide.</p>

<p>If you get an output that says something similar to <code>command not found: ffmpeg -version</code>, then check the <strong>Prerequisites</strong> section above and make sure you have FFmpeg installed on your system.</p>

<h2 id="convert-to-an-animated-gif-usingffmpeg">Convert to an animated GIF using&nbsp;FFmpeg</h2>

<h3 id="convert-a-whole-video-togif">Convert a whole video to&nbsp;GIF</h3>

<p><strong>Base command</strong></p>

<div><div><pre><code>ffmpeg -i $INPUT_FILENAME \
-vf "fps=$OUTPUT_FPS,scale=$OUTPUT_WIDTH:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" \
-loop $NUMBER_OF_LOOPS $OUTPUT_FILENAME

# Change these placeholders:
# * $INPUT_FILENAME - path to the input video.
# * $OUTPUT_FPS - ouput frames per second. Start with `10`.
# * $OUTPUT_WIDTH - output width in pixels. Aspect ratio is maintained.
# * $NUMBER_OF_LOOPS - use `0` to loop forever, or a specific number of loops.
# * $OUTPUT_FILENAME - the name of the output animated GIF.
</code></pre></div></div>

<p><strong>Example usage of this command</strong></p>

<p>Here is an example of this command with the input options filled out:</p>

<div><div><pre><code>ffmpeg -i "sample_recording.mp4" \
-vf "fps=10,scale=720:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" \
-loop 0 sample_recording.gif
</code></pre></div></div>

<h3 id="convert-part-of-a-video-togif">Convert part of a video to&nbsp;GIF</h3>

<p><strong>Base command</strong></p>

<p>This is the base command with various options for converting part of a video to an animated GIF:</p>

<div><div><pre><code>ffmpeg -ss $INPUT_START_TIME -t $LENGTH -i $INPUT_FILENAME \
-vf "fps=$OUTPUT_FPS,scale=$OUTPUT_WIDTH:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" \
-loop $NUMBER_OF_LOOPS $OUTPUT_FILENAME

# Change these placeholders:
# * $INPUT_START_TIME - number of seconds in the input video to start from.
# * $LENGTH - number of seconds to convert from the input video.
# * $INPUT_FILENAME - path to the input video.
# * $OUTPUT_FPS - ouput frames per second. Start with `10`.
# * $OUTPUT_WIDTH - output width in pixels. Aspect ratio is maintained.
# * $NUMBER_OF_LOOPS - use `0` to loop forever, or a specific number of loops.
# * $OUTPUT_FILENAME - the name of the output animated GIF.
</code></pre></div></div>

<p><strong>Example usage of this command</strong></p>

<p>Here is an example of this command with the input options filled out:</p>

<div><div><pre><code>ffmpeg -ss 32.5 -t 7 -i "sample_recording.mp4" \
-vf "fps=10,scale=720:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" \
-loop 0 sample_recording.gif
</code></pre></div></div>

<h2 id="convert-to-an-animated-webp-usingffmpeg">Convert to an animated WebP using&nbsp;FFmpeg</h2>

<h3 id="convert-a-whole-video-to-animatedwebp">Convert a whole video to animated&nbsp;WebP</h3>

<p><strong>Base command</strong></p>

<p>This is the base command with various options for converting an entire video to an animated WebP. You can use options like FPS, output width, and quality to determine the file size and quality of your output:</p>

<div><div><pre><code>ffmpeg -i $INPUT_FILENAME \
-vf "fps=$OUTPUT_FPS,scale=$OUTPUT_WIDTH:-1:flags=lanczos" \
-vcodec libwebp -lossless 0 -compression_level 6 \
-q:v $OUTPUT_QUALITY -loop $NUMER_OF_LOOPS \
-preset picture -an -vsync 0 $OUTPUT_FILENAME

# Change these placeholders:
# * $INPUT_FILENAME - path to the input video.
# * $OUTPUT_FPS - ouput frames per second. Start with `10`.
# * $OUTPUT_WIDTH - output width in pixels. Aspect ratio is maintained.
# * $OUTPUT_QUALITY - quality of the WebP output. Start with `50`.
# * $NUMBER_OF_LOOPS - use `0` to loop forever, or a specific number of loops.
# * $OUTPUT_FILENAME - the name of the output animated WebP.
</code></pre></div></div>

<p><strong>Example usage of this command</strong></p>

<p>Here is an example of this command with the input options filled out:</p>

<div><div><pre><code>ffmpeg -i "sample_recording.mp4" \
-vf "fps=10,scale=720:-1:flags=lanczos" \
-vcodec libwebp -lossless 0 -compression_level 6 \
-q:v 50 -loop 0 \
-preset picture -an -vsync 0 sample_recording.webp
</code></pre></div></div>

<h3 id="convert-part-of-a-video-to-animatedwebp">Convert part of a video to animated&nbsp;WebP</h3>

<p><strong>Base command</strong></p>

<p>This is the base command with various options for converting part of a video to an animated WebP:</p>

<div><div><pre><code>ffmpeg -ss $INPUT_START_TIME -t $LENGTH -i $INPUT_FILENAME \
-vf "fps=$OUTPUT_FPS,scale=$OUTPUT_WIDTH:-1:flags=lanczos" \
-vcodec libwebp -lossless 0 -compression_level 6 \
-q:v $OUTPUT_QUALITY -loop $NUMER_OF_LOOPS \
-preset picture -an -vsync 0 $OUTPUT_FILENAME

# Change these placeholders:
# * $INPUT_START_TIME - number of seconds in the input video to start from.
# * $LENGTH - number of seconds to convert from the input video.
# * $INPUT_FILENAME - path to the input video.
# * $OUTPUT_FPS - ouput frames per second. Start with `10`.
# * $OUTPUT_WIDTH - output width in pixels. Aspect ratio is maintained.
# * $OUTPUT_QUALITY - quality of the WebP output. Start with `50`.
# * $NUMBER_OF_LOOPS - use `0` to loop forever, or a specific number of loops.
# * $OUTPUT_FILENAME - the name of the output animated WebP.
</code></pre></div></div>

<p><strong>Example usage of this command</strong></p>

<p>Here is an example of this command with the input options filled out:</p>

<div><div><pre><code>ffmpeg -ss 32.5 -t 7 -i "sample_recording.mp4" \
-vf "fps=10,scale=720:-1:flags=lanczos" \
-vcodec libwebp -lossless 0 -compression_level 6 \
-q:v 50 -loop 0 \
-preset picture -an -vsync 0 sample_recording.webp
</code></pre></div></div>

<h2 id="when-should-i-use-an-animated-gif-versus-an-animatedwebp">When should I use an animated GIF versus an animated&nbsp;WebP?</h2>

<p>This depends on the quality, size, and support you want for your output. Modern browsers have support for animated WebP and the quality tends to be higher, but the processing power required is also higher.</p>

<h2 id="next-steps">Next steps</h2>

<p>This guide serves as a brief introduction to using FFmpeg to create an animated GIF or animated WebP from a video, but there is so much more you can do with the tool. There are also many options that FFmpeg supports for these formats that are not covered.</p>

<p>You can also get the code for all the commands and examples in one place by <a href="https://gist.github.com/devadvance/f2ad3cfe38afe3eeef64c72c46692158" target="_blank" rel="noopener">visiting the GitHub Gist here</a>.</p>


      </div>
    </div>
  </article></div>]]>
            </description>
            <link>https://mattj.io/posts/2021-02-27-create-animated-gif-and-webp-from-videos-using-ffmpeg/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26296315</guid>
            <pubDate>Sun, 28 Feb 2021 19:35:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Weird architectures weren't supported to begin with]]>
            </title>
            <description>
<![CDATA[
Score 327 | Comments 203 (<a href="https://news.ycombinator.com/item?id=26294397">thread link</a>) | @woodruffw
<br/>
February 28, 2021 | https://blog.yossarian.net/2021/02/28/Weird-architectures-werent-supported-to-begin-with | <a href="https://web.archive.org/web/*/https://blog.yossarian.net/2021/02/28/Weird-architectures-werent-supported-to-begin-with">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">


<h2><em>Programming, philosophy, pedaling.</em></h2>

<ul>
    <li><a href="https://blog.yossarian.net/">Home</a></li>
    <li><a href="https://blog.yossarian.net/tags">Tags</a></li>
    <li><a href="https://blog.yossarian.net/favorites">Favorites</a></li>
    <li><a href="https://blog.yossarian.net/archive">Archive</a></li>
    
      <li><a href="https://yossarian.net/">Main Site</a></li>
    
</ul>

<hr>



<h2>
  <em>Feb 28, 2021</em>
</h2>

  <p>Tags:
  
    
    <a href="https://blog.yossarian.net/tags#rant">rant</a>,
    
  
    
    <a href="https://blog.yossarian.net/tags#programming">programming</a>
    
  
  </p>


<hr>

<h4 id="preword">Preword</h4>

<p>This post contains my own opinions, not the opinions of my employer or any open source groups I
belong or contribute to.</p>

<p>It‚Äôs also been rewritten 2¬Ω times, and (I think) reads confusingly in places. But I promised
myself that I‚Äôd get it out of the door instead of continuing to sit on it, so here we go.</p>

<hr>

<p>There‚Äôs been a decent amount of <del>drama</del> debate in the open source community about <em>support</em>
recently, originating primarily from
<a href="https://github.com/pyca/cryptography/issues/5771">pyca/cryptography‚Äôs decision to use Rust for some ASN.1 parsing routines</a><sup id="fnref:asn1" role="doc-noteref"><a href="#fn:asn1">1</a></sup>.</p>

<p>To summarize the situation: building the latest <code>pyca/cryptography</code> release from scratch now requires
a Rust toolchain. The only currently<sup id="fnref:gccrust" role="doc-noteref"><a href="#fn:gccrust">2</a></sup> Rust toolchain is built on <a href="https://llvm.org/">LLVM</a>, which
supports a (relatively) limited
<a href="https://llvm.org/docs/CompilerWriterInfo.html">set of architectures</a>. Rust further whittles this
set down into <a href="https://doc.rust-lang.org/nightly/rustc/platform-support.html">support tiers</a>, with
some targets not receiving automated testing (tier 2) or official builds (tier 3).</p>

<p>By contrast, upstream<sup id="fnref:gcchell" role="doc-noteref"><a href="#fn:gcchell">3</a></sup> GCC supports a <a href="https://gcc.gnu.org/backends.html">somewhat larger</a>
set of architectures. But C<sup id="fnref:c" role="doc-noteref"><a href="#fn:c">4</a></sup>, cancer that it is, finds its way onto every architecture with or
without GCC (or LLVM‚Äôs) help, and thereby bootstraps <a href="https://github.com/python/cpython">everything</a>
<a href="https://www.gnome.org/">else</a>.</p>

<p>Program packagers and distributors (frequently separate from project maintainers themselves)
are very used to C‚Äôs universal presence. They‚Äôre so used to it that they‚Äôve built generic
mechanisms for putting entire distributions onto new architectures with
only a single assumption: the presence of a serviceable C compiler.</p>

<p>This is the heart of the conflict: Rust (and many other modern, safe languages) use LLVM for its
relative simplicity<sup id="fnref:simple" role="doc-noteref"><a href="#fn:simple">5</a></sup>, but LLVM does not support either native or cross-compilation to many
less popular (read: niche) architectures. Package managers are increasingly finding that one of
their oldest assumptions can be easily violated, and they‚Äôre not happy about that.</p>

<p>But here‚Äôs the problem: <em>it‚Äôs a bad assumption</em>. The fact that it‚Äôs the default
represents an <strong>unmitigated</strong> security, reliability, and reproducibility <em>disaster</em>.</p>

<h2 id="a-little-thought-problem">A little thought problem</h2>

<p>Imagine, for a moment, that you‚Äôre a maintainer of a popular project.</p>

<p>Everything has gone right for you: you have happy users, an active development base, and maybe even
corporate sponsors. You‚Äôve also got a CI/CD pipeline that produces canonical releases of your
project on tested architectures; you treat any issues with uses of those releases as a bug in the
project itself, since you‚Äôve taken responsibility for packaging it.</p>

<p>Because your project is popular, <strong>others</strong> also distribute it: Linux distributions, third-party
package managers, and corporations seeking to deploy their own controlled builds. These others have
slightly different needs and setups and, to varying degrees, will:</p>

<ul>
  <li>Build your project with slightly (or completely) different versions of dependencies</li>
  <li>Build your project with slightly (or completely) different optimization flags and other potentially
ABI-breaking options</li>
  <li>Distribute your project with insecure or outright broken defaults</li>
  <li>Disable important security features because other parts of their ecosystem haven‚Äôt caught up</li>
  <li>Patch your project or its build to make it ‚Äúwork‚Äù (read: compile and not crash immediately) with
completely new dependencies, compilers, toolchains, architectures, and environmental constraints</li>
</ul>

<p>You don‚Äôt know about <em>any</em> of the above until the bug reports start rolling in: users will report
bugs that have already been fixed, bugs that you explicitly document as caused by unsupported
configurations, bugs that <em>don‚Äôt make any sense whatsoever</em>.</p>

<p>You struggle to debug your users‚Äô reports, since you don‚Äôt have access to the niche
hardware, environments, or corporate systems that they‚Äôre running on. You slowly burn out
as an unending torrent of already fixed bugs that never seem to make it to your users. Your
user base is unhappy, and you start to wonder why you‚Äôre putting all this effort into
project maintenance in the first place. Open source was supposed to be fun!</p>

<p>What‚Äôs the point of this spiel? It‚Äôs <em>precisely</em> what happened to <code>pyca/cryptography</code>:
nobody asked them whether it was a good idea to try to run their code on
<a href="https://en.wikipedia.org/wiki/PA-RISC">HPPA</a>, much less
<a href="https://en.wikipedia.org/wiki/IBM_System/390">System/390</a><sup id="fnref:s390" role="doc-noteref"><a href="#fn:s390">6</a></sup>; some packagers just went ahead
and did it, and are frustrated that it no longer works. People just <em>assumed</em> that it
would, because there is <em>still</em> a norm that everything flows from C, and that any
host with a halfway-functional C compiler should have the entire open source ecosystem
at its disposal.</p>

<h3 id="reflections-on-trusting-random-platforms">Reflections on trusting random platforms<sup id="fnref:rott" role="doc-noteref"><a href="#fn:rott">7</a></sup></h3>

<p>Security-sensitive software<sup id="fnref:security" role="doc-noteref"><a href="#fn:security">8</a></sup><sup>,</sup><sup id="fnref:reliability" role="doc-noteref"><a href="#fn:reliability">9</a></sup>, <em>particularly</em> software written
in unsafe languages, is <strong>never</strong> secure in its own right.</p>

<p>The security of a program is a function of its own design and testing,
<em>as well as</em> the design, testing, and basic correctness of its underlying platform: everything from
the userspace, to the kernel, to the compilers themselves. The latter
is an <strong>unsolved problem</strong> in the <em>very best of cases</em>: bugs are <em>regularly</em>
found in even the most mature compilers (Clang, GCC) and their most mature backends (x86, ARM). Tiny
changes to or differences in build systems can have profound effects at the binary level, like
<a href="https://insights.sei.cmu.edu/cert/2018/08/when-aslr-is-not-really-aslr---the-case-of-incorrect-assumptions-and-bad-defaults.html">accidentally removing security mitigations</a>.
Seemingly innocuous patches can make otherwise safe code
<a href="https://wiki.gentoo.org/wiki/Hardened/GNU_stack_quickstart">exploitable</a> in the context of
other vulnerabilities.</p>

<p>The problem gets worse as we move towards niche architectures and targets that are used
primarily by small hobbyist communities.
Consider <a href="https://en.wikipedia.org/wiki/Motorola_68000_series">m68k</a>
(one of the other architectures affected by <code>pyca/cryptography</code>‚Äôs move to Rust): even
GCC <a href="https://gcc.gnu.org/legacy-ml/gcc-patches/2019-10/msg02044.html">was considering</a> removing
support due to lack of maintenance, until hobbyists stepped in. That isn‚Äôt to say that any
<em>particular</em> niche target is full of bugs<sup id="fnref:although" role="doc-noteref"><a href="#fn:although">10</a></sup>; only to say that it‚Äôs a greater likelihood
for niche targets <em>in general</em>. <strong>Nobody</strong> is regularly testing the mountain of userspace
code that implicitly forms an operating contract with arbitrary programs on these platforms.</p>

<p>Project maintainers don‚Äôt want to chase down compiler bugs on ISAs or systems that they never
intended to support in the first place, and aren‚Äôt receiving any active support feedback about.
They <em>especially</em> don‚Äôt want to have vulnerabilities associated
with their projects because of buggy toolchains <em>or</em> tooling inertia when working on security
improvements.</p>

<h3 id="some-more-finger-pointing">Some more finger-pointing</h3>

<p>As someone who <em>likes</em> C: this is all C‚Äôs fault. Really.</p>

<p>Beyond language-level unsafety (plenty of people have
<a href="https://fishinabarrel.github.io/">covered that already</a>), C is <em>organizationally</em> unsafe:</p>

<ul>
  <li>
    <p>There‚Äôs no standard way to write tests for C.</p>

    <p>Functional and/or unit tests <em>alone</em> would go a long
way in assuring baseline correctness on weird architectures or platforms, but the cognitive
overhead of testing C <em>and</em> getting those tests running ensures that well-tested builds of C
programs will continue to be the exception, rather than the rule.</p>
  </li>
  <li>
    <p>There‚Äôs no standard way to build C programs.</p>

    <p><a href="https://blog.yossarian.net/2019/04/23/Make-is-probably-fine">Make is fine</a>, but it‚Äôs not standard.
Disturbingly large swathes of critical open source infrastructure are compiled using a hodgepodge
of Make, autogenerated rules from autotools, and the maintainer‚Äôs boutique shell scripts. One
consequence of this is that C builds tend to be flexible <em>to a fault</em>: prospective packagers
can inject all sorts of behavior-modifying flags that may not be attested directly
in the compiled binary or other build products. The result: it‚Äôs almost impossible to prove that
two separate builds on different machines are the same, which means more maintainer pain.</p>
  </li>
  <li>
    <p>There‚Äôs no standard way to distribute C programs.</p>

    <p>Yes, I know that package managers exist. Yes, I know how to statically link. Yes, I know how to
vendor libraries and distribute self-contained program ‚Äúbundles‚Äù. None of these are or amount to
a <em>complete</em> standard, and each introduces additional logistical or security problems.</p>
  </li>
  <li>
    <p>There‚Äôs no such thing as truly cross-platform C.</p>

    <p>The C abstract machine, despite looking a lot like a PDP-11, leaks the underlying memory
and ordering semantics of the architecture being targeted. The result is that even seasoned
C programmers regularly rely on architecture-specific assumptions when writing ostensibly
cross-platform code: assumptions about the atomicity of reads and writes, operation ordering,
coherence and visibility in self-modifying code, the safety and performance of unaligned accesses,
and so forth. Each of these, apart from being a potential source of unsafety, are <strong>impossible
to detect</strong> statically in the general case: they are, after all, perfectly correct
(and frequently intended!) on the programmer‚Äôs host architecture.</p>
  </li>
</ul>

<p>By contemporary programming language standards, these are conspicuous gaps in functionality:
we‚Äôve long since learned to bake testing, building, distribution, and sound abstract machine
semantics into the standard tooling for languages (and language design itself). But their absence
is <strong>doubly pernicious</strong>: they ensure that C remains a perpetually
unsafe development ecosystem, <em>and</em> an appealing target when bootstrapping a new platform.</p>

<h2 id="the-life-of-a-package-maintainer-is-hard">The life of a package maintainer is hard</h2>

<p>The project maintainer isn‚Äôt the only person hurting in the status quo.</p>

<p>Everything stated above <em>also</em> leads to a bum job for the lowly package maintainer<sup id="fnref:yt" role="doc-noteref"><a href="#fn:yt">11</a></sup>. They‚Äôre
(probably) also an unpaid open source hobbyist, and they‚Äôre operating with constraints that
the upstream isn‚Äôt likely to immediately understand:</p>

<ul>
  <li>The need to link against versions of dependencies that have already been packaged (and perhaps patched)</li>
  <li>ABI and ISA subset constraints, stemming from a need to distribute binaries that function with
relatively old versions of <code>glibc</code> or x86-64 CPUs without modern extensions</li>
  <li>Limited visibility into each project‚Äôs test suite and how to run it, much less what to do when
it fails</li>
</ul>

<p>They <em>also</em> have to deal with users who are unsympathetic to those reports, and who:</p>

<ul>
  <li>Rarely submit reports to the packager (they bug the project directly instead!), or ‚Ä¶</li></ul></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.yossarian.net/2021/02/28/Weird-architectures-werent-supported-to-begin-with">https://blog.yossarian.net/2021/02/28/Weird-architectures-werent-supported-to-begin-with</a></em></p>]]>
            </description>
            <link>https://blog.yossarian.net/2021/02/28/Weird-architectures-werent-supported-to-begin-with</link>
            <guid isPermaLink="false">hacker-news-small-sites-26294397</guid>
            <pubDate>Sun, 28 Feb 2021 16:22:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hunting two PDP-1 photos (which are not what they seem)]]>
            </title>
            <description>
<![CDATA[
Score 93 | Comments 30 (<a href="https://news.ycombinator.com/item?id=26292781">thread link</a>) | @masswerk
<br/>
February 28, 2021 | https://www.masswerk.at/nowgobang/2021/train-spotting-1 | <a href="https://web.archive.org/web/*/https://www.masswerk.at/nowgobang/2021/train-spotting-1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.masswerk.at/nowgobang/2021/train-spotting-1</link>
            <guid isPermaLink="false">hacker-news-small-sites-26292781</guid>
            <pubDate>Sun, 28 Feb 2021 12:02:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Actually Portable Executables]]>
            </title>
            <description>
<![CDATA[
Score 649 | Comments 143 (<a href="https://news.ycombinator.com/item?id=26292166">thread link</a>) | @krab
<br/>
February 28, 2021 | https://ahgamut.github.io/c/2021/02/27/ape-cosmo/ | <a href="https://web.archive.org/web/*/https://ahgamut.github.io/c/2021/02/27/ape-cosmo/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <div>  <p><span>27 Feb 2021</span></p><p>I came across <a href="https://github.com/jart/cosmopolitan">Cosmopolitan</a> on Hacker News, and I was initially confused, due to a few memories of cross-compilation nightmares: while it should be possible to compile for the same architecture regardless of operating system, wouldn‚Äôt the OS get confused by the leading bytes of the executable? I read the <a href="https://justine.lol/ape.html">article</a> explaining how it works, but most of it went over my head.</p> <p>The example on the <a href="https://github.com/jart/cosmopolitan">Github README</a> used the following script for compilation:</p> <div><div><pre><code>gcc <span>-g</span> <span>-O</span> <span>-static</span> <span>-nostdlib</span> <span>-nostdinc</span> <span>-fno-pie</span> <span>-no-pie</span> <span>-mno-red-zone</span> <span>\</span>
  <span>-o</span> hello.com.dbg hello.c <span>-fuse-ld</span><span>=</span>bfd <span>-Wl</span>,-T,ape.lds <span>\</span>
  <span>-include</span> cosmopolitan.h crt.o ape.o cosmopolitan.a
objcopy <span>-S</span> <span>-O</span> binary hello.com.dbg hello.com
</code></pre></div></div> <p>I converted it into a simple Makefile to run the compilation commands. I tried a bunch of simple C programs (basic arithmetic, reading and writing to files) on Linux+Windows (compiled on Linux), and all of them worked.</p> <h2 id="compiling-lua-with-cosmopolitan">Compiling Lua with Cosmopolitan</h2> <p>I decided to try compiling a high-level language built on C. I originally picked Python, but the Makefile for Python seemed too complicated to mess with, so I then picked <a href="https://www.lua.org/download.html">Lua</a>, which looked much simpler in comparison.</p> <p>I started out by blindly copy-pasting the flags and includes used in the sample compilation on Github. Ah, it would have been wonderful for my laziness if it compiled out of the box. Following is a play-by-play commentary of trying to compile Lua.</p> <p>First problem I ran into was header clashes: if I didn‚Äôt put <code>-nostdlib -nostdinc</code> while compiling each object file, <code>-include cosmopolitan.h</code> would clash with the system headers. But blocking the system headers meant I would have to change every <code>#include</code> of a system header. I created a bunch of dummy headers with the same names as those in the <a href="https://en.cppreference.com/w/c/header">C stdlib</a> and and included to those instead.</p> <p>Naming clashes: some of the macros in <code>cosmopolitan.h</code> clashed with macro/function names in Lua: <code>reverse</code> and <code>isempty</code>. I changed the Lua source to avoid this.</p> <p>A macro <code>FIRST_RESERVED</code> was broken because <code>UCHAR_MAX</code> was missing. I thought <code>UCHAR_MAX</code> was supposed to be in <code>limits.h</code> ‚Äì the <code>limits.h</code> part of <code>cosmopolitan.h</code> did not have <code>UCHAR_MAX</code> (It had <code>SCHAR_MAX</code>, though.) I added in a <code>#define</code> stating <code>UCHAR_MAX</code> as <code>__UINT8_MAX__</code> (ie 255).</p> <p>The default Lua Makefile attempts to use <code>_setjmp</code>/<code>_longjmp</code> in <code>ldo.c</code> when on Linux. I disabled the <code>LUA_USE_LINUX</code> flag for compiling the object files, but this caused an issue with <code>tmpnam</code> in <code>loslib.c</code> (<code>mkstemp</code> is available in Cosmopolitan). I changed the Lua source to use <code>setjmp</code>/<code>longjmp</code>. A similar issue showed in <code>lauxlib.c</code> for <code>sys/wait.h</code> (which is a no-op in non-POSIX systems, as per the Lua source code), and in <code>liolib</code> for <code>sys/types.h</code> so disabled <code>LUA_USE_POSIX</code> over there as well.</p> <p>The <code>localeconv()</code> function (part of <code>locale.h</code>) was not implemented in <code>cosmopolitan.h</code>, and this caused an error while compiling <code>lobject.c</code> (macro <code>lua_getlocaledecpoint()</code> depended on <code>localeconv()</code>). Changed the macro to just return <code>'.'</code>.</p> <p>The <code>panic</code> function in Lua <code>static int panic (lua_state*)</code> clashed with that in Cosmopolitan <code>void panic(void)</code>. Renamed the lua function to <code>lua_panic</code>. This triggered an error where the <code>panic</code> function was being called in <code>luaL_newstate</code>, so I changed the name there as well.</p> <p><code>luaL_loadfilex</code> caused a <em>frame size error</em> ‚Äì I have never seen this before. A quick internet search shows that this is because a large buffer is allocated on stack when entering the function, and yes, <code>luaL_loadfilex</code> allocates a <code>loadF</code> object containing a <code>char</code> buffer of <code>BUFSIZ</code>. I reduced the size of the buffer to <code>BUFSIZ - 64</code>.</p> <p><code>loslib.c</code> reuiqres the <code>setlocale()</code> and <code>LC_*</code> from <code>locale.h</code>, which is defined as an extern value in <code>cosmopolitan.h</code>, but that definition is somehow not enough.. screw it, I just disabled <code>os_setlocale</code> in <code>loslib.c</code>, and then it compiles.</p> <h2 id="linking-the-object-files">Linking the object files</h2> <p>Ok, time for linking ‚Ä¶</p> <div><div><pre><code>gcc -std=gnu99 -o lua   lua.o liblua.a -lm -Wl,-E -ldl
/usr/bin/ld: errno: TLS definition in //lib/x86_64-linux-gnu/libc.so.6 section
.tbss mismatches non-TLS reference in liblua.a(lauxlib.o)
/usr/bin/ld: //lib/x86_64-linux-gnu/libc.so.6: error adding symbols: bad value
collect2: error: ld returned 1 exit status
</code></pre></div></div> <p>I forgot, I shouldn‚Äôt <code>-lm</code> or <code>-ldl</code>. Ok, let‚Äôs try with all the object files instead of <code>liblua.a</code>:</p> <div><div><pre><code>/usr/bin/ld.bfd: lvm.o: in function `l_strcmp':
lvm.c:(.text+0x59): undefined reference to `strcoll'
/usr/bin/ld.bfd: lmathlib.o: in function `math_tanh':
lmathlib.c:(.text+0x21f): undefined reference to `tanh'
/usr/bin/ld.bfd: lmathlib.o: in function `math_sinh':
lmathlib.c:(.text+0x24f): undefined reference to `sinh'
/usr/bin/ld.bfd: lmathlib.o: in function `math_cosh':
lmathlib.c:(.text+0x27f): undefined reference to `cosh'
collect2: error: ld returned 1 exit status
</code></pre></div></div> <p>Umm‚Ä¶ okay, it looks like some of the functions defined in the cosmopolitan header are yet to be implemented in the static library. That‚Äôs okay, I can just quickly fill in the math functions, and I‚Äôll comment out <code>strcoll</code> for now, just because I want to see it compile‚Ä¶. and it successfully compiles!! Let‚Äôs run <code>objcopy</code> before trying it out on a system though.</p> <div><div><pre><code>$ objcopy -S -O binary lua lua.exe
$ ls -al
-rwxr-xr-x 1 1953720 Feb 27 01:33 lua
-rwxr-xr-x 1 344064 Feb 27 01:39 lua.exe
</code></pre></div></div> <p>That size reduction seems a little too drastic, but let‚Äôs see if it runs on Linux:</p> <p><img src="https://ahgamut.github.io/assets/images/linux_screen.png" alt=""></p> <p>Awesome. How about Windows?</p> <p><img src="https://ahgamut.github.io/assets/images/windows_screen.png" alt=""></p> <h2 id="summary-it-is-actually-portable">Summary: it <em>is</em> actually portable</h2> <p>This is pretty incredible: I just had to modify a few lines in a Makefile and some C source files, and I got a Lua executable that works both on Linux and Windows (and possibly others as well). Granted, there are still some details to be filled out (floating point calculation above prints a <code>g</code>), but Cosmopolitan is currently at release 0.0.2, so there is a lot of time.</p> <p>Hopefully this means that other languages that have source code completely in C can also be compiled once and run anywhere. Actually Portable Python next, maybe?</p> </div> </div></div>]]>
            </description>
            <link>https://ahgamut.github.io/c/2021/02/27/ape-cosmo/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26292166</guid>
            <pubDate>Sun, 28 Feb 2021 10:06:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Take a look at Nomad before jumping on Kubernetes]]>
            </title>
            <description>
<![CDATA[
Score 158 | Comments 76 (<a href="https://news.ycombinator.com/item?id=26291975">thread link</a>) | @sofixa
<br/>
February 28, 2021 | https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/ | <a href="https://web.archive.org/web/*/https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
        
        <h2>Table of Contents</h2>
        
         
      
      <h2 id="pre-introduction">Pre-introduction</h2>
<p>Recently I stumbled upon and then stumbled upon again on <a href="https://blog.dave.tf/post/new-kubernetes/">David Anderson</a>‚Äôs interesting post about ‚Äúnew Kubernetes‚Äù, based on a discussion he had with <a href="https://timewitch.net/">Vallery Lancey</a> about what they would do differently if they were rewriting Kubernetes from scratch. Interestingly, a decent part of the proposals for a ‚Äúnew Kubernetes‚Äù are design choices made by Hashicorp for <a href="https://www.nomadproject.io/">Nomad</a>, which is a pretty underrated orchestrator, and drastically simpler ( one of the main goals of said ‚Äúnew Kubernetes‚Äù).</p>
<p>Some people are aware that Docker Swarm kinda exists but is abandonware/on life support, and isn‚Äôt really recommended anymore, but it still comes up in discussions due to how easy it is to use. For most, that leaves Kubernetes as the only ‚Äúserious‚Äù option, but it is a <em>very</em> complex piece of software, with a lot of moving parts, which isn‚Äôt actually required or need in most cases.</p>

  <figure>
    <img src="https://atodorov.me/img/nomad/kubernetes.jpg#center">
    
  </figure>


<p>This inspired me to write a series on Nomad, what it is, why it‚Äôs great, where it‚Äôs lacking and how to use it.</p>
<h2 id="introduction---what-is-nomad-and-why-its-great">Introduction - what is Nomad and why it‚Äôs great</h2>
<p>Hashicorp‚Äôs Nomad is a simple to run and maintain, yet very flexible task scheduler/orchestrator. It relies on plugins for execution, autoscaling and other features, and can run pretty much anything via its <code>task drivers</code> - Docker, contairnerd, LXC, rkt, podman, Java, fork/exec, QEMU, firecracker, FreeBSD jails.</p>
<p>It comes in the form of a single binary, run in two modes (<code>server</code>, in groups of 3 or 5, which make scheduling decisions and host the APIs and configuration, and an unlimited number of <code>worker</code>s which actually run whatever it is you want to run), and can be automatically clustered via <a href="https://consul.io/">Consul</a>. The configuration ( both for jobs and of Nomad itself) is in <a href="https://github.com/hashicorp/hcl">HCL</a> (I‚Äôll get into more detail about how great that is a bit later) or JSON (mainly for when the jobs are submitted by machines/scripts/tooling and not humans). Multiple clusters can be connected via <a href="https://learn.hashicorp.com/tutorials/nomad/federation?in=nomad/manage-clusters">multi-region federation</a> for sharing ACLs and for API forwarding ( you can submit a job or request logs to any server for any region and it will be forwarded to the appropriate server). Deployments can be complex out of the box ( rolling, canary, blue/green), and everything is version controlled and rollbackable.</p>
<p>Like most HashiCorp tools, it‚Äôs ‚Äúopen core‚Äù, meaning that the majority of features are available in an <a href="https://github.com/hashicorp/nomad">open source</a> version, and some more advanced/enterprise-y ones ( in Nomad‚Äôs case, <a href="https://www.hashicorp.com/blog/hashicorp-nomad-multi-cluster-deployment">multi-region/cluster deployments</a> - deploying something simultaneously to multiple separate clusters, policy as code with <a href="https://docs.hashicorp.com/sentinel/nomad">Sentinel</a> and similar ) require upgrading to Nomad Enterprise.</p>
<h2 id="primitives">Primitives</h2>
<ul>
<li><code>job</code> is a declarative file which contains groups of tasks, each task being a container/binary/anything run by an exec driver</li>
<li><code>system</code> jobs (run on all client nodes, equivalent to Kubernetes DaemonSets, for monitoring/logging agents/load balancers)</li>
<li><code>periodic</code> jobs (equivalent to cronjobs)</li>
<li><code>service</code>, which registers as a Consul service and is thus discoverable ( via API or DNS)</li>
<li><code>deployment</code>, each version of a job, they‚Äôre tracked and can be rollbacked to</li>
<li><code>allocation</code>, each instance of a task ( group ) on a node</li>
<li><code>namespace</code>, a logical unit to organise jobs in and ACLs around</li>
</ul>
<h3 id="jobs">Jobs</h3>
<p>Example of a very basic job that runs a Docker container (<code>jaegertracing/all-in-one:1.21</code>), with limits of 1000Mhz of CPU and 1024MB of RAM, and registers the service with Consul:</p>
<div><pre><code data-lang="hcl"><span>job</span> <span>"jaeger"</span> {
        type <span>=</span> <span>"service"</span>
        <span>group</span> <span>"api"</span> {
            <span>task</span> <span>"jaeger"</span> {
                driver <span>=</span> <span>"docker"</span>
                <span>config</span> { 
                  image <span>=</span> <span>"jaegertracing/all-in-one:1.21"</span>
                }
                <span>resources</span> {
                  cpu <span>=</span> <span>1000</span>
                  memory <span>=</span> <span>1024</span>
                }
                <span>service</span> {
                  name <span>=</span> <span>"jaeger-query"</span>
                }
            }
        }            
}
</code></pre></div><p>Note that this is a <em>very</em> basic job, there are no healthchecks, no persistent storage, no extra configuration, no update strategy, no autoscaling, no exposed ports.</p>
<h4 id="deployment-history-and-rollback">Deployment history and rollback</h4>
<p>Nomad tracks each job‚Äôs full definitions and deployment history, and allows you to easily rollback and compare them, via the UI, CLI or API, e.g.:</p>
<div><pre><code data-lang="bash"><span># List the versions of the job named "opentelemetry-collector"</span>
$ nomad job history opentelemetry-collector
Version     <span>=</span> <span>1</span>
Stable      <span>=</span> false
Submit Date <span>=</span> 2021-01-08T21:30:30+01:00

Version     <span>=</span> <span>0</span>
Stable      <span>=</span> true
Submit Date <span>=</span> 2021-01-08T21:29:48+01:00

<span># Check the difference between versions</span>
$ nomad job history -p opentelemetry-collector
Version     <span>=</span> <span>1</span>
Stable      <span>=</span> false
Submit Date <span>=</span> 2021-01-08T21:30:30+01:00
Diff        <span>=</span>
+/- Job: <span>"opentelemetry-collector"</span>
+/- Task Group: <span>"opentelemetry-collector"</span>
  +/- Task: <span>"opentelemetry-collector"</span>
    +/- Config <span>{</span>
          args<span>[</span>0<span>]</span>:  <span>"--config=local/otel/config.yaml"</span>
      +/- image:    <span>"otel/opentelemetry-collector-contrib:0.15.0"</span> <span>=</span>&gt; <span>"otel/opentelemetry-collector-contrib:0.16.0"</span>
          ports<span>[</span>0<span>]</span>: <span>"health"</span>
          ports<span>[</span>1<span>]</span>: <span>"jaeger_thrift_compact"</span>
        <span>}</span>

Version     <span>=</span> <span>0</span>
Stable      <span>=</span> true
Submit Date <span>=</span> 2021-01-08T21:29:48+01:00

<span># Revert job "opentelemetry-collector" to version 0</span>
$ nomad job revert opentelemetry-collector <span>0</span>

</code></pre></div><h4 id="state-tracking-and-job-planning">State tracking and job planning</h4>
<p>Nomad keeps the desired state and its history, and with <code>nomad job plan</code>, similar to <code>terraform plan</code>, allows us to preview what will change upon applying a new job file. There‚Äôs also a feature to verify nothing has changed between the <code>plan</code> and <code>run</code> (equivalent to <code>terraform apply</code> with a plan file) with the <code>-check-index</code> flag:</p>
<div><pre><code data-lang="bash">$ nomad job plan otel.nomad
+/- Job: <span>"otel"</span>
+/- Task Group: <span>"opentelemetry"</span> <span>(</span><span>1</span> create/destroy update<span>)</span>
  +/- Task: <span>"opentelemetry-collector"</span> <span>(</span>forces create/destroy update<span>)</span>
    +/- Config <span>{</span>
          args<span>[</span>0<span>]</span>:  <span>"--config=local/otel/config.yaml"</span>
      +/- image:    <span>"otel/opentelemetry-collector-contrib:0.15.0"</span> <span>=</span>&gt; <span>"otel/opentelemetry-collector-contrib:0.20.0"</span>
          ports<span>[</span>0<span>]</span>: <span>"health"</span>
          ports<span>[</span>1<span>]</span>: <span>"jaeger_thrift_compact"</span>
        <span>}</span>
Scheduler dry-run:
- All tasks successfully allocated.

Job Modify Index: <span>413</span>
To submit the job with version verification run:

nomad job run -check-index <span>413</span> otel.nomad

When running the job with the check-index flag, the job will only be run <span>if</span> the
job modify index given matches the server-side version. If the index has
changed, another user has modified the job and the plan<span>'</span>s results are
potentially invalid.
</code></pre></div><p>Overall, it‚Äôs a very useful feature, especially when collaborating, locally or via CI/CD.</p>
<h4 id="checking-the-status-and-logs">Checking the status and logs</h4>
<p>To check the status of a job, there are a few commands under <code>nomad job</code> and <code>nomad alloc</code></p>
<div><pre><code data-lang="bash">$ nomad job status otel
ID            <span>=</span> otel
Name          <span>=</span> otel
Submit Date   <span>=</span> 2021-02-27T20:41:29+01:00
Type          <span>=</span> service
Priority      <span>=</span> <span>50</span>
Datacenters   <span>=</span> dc1
Namespace     <span>=</span> default
Status        <span>=</span> running
Periodic      <span>=</span> false
Parameterized <span>=</span> false

Summary
Task Group  Queued  Starting  Running  Failed  Complete  Lost
otel      <span>0</span>       <span>0</span>         <span>1</span>        <span>0</span>       <span>0</span>         <span>0</span>

Latest Deployment
ID          <span>=</span> ea533b6f
Status      <span>=</span> successful
Description <span>=</span> Deployment completed successfully

Deployed
Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
otel      <span>1</span>        <span>1</span>       <span>1</span>        <span>0</span>          2021-02-27T20:51:45+01:00

Allocations
ID        Node ID   Task Group  Version  Desired  Status   Created  Modified
89031cfd  d3cbeb7e  otel      <span>0</span>        run      running  20s ago  4s ago

<span># logs are at the allocation level ( similar to Kubernetes, where they're at the container level), so we get them with the alloc id</span>
$ nomad alloc logs 89031cfd
<span>[</span>...<span>]</span>
</code></pre></div><h4 id="lifecycle-and-sidecars">lifecycle and sidecars</h4>
<p>Nomad allows defining the lifecycle of tasks in task groups, and their status, with the <code>lifecycle</code> stanza. We can have <code>prestart</code> ( for initialisation ), <code>poststart</code> ( companion, for proxying (aka ambassador and adapter pattern in Kubernetes )) or <code>poststop</code> for clean up, and via the <code>sidecar</code> bool we define whether or not it should run as long as the main task(s), e.g.:</p>
<div><pre><code data-lang="hcl">  <span>task</span> <span>"init"</span> {
    <span>lifecycle</span> {
      hook <span>=</span> <span>"prestart"</span>
      sidecar <span>=</span> <span>false</span>
    }
    driver <span>=</span> <span>"docker"</span>
    <span>config</span> {
      image <span>=</span> <span>"alpine/httpie"</span>
      command <span>=</span> <span>"http"</span>
      args <span>=</span> [
        <span>"POST"</span>,
        <span>"https://some-internal-service-for-provisioning-stuff.local/v1/new"</span>,
        "job_id<span>=</span><span>'</span><span>${</span><span>NOMAD_JOB_ID</span><span>}</span><span>!'"</span>
      ]
    }
  }

  <span>task</span> <span>"fluentd"</span> {
    <span>lifecycle</span> {
      hook <span>=</span> <span>"poststart"</span><span> # should start after the main task
</span><span></span>      sidecar <span>=</span> <span>true</span><span> # should run as long as the main task does, and be restarted if it fails
</span><span></span>    }
    driver <span>=</span> <span>"docker"</span>
    <span>config</span> {
      image <span>=</span> <span>"fluentd/fluentd"</span>
    }
    ...
  }

  <span>task</span> <span>"main-app"</span> {
    ...
  }

  <span>task</span> <span>"cleanup"</span> {
    <span>lifecycle</span> {
      hook <span>=</span> <span>"poststop"</span>
    }
    driver <span>=</span> <span>"docker"</span>
    <span>config</span> {
      image <span>=</span> <span>"alpine"</span>
      command <span>=</span> <span>"rm -rf"</span>
      args <span>=</span> [
        <span>"/var/lib/volume-with-super-secret-data"</span>
      
    }
  }
</code></pre></div><h3 id="aclrbac">ACL/RBAC</h3>
<p>ACL ( access-control list ), or RBAC ( role-based access control ) as it‚Äôs known in Kubernetes, allow defining who can do what, so that not everyone with network access can have full administrator privileges and run/stop whatever. Nomad‚Äôs ACL system is pretty similar to Consul and Vault‚Äôs, and uses JSON ( mostly for non-humans ) or HCL to define <code>policies</code> with <code>rules</code>, which describe what action is allowed on what object.</p>
<div><pre><code data-lang="hcl"><span># a basic policy which allows the predefined read policy with read-only access to list and read:
</span><span># job, volume and scaling details, and extra capabilities for job creation and log access within the default namespace
</span><span></span><span>namespace</span> <span>"default"</span> {
  policy <span>=</span> <span>"read"</span>
  capabilities <span>=</span> [<span>"submit-job","dispatch-job","read-logs"</span>]
}
</code></pre></div><p>Assignment of policies is done only via the CLI, unlike Kubernetes where that happens in YAML, as does policy management:</p>
<div><pre><code data-lang="bash"><span># create/update the policy within Nomad</span>
nomad acl policy apply -description <span>"Application Developer policy"</span> my-policy my-policy.hcl
nomad acl token create -name<span>=</span><span>"Test token"</span> -policy<span>=</span>my-policy -type<span>=</span>client
Accessor ID  <span>=</span> ‚Ä¶</code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/">https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/</a></em></p>]]>
            </description>
            <link>https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26291975</guid>
            <pubDate>Sun, 28 Feb 2021 09:30:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How fighting games use delay-based and rollback netcode (2019)]]>
            </title>
            <description>
<![CDATA[
Score 284 | Comments 103 (<a href="https://news.ycombinator.com/item?id=26289933">thread link</a>) | @Kinrany
<br/>
February 27, 2021 | https://ki.infil.net/w02-netcode.html | <a href="https://web.archive.org/web/*/https://ki.infil.net/w02-netcode.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<div>
					<div id="content">

						<!-- Content -->
					
							<article>

								<div>
								
								<div>
								<p><a href="https://ki.infil.net/words.html">
								<img src="https://ki.infil.net/images/words/header.gif"></a>
								
								</p></div>
								
								
								
								
								<hr>
								
								<div>
								
								<div>
								<p>Netcode</p>
								<p>Explaining how fighting games use delay-based and rollback netcode</p>
								<p>October 16, 2019</p>
								
								
								
								
								</div>
								
								</div>
							
								
								<!-- blog navigation -->
								
								
								

								<div>
								<div><p>
								<em>I would like to thank <a href="https://twitter.com/Krazhier">krazhier</a> and <a href="https://twitter.com/TheKeits">Keits</a> for taking hours out of their busy schedules to discuss technical aspects of netcode with me, and <a href="https://twitter.com/Sajam">Sajam</a> for taking time to answer interview questions and being supportive throughout the writing process. I would also like to especially thank <a href="https://twitter.com/MagicMoste">MagicMoste</a> for making all the wonderful videos you see in this article. All their help was offered for free and I am thankful for their friendship.
								</em></p><p><em>

								This article has been <a href="https://arstechnica.com/gaming/2019/10/explaining-how-fighting-games-use-delay-based-and-rollback-netcode/">cross-posted on Ars Technica</a>.
								</em></p><p><em>

								You may also enjoy <a href="https://www.youtube.com/watch?v=1RI5scXYhK0">watching a video feature</a> on the topics in this article.
								</em>							
								</p></div>
								</div>

								
															
								<!-- Infil -->
								<div><p><img src="https://ki.infil.net/images/words/infil_smiling_2.jpg"></p>
								<p>
								Welcome back to Fightin‚Äô Words! It‚Äôs been a while since we last discussed how the <a href="https://ki.infil.net/w01-bugs.html">most famous fighting game bugs</a> have impacted the community‚Äôs favorite games. Today‚Äôs topic is a bit more technical, but it‚Äôs an equally important factor in how our favorite modern games are played -- we‚Äôre going to be doing a <strong>deep dive into netcode</strong>.								
								</p>
								</div>

								<hr><!-- Infil -->
								<div><p><img src="https://ki.infil.net/images/words/infil_lecture.jpg"></p>
								<div><p>
								At its core, netcode is simply a method for two or more computers, each trying to play the same game, to talk to each other over the internet. While local play always ensures that all player inputs arrive and are processed at the same time, <strong>networks are constantly unstable</strong> in ways the game cannot control or predict. Information sent to your opponent may be delayed, arrive out of order, or become lost entirely depending on dozens of factors, including the physical distance to your opponent, if you‚Äôre on a WiFi connection, and whether your roommate is watching Netflix.
								</p><p>
								Online play in games is nothing new, but fighting games have their own set of unique challenges. They tend to involve direct connections to other players, unlike many other popular game genres, and <strong>low, consistent latency</strong> is extremely important because muscle memory and reactions are at the core of virtually every fighting game. As a result, two prominent strategies have emerged for playing fighting games online: <strong>delay-based netcode</strong> and <strong>rollback netcode</strong>. 
								</p></div>
								</div>

								<div><p><img src="https://ki.infil.net/images/words/infil_thoughtful.jpg"></p>
								<div><p>
								There‚Äôs been a renewed passion in the fighting game community that <strong>rollback is the best choice</strong>, and fighting game developers who <a href="https://www.youtube.com/watch?v=qW61xJNJ9m8">choose to use delay-based netcode</a> are <a href="https://www.youtube.com/watch?v=iTUtnclr2hs">preventing the growth of the genre</a>. While people have been passionate about this topic <a href="https://www.youtube.com/watch?v=Tu2kAdmUCaI&amp;t=42m34s">for many years</a>, frustrations continue to rise as new, otherwise excellent games repeatedly have bad online experiences.
								</p><p>

								There are relatively few easy-to-follow explanations for what exactly rollback netcode is, how it works, and why it is so good at hiding the effects of bad connections (though <a href="http://mauve.mizuumi.net/2012/07/05/understanding-fighting-game-networking.html">there are some</a>). Because I feel this topic is extremely important for the future health of the fighting game community, I want to help squash some misconceptions about netcode and explain both netcode strategies thoroughly so everyone can be informed as they discuss. If you stick around to the end, I‚Äôll even <strong>interview some industry experts and community leaders</strong> on the topic!
								</p><p>
								Before we dig into the details, though, let‚Äôs get one thing straight.
										
								</p></div>
								</div>

								<hr><!-- Infil -->
								<!-- header -->
								

								<div><p><img src="https://ki.infil.net/images/words/infil_smiling_3.jpg"></p>
								<div><p>
										Both companies and players should care about good netcode because <strong>playing online is no longer the future -- it's the present</strong>. 
								</p><p>
										While most other video game genres have been this way for a decade or longer, fighting game developers seem to be resistant to embracing online play, perhaps because of the genre‚Äôs roots in offline settings such as arcades and tournaments. Playing offline is great, and it will always have considerable value in fighting games, but it‚Äôs simply a reality that <strong>a large percentage of the player base will never play offline</strong>. For many fighting game fans, playing online <em>is</em> the game, and a bad online experience prevents them from getting better, playing or recommending the game to their friends, and ultimately causes them to <a href="https://youtu.be/iTUtnclr2hs?t=758">simply go do something else</a>.
								</p><p>
										Even if you think you have a good connection, or live in an area of the world with robust internet infrastructure, good netcode is still mandatory. Plus, lost or delayed information happens regularly even on the best networks, and poor netcode can <a href="https://twitter.com/john_takeuchi/status/1162562266027327488">actively hamper matches</a> no matter how smooth the conditions may be. Good netcode also has the benefit of connecting regions across greater distances, effectively uniting the global player base as much as possible.

										<!-- gif 1A -->
										</p><div><!-- image -->
											<figure>
												
											<figcaption>Bad netcode can ruin matches. This match, played online between two Japanese players, impacted who gets to attend the Capcom Pro Tour finals. <a href="https://twitter.com/john_takeuchi/status/1162562266027327488">(source)</a>
											</figcaption></figure></div>
										
								</div>
								</div>

								<div><p><img src="https://ki.infil.net/images/words/infil_content.jpg"></p>
								<div><p>
										What about those who never play online because they much prefer playing offline with their friends? The <strong>healthy ecosystem</strong> that good netcode creates around a game benefits everyone. There will be more active players, more chances to consume content for your favorite game -- from tech videos to spectating online tournaments to expanding the strategy of lesser-used characters -- and more excitement surrounding your game in the FGC. Despite <a href="http://ki.infil.net/">Killer Instinct</a>‚Äôs pedigree as an excellent game, there‚Äôs no doubt that its superb rollback netcode has played a huge part in the sustained growth of its community.
								</p><p>

										Good netcode matters, period. So let‚Äôs talk about it.
										
								</p></div>
								</div>
								
								
								<!-- blog navigation -->
								
								
								

								</div>
								
							</article>
				
					</div>
				</div>
			</section></div>]]>
            </description>
            <link>https://ki.infil.net/w02-netcode.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26289933</guid>
            <pubDate>Sun, 28 Feb 2021 01:47:03 GMT</pubDate>
        </item>
    </channel>
</rss>
