<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 24 Oct 2020 01:14:59 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 24 Oct 2020 01:14:58 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[How Google obliterated my 4 year old Chrome extension with 24k+ users (2016)]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 38 (<a href="https://news.ycombinator.com/item?id=24855582">thread link</a>) | @Fiveplus
<br/>
October 22, 2020 | https://by.graffino.com/how-google-obliterated-my-24k-users-chrome-extension-eeb14c040a39#.13m3awp18 | <a href="https://web.archive.org/web/*/https://by.graffino.com/how-google-obliterated-my-24k-users-chrome-extension-eeb14c040a39#.13m3awp18">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><div><div><p>
              After 3 months of trying everything I could think of, I give up.
              I don‚Äôt think I will ever develop anything for the Google
              ecosystem again. I‚Äôm not angry, I‚Äôm not doing this out of spite.
              I just don‚Äôt think it is worth it to invest any amount of effort
              to build something on a platform that turned out to be so
              unreliable.
            </p>  <h2>Please scroll down to read the latest update.</h2>  <p>
              I started developing my first extension back in 2010, when Apple
              finally decided to implement proper extension support in Safari.
              Exciting times. I spent a weekend writing my first Safari
              Extension, an ad blocker called ‚ÄúCleaner Facebook‚Äù. I must
              confess it was ugly, and by today‚Äôs standards, very poorly
              written, but it did the job.
            </p> <p>
              After a few weeks while browsing Facebook using Chrome, the
              annoying ads popped up and I figured that it‚Äôll be worthwhile to
              port it over, so I did. I also made it available to everyone via
              <a href="http://apps.graffino.com/" target="_blank" title="App Development">Graffino‚Äôs apps playground.</a></p> <p>
              Months later the due to some policy changes in Chrome, I moved
              to the Chrome Web Store where I made it available for free under
              the name ‚ÄúCleaner Facebook‚Äù.
            </p> <p>
              What happened next was beyond my wildest expectations. It
              quickly gathered around 4.000 users and over the last year it
              surged to 24.000+ users. Whenever I pushed an update, the
              numbers would spike.
            </p> <p>
              I got quite a few offers to sell it, so others could push
              advertising through it. Thing is, I never wanted to make any
              money off it and I felt like I was betraying my users. So, I
              kept it and improved it constantly. It worked so well because I
              needed it to work well, I was using it daily and my users loved
              it. It got a 5 stars rating and over 85 reviews.
            </p> <blockquote>
              Google has been notified that some of your materials allegedly
              infringe upon the trademarks of others.
            </blockquote> <p>
              At the end of May this year, I received an email from Google
              telling me that my extension violated Facebook‚Äôs trademark and
              got taken down. There was no information how to solve the issue,
              no way to appeal. Google washed its hands clean and directed all
              inquiries to an automatically generated email address. This
              email address was a black hole that never responded to any of my
              emails.
            </p> <figure><img src="https://by.graffino.com/_nuxt/img/bcfd01f.png" alt="Google's notification that they removed Graffino's Ads removal app from the store."> <figcaption>
                The original infringement and taken down notice (shortened).
              </figcaption></figure> <p>
              I figured out that the name might be an issue although there
              were
              <a href="https://chrome.google.com/webstore/search/facebook?hl=en-US" target="_blank">a lot of other extensions</a>
              with the same name in the store. Even so, I changed the name of
              my extension to ‚ÄúCleaner ‚Äî for Facebook‚Äù and resubmitted it to
              the store where it got placed ‚ÄúIn review‚Äù.
            </p> <p>
              Over the next weeks I never received any email from the Chrome
              Store other than the initial take down notice. When I logged in
              again, I found my extension was still ‚ÄúTaken down‚Äù and the ‚ÄúIn
              review‚Äù flag gone.
            </p> <p>
              I went on to search for a Developer Support page, but after half
              an hour of searching I found out that there is none. There is no
              support whatsoever for the developer besides Google‚Äôs own
              documentation. If you encounter an issue that you can‚Äôt solve
              yourself, you‚Äôre stranded. There‚Äôs no contact info. No one to
              write to.
            </p> <p>
              After getting really frustrated that day, I vented out sending
              out a nasty email to removals@google.com. Not sure it helped.
            </p> <p>
              The next day I poked around and found a support form intended
              for Chrome Web Store users. In my desperation I figured this was
              my only option left.
            </p> <p>
              About a week later, I got my first reply from <em>Google</em>.
              Although it sounded nice, it wasn‚Äôt really helpful. Most
              probably due to the fact it was a canned response, totally
              unrelated to my issue.
            </p> <figure><img src="https://by.graffino.com/_nuxt/img/a38f41d.png" alt="Google's first email response to Graffino's complaint."> <figcaption>First canned response</figcaption></figure> <p>
              Later that day, I fired a few other emails, telling them that my
              extension was not under review and the cause for the take down
              was not flagging. I also submitted another message via the
              support form I used before. My requests got closed with a
              <em>‚ÄúThanks for your feedback.‚Äù</em> canned response.
            </p> <figure><img src="https://by.graffino.com/_nuxt/img/c1a157e.png" alt="Google's first email response to Graffino's complaint."> <figcaption>‚ÄúThank you for your feedback‚Äù response</figcaption></figure> <p>
              Meanwhile my extension‚Äôs user base was dwindling, and I could do
              absolutely nothing about it.
            </p> <figure><img src="https://by.graffino.com/_nuxt/img/ee3fd6a.png" alt="Google's first email response to Graffino's complaint."> <figcaption>
                Screenshot from the Developer Dashboard made on the 18th of
                July.
              </figcaption></figure> <p>
              After another two weeks I finally gave up and sent this final
              email. I got a reply that my case was ‚ÄúReopened‚Äù. It‚Äôs been a
              month and I didn‚Äôt hear back since, so I don‚Äôt think I will ever
              get my extension back on the store.
            </p> <figure><img src="https://by.graffino.com/_nuxt/img/3e2eaf4.png" alt="Google's first email response to Graffino's complaint."> <figcaption>Final email from my side.</figcaption></figure> <p>
              I‚Äôm not playing the victim here, and I don‚Äôt expect this post to
              solve anything for my extension. I just want you to think twice
              before creating a revenue stream based on the Chrome Web Store.
            </p> <p>
              Google has great automated tools and services, to help
              developers ramp up and deliver their apps to users quickly and
              painless. Most of the time these tools work perfectly. The
              problems arise when they don‚Äôt.
            </p> <p>
              I have never experienced such frustration, and such feelings of
              helplessness from any other major player out there. A similar
              issue with the
              <a href="https://safari-extensions.apple.com/details/?id=com.graffino.cleaner-for-facebook-86755BRK69" target="_blank">‚ÄúCleaner Facebook‚Äù Safari extension‚Äù</a>, got promptly solved by Apple in under three days.
            </p> <p>
              Google has a record of very bad user support. Basically, if your
              issue is not listed in the ‚ÄúSupport Forums‚Äù you‚Äôre screwed. If I
              would have based my business on the Chrome Web store, I would be
              out of business by now.
            </p>  <h2>Update 8.09.2016</h2> <p>
              I never imagined this would generate this amount of attention:
              featured by
              <strong>Medium, 100k+</strong> views, heated discussions on
              <a href="https://www.reddit.com/r/programming/comments/51mgix/how_google_obliterated_my_4_year_old_chrome/" target="_blank">Reddit</a>
              or
              <a href="https://news.ycombinator.com/item?id=12442048" target="_blank">Hackernews</a>. I don‚Äôt really expect this to change anything for me, but I‚Äôm
              glad people are speaking up. If Google would implement a paid
              support option like Apple does for its developers, I‚Äôm sure a
              lot of pain would be avoided.
            </p> <p>
              I try to read everything, but I don‚Äôt respond on Medium (hate
              the commenting system), but you can ping me on Twitter.
            </p> <h2>To answer the most common questions:</h2> <p>1.<em> ‚ÄúWhy don‚Äôt you change the name completely?‚Äù</em></p> <p>
              Well, because using ‚Äúfor Facebook‚Äù in the name is considered
              <a href="http://www.inta.org/TrademarkBasics/FactSheets/Pages/FairUse.aspx" target="_blank">fair use</a>. Also, when managing an app or extension with that many users,
              you don‚Äôt really want to confuse the users by renaming it to a
              completely different thing over night. It‚Äôs also an extension
              that only works on Facebook. I might also be infringing with the
              icon.
            </p> <p>
              So, you see, I really need guidelines to do this. Again, my
              problem is with the way this was handled, not the takedown
              itself.
            </p> <p>
              2.
              <em>‚ÄúThe extension was taken down because it was blocking ads.‚Äù
              </em></p> <p>
              This is simply untrue. I don't believe I was intentionally
              targeted. I was simply unlucky enough to be targeted by
              automated tools. Even the way my requests were handled weren‚Äôt
              done with malicious intent by Google. I think I‚Äôm just an edge
              case for which their support tools aren‚Äôt equipped to deal with.
            </p> <h2>Update: 9.09.2016</h2> <p>
              I received an email from Google today in which they‚Äôre
              expressing their apologies about the incident. It seems that
              they cannot do anything for the current store entry until the
              original complainant retracts his complain.
            </p> <figure><img src="https://by.graffino.com/_nuxt/img/3e2eaf4.png" alt="Google's first email response to Graffino's complaint."> <figcaption>Final email from my side.</figcaption></figure> <p>
              They offered me clear options though. We‚Äôre currently assessing
              what the best option might be. As I said in the article, this
              wasn‚Äôt an intentional targeting but a slip-up of their support
              which they‚Äôre trying to fix.
            </p> <p>
              Hopefully I will have the time to write a follow-up once all
              this is over. I didn‚Äôt expect or wanted this kind of attention,
              these have been 2 stressful days for me, trying to keep up with
              all the requests and messages, as I do have a job and my own
              company to run.
            </p> <p>
              It also happens that today is my birthday. I guess receiving a
              response from Google was my birthday present :).
            </p>  <h2>Update: 10.09.2016</h2> <p>
              You can still find the
              <a href="http://apps.graffino.com/" target="_blank">ad blocker extension</a>
              here. However, you will not be able to run it. Chrome made
              changes so that extensions cannot be side-loaded without an
              enterprise profile.
            </p> <h2>Final Update: 12.09.2016</h2> <p>
         ‚Ä¶</p></div></div></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://by.graffino.com/how-google-obliterated-my-24k-users-chrome-extension-eeb14c040a39#.13m3awp18">https://by.graffino.com/how-google-obliterated-my-24k-users-chrome-extension-eeb14c040a39#.13m3awp18</a></em></p>]]>
            </description>
            <link>https://by.graffino.com/how-google-obliterated-my-24k-users-chrome-extension-eeb14c040a39#.13m3awp18</link>
            <guid isPermaLink="false">hacker-news-small-sites-24855582</guid>
            <pubDate>Thu, 22 Oct 2020 07:10:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Build a startup financial model from scratch]]>
            </title>
            <description>
<![CDATA[
Score 162 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24853787">thread link</a>) | @aaronbski
<br/>
October 21, 2020 | https://www.mathventurepartners.com/blog/2016/9/15/startup-financial-modeling-part-1-what-is-a-financial-model | <a href="https://web.archive.org/web/*/https://www.mathventurepartners.com/blog/2016/9/15/startup-financial-modeling-part-1-what-is-a-financial-model">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
  <article id="article-57daf9aad2b857a2108683be" data-item-id="57daf9aad2b857a2108683be">

    

    <div>
      
        <div data-layout-label="Post Body" data-type="item" data-updated-on="1473969767493" id="item-57daf9aad2b857a2108683be"><div><div><div data-block-type="2" id="block-7c825fcce3d916c3aa66"><div><p><em>This article was originally published over on Startup Rocket&nbsp;</em><a href="https://www.startuprocket.com/articles/startup-financial-modeling-part-1-what-is-a-financial-model" target="_blank"><em>here</em></a><em>, and written by Will Little and Troy Henikoff.</em></p><p><em>This series is the result of a friendly debate I had recently with&nbsp;</em><a href="https://www.mathventurepartners.com/troy-henikoff" target="_blank">Troy Henikoff</a><em>&nbsp;(former Techstars Chicago Accelerator Managing Director)&nbsp;regarding the best approach for founders to take when building a financial model. More accurately, the ‚Äúdebate‚Äù was a strong adverse reaction from Troy after I shared a template I built for Prota Ventures‚Äô portfolio companies. His feedback was, essentially, to never use a template and instead build each model from scratch.</em></p><p><em>He invited me to a 90-minute lecture he gave where he overwhelmingly convinced me and the room that, indeed, founders need to take the time necessary to build their models from scratch. After I asked him where I could find his lecture material online, he suggested we co-author this article series since there weren‚Äôt many solid resources available. We sincerely hope you find this series helpful.&nbsp;</em></p><p>Our plan is to break this out into a four-part series and guide you through the components necessary for building your own financial model from scratch:</p><ul data-rte-list="default"><li><p><a href="https://www.mathventurepartners.com/blog/2016/9/15/startup-financial-modeling-part-1-what-is-a-financial-model" target="_blank">Part 1: The Why and What of Financial Modeling</a></p></li><li><p><a href="https://www.mathventurepartners.com/blog/2016/10/2/startup-financial-modeling-part-2-start-with-your-assumptions" target="_blank">Part 2: Assumptions</a></p></li><li><p><a href="https://www.mathventurepartners.com/blog/2016/10/2/startup-financial-modeling-part-3-the-income-statement-and-custom-detail-tabs" target="_blank">Part 3: Income Statement and Custom Detail Tabs</a></p></li><li><p><a href="https://www.mathventurepartners.com/blog/2016/10/7/startup-financial-modeling-part-4-the-balance-sheet-cash-flow-and-unit-economics" target="_blank">Part 4: Cash Flow, Balance Sheet and Keeping the Model Updated</a></p></li></ul><p>In short, a financial model is an abstract mathematical representation of how a company works (and more importantly, how it will work going forward). The model has inputs and outputs. The inputs are the assumptions that drive the model, things like what drives your customer acquisition cost, what your churn rates are, how much you pay people, etc. The outputs are a set of projections that show how the company will perform if the assumptions are true. One model can produce multiple sets of projections given different assumptions.</p><p>Based on a set of assumptions, a financial model is used to make smart decisions (e.g. how many sales people to hire and what to pay them). The model includes financial projections that are tied mathematically to the assumptions, which allows operators to ‚Äúplay with the variables‚Äù in order to understand how certain decisions might affect the future health of their company.</p><p><strong>Troy has an important story to share on this topic:</strong></p><p>‚ÄúWhen fundraising for SurePayroll, we had some very high level financials in the pitch deck. Inevitably, VC‚Äôs would ask where the numbers came from. I would tell them that we had a very detailed financial model that drove it, I was setting the bait‚Ä¶</p><p>They would ask to be sent a copy of the model and I would refuse. I would only share it by first sitting down with them and an associate and reviewing the model in person and after that 90 minute session, I would leave them a copy of the model to play with further.</p><blockquote><p>They would insist that they could figure it out without the meeting, but I ALWAYS held my ground. I wanted the meeting not just to save them time and frustration learning a new model, but more importantly to get more face time with them in a situation where I was going to shine.</p></blockquote><p>I knew the model inside and out since I built it; I could answer any question about any cell and look like a genius. In the end, I did eight&nbsp;of these meetings and EVERY ONE of the firms that did the 90 minute meeting with me on the financial model either made an investment in the company or made an offer to invest in the company.&nbsp;<strong>Every single one</strong>.‚Äù</p><p>While it‚Äôs easy to search around and find a template to use, those templates were built by someone with a particular business in mind. Since every business is unique, this will lead you into trouble.</p><p>While it‚Äôs often helpful to&nbsp;<em>learn&nbsp;</em>from other people‚Äôs models to ensure, for example, that you aren‚Äôt missing anything important, you should never build your model using their template. You‚Äôll end up banging your head against a wall when you need to change things, and you‚Äôll inevitably be confused about some nuance that will come back to haunt you since you don‚Äôt understand it.</p><blockquote><p>In other words, while you may think that a template will help you save time, what you are actually doing is acquiring ‚Äútechnical debt‚Äù that will end up costing you more time in the long run.</p></blockquote><p>Plus, it‚Äôs critical to understand every column, row, cell and tab in your spreadsheet for two key reasons; it will help you better manage your business, and when the time comes to explain it to an investor, you‚Äôll be able to explain exactly how it works and increase your odds of landing funding.</p><p>Since most people are using the financial model to communicate projections to investors, it is critical that you speak the investors‚Äô language. They are used to having financials in Excel, so you should build your model in Excel.</p><blockquote><p>Google sheets is convenient for making changes and having multiple people editing, but sending an investor a model in Google sheets signals that you are not financially savvy.</p></blockquote><p>Investors are also used to seeing three standard statements; an income statement, a balance sheet, and a statement of cash flow. &nbsp;Each of these is more credible if it has BOTH the past performance and the future projections in the same spreadsheet.</p><p>Your spreadsheet should contain a tab for each of these outputs along with an ‚Äúassumptions‚Äù tab and custom detail tabs needed to help calculate the main outputs. We‚Äôll walk through a specific example later in this series so you have a better understanding of what this should look like.</p><p>Because of various accounting nuances ‚Äì such as fixed asset depreciation and deferring revenue ‚Äì if you assign ten accountants to finish your books at the end of the year, you‚Äôll get ten different answers for how much profit (or loss) you had in the year. While hopefully not far off from the others, each will have a slightly different report of your ‚Äúprofit‚Äù based on their accounting opinions.</p><blockquote><p>However, the balance of your bank account is a specific number to point at; it‚Äôs a fact that your ten accountants should agree on.</p></blockquote><p>Therefore, it‚Äôs important to remember that your financial model will have your own opinions baked in regarding your profit. This means that examining your cash flow carefully as you fine-tune your business assumptions is critical.</p><p>Having a solid financial model is a significant step in communicating to investors that you are a logical thinker with a defensible plan and clearly understand your business and the levers that drive it. &nbsp;&nbsp;</p><p>Nobody expects your model to be perfect, as a matter of fact, when we present a model, we always open with the same line:</p><blockquote><p><em>‚ÄúThe only thing we know for sure about this model is that it is wrong. But, if we look critically at it we can better understand the drivers of the business and what we need to be focused on to reduce our risk.‚Äù &nbsp;</em></p></blockquote><p>Keep in mind, investors are looking for the big home runs, but they are also looking at reducing their risk. The model can help them get comfortable with the risk.</p><p>‚Äì ‚Äì ‚Äì</p><p><em>In our next post in this series, we‚Äôll dive in a step-by-step guide of how to build a financial model, starting with the assumptions tab.&nbsp;</em><a href="https://satchel.works/@wclittle/subscribe" target="_blank">Subscribe</a><em>&nbsp;to Will‚Äôs newsletter to get notified when the next articles are up. As we mentioned above, feel free to ping us on Twitter (</em><a href="https://twitter.com/wclittle" target="_blank"><em>@wclittle</em></a><em>,&nbsp;</em><a href="https://twitter.com/troyhenikoff" target="_blank"><em>@troyhenikoff</em></a><em>) with any questions.</em></p></div></div></div></div></div>
      
    </div>


    

    
      
    

    

  </article>

  

  

  
  
  

</div></div>]]>
            </description>
            <link>https://www.mathventurepartners.com/blog/2016/9/15/startup-financial-modeling-part-1-what-is-a-financial-model</link>
            <guid isPermaLink="false">hacker-news-small-sites-24853787</guid>
            <pubDate>Thu, 22 Oct 2020 00:28:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Notes on My Colon Cancer]]>
            </title>
            <description>
<![CDATA[
Score 336 | Comments 182 (<a href="https://news.ycombinator.com/item?id=24853503">thread link</a>) | @whatrocks
<br/>
October 21, 2020 | https://www.charlieharrington.com/colon-cancer | <a href="https://web.archive.org/web/*/https://www.charlieharrington.com/colon-cancer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>The giant robot looks like a WED Treadwell, <a href="https://www.charlieharrington.com/robots-i-love">my favorite robot</a> of all the Star Wars droids. I admit, I was worried that it would look organic, like a Sentinel from The Matrix, with wriggling Dr. Octopus arms and pinchy pincers that pinch. But I'm calmed by the robot's EVE-like exterior.</p>
<p>The room is sterile. A dozen masked, gloved attendants in blue buzz. I imagine I'm an astronaut about to step into the rocketship capsule.</p>
<p>Except I won't be going anywhere on this particular journey, unless something goes very, very wrong. In fact, I've already been asked repeatedly by various staffers to describe what I'm expecting to happen in this room over the next few hours:</p>
<blockquote>
<p>"I'm here to remove my sigmoid colon via robotic surgery because of the cancerous tumor inside."</p>
</blockquote>
<p>I'm 34 years old. It's October 12th, 2020. Five weeks ago I was diagnosed with colon cancer.</p>
<h2>Stool, bloody stool</h2>
<p>I've always been a standing wiper. Not sure entirely why. I must have once, accidentally, touched a load of poo during a seated wipe. That sort of thing can change a person.</p>
<p>This charming anecdote does factor into our story, because it means I've always had a pretty good sense for my poo. Consistency, quality, and color, both in the bowl and on the TP. Did you know, there's even a seven-stage scientific classification system for your poo, called the <a href="https://en.wikipedia.org/wiki/Bristol_stool_scale">Bristol stool scale</a>?!</p>
<p><span>
      <span></span>
  <img alt="Bristol stool scale" title="Bristol stool scale" src="https://www.charlieharrington.com/static/aa89064dc77b216479fbe409ba4b8653/a6d36/bristol.png" srcset="https://www.charlieharrington.com/static/aa89064dc77b216479fbe409ba4b8653/222b7/bristol.png 163w,
https://www.charlieharrington.com/static/aa89064dc77b216479fbe409ba4b8653/ff46a/bristol.png 325w,
https://www.charlieharrington.com/static/aa89064dc77b216479fbe409ba4b8653/a6d36/bristol.png 650w,
https://www.charlieharrington.com/static/aa89064dc77b216479fbe409ba4b8653/be86f/bristol.png 662w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy">
    </span></p>
<p>I first noticed blood two or three years ago. On a monthly or so cadence, I'd wipe and notice a reddish tinge. Not bright red, more like muddy-red. Poopy-red. Initially, I thought little of it. Just a minor curiousity. It certainly didn't happen every time. Still, I decided to check off the <code>Blood in stool</code> box on the forms at my annual physical with my primary care doctor that year.</p>
<p>A brief aside on the phrase "your primary care doctor." Like in <em>Forgetting Sarah Marshall</em>, the last doctor I really thought of as "my doctor" was my pediatrician. Since "becoming an adult", I've lived in three cities in two countries, which means that I've generally had no idea who my primary care doctor is or was, only that I'd need to find one to give me a referral to get this wart on my foot removed.</p>
<p><span>
      <span></span>
  <img alt="Firetruck" title="Firetruck" src="https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/a6d36/firetruck.png" srcset="https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/222b7/firetruck.png 163w,
https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/ff46a/firetruck.png 325w,
https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/a6d36/firetruck.png 650w,
https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/e548f/firetruck.png 975w,
https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/3c492/firetruck.png 1300w,
https://www.charlieharrington.com/static/77fe5d6a0fb7c7f56ef4920d22c1efa9/6c2de/firetruck.png 1334w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy">
    </span></p>
<p>Anyway, this season's Dr. Who told me not to worry about the blood. "It's probably hemorrhoids."</p>
<p><em>WTF is a hemorrhoid?</em> I wondered to myself as I said to him, "Sounds good." Googled it after, and I learned that it's a vein that pokes out a little in your butt and doesn't really want to be poking out a little, so it bleeds. Seems like a thing that can happen, so I more or less returned to regularly scheduled programming and just dealt with the occasional poopy-red wipe. This doc also said I probably don't need to come for a physical for a few years, that annual physicals are a myth, dry land in a water world.</p>
<p>Fast-forward to 2020. Everything sucks. And the bloody wipes are making a resurgence. Because, of course, they are. About four months ago I noticed that my first poop of the day (I usually go 2x) would have this purple-red streak embedded in it, like a racing stripe from hell. And it would happen almost without fail every single morning. That just didn't seem right, no matter what Doctors of Physicals Past told me. And then one morning I felt like I had actual blood dripping from my butt.</p>
<p>Now I consider myself to be a mostly healthy person. I eat fairly well (even though I enjoy the occasional sourdough loaf and hazy IPA), I run and bike and hike regularly, I ran an IRONMAN in 2016 and a few ultramarathons since. I also don't like being sick (who does?). But, like with most things in my life, I want to be "good" at health. An ideal dental appointment for me would go something like this, "Wow, Charlie, these are the straightest, whitest teeth we've ever seen. We'd like you to come in and be the model for our Instagram ads and also be our 3D teeth model for dentures. Congratulations. Here's <em>two</em> free toothbrushes. You also never need to floss again."</p>
<p>Anything that deviates from that ideal makes me squirm and I do think I can fix anything. For what it's worth I still believe that, if I ever encounter a blue flower on a mountain-top, I'm only a few months of mystical training away from becoming Batman. I already have the cape (it's actually a Harry Potter robe, but, hey, I'm scrappy).</p>
<p>At the same time, I counterweight this with a mild touch of hypochondria. I'll see the poison oak in the mistletoe, so to speak. In this case it was a gift.  I googled again for <em>stool, bloody stool</em> and the dreaded <em>colon cancer</em> came back. Last time, I averted my eyes from these search results. But the bloody racing stripes weren't going away. I needed to get myself checked out.</p>
<p>Then I remembered an email from work: I was eligible for a <a href="http://members.onemedical.com/membership_referrals?code=cha0014&amp;source=sa">OneMedical</a> membership. I knew there was hype about OneMedical, certainly I've seen the billboards, but I still wasn't exactly sure what they were all about. It had been a few years since my last physical, as you know, so I was primary-care-less, with a bloody problem on my hands. I downloaded the OneMedical app, uploaded a photo of my insurance card, beep-boop, and I've got an appointment with a new doc in a few days in one of their nearby clinics. Already, I loved the experience - I could text my questions any time (see <em>foot wart</em> above). I'd describe OneMedical as a network of clinics with an app for scheduling appointments and texting with a doc. Sure, ZocDoc kinda does the scheduling thing, but Zocdoc feels like you're sifting through the classifieds. Gimme some non-user-generated-ratings-based curation, please. </p>
<p>So, I met with the doc, liked him a lot, discussed my bloody poops, and sheepishly asked if he'd be my new primary care. He agreed, and he also referred me to UCSF for a colonoscopy. Sure, I'm young, and it's probably hemorrhoids, we agreed, but it's the only way to be sure.</p>
<p>After some jiggling about with the referral documentation, we finally get the colonoscopy scheduled for a few weeks later on Sept 9th.</p>
<p>Then, on August 28th, <a href="https://en.wikipedia.org/wiki/Chadwick_Boseman">Chadwick Boseman</a> died of complications from colon cancer.</p>
<p>I wasn't freaked out. Okay, yes, I was very freaked out.</p>
<h2>Colonoscopies are not bad</h2>
<p>What's a colonoscopy? It's a surgical procedure where the doctor goes all the way up your butt to see what's going on in there. You are completely knocked out, so you feel nothing. The only thing you need to do is what we in the business like to call "bowel prep."</p>
<p>Allow me to describe bowel prep: the day before the procedure, you will poop your ever-living guts out for a few hours until you are clean-as-a-whistle, stem to stern. They'll give you a prescription for a gigantic jug of clear laxatives that you'll drink every 15 minutes or so for a few hours. In today's toilet-paper hoarding economy, I'd make sure that you are stocked up, because this gets messy.</p>
<p>Other then the laxatives, you're allowed to drink clear liquids - which is confusing because you can enjoy such clear liquids as black coffee, Gatorade, broth, even green jello.</p>
<p>But that's it. Easy. I watched Stranger Things season 3 again during my bowel prep day. Might not have been the best choice, as I intermittently had to pause Netflix to contribute my own liquified form of the Mind-Flayer, but it got the job done, and I cried my way thru Dustin and Suzie's hymn to childhood, again, as expected.</p>
<p>Okay, next, I woke up on September 9th. My appointment is around 2 PM. Normal day, right?</p>
<p><span>
      <span></span>
  <img alt="sf" title="sf" src="https://www.charlieharrington.com/static/dfe0aae59da7dc3715e8f289c1ae8f7b/6aca1/sf.jpg" srcset="https://www.charlieharrington.com/static/dfe0aae59da7dc3715e8f289c1ae8f7b/d2f63/sf.jpg 163w,
https://www.charlieharrington.com/static/dfe0aae59da7dc3715e8f289c1ae8f7b/c989d/sf.jpg 325w,
https://www.charlieharrington.com/static/dfe0aae59da7dc3715e8f289c1ae8f7b/6aca1/sf.jpg 650w,
https://www.charlieharrington.com/static/dfe0aae59da7dc3715e8f289c1ae8f7b/8e1fc/sf.jpg 900w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy">
    </span></p>
<blockquote><p lang="en" dir="ltr">A strange, apocalyptic orange-red sky looms over the Bay Area. Here's what you need to know.<br>Read more: <a href="https://t.co/JxpYSnUPG9">https://t.co/JxpYSnUPG9</a> <a href="https://t.co/ZPOj4X3jRg">pic.twitter.com/ZPOj4X3jRg</a></p>‚Äî San Francisco Chronicle (@sfchronicle) <a href="https://twitter.com/sfchronicle/status/1303799596515172352?ref_src=twsrc%5Etfw">September 9, 2020</a></blockquote> 
<p>Nope.</p>
<p>I decide to walk over to the UCSF Parnassus building in the creepy Mars firelight, imagining I'm the last man on Earth (and hoping I don't step on my reading glasses). Carly makes a plan to pick me up in a few hours in our car.</p>
<p>As expected, the procedure was painless. My only bit of further colonoscopy advice here is to ALWAYS bring a book with you, to every single medical appointment you have, because there's always going to be some sort of delay or waiting room.</p>
<p>An hour or so later, I woke up feeling the feels of that post-anesthesia giddiness. Except no one else was happy. Carly was in the room, a surprise to me. And my doctor looked quite serious.</p>
<p>In addition to two small polyps (which she removed), my colonoscopy surgeon found a tumor in my sigmoid colon. At this point, I don't know a sigmoid colon from a semi-colon, but I knew it wasn't good news. Go 2020!</p>
<p>Despite the odds (my youth, my health), I now had cancer. Well, I probably had it for awhile, but we just found out I had it.</p>
<p>My doc said I'd need to meet with <a href="https://www.ucsfhealth.org/clinics/center-for-colorectal-surgery">UCSF's colorectal surgery team</a>, and I'd also need to get CT scans ("cat scans") to see if the cancer had spread anywhere else in my body.</p>
<p>And so began one of the worst weeks of our lives.</p>
<h2>A brief family history</h2>
<p>Let's talk about the odds for a moment.</p>
<p><img src="https://www.charlieharrington.com/899a4e01fcef3d2113e4588727bc0834/odds.gif" alt="odds"></p>
<p>We've already discussed my vigorous, proto-Batman level of health. And how I'm a fresh-faced, occasionally-bearded, 34 year old with the heart of a child and the strength of a chimpanzee (no, that's a <a href="https://en.wikipedia.org/wiki/Humanzee">humanzee</a>).</p>
<p>Speaking of unfortunate genetics, it turns out that I have some family history of colon cancer. </p>
<p>Here's the scoop: my pops (that's cool talk for Dad) has had benign (non-cancerous) polyps in his previous colonoscopies. What's a poylp? It's a little growth thingy in your colon that may evolve into a tumor. Just like how a Charmander becomes a Charmeleon, polyps can grow bigger and more serious with more destructive power. Polyps are usually just snipped out during your colonoscopy and sent off for pathology (aka to see if they have cancer in them). Most do not. This is the case with my dad's polyp experience. Even though none of his have been cancerous, he still needs to go in for colonoscopies more regularly than those who don't have polyps.</p>
<p>My own tumor began as a lowly polyp, perhaps some ten years ago. We don't know exactly. But if I'd ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.charlieharrington.com/colon-cancer">https://www.charlieharrington.com/colon-cancer</a></em></p>]]>
            </description>
            <link>https://www.charlieharrington.com/colon-cancer</link>
            <guid isPermaLink="false">hacker-news-small-sites-24853503</guid>
            <pubDate>Wed, 21 Oct 2020 23:40:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Talking, Typing, Thinking: Software Is Not a Desk Job]]>
            </title>
            <description>
<![CDATA[
Score 129 | Comments 60 (<a href="https://news.ycombinator.com/item?id=24851861">thread link</a>) | @danielfone
<br/>
October 21, 2020 | https://daniel.fone.net.nz/blog/2020/10/21/talking-typing-thinking-software-is-not-a-desk-job/ | <a href="https://web.archive.org/web/*/https://daniel.fone.net.nz/blog/2020/10/21/talking-typing-thinking-software-is-not-a-desk-job/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <h2>
      Talking, Typing, Thinking: Software Is Not a Desk Job
    </h2>
    <p>
      October 2020
      ¬∑
      about 7  minutes to read
    </p>

      <summary>
        <p><strong>tl;dr</strong> Developers over-optimise for the ergonomics of typing and not enough for the ergonomics of thinking.</p>
      </summary>

    <p>I had a wonderful shower the other day.</p>

<p>It was late morning (as the best showers often are) and I was reflecting on how I spend my time during the day. As a work-from-home consultant, I constantly need to justify my billing and my time, and in this case I was justifying spending more of it in the shower.</p>

<p>Like most of us, I started my career with the impression that a productive day was spent ergonomically poised over a keyboard typing hundreds of lines of code into Microsoft Visual Basic 6.0 Professional Edition, and <em>not</em> standing in a perfectly hot stream of high pressure fresh water. However, the longer I spend as a developer, the less I‚Äôm convinced I need to be at my desk to deliver the truly astounding spreadsheet-to-web-application business value us senior engineering consultants deliver.</p>

<p>So that you too can justify spending the good part of a morning enveloped in a cocoon of cleansing warmth, let‚Äôs break this down and look at 5 physical activities of effective software development. Like all good listicles, this is ordered roughly in order of increasing time and importance.</p>

<h2 id="talking">5. Talking</h2>

<p>Some software development probably doesn‚Äôt need any talking to be effective. I understand for example that it is universally considered bad manners to talk about linux kernel development out loud. The contents of ~/bin too, we do not speak of.</p>

<p>But every commercial project I‚Äôve worked on has needed at least some talking. When people are too busy or just too shy to talk, the lack of high-bandwidth communication can make it hard to tease out requirements and unpack poorly explained business problems.</p>

<p>But more importantly, a lack of talking makes it hard to build trust and rapport ‚Äî critical&nbsp;in early stages of any new relationship. As social animals, we are particularly good at doing this verbally, and not particularly good at doing this with emails and spicy subtweets.</p>

<p>On the other hand I‚Äôve worked on projects where talking is a prop to disguise that no-one knows what to do. Where a dozen people sit in a room and talk for an hour without saying anything and we all walk out dumber than when we walked in.</p>

<p>So for most cases: talking is critical, but in the right amount.</p>

<h2 id="listening">4. Listening</h2>

<p>Honestly I just included this for symmetry. The only thing I‚Äôll add is that we have two ears and one mouth so either binaural hearing offers an evolutionary advantage against some selection pressure or we‚Äôre supposed to listen twice as much as we talk.</p>

<p>Take your pithy wisdom however you like it.</p>

<p><em>(quickly googles why do we have two ears)</em></p>

<h2 id="writing">3. Writing</h2>

<p>Writing code of course! But also‚Ä¶ READMEs, comments, inline documentation, PR descriptions, code reviews, git commits; this is all part of the <em>core work</em>. It‚Äôs tempting to see this meta-writing as overhead on top of the real ‚Äòcode‚Äô writing. But effective writing in these other places is a force multiplier for your code.</p>

<p>Much more importantly though, in my experience the best communication is written.</p>

<ul>
  <li>It‚Äôs async, meaning it can be consumed whenever convenient for each reader (i.e. after a late morning shower).</li>
  <li>It can be easily distributed and has no fidelity loss when shared (compared to talking to John about what Sarah told you Steve said in meeting that none of you were at).</li>
  <li>It creates a record, as opposed to ‚Äúwait, why did we‚Ä¶?‚Äù</li>
  <li><a href="https://alistapart.com/article/writing-is-thinking/">Writing is thinking</a>! Writing forces you to structure your ideas coherently (at least, it seems to for some people). It reveals shortcomings or gaps in your understanding or plan.</li>
</ul>

<p>Because of this I encourage team comms to be mostly written. Jira, slack, emails, trello, blog posts, whatever. Even a hi-res photo of a wall of post-it notes has been an indispensable architectural road-map at times. However it‚Äôs published, detailed, well-thought out writing is üíØ.</p>

<p>Perhaps even more important than writing though is‚Ä¶</p>

<h2 id="reading">2. Reading</h2>

<p>Having just extolled the virtues of writing READMEs, commit messages, PR descriptions, etc, I should obviously encourage you to read them. It‚Äôs called README IN CAPITALS for a reason, and it‚Äôs not just because it‚Äôs an acronym. Yet if I had a dollar for every time someone asked me a question that was already answered in the README I would have three dollars. üí∞</p>

<p>This is because of what I succinctly call the vicious-reading-writing-cycle-feedback-loop. When people don‚Äôt update the commentary, people become trained to ignore it, so people don‚Äôt update it, etc. Truthfully, if you know someone‚Äôs reading your git commits, their quality will rapidly improve. Even if you‚Äôve never read a coherent git commit from your colleagues before, it‚Äôs never too late to ask them to elaborate on what <code>finally fix it</code> means.</p>

<p>But like writing, the value of reading extends well beyond the code repository.</p>

<p>I recently started a project involving a completely unfamiliar field of medical technology (ps you‚Äôre <a href="https://twitter.com/danielfone/status/1318026784454045703">still my favourite</a> patient <code>01-004</code> üìä‚ù§Ô∏è). The most valuable activity I find at this stage of a project is to read.</p>

<p>We have to parse a <a href="https://en.wikipedia.org/wiki/European_Data_Format">specialised file format</a>, for which there is <a href="https://github.com/nsrr/edfize">a gem</a>. But why leave all that useful context buried inside the gem? The <a href="https://www.edfplus.info/specs/edf.html">file specification</a> is not that long, even if it takes many attempts to understand it. Reading the file format spec makes it much easier to understand why the gem needs to <a href="https://github.com/nsrr/edfize/blob/93566cdc82b160ef319c51908c1c4a19666e2625/lib/edfize/edf.rb#L243">load_digital_signals_by_epoch</a>, which in turn suggests alternative solutions to the problem you have in hand.</p>

<p>None of the <em>adjacent possible</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> is discoverable without the insights gained from reading these sources, so whatever you‚Äôre dealing with, go to the source and read read read‚Ä¶</p>

<ul>
  <li>documentation (<em>reading the very well-written <a href="https://www.postgresql.org/docs/current/index.html">postgres manual</a> or <a href="https://redis.io/documentation">redis docs</a> is the closet experience I‚Äôve had to Neo downloading kung-fu into his brain</em>)</li>
  <li>code (<em>vastly underrated - there‚Äôs not a gem in my Gemfiles I haven‚Äôt <code>bundle open</code>d at least once</em>)</li>
  <li>log files, error messages, that tutorial on how to read flame graphs</li>
  <li>the specification, legislation, policy document, NIST guideline, the original paper in the open access journal</li>
</ul>

<p>‚Ä¶ you get the idea, just find the authoritative document and slurp it into your brain. Even if it seems like nothing sticks, a brief encounter with the text will leave a long-lasting impression. It‚Äôs like homeopathy but real.</p>

<p>So talking/listening‚Ä¶ writing/reading‚Ä¶ and finally‚Ä¶ <em>drumroll noises</em></p>

<h2 id="thinking">1. Thinking</h2>

<p>When you boil it down, <em>this</em> is the main effort for me, and yet it‚Äôs kind of the hidden one.</p>

<p>How much of my programming/coding/dev time is actually just spent <em>thinking</em> about the problems?
Modelling the domain,
thinking through the edge cases,
mentally playing with abstractions.</p>

<p>And it‚Äôs obvious when you think about what makes good developers. The people I value working with most aren‚Äôt accurate typists, they‚Äôre <em>clear thinkists</em>.</p>

<p>Yet the image persists that typing is working and working is typing and a productive day is in your chair at your desk.
So we have dual 4k monitors, mechanical keyboards, aeron chairs, touchbar, vim shortcuts, whatever optimises for us tapping away at our computers.</p>

<p>But how much attention do we pay to the <strong>ergonomics of thinking</strong>?</p>

<p>When we elevate ‚Äòthinking‚Äô to core work, we naturally start to optimise for it specifically. In general, we don‚Äôt need to be in front of anything to think effectively, and often I find it better not to be. My times of greatest clarity are invariably when I‚Äôm moving, often when I‚Äôm exercising. Further, I can read on my phone practically anywhere, and the best conversations are often had while strolling.</p>

<p>So while I‚Äôm glad for all the ergonomics of my workspace, increasingly I find that writing code is the brief part where I‚Äôm simply harvesting all the mental crop that I‚Äôve sown from the talking and listening and reading and thinking.</p>

<p>To distill this into something a little more alliterative, I have sometimes described this as the 3 Ts of software development‚Ä¶</p>

<h2>Talking ¬∑ Typing ¬∑ Thinking</h2>

<p><strong>Talking</strong> and listening; the verbal discussions. Most of the time we need a small but critical amount of high bandwidth synchronous comms.</p>

<p><strong>Typing</strong> code commentary: READMEs, code reviews, PR descriptions; and all asynchronous communication: project updates, technical overviews, emails with next steps; These are all an essential part of the job and not just ancillary or busywork. Also typing actual code at some point. But I find the more time you spend typing the other stuff, the less time you need to spend (re)typing code.</p>

<p><strong>Thinking</strong>: (including, for the purposes of alliteration, reading)</p>

<p>Talking, typing, thinking: this is the work we do. And I for one want to give myself the space to do all parts of it really well.</p>

<p>Anyway, I gotta go take a shower. üöø</p>



  </article></div>]]>
            </description>
            <link>https://daniel.fone.net.nz/blog/2020/10/21/talking-typing-thinking-software-is-not-a-desk-job/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24851861</guid>
            <pubDate>Wed, 21 Oct 2020 20:33:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stable 1.2 Gigabit/s Internet achieved in moving train in Switzerland]]>
            </title>
            <description>
<![CDATA[
Score 366 | Comments 303 (<a href="https://news.ycombinator.com/item?id=24851365">thread link</a>) | @richx
<br/>
October 21, 2020 | https://www.swisscom.ch/en/about/news/2020/10/21-mehr-bandbreite-im-zug.html | <a href="https://web.archive.org/web/*/https://www.swisscom.ch/en/about/news/2020/10/21-mehr-bandbreite-im-zug.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	

		
		
		

		
		

		
	
	<section>
		

<div>
	
	<div>
		<div>
			<div>







	<p>Mobile phone reception</p>


	


	<p>Uninterrupted, good quality mobile phone reception is extremely important to rail passengers. In technical terms, it‚Äôs the pi√®ce de r√©sistance for every network provider because the demands on bandwidth increase with data-intensive applications. Swisscom has now successfully achieved a transmission speed of over 1 Gigabit per second on a moving train under test conditions. This result sets a new benchmark for the mobile phone industry.</p>



	
	
		<div>
			
			<p><img src="https://rcp.scsstatic.ch/content/dam/swisscom/de/about/news/authors/armin-schaedeli-260x260-2x.png.scsimg.89x89.ts1543323485216.png/armin-schaedeli-260x260-2x.png" width="89" height="89" alt="Armin Sch√§deli" loading="lazy">
			</p>
			
			<p>
				Armin
				Sch√§deli,
				Deputy Head of Media Relations<br>
				21 October 2020
			</p>
		</div>
	
	


</div>

		</div>
		<div>
			<div><div>

	

	
	
	
		
		
			<div data-column-count="3">
				
					
				
					<div>
						



	
		
		
			


	<div>


	
	
	
	<div>
		
			
		
		
		



	
	
		
	





		<div>
			
				<p>How do most people use their phones when on a train? Besides checking their mail and reading the paper, they also stream videos, play online games or work in virtual offices. This requires a great deal of bandwidth, meaning that capacity issues can prove particularly annoying. A Swisscom team has been researching and working on continuously improving mobile coverage for rail travellers and commuters for more than ten years.</p>

<p>The invention of a special type of glass for the train windows, which lets mobile telephone signals through, has made it possible to bring mobile coverage directly onto the train without intermediate components. However, coverage along train routes remains challenging as much more data is transmitted under the same conditions with each mobile phone generation. One possible solution is a specially designed antenna corridor along railway lines.</p>

			
		</div>
		
	</div>
</div>


	


	<div>


	
	
	
	<div>
		
			
		
		
		



	
	
		
	





		<div>
			
				<p>Swisscom has now made a major breakthrough on a test route between Biberlikopf and Kerenzerberg at Lake Walen with a newly designed four-kilometre antenna corridor: Swisscom engineers achieved a connection with 1.2 Gbit/s on a moving train. Christoph Aeschlimann, Head of IT, Network &amp; Infrastructure at Swisscom says: ‚ÄúThis concept sets a new benchmark for the mobile phone industry. Just one year ago, we had no idea whether this would be possible. We now have a solution that provides stable and reliable coverage for passengers as well as important insights for safety-relevant applications in rail transport.‚Äù</p>
<p>Another positive side effect is the lower transmitting power required due to the shorter distances between antennas and devices.</p>

			
		</div>
		
	</div>
</div>


	


	<div>


	
	
	
	<div>
		
			
		
		
		



	
	
		
	





		<p>After evaluating the results, the test corridor will be further optimised and validated with measurements in the first quarter of 2021. The long-term goal is to achieve uninterrupted mobile phone coverage along the main routes for all mobile phone users and providers in Switzerland In terms of the antenna corridor, Swisscom has developed a feasible solution that is also available to other providers.</p>
		
	</div>
</div>


	<div>

	


























	



	
	
	<div>
		


	<div>

	
	

	
	

	

	
	
	



	
	
	
		
	



	
	
		
	
	




















	












	<div data-interactive-name="" data-interactive-value="">
		<div>
			
				
			
			
				
					
						
						
							


	<div>

	


























	



	
	
	<div>
		


	


	<div>


	
	
	
	<div>
		
			
		
		
		



	
	
		
	





		<div>
			
				<p>A four-kilometre antenna corridor was constructed along the railway line at Lake Walen for the test, in conjunction with network equipment supplier Ericsson. The proximity of the antenna to devices means the transmitting power is lower and the coverage along the railway corridor is more targeted.</p>

<p>In a step-by-step procedure, numerous combinations (4G and 5G mobile phone generations, seating position, type of train car, transmitting power, train windows, mast antennas, smartphone models, etc.) were measured and analysed over more than 200 train journeys. The project has shown that the antenna corridor is possible and offers good performance. Download speeds of over 1.2 Gbit/s were possible on a moving train with a combination of 4G and 5G. The 5G response time was four times shorter than 4G ‚Äì an impressive 8 milliseconds.</p>

<p>In addition to network coverage, safety-critical applications on rail transport are another consideration. The existing GSM-R railway communication network standard will be replaced in the coming years by the new Future Railway Mobile Communication System (FRMCS). Good mobile phone coverage is also therefore crucial for rail companies as well as passengers.</p>

			
		</div>
		
	</div>
</div>



	</div>
</div>



						
					
				
			
		</div>
		
	</div>

</div>



	</div>
</div>


	


	

	


	


	<div>





	
	
	
		
		
			
			
				<div>
					
						
						
							<div>






	










	

















	



	
	<div>
		
			
				
				
					

				
			
		
			
				
				
					<div>

	










	





	
	
	















	
	
	<div>
		


	


	<div>

	

	
	
	
		
			<div data-column-count="3">
				
					<div>
						



	
		
		
			


	


	<div>


	
	
	
	<div>
		
			
		
		
		



	
	
		
	





		<div>
			
				<div><p>Swisscom<br> Media Relations<br> Alte Tiefenaustrasse 6<br> 3048 Worblaufen</p><p>  Postal address:<br> Postfach, CH-3050 Bern<br> Switzerland</p></div>
			
		</div>
		
	</div>
</div>



		
	


					</div>
				
					
				
					<div>
						



	
		
		
			


	


	<div>


	
	
	
	<div>
		
			
		
		
		



	
	
		
	





		<p>Tel. +41 58 221 98 04<br> Fax +41 58 221 81 53<br> media@swisscom.com</p>
		
	</div>
</div>


	


	



		
	


					</div>
				
			</div>
		
		
	

</div>



	</div>
</div>

				
			
		
		
	</div>

</div>

						
					
				</div>
			
		
	
	

</div>



		
	


					</div>
				
					
				
			</div>
		
	

</div>

</div>

		</div>
	</div>
</div>

	</section>
</div></div>]]>
            </description>
            <link>https://www.swisscom.ch/en/about/news/2020/10/21-mehr-bandbreite-im-zug.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24851365</guid>
            <pubDate>Wed, 21 Oct 2020 19:52:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Escaping Science's Paradox]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24851141">thread link</a>) | @stuart_buck
<br/>
October 21, 2020 | https://worksinprogress.co/issue/escaping-sciences-paradox/ | <a href="https://web.archive.org/web/*/https://worksinprogress.co/issue/escaping-sciences-paradox/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<div>
						<p>Science has two stark problems: replication and innovation. Many scientific findings aren‚Äôt reproducible. That is to say, you can‚Äôt be sure that another study or experiment on the same question would get similar results. At the same time, the pace of scientific innovation could be slowing down.</p>
<p>Does attempting to solve one problem make the other worse? Many have argued that policies seeking to avoid reproducibility issues will create a constrictive atmosphere that inhibits innovation and discovery.</p>
<p>Indeed, top policymakers are worried about just this. Along with other prominent philanthropists and academics, I attended a White House meeting on scientific reproducibility early in 2020 (just before COVID-19 really hit). One of the key questions on a sheet of paper that the White House Office of Science and Technology Policy circulated for discussion was whether a tradeoff existed: Would efforts to improve reproducibility risk harming the creativity and innovation of federally-funded research?</p>
<p>I do <i>not</i> think there‚Äôs a contradiction between reproducibility and innovation. Contrary to common belief, we can improve <i>both</i> at once ‚Äì by incentivizing failed results, and by funding ‚ÄúRed Teams‚Äù that would aim to refute existing dogma or would be entirely outside it.</p>
<p>First, though, let‚Äôs take a step back, and briefly review the evidence that significant areas of science could be more reproducible and innovative.</p>
<h2><b>Is science reproducible?<br>
</b></h2>
<p>Many people have written about scientific irreproducibility over the past several decades. But the issue became more prominent in the mid-2000s with the publication of what soon became one of the most downloaded research papers of all time: The 2005 piece ‚Äú<a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124">Why Most Published Research Findings Are False</a>,‚Äù by Stanford‚Äôs John Ioannidis. (Disclaimer: he is a long-time grantee of Arnold Ventures, where I work.)</p>
<p>To be sure, Ioannidis‚Äôs finding was mostly theoretical; it‚Äôs not as if he actually redid ‚Äúmost‚Äù published research (i.e., tens of millions of studies). Instead, he showed that given the way most studies are carried out, if journals have even a slight bias towards positive results (and they most definitely do), then most of the results that end up getting published would inevitably be statistical flukes or the results of p-hacking.</p>
<p>His theoretical case has been confirmed by many empirical studies in fields from drug development to psychology. Pharmaceutical companies such as <a href="https://www.nature.com/articles/483531a">Amgen</a> and <a href="https://www.nature.com/articles/nrd3439-c1">Bayer</a> have reported that they are unable to reproduce 80+% of experiments from prestigious journals. To quote Bayer‚Äôs scientists, ‚Äúprojects that were started in our company based on exciting published data have often resulted in disillusionment when key data could not be reproduced.‚Äù</p>
<p>Then there was the <a href="https://osf.io/ezcuj/">Reproducibility Project in Psychology</a>, which we funded, and which was carried out by our grantee Center for Open Science. That project organized well over 200 psychology labs around the world to systematically redo 100 experiments published in top psychology journals. It found that only about 40% could be reliably replicated (another 40% were inconclusive, and around 20% were decisively <i>not</i> replicated). Since <a href="http://science.sciencemag.org/content/349/6251/aac4716">those results were published</a> in 2015, the study has already been cited <a href="https://scholar.google.com/scholar?cites=10200793109432081889&amp;as_sdt=5,44&amp;sciodt=0,44&amp;hl=en">over 4,400 times</a> according to Google Scholar. Many of the most famous results in psychology have turned out to be <a href="https://www.theguardian.com/science/2018/apr/16/a-real-life-lord-of-the-flies-the-troubling-legacy-of-the-robbers-cave-experiment">unreliable</a> and possibly fraudulent (such as Zimbardo‚Äôs <a href="https://www.vox.com/2018/6/13/17449118/stanford-prison-experiment-fraud-psychology-replication">Stanford prison experiment</a>), and the best recent treatment of this issue is Stuart Ritchie‚Äôs 2020 book ‚ÄúScience Fictions.‚Äù</p>
<p>To be sure, the problem seems much less acute in harder sciences ‚Äì e.g., physics, chemistry, cosmology ‚Äì that have an established tradition of skepticism, replication, or even <a href="https://www.law.berkeley.edu/wp-content/uploads/2018/01/Paper-MacCounPerlmutter2017ch15.pdf">blinding researchers</a> to their own conclusions. The bulk of the reproducibility and publication bias problem seems to be in social science and biomedicine. In many of those fields and subfields ‚Äì such as <a href="https://pubmed.ncbi.nlm.nih.gov/19160345/">clinical trials in medicine</a>, <a href="https://arxiv.org/pdf/1010.1092.pdf">high-throughput bioinformatics</a>, <a href="https://www.sciencedirect.com/science/article/pii/S2213158213000090">neuroimaging</a>, <a href="https://www.sciencedirect.com/science/article/abs/pii/S1364661314000540">cognitive science</a>, <a href="https://www.niss.org/publications/deming-data-and-observational-studies-process-out-control-and-needing-fixing">public health and epidemiological research</a>, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12032">economics</a>, <a href="https://www.semanticscholar.org/paper/The-fault-in-our-stars-%3A-Measuring-and-correcting-Esarey-Wu/48403d5dce1f2972c7a91af3bb2d41635221cb3f">political science</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/23696308/">psychiatry</a>, <a href="https://eric.ed.gov/?id=EJ1019294">education</a>, <a href="https://journals.sagepub.com/doi/10.1177/0049124108318973">sociology</a>, <a href="http://cs.brown.edu/~sk/Memos/Examining-Reproducibility/">computer science</a>, and <a href="https://dennybritz.com/blog/ai-replication-incentives/">machine learning and AI</a> ‚Äì the published literature features too many false positives as well as conclusions that may well be p-hacked. It‚Äôs enough to make folks at the White House, NIH, and NSF worried about the quality of federally-funded science.</p>
<h2><b>Is science innovative enough?<br>
</b></h2>
<p>At the same time, numerous observers have pointed to an entirely different problem: science has grown less innovative these days. (And even if it hasn‚Äôt, we could always benefit from faster innovation.)</p>
<p>In a recent piece, Patrick Collison, the founder of Stripe, and Michael Nielsen, a theoretical physicist, <a href="https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/">made the case</a> that the rate of scientific advancement is slowing down in recent years per dollar spent. Based on surveys of noted leaders in physics, chemistry, and medicine, they concluded, ‚ÄúOver the past century, we‚Äôve vastly increased the time and money invested in science, but in scientists‚Äô own judgement, we‚Äôre producing the most important breakthroughs at a near-constant rate. On a per-dollar or per-person basis, this suggests that science is becoming far less efficient.‚Äù</p>
<p>Collison and Nielsen are <a href="https://www.nber.org/papers/w26752.pdf">far from alone</a>. Cowen and Southwood <a href="https://www.brown.edu/academics/political-theory-project/sites/brown.edu.academics.political-theory-project/files/uploads/Innovation%20%26%20scientific%20progress.pdf">argue</a> that ‚Äúthere is good and also wide-ranging evidence that the rate of scientific progress has indeed slowed down.‚Äù The 2019 <a href="https://web.stanford.edu/~chadj/IdeaPF.pdf">paper</a>, ‚ÄúAre Good Ideas Getting Harder to Find?‚Äù argues that in semiconductors, agriculture, and medical innovations, ‚Äúresearch effort is rising substantially while research productivity is declining sharply.‚Äù	<sup data-close="x" data-content="[1]">[1]</sup>
	<span>They attempted to replicate this analysis for ‚Äúthe internal combustion engine, the speed of air travel, the efficiency of solar panels, the Nordhaus (1997) ‚Äòprice of light‚Äô evidence, and the sequencing of the human genome.‚Äù But they couldn‚Äôt do so because there was no accurate measure of the amount of R&amp;D on those issues.</span>
	 That paper concludes by predicting that ‚Äújust to sustain constant growth in GDP per person, the U.S. must double the amount of research effort searching for new ideas every 13 years to offset the increased difficulty of finding new ideas.‚Äù</p>
<p>Of course, some of these assessments might be too <a href="https://guzey.com/how-life-sciences-actually-work/">pessimistic</a>. But it is depressingly common to hear the world‚Äôs most innovative scientists lament that they would never have succeeded in today‚Äôs academic or funding system because their work was too outside the box:</p>
<ul>
<li>Roger Kornberg (a Nobel-winning biochemist) <a href="http://www.washingtonpost.com/wp-dyn/content/article/2007/05/27/AR2007052700794.html">told the Washington Post in 2007</a> that his 1970s research on DNA ‚Äúwould never have gotten the necessary funding‚Äù if he had come along in the 2000s: ‚ÄúIn the present climate especially, the funding decisions are ultraconservative. If the work that you propose to do isn‚Äôt virtually certain of success, then it won‚Äôt be funded.‚Äù</li>
<li>As <a href="https://www.kqed.org/forum/201310090930/cuts-in-federal-funding-hurt-scientific-research">reported</a> in 2013, ‚ÄúUC Berkeley molecular biologist Randy Schekman won the Nobel Prize for Medicine with two other scientists this week. But he says the kind of basic science research that led to his prize might have never gotten funded if he were applying for grants today.‚Äù</li>
<li>David Deutsch, who pioneered quantum computing, <a href="https://twitter.com/DavidDeutschOxf/status/982233180081029128">says</a> that he would never have gotten his ‚Äúfirst research grant on quantum computers . . . under today‚Äôs criteria.‚Äù</li>
<li>Peter Higgs, the Nobel Laureate for whom the Higgs Boson is named, ‚Äú<a href="https://www.theguardian.com/science/2013/dec/06/peter-higgs-boson-academic-system">believes</a> no university would employ him in today‚Äôs academic system because he would not be considered ‚Äòproductive‚Äô enough. . . . ‚ÄòToday I wouldn‚Äôt get an academic job. It‚Äôs as simple as that. I don‚Äôt think I would be regarded as productive enough.‚Äô‚Äù</li>
</ul>
<p>When so many top scientists say that their own work would never have passed muster in the current system, we must take stock of the current system. As prominent scientists <a href="https://science.sciencemag.org/content/364/6441/613">have asked</a>, ‚ÄúHow successful would Silicon Valley be if nearly 99% of all investments were awarded to scientists and engineers aged 36 years or older, along with a strong bias toward funding only safe, non-risky projects?‚Äù Moreover, a <a href="https://www.telegraph.co.uk/business/2018/06/07/science-holds-key-unlocking-economic-success/">common complaint</a> is that ‚Äúscientists are forced to specify years in advance what they intend to do, and spend their time continually applying for very short, small grants‚Äù ‚Äì hardly a system that would encourage innovation.</p>
<p>In short, we have evidence that US science funding is often fairly tame and incremental, that some of the most innovative science of the past would <a href="https://www.npr.org/2019/05/19/723326933/billion-dollar-gamble-how-a-singular-hero-helped-start-a-new-field-in-physics?fbclid=IwAR28fazRNhzzT5ijMINwzZkNqlP6fEfBlI4OLyY-eesrUyOfbZfF7-P4qUc">never have been funded</a> by today‚Äôs bureaucracy, and that scientific review panels are <a href="https://www.nature.com/articles/492034a">dominated by insiders</a>.</p>
<p>Thus, innovation in science is imperiled. If Einstein had to navigate such a system, we might <a href="https://www.wsj.com/articles/could-einstein-get-published-today-11600974323">never have heard of relativity</a>. And even if innovation weren‚Äôt slowing down <em>per se</em>, we could still do better.</p>
<h2><b>What next?</b></h2>
<p>There are lots of ideas about how to improve scientific reproducibility in how federal research is funded. After all, quality control and assurance are hardly new ideas.</p>
<p>For example, we could require that data and computer code be shared openly so that others can scrutinize and rerun it. In too many cases to list, this sort of reanalysis has led to revisions, retractions, and even the discovery of <a href="https://www.newyorker.com/science/maria-konnikova/how-a-gay-marriage-study-went-wrong">outright fraud</a>.</p>
<p>Next, we could require that experiments and other empirical studies be pre-registered, so that the analysis and results are less likely to be cherry-picked later. We already do this for clinical trials in medicine, and a <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382">review</a> of federally-sponsored clinical trials found that the rate of positive results went down dramatically as soon as researchers were required to pre-register their studies. We could do the same for much else in science. We could even move towards more widespread use of the Registered Reports format, in which journals accept an article for publication before the final results are even available.</p>
<p>It‚Äôs less obvious how to reform government funding so as to improve scientific <i>innovation</i>. Let‚Äôs try a thought experiment:</p>
<p>Imagine that you were the President 100 years ago, instead of Woodrow Wilson. Imagine that a time-traveling genie from the future tells you that over the ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://worksinprogress.co/issue/escaping-sciences-paradox/">https://worksinprogress.co/issue/escaping-sciences-paradox/</a></em></p>]]>
            </description>
            <link>https://worksinprogress.co/issue/escaping-sciences-paradox/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24851141</guid>
            <pubDate>Wed, 21 Oct 2020 19:26:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[To write better, develop a habit of writing]]>
            </title>
            <description>
<![CDATA[
Score 160 | Comments 68 (<a href="https://news.ycombinator.com/item?id=24849485">thread link</a>) | @pavelegorkin
<br/>
October 21, 2020 | https://bookpub.club/post/to-write-better-you-need-to-develop-a-habit-of-writing--1603298302647x354487348376371200 | <a href="https://web.archive.org/web/*/https://bookpub.club/post/to-write-better-you-need-to-develop-a-habit-of-writing--1603298302647x354487348376371200">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://bookpub.club/post/to-write-better-you-need-to-develop-a-habit-of-writing--1603298302647x354487348376371200</link>
            <guid isPermaLink="false">hacker-news-small-sites-24849485</guid>
            <pubDate>Wed, 21 Oct 2020 16:39:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[LinkedIn will scan the browser to identify extensions]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24849282">thread link</a>) | @youeseh
<br/>
October 21, 2020 | https://prophitt.me/a-look-at-how-linkedin-exfiltrates-extension-data-from-your-browser | <a href="https://web.archive.org/web/*/https://prophitt.me/a-look-at-how-linkedin-exfiltrates-extension-data-from-your-browser">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><header>
  <a href="https://prophitt.me/">Corey Prophitt's Website</a>
  <hr>
  <ul>
    <li><a href="mailto:corey@prophitt.me">Email</a></li>
    <li><a href="https://code.prophitt.me/corey" target="_blank" rel="noreferrer noopener">Code</a></li>
  </ul>
</header><section>
      <p>June 22, 2019</p>
      
      <hr>
      <p>A look at how LinkedIn exfiltrates browser extension data from your browser.</p>
    </section>

    <section>
      
      <hr>
      <p>How would you feel if you opened a program and the program started to check your file system to see what other programs you
      had installed? You would probably feel the software was overstepping. This is essentially what LinkedIn does when you visit their
      website. LinkedIn will scan your local browser files in an attempt to identify a number of different browser extensions you may
      have installed. The data collected by LinkedIn is then exfiltrated from the browser.</p>
      <p>
        This whole adventure started when I was browsing LinkedIn and happened to have my browser console open. While on my LinkedIn
        profile I noticed a large number of 404 errors and as a developer it piqued my interest.
        <a href="https://prophitt.me/assets/images/spying-xs.gif" target="blank" rel="nopener noreferrer">
          <img src="https://prophitt.me/assets/images/spying-xs.gif" alt="LinkedIn's website making local host web requests.">
        </a>
      </p>
      <p>What really piqued my curiosity was the fact all of these failed web requests were for <strong>chrome-extension://</strong>
      resources. After inspecting a few of the extension IDs in the resources I began to suspect LinkedIn was attempting to determine
      if I had certain extensions installed by executing local web requests to the browser itself.</p>
      <p>I spent some time toying around with my browser and LinkedIn's assets. Note, reverse engineering LinkedIn's source code is
      apparently <a href="https://www.linkedin.com/legal/user-agreement#dos" target="_blank" rel="noopener noreferrer">against their terms of service</a>. If I was
      LinkedIn I would probably not want people figuring out how I spy on them either.</p>
      <p>After poking around and doing some investigation I found an interesting object in one of LinkedIn's local storage values.</p>
    </section>

    <section>
      
      <hr>

      <p>One of LinkedIn's local storage keys is <strong>C_C_M</strong>. The value itself is a base64 encoded string (which isn't too
      abnormal). However, if you decode the string you will see a large JSON blob that seems to be encoded with unicode code points (not
      human readable).</p>
      <a href="https://prophitt.me/assets/images/unicode-resized.png" target="blank" rel="nopener noreferrer">
        <img src="https://prophitt.me/assets/images/unicode-resized.png" alt="LinkedIn's local storage printed to a browser console encoded with unicode.">
      </a>
      <p>I am not too sure how or why they encoded it that way, but it seems to me like it was in an attempt to obfuscate the data. The
      encoding is easy enough to reverse, simply parse the JSON. You can do it yourself with the following snippet:</p>
      <p><code>JSON.parse(atob(localStorage.getItem("C_C_M")));</code></p>
      <p>Doing so will display a large JavaScript object with some interesting data in it. Note, it appears the data held in this JSON
      blob is personalized to some degree. In other words, my JSON blob may be larger or smaller than yours. I am unsure which heuristic
      LinkedIn uses to determine which extensions to scan for but they must be using some. Curious what the complete JSON object looks
      like? <a href="https://prophitt.me/assets/files/linkedin-extension.json" target="_blank">Here's mine</a>.</p>
      <a href="https://prophitt.me/assets/images/localstorage.png" target="blank" rel="nopener noreferrer">
        <img src="https://prophitt.me/assets/images/localstorage.png" alt="LinkedIn's local storage printed to the console as a javascript object.">
      </a>
    </section>

    <section>
      
      <hr>
      <p>After examing the JSON file it became pretty clear what was going on. LinkedIn is using two different methods to determine if
      you have an extension installed.</p>
      <ol>
        <li>Content Changed</li>
        <li>Web Accessible Resources</li>
      </ol>
      <p>The first method is the simplest. LinkedIn simply looks for certain content on the page they know doesn't belong there. For
      instance, if they find a div with the ID <strong>email-hunter</strong>, they know you have the Email Hunter extension installed
      (and now your account is probably restricted, or at least on a blacklist).</p>
      <p>The second method is a lot more interesting to me. When building an extension you can specify web accessible resources. These
      resources are typically used via a content script to build a custom interface. However, there's a bit of a gotcha. If the content
      script can make a request for the web accessible resource, so can the underlying website. LinkedIn abuses this fact and sprays
      web requests to your local browser in attempt to find extensions.</p>
      <p>I built a simple extension to automatically parse the extension file and display extensions LinkedIn is looking for.
        <a href="https://code.prophitt.me/corey/nefarious-linkedin" target="_blank" rel="noopener noreferrer">Check it out here</a>.
      </p>
    </section>

    <section>
      
      <hr>
      <p>So, as a developer of an extension what can you do about this? I recommend not using
      <a href="https://developer.chrome.com/extensions/manifest/web_accessible_resources" target="_blank" rel="noopener noreferrer">web accessible resources</a>. Out
      of all extensions LinkedIn finds, a majority of them are due to web accessible resources.</p>
      <p>I would recommend not modifying or injecting user interface features into the underlying page. Alternatively, I would use a
      browser action and communicate with the content page through messaging passing.</p>
    </section>
  


</div>]]>
            </description>
            <link>https://prophitt.me/a-look-at-how-linkedin-exfiltrates-extension-data-from-your-browser</link>
            <guid isPermaLink="false">hacker-news-small-sites-24849282</guid>
            <pubDate>Wed, 21 Oct 2020 16:19:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gardening Your Twitter]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 47 (<a href="https://news.ycombinator.com/item?id=24848782">thread link</a>) | @tosh
<br/>
October 21, 2020 | https://steipete.com/posts/growing-your-twitter-followers/ | <a href="https://web.archive.org/web/*/https://steipete.com/posts/growing-your-twitter-followers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <p><img src="https://d33wubrfki0l68.cloudfront.net/f99dfb0a1f04e2577108899ca48dca03f49f0d67/039b8/assets/img/2020/make-twitter-work/gardening-growing.jpg"></p><p>I‚Äôve been using Twitter for almost 12 years now. It can be challenging to navigate your timeline, so today I‚Äôm sharing some tips to keep it fun.</p><p>This is the first part of my Twitter series about Gardening Your Twitter. Don‚Äôt miss out on the second part, where you can learn how to best <a href="https://steipete.com/posts/curating-your-twitter-timeline/">curate your timeline</a> and manage who to follow and unfollow.</p><h2 id="your-online-persona">Your Online Persona</h2><p>There are many strategies for online personas, but I can only share what works well for me. I follow people to cover specific areas/topics and also for their commentary/personality. In a way, Twitter is a newsfeed where the comments are presented before the content, and you pick people for both content and comments.</p><p>I‚Äôm known for talking about iOS and bootstrapping a company, and I have a pretty sharp tongue on tech news. I used to keep politics out of my feed, but since 2016, I do sprinkle in topics that are important to me ‚Äî from US politics to climate change and LGBTQIA rights.</p><p>There will always be people who complain that XY topic shouldn‚Äôt be on Twitter, but in the end, it‚Äôs <em>your choice</em> what you talk about and it‚Äôs their choice to follow you.</p><p>I‚Äôm openly gay on Twitter, but only in the last few years have I also started talking about that. Being open does allow me to add a unique perspective to some content, and it adds more complexity to my persona. I almost never share pictures or private content though; <a href="https://www.instagram.com/sportg33k/">that stuff is for Instagram</a>.</p><p>Whatever you go with, be authentic. I don‚Äôt share everything on Twitter, but what I do share is honest and is usually done with passion. Additionally, it can be interesting or funny. I do not share content for money or for favors, rather I only share things if I find them interesting.</p><h3 id="your-avatar">Your Avatar</h3><p>Pick an avatar you like and stick with it. I recommend a real face and not a sketch or something more abstract, as it‚Äôll help folks identify you at conferences or events. Make sure you use the same picture and use it everywhere (GitHub, Gravatar, email, etc.) so that you have one universal online identity. People will scan the picture much faster than your name ‚Äî changing it is usually something folks dislike, and it‚Äôll result in a temporary loss of engagement. You can change it, but I recommend not doing that, or at least doing so only every few years.</p><p>Or, you can be really sneaky and just <a href="https://krausefx.com/blog/continuous-delivery-for-your-profile-picture">remake your picture so it changes slightly every year</a>.</p><h3 id="direct-messages">Direct Messages</h3><p>I highly recommend going into Settings and privacy &gt; Privacy and safety &gt; Direct Messages and enabling ‚ÄúReceive messages from anyone.‚Äù There‚Äôs a lot of great commentary from people that I received via DM since they‚Äôre not comfortable replying publicly. There are the occasional odd messages (and inappropriate offers), but if you‚Äôre a cis white male, you likely are good. Minority groups might want to reconsider this setting or at least enable the Quality filter.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/863f9afff497ec9faee70ce7485680fbc999fde1/a1446/assets/img/2020/make-twitter-work/settings.png" alt="Twitter Settings"></p><p>If in doubt, I suggest you experiment with this ‚Äî the settings are easy to change if it turns out to be a bad idea.</p><h3 id="multiple-profiles">Multiple Profiles</h3><p>Quite a few of my friends have ‚Äúalt‚Äù accounts for the hot takes or for talking with friends. If you work at a Big Corp, you might be required to filter what you say, and having an alt can be a solution. In general, I don‚Äôt recommend making an alt account, as it‚Äôs simply too much work to maintain multiple accounts. Just tweet out your hot takes and attract the right followers on your main account.</p><h2 id="extending-reach">Extending Reach</h2><p>The more active followers you have on Twitter, the more fun it becomes. There‚Äôs no hack or shortcut for gaining followers, but there are various things you can do that can help you steadily grow your audience.</p><h3 id="blog-posts">Blog Posts</h3><p>Twitter is a great indicator for topics that people find interesting ‚Äî <a href="https://twitter.com/steipete/status/1297956386836566016">I often get my best ideas for blog posts out of Twitter conversations</a>, and I also already have half the content there. Twitter is great for inspiration and to learn, but it‚Äôs often hard to read and follow conversations. Go the extra mile and convert some of these interactions to blog posts. This will greatly extend your reach, and in turn, it‚Äôll attract new followers who find your content interesting.</p><h3 id="conference-talks">Conference Talks</h3><p><a href="https://steipete.tv/">Speaking at conferences</a> is a great way to meet new people and extend your social circle. I often meet folks at conferences, and either we connect on Twitter or we find out that we already know each other on there! Either way ‚Äî this will increase the bond and will make it more likely that people reach out to you. <strong>Conferences are work, but they are so worth it.</strong></p><p>Bonus: <a href="https://pspdfkit.com/blog/2018/binary-frameworks-swift/">Convert your conference talk to a blog post</a>. Very few people will actually watch a recording, so via recycling and reshaping content you already have, you can extend your reach again.</p><p>If you plan on starting to speak, create a website where you list what topics you can talk about and your bio. I‚Äôm using <a href="https://github.com/steipete/speaking">a simple GitHub repo</a> that has been proven extremely useful for me to track past events, attract new speaking gigs, and help conference organizers with getting the information they need to announce me.</p><h3 id="engage-with-your-audience">Engage with Your Audience</h3><p>I try to reply to almost everyone who interacts with me on Twitter. This doesn‚Äôt take much time, and sometimes I just reply with an emoji, but taking time to engage shows your audience you care, and they‚Äôre much more likely to interact with your content again if they know that it‚Äôs not a one-way street. Same goes for your feed ‚Äî don‚Äôt just read, reply. This can range from helping others with questions/problems to just posting a ‚Äúme too‚Äù retweet. Sometimes I get content in my feed via a retweet, and by interacting with that, I get a new follower.</p><h3 id="tracking-statistics">Tracking Statistics</h3><p>Be consistent. You won‚Äôt grow an audience overnight. Make Twitter a daily thing. Share content. Be present ‚Äî and you‚Äôll grow your audience every day.</p><p><a href="https://analytics.twitter.com/">Twitter Analytics</a> is great to understand which tweets work. To track long-term performance, I‚Äôm using <a href="http://birdbrainapp.com/">Birdbrain</a>. It‚Äôs one of the oldest apps on my phone, so I have data since 2014. Interestingly, my follower count has been growing pretty much linearly:</p><p><img src="https://d33wubrfki0l68.cloudfront.net/b753fb6d7ed16ff2e572edcc90d8143bf73653eb/38f85/assets/img/2020/make-twitter-work/follower.png" alt="Birdbrain Follower Count of @steipete" width="50%"></p><h2 id="tweets-that-work">Tweets that Work</h2><p>I do share a lot of news articles. I often just quote something interesting from the news if it doesn‚Äôt need strong commentary, but the inclusion of a pull quote helps show that it‚Äôs worth reading.</p><p>The tweets that are the most engaging, however, usually are original content, particularly in context with your audience and topics of interest. Here are some of my top performing tweets from the last few months, with about 80K‚Äì450K impressions each. Sometimes it‚Äôs the <a href="https://twitter.com/steipete/status/1310331623729229827">ridiculous tweets that explode</a>, and sometimes you <a href="https://twitter.com/steipete/status/1306884214252613632?s=20">don‚Äôt need words</a>. It also can be news commentary if the comment <a href="https://twitter.com/steipete/status/1288151223028322304">really nails it</a> or just <a href="https://twitter.com/steipete/status/1281547449660825601">really fits</a>.</p><h3 id="using-threads">Using Threads</h3><p>Lately I‚Äôve been using more and more threads to connect tweets over time ‚Äî this has been proven to be really great, as it immediately gives people context, they can read more, and the official Twitter client also usually shows 2‚Äì3 tweets in a thread, giving you more ‚Äúspace‚Äù in the timeline. Here‚Äôs an example:</p><div><blockquote><div lang="en" dir="ltr"><p>Been clicking around for a minute with Apple's new Fruta SwiftUI sample. </p><p>Things jump around wildly, fav' doesn't work, and it crashes once you open a second window. I understand it's b1, but looking at how SwiftUI went last year I doubt this will all be fixed. <a href="https://t.co/zGRRYswRde">pic.twitter.com/zGRRYswRde</a></p></div>‚Äî Peter Steinberger (@steipete) <a href="https://twitter.com/steipete/status/1277623561604214784?ref_src=twsrc%5Etfw">June 29, 2020</a></blockquote></div><h2 id="curating-your-timeline">Curating Your Timeline</h2><p>Who you follow defines your Twitter experience. Learn how you can curate your Twitter timeline to keep it fun and interesting by reading <a href="https://steipete.com/posts/curating-your-twitter-timeline/">the second part of this series</a>.</p><h2 id="addendum-building-personal-brands-for-introverts">Addendum: Building Personal Brands for Introverts</h2><p>I gave a talk at UIKonf in Berlin in 2018 about Building Personal Brands for Introverts. This talk is still highly relevant and goes even deeper into defining your online identity. Check it out if you want to know more.</p><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/0c6izSzP-KQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div>]]>
            </description>
            <link>https://steipete.com/posts/growing-your-twitter-followers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24848782</guid>
            <pubDate>Wed, 21 Oct 2020 15:28:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fear and Loathing in YAML]]>
            </title>
            <description>
<![CDATA[
Score 60 | Comments 83 (<a href="https://news.ycombinator.com/item?id=24848511">thread link</a>) | @mooreds
<br/>
October 21, 2020 | https://chrisshort.net/fear-and-loathing-in-yaml/ | <a href="https://web.archive.org/web/*/https://chrisshort.net/fear-and-loathing-in-yaml/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main"><div><div><article role="main"><blockquote><p>This post was originally written as the introduction to <a href="https://devopsish.com/188/">DevOps‚Äôish 188</a>, has been modified based on feedback, and deemed worthy to share as its own blog post.</p><p>Photo by <a href="https://twitter.com/wocintechchat">Christina Morillo</a> from <a href="https://www.pexels.com/photo/adult-computer-eyewear-female-1181325/">Pexels</a></p></blockquote><p><a href="https://twitter.com/brunoborges/status/1315230767207784450">We kinda went down a rabbit hole</a> the other day when I suggested folks check out <a href="https://dev.to/vikcodes/yq-a-command-line-tool-that-will-help-you-handle-your-yaml-resources-better-8j9"><em>yq</em></a>, ‚ÄúThe aim of the project is to be the jq or sed of yaml files.‚Äù First, there‚Äôs nothing wrong with this project. I like it, I find the tool useful, and that‚Äôs that. But the great debate started over our lord and savior, <a href="https://yaml.org/">YAML</a>. Yeah, I know, XML vs. JSON vs. YAML vs. TOML vs. the next thing is a tired and old debate.</p><p>Let me level set here. I routinely joke about how I‚Äôm a ‚ÄúCalendar Driven YAML Engineer‚Äù and have been for years on <a href="https://openshift.tv/">openshift.tv</a>. But I‚Äôm not too fond of YAML. Let me tell you a story‚Ä¶</p><p>In 2012, I worked at McClatchy Interactive (before the really dark times) and enjoyed the systems and security work I was doing. We had our machine creation down to a finite science. Bare metal spun up, you punched the MAC address into a database file, and off the machine went to get all the needed packaging and code to run as its defined purpose in our infrastructure.</p><p><a href="https://en.wikipedia.org/wiki/CFEngine">CFEngine</a> provisioned the machine accordingly based on purpose and positioned it in the network ready for code deployment. DevOps was something the company was embracing at the time. So instead of using the existing CFEngine infrastructure, the DevOps tandem at the time was using <a href="https://en.wikipedia.org/wiki/Puppet_(company)">Puppet</a> for code deploys. This system worked fine until it didn‚Äôt. There were clear lines between infrastructure (typical IT in the datacenter) and software deployment and configuration (developers). In our case, DevOps represented the development team more so than the Operations team. Sound familiar?</p><p>But, as you can imagine, even with all the automation in place, it was still a throw over the wall kind of scenario. When Puppet needed system packages installed because of modifications to the codebase (requiring a newer version of Perl, for example) or new services coming online using different OS packages, Puppet now had to do the task CFEngine was doing; systems management. The idea was to build an overarching WebOps team that was cross-functional, spirited, and deeply technical. The first edict laid down to the team by the DevOps lead was, ‚Äúread the <a href="https://yaml.org/spec/1.2/spec.html">YAML spec</a>.‚Äù We were all jumping into the Puppet pool to help integrate our processes and procedures better.</p><p>‚ÄúUgh‚Ä¶‚Äù I thought to myself. ‚ÄúI have to read this horribly written spec.‚Äù It was not an RFC, which I am fond of reading, but something about the YAML spec made me sad and frustrated. Syntax <em>really</em> mattered. Whitespace <em>really</em> mattered. My experiences have taught me that rote memorization and getting humans to see the absence of something were incredibly difficult tasks. These are things that most hackers take advantage of when infiltrating systems. Humans aren‚Äôt as good as computers at finding the absence of something or memorizing things. I was not too fond of this non-markup language for these reasons.</p><p>It irked me that the YAML creators laid out goal #1 as ‚ÄúYAML is easily readable by humans.‚Äù It is human-readable because you see the human-readable words in the scalars and structures, but there was something off-putting about YAML. It was a markup language claiming not to be a markup language. I held the firm belief that markup languages are supposed to make things simpler for humans, not harder (XML is the antithesis of markup languages, in my opinion).</p><p>Here I was, relatively fresh to the DevOps game, learning some core developer concepts to understand a markup language, the crux of which was two Achilles heels. I also didn‚Äôt like how big, bulky, and cumbersome Puppet was to work with. But, here I was thrust headfirst into this world. Might as well make the best of it. I‚Äôve since embraced YAML, but it doesn‚Äôt mean I‚Äôm writing my notes in YAML format.</p><div><div id="sib-form-container"><div id="error-message"><div><svg viewBox="0 0 512 512"><path d="M256 40c118.621.0 216 96.075 216 216 0 119.291-96.61 216-216 216-119.244.0-216-96.562-216-216 0-119.203 96.602-216 216-216m0-32C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm-11.49 120h22.979c6.823.0 12.274 5.682 11.99 12.5l-7 168c-.268 6.428-5.556 11.5-11.99 11.5h-8.979c-6.433.0-11.722-5.073-11.99-11.5l-7-168c-.283-6.818 5.167-12.5 11.99-12.5zM256 340c-15.464.0-28 12.536-28 28s12.536 28 28 28 28-12.536 28-28-12.536-28-28-28z"></path></svg><p><span>Your subscription could not be saved. Please try again.</span></p></div></div><div id="success-message"><div><svg viewBox="0 0 512 512"><path d="M256 8C119.033 8 8 119.033 8 256s111.033 248 248 248 248-111.033 248-248S392.967 8 256 8zm0 464c-118.664.0-216-96.055-216-216 0-118.663 96.055-216 216-216 118.664.0 216 96.055 216 216 0 118.663-96.055 216-216 216zm141.63-274.961L217.15 376.071c-4.705 4.667-12.303 4.637-16.97-.068l-85.878-86.572c-4.667-4.705-4.637-12.303.068-16.97l8.52-8.451c4.705-4.667 12.303-4.637 16.97.068l68.976 69.533 163.441-162.13c4.705-4.667 12.303-4.637 16.97.068l8.451 8.52c4.668 4.705 4.637 12.303-.068 16.97z"></path></svg><p><span>Your subscription has been successful.</span></p></div></div></div></div><p>Close to ten years later, I see YAML in the same somewhat offputting light. It‚Äôs not friendly to new people in the same sense <a href="https://git-scm.com/">git</a> isn‚Äôt. <a href="https://www.kubernetes.dev/">Kubernetes</a> has almost abused YAML to the point that it has become a punchline. And we‚Äôve stuck ourselves with it for a long time to come too. If Kubernetes is the platform of the future, that means we‚Äôll be using a spec written in 2009 well into the 2030s (and likely beyond).</p><p>I hope that a drop in replacement is possible. The fact that we need tools like <a href="https://github.com/mikefarah/yq/">yq</a> does show that there is some work to be done when it comes to wrangling the YAML beast at scale. In 2009, when the latest version of the YAML spec was written, no one thought of applying pod security policies to massive Kubernetes deployments spread out across data centers the world over. Something better will come along and I hope adopting it isn‚Äôt as painful as adopting YAML is.</p><p>Remember, comparing things relatively to like something (YAML vs. XML or YAML vs. JSON) completely throws out the beginner‚Äôs journey. Start from the newb and go forward from there. YAML doesn‚Äôt. Git doesn‚Äôt. Incrementally, YAML is better than XML but, it sucks compared to something like HTML or Markdown (which I can teach to execs and children alike). Yes, balancing machine and human readability is hard. The compromises suck, but, at some point, there‚Äôs enough compute to run a process to take in something 100% human-readable and make it 100% machine-readable. In the same sense that compute has become so readily available that we gzip and encrypt almost all HTTP traffic today, I hope we can do the same with systems configuration languages. Move the complexity from the human to code. Computers are better at remembering things and syntax-semantics than humans could ever hope to be.</p><p>There‚Äôs always a happy medium between human and machine readability. However, I‚Äôd much rather see a human first, <a href="https://en.wikipedia.org/wiki/Pareto_efficiency">80-20</a> approach here where entry-level skills can solve 80% of the markup language‚Äôs use cases. That‚Äôs the true nirvana, in my opinion. There will always be complexity and a need to understand the tool you‚Äôre using. But, YAML gives us an example that there can and should be better things.</p><hr><h4>See also</h4><ul><li><a href="https://chrisshort.net/2019-learnings-2020-expectations/">2019 Learnings, 2020 Expectations</a></li><li><a href="https://chrisshort.net/docker-inc-is-dead/">Docker, Inc is Dead</a></li><li><a href="https://chrisshort.net/live-streaming-on-openshift.tv-and-some-lessons-learned/">Live streaming on openshift.tv and some lessons learned</a></li></ul></article></div></div></div></div>]]>
            </description>
            <link>https://chrisshort.net/fear-and-loathing-in-yaml/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24848511</guid>
            <pubDate>Wed, 21 Oct 2020 15:01:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Attempts to make Python fast]]>
            </title>
            <description>
<![CDATA[
Score 128 | Comments 145 (<a href="https://news.ycombinator.com/item?id=24848318">thread link</a>) | @Queue29
<br/>
October 21, 2020 | https://sethops1.net/post/attempts-to-make-python-fast/ | <a href="https://web.archive.org/web/*/https://sethops1.net/post/attempts-to-make-python-fast/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemprop="articleBody" id="content">
        <p>Posted on Hacker News there was an <a href="https://github.com/markshannon/faster-cpython/blob/master/plan.md">Implementation Plan</a>
for making CPython (the official Python implementation) faster. The author claims a 5x speedup
is possible for the low cost of <a href="https://github.com/markshannon/faster-cpython/blob/master/funding.md">$2 million</a> USD.</p>
<p>The four step plan includes</p>
<ul>
<li>creating an adaptive interpreter</li>
<li>improvements to internal types</li>
<li>creating a JIT compiler</li>
<li>extending the JIT compiler</li>
</ul>
<p>We have witnessed other attempts at making Python fast, each achieving their own degree
of success in terms of performance and compatibility. For posterity I started keeping a
list of them here, in no particular order.</p>
<p>(update: added more projects from HN comments - ones that make at least some claim about
performance, thanks!)</p>
<h3 id="pyston">Pyston</h3>
<blockquote>
<p><em>Pyston is a performance-oriented Python implementation built using LLVM and modern JIT techniques.</em></p>
</blockquote>
<ul>
<li><a href="https://github.com/pyston/pyston">https://github.com/pyston/pyston</a></li>
</ul>
<h3 id="unladen-swallow">Unladen Swallow</h3>
<blockquote>
<p><em>An optimization branch of CPython, intended to be fully compatible and significantly faster.</em></p>
</blockquote>
<ul>
<li><a href="https://code.google.com/archive/p/unladen-swallow/">https://code.google.com/archive/p/unladen-swallow/</a></li>
</ul>
<h3 id="stackless-python">Stackless Python</h3>
<blockquote>
<p><em>Stackless Python is an enhanced version of the Python programming language. It allows programmers to reap the benefits of thread-based programming without the performance and complexity problems associated with conventional threads.</em></p>
</blockquote>
<ul>
<li><a href="https://github.com/stackless-dev/stackless">https://github.com/stackless-dev/stackless</a></li>
</ul>
<h3 id="pypy">PyPy</h3>
<blockquote>
<p><em>A fast, compliant alternative implementation of Python.</em></p>
</blockquote>
<ul>
<li><a href="https://www.pypy.org/">https://www.pypy.org/</a></li>
</ul>
<h3 id="jython">Jython</h3>
<blockquote>
<p><em>Jython is approximately as fast as CPython‚Äìsometimes faster, sometimes slower. Because most JVMs‚Äìcertainly the fastest ones‚Äìdo long running, hot code will run faster over time.</em></p>
</blockquote>
<ul>
<li><a href="https://www.jython.org/">https://www.jython.org/</a></li>
</ul>
<h3 id="hotpy">HotPy</h3>
<blockquote>
<p><em>The HotPy virtual machine is a high-performance virtual machine for Python.</em></p>
</blockquote>
<ul>
<li><a href="https://code.google.com/archive/p/hotpy/">https://code.google.com/archive/p/hotpy/</a></li>
</ul>
<h3 id="iron-python">Iron Python</h3>
<blockquote>
<p><em>Performance is comparable to CPython - much faster for some things ‚Ä¶ but slower for other things.</em></p>
</blockquote>
<ul>
<li><a href="https://wiki.python.org/moin/IronPython">https://wiki.python.org/moin/IronPython</a></li>
</ul>
<h3 id="psyco">Psyco</h3>
<blockquote>
<p><em>Psyco is a Python extension module which can greatly speed up the execution of any Python code.</em></p>
</blockquote>
<ul>
<li><a href="http://psyco.sourceforge.net/">http://psyco.sourceforge.net/</a></li>
</ul>
<h3 id="2c-python">2c-python</h3>
<blockquote>
<p><em>Using the generated binary code gives a speed boost from 2 to 4.5 times.</em></p>
</blockquote>
<ul>
<li><a href="https://github.com/DarrenRainey/2c-python">https://github.com/DarrenRainey/2c-python</a></li>
</ul>
<h3 id="cython">Cython</h3>
<blockquote>
<p><em>Easily tune readable Python code into plain C performance by adding static type declarations.</em></p>
</blockquote>
<ul>
<li><a href="https://cython.org/">https://cython.org/</a></li>
</ul>
<h3 id="nuitka">Nuitka</h3>
<blockquote>
<p><em>Nuitka is more than 2 times faster than CPython ‚Ä¶</em></p>
</blockquote>
<ul>
<li><a href="http://nuitka.net/pages/overview.html">http://nuitka.net/pages/overview.html</a></li>
</ul>
<h3 id="pyc">Pyc</h3>
<blockquote>
<p><em>Pyc is a python compiler intended for high performance computing and programming-in-the-large</em></p>
</blockquote>
<ul>
<li><a href="https://sourceforge.net/projects/pyc/">https://sourceforge.net/projects/pyc/</a></li>
</ul>
<h3 id="shedskin">Shedskin</h3>
<blockquote>
<p><em>For a set of 75 non-trivial programs ‚Ä¶, measurements show a typical speedup of 2-200 times over CPython.</em></p>
</blockquote>
<ul>
<li><a href="https://code.google.com/archive/p/shedskin/">https://code.google.com/archive/p/shedskin/</a></li>
</ul>
<h3 id="numba">Numba</h3>
<blockquote>
<p><em>Numba makes Python code fast</em></p>
</blockquote>
<ul>
<li><a href="http://numba.pydata.org/">http://numba.pydata.org/</a></li>
</ul>
<h3 id="parakeet">Parakeet</h3>
<blockquote>
<p><em>Parakeet was a runtime accelerator for an array-oriented subset of Python.</em></p>
</blockquote>
<ul>
<li><a href="https://pypi.org/project/parakeet/">https://pypi.org/project/parakeet/</a></li>
</ul>
<h3 id="cannoli">Cannoli</h3>
<blockquote>
<p><em>Cannoli is a compiler for a subset of Python 3.6.5 and is designed to evaluate the language features of Python that negatively impact performance.</em></p>
</blockquote>
<ul>
<li><a href="https://github.com/joncatanio/cannoli">https://github.com/joncatanio/cannoli</a></li>
</ul>
<p>Happy hacking!</p>

      </article></div>]]>
            </description>
            <link>https://sethops1.net/post/attempts-to-make-python-fast/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24848318</guid>
            <pubDate>Wed, 21 Oct 2020 14:40:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Does Julia Work So Well?]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 79 (<a href="https://news.ycombinator.com/item?id=24846033">thread link</a>) | @Tomte
<br/>
October 21, 2020 | https://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia | <a href="https://web.archive.org/web/*/https://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div tabindex="-1" id="notebook">
    <div id="notebook-container">

<div>
<div>
<div>
<p>There is an obvious reason to choose Julia:</p>
<blockquote><p>it's faster than other scripting languages, allowing you to have the rapid development of Python/MATLAB/R while producing code that is as fast as C/Fortran</p>
</blockquote>
<p>Newcomers to Julia might be a little wary of that statement.</p>
<ol>
<li>Why not just make other scripting languages faster? If Julia can do it, why can't others? </li>
<li>How do you interpert Julia benchmarks to confirm this? (This is surprisingly difficult for many!)</li>
<li>That sounds like it violates the No-Free-Lunch heuristic. Is there really nothing lost?</li>
</ol>
<p>Many people believe Julia is fast <strong>because it is Just-In-Time (JIT) compiled</strong> (i.e. every statement is run using compiled functions which are either compiled right before they are used, or cached compilations from before). This leads to questions about what Julia gives over JIT'd implementations of Python/R (and MATLAB by default uses a JIT). These JIT compilers have been optimized for far longer than Julia, so why should we be crazy and believe that somehow Julia quickly out-optimized all of them? However, that is a complete misunderstanding of Julia. What I want show, in a very visual way, is that Julia is fast because of its design decisions. The core design decision, <strong>type-stability through specialization via multiple-dispatch</strong> is what allows Julia to be very easy for a compiler to make into efficient code, but also allow the code to be very concise and "look like a scripting language". This will lead to some very clear performance gains.</p>
<p>But what we will see in this example is that Julia does not always act like other scripting languages. There are some "lunches lost" that we will have to understand. Understanding how this design decision effects the way you must code is crucial to producing efficient Julia code.</p>
<p>To see the difference, we only need to go as far as basic math.</p>

</div>
</div>
</div>
<div>
<div>
<div>
<h2 id="Arithmetic-in-Julia">Arithmetic in Julia<a href="#Arithmetic-in-Julia">¬∂</a></h2><p>In general, math in Julia looks the same as in other scripting languages. One detail to note is that the numbers are "true numbers", as in a <code>Float64</code> is truly the same thing as a 64-bit floating point number or a "double" in C. A <code>Vector{Float64}</code> is the same memory layout as an array of doubles in C, both making interop with C easy (indeed, in some sense "Julia is a layer on top of C") and it leads to high performance (the same is true for NumPy arrays).</p>
<p>Some math in Julia:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[1]:</p>
<div>
    <div>
<div><pre><span></span><span>a</span> <span>=</span> <span>2</span><span>+</span><span>2</span>
<span>b</span> <span>=</span> <span>a</span><span>/</span><span>3</span>
<span>c</span> <span>=</span> <span>a√∑3</span> <span>#\div tab completion, means integer division</span>
<span>d</span> <span>=</span> <span>4</span><span>*</span><span>5</span>
<span>println</span><span>([</span><span>a</span><span>;</span><span>b</span><span>;</span><span>c</span><span>;</span><span>d</span><span>])</span>
</pre></div>

</div>
</div>
</div>

<div>
<div>


<div>




<div>
<pre>[4.0, 1.33333, 1.0, 20.0]
</pre>
</div>
</div>

</div>
</div>

</div>
<div>
<div>
<p>Note here that I showed off Julia's unicode tab completion. Julia allows for unicode characters, and these can be used by tab completing Latex-like statements. Also, multiplication by a number is allowed without the * if followed by a variable. For example, the following is allowed Julia code:</p>
</div>
</div>
<div>
<div>
<p>In&nbsp;[2]:</p>
<div>
    <div>
<div><pre><span></span><span>Œ±</span> <span>=</span> <span>0.5</span>
<span>‚àáf</span><span>(</span><span>u</span><span>)</span> <span>=</span> <span>Œ±</span><span>*</span><span>u</span><span>;</span> <span>‚àáf</span><span>(</span><span>2</span><span>)</span>
<span>sin</span><span>(</span><span>2</span><span>œÄ</span><span>)</span>
</pre></div>

</div>
</div>
</div>



</div>
<div>
<div>
<div>
<h2 id="Type-stability-and-Code-Introspection">Type-stability and Code Introspection<a href="#Type-stability-and-Code-Introspection">¬∂</a></h2><p>Type stability is the idea that there is only 1 possible type which can be outputtted from a method. For example, the reasonable type to output from <code>*(::Float64,::Float64)</code> is a <code>Float64</code>. No matter what you give it, it will spit out a <code>Float64</code>. This right here is multiple-dispatch: the <code>*</code> operator calls a different method depending on the types that it sees. When it sees floats, it will spit out floats. Julia provides code introspection macros so that way you can see what your code actually compiles to. Thus Julia is not just a scripting language, it's a scripting language which lets you deal with assembly! Julia, like many languages, compiles to LLVM (LLVM is a type of portable assembly language).</p>

</div>
</div>
</div>
<div>


<div>
<div>


<div>




<div>
<pre>; Function *
; Location: int.jl:54
define i64 @"julia_*_33751"(i64, i64) {
top:
  %2 = mul i64 %1, %0
  ret i64 %2
}
</pre>
</div>
</div>

</div>
</div>

</div>
<div>
<div>
<p>This output is saying that a floating point multiplication operation is performed and the answer is returned. We can even look at the assembly</p>
</div>
</div>
<div>


<div>
<div>


<div>




<div>
<pre>	.text
; Function * {
; Location: int.jl:54
	imulq	%rsi, %rdi
	movq	%rdi, %rax
	retq
	nopl	(%rax,%rax)
;}
</pre>
</div>
</div>

</div>
</div>

</div>
<div>
<div>
<div>
<p>This shows us that the <code>*</code> function has compiled down to exactly the same operation as what happens in C/Fortran, meaning it achieves the same performance (even though it's defined in Julia). Thus it is possible to not just get "close" to C, but actually get the same C code out. In what cases does this happen?</p>
<p>The interesting thing about Julia is that, asking which cases this happens is not the right question. the right question is, in what cases does the code not compile to something as efficient as C/Fortran? The key here is type-stability. If a function is type-stable, then the compiler can know what the type will be at all points in the function and smartly optimize it to the same assembly as C/Fortran. If it is not type-stable, Julia has to add expensive "boxing" to ensure types are found/known before operations.</p>
<h4 id="This-is-the-key-difference-between-Julia-and-other-scripting-languages">This is the key difference between Julia and other scripting languages<a href="#This-is-the-key-difference-between-Julia-and-other-scripting-languages">¬∂</a></h4><p>The upside is that Julia's functions, when type stable, are essentially C/Fortran functions. Thus <code>^</code> (exponentiation) is fast. However, <code>^(::Int64,::Int64)</code> is type-stable, so what type should it output?</p>

</div>
</div>
</div>


<div>
<div>
<p>Here we get an error. In order to guarantee to the compiler that <code>^</code> will give an Int64 back, it has to throw an error. If you do this in MATLAB, Python, or R, it will not throw an error. That is because those languages do not have their entire language built around type stability.</p>
</div>
</div>
<div>
<div>
<p>What happens when we don't have type stability? Let's inspect this code:</p>
</div>
</div>
<div>


<div>
<div>


<div>




<div>
<pre>	.text
; Function ^ {
; Location: intfuncs.jl:220
	pushq	%rax
	movabsq	$power_by_squaring, %rax
	callq	*%rax
	popq	%rcx
	retq
	nop
;}
</pre>
</div>
</div>

</div>
</div>

</div>
<div>
<div>
<p>Now let's define our own exponentiation on integers. Let's make it "safe" like the form seen in other scripting languages:</p>
</div>
</div>
<div>
<div>
<p>In&nbsp;[8]:</p>
<div>
    <div>
<div><pre><span></span><span>function</span> <span>expo</span><span>(</span><span>x</span><span>,</span><span>y</span><span>)</span>
    <span>if</span> <span>y</span><span>&gt;</span><span>0</span>
        <span>return</span> <span>x</span><span>^</span><span>y</span>
    <span>else</span>
        <span>x</span> <span>=</span> <span>convert</span><span>(</span><span>Float64</span><span>,</span><span>x</span><span>)</span>
        <span>return</span> <span>x</span><span>^</span><span>y</span>
    <span>end</span>
<span>end</span>
</pre></div>

</div>
</div>
</div>

<div>
<div>


<div>

<p>Out[8]:</p>




<div>
<pre>expo (generic function with 1 method)</pre>
</div>

</div>

</div>
</div>

</div>
<div>
<div>
<p>Let's make sure it works:</p>
</div>
</div>
<div>
<div>
<p>In&nbsp;[9]:</p>
<div>
    <div>
<div><pre><span></span><span>println</span><span>(</span><span>expo</span><span>(</span><span>2</span><span>,</span><span>5</span><span>))</span>
<span>expo</span><span>(</span><span>2</span><span>,</span><span>-</span><span>5</span><span>)</span>
</pre></div>

</div>
</div>
</div>



</div>
<div>
<div>
<p>What happens if we inspect this code?</p>
</div>
</div>
<div>


<div>
<div>


<div>




<div>
<pre>	.text
; Function expo {
; Location: In[8]:2
	pushq	%rbx
	movq	%rdi, %rbx
; Function &gt;; {
; Location: operators.jl:286
; Function &lt;; {
; Location: int.jl:49
	testq	%rdx, %rdx
;}}
	jle	L36
; Location: In[8]:3
; Function ^; {
; Location: intfuncs.jl:220
	movabsq	$power_by_squaring, %rax
	movq	%rsi, %rdi
	movq	%rdx, %rsi
	callq	*%rax
;}
	movq	%rax, (%rbx)
	movb	$2, %dl
	xorl	%eax, %eax
	popq	%rbx
	retq
; Location: In[8]:5
; Function convert; {
; Location: number.jl:7
; Function Type; {
; Location: float.jl:60
L36:
	vcvtsi2sdq	%rsi, %xmm0, %xmm0
;}}
; Location: In[8]:6
; Function ^; {
; Location: math.jl:780
; Function Type; {
; Location: float.jl:60
	vcvtsi2sdq	%rdx, %xmm1, %xmm1
	movabsq	$__pow, %rax
;}
	callq	*%rax
;}
	vmovsd	%xmm0, (%rbx)
	movb	$1, %dl
	xorl	%eax, %eax
; Location: In[8]:3
	popq	%rbx
	retq
	nopw	%cs:(%rax,%rax)
;}
</pre>
</div>
</div>

</div>
</div>

</div>
<div>
<div>
<p>That's a very visual demonstration on why Julia achieves such higher performance than other scripting languages by how it uses type inference.</p>
</div>
</div>
<div>
<div>
<div>
<p>Type stability is one crucial feature which separates Julia apart from other scripting languages.  In fact, the core idea of Julia is the following statement:</p>
<h4 id="Multiple-dispatch-allows-for-a-language-to-dispatch-function-calls-onto-type-stable-functions.">Multiple dispatch allows for a language to dispatch function calls onto type-stable functions.<a href="#Multiple-dispatch-allows-for-a-language-to-dispatch-function-calls-onto-type-stable-functions.">¬∂</a></h4><p>This is what Julia is all about, so let's take some time to dig into it.If you have type stability inside of a function (meaning, any function call within the function is also type-stable), then the compiler can know the types of the variables at every step. Therefore it can compile the function with the full amount of optimizations since at this point the code is essentially the same as C/Fortran code.  Multiple-dispatch works into this story because it means that <code>*</code> can be a type-stable function: it just means different things for different inputs. But if the compiler can know the types of <code>a</code> and <code>b</code> before calling <code>*</code>, then it knows which <code>*</code> method to use, and therefore it knows the output type of <code>c=a*b</code>. Thus it can propogate the type information all the way down, knowing all of the types along the way, allowing for full optimiziations. Multiple dispatch allows <code>*</code> to mean the "right thing" every time you use it, almost magically allowing this optimization.</p>
<p>There are a few things we learn from this. For one, in order to achieve this level of optimization, you must have type-stability. This is not featured in the standard libraries of most languages, and was choice that was made to make the experience a little easier for users. Secondly, multiple dispatch was required to be able to specialize the functions for types which allows for the scripting language syntax to be "more explicit than meets the eye". Lastly, a robust type system is required. In order to build the type-unstable exponentiation (which may be needed) we needed functionalities like convert. Thus the language must be designed to be type-stable with multiple dispatch and centered around a robust type system in order to achieve this raw performance while maintaining the syntax/ease-of-use of a scripting language. You can put a JIT on Python, but to really make it Julia, you would have to design it to be Julia.</p>

</div>
</div>
</div>
<div>
<div>
<div>
<h2 id="The-Julia-Benchmarks">The Julia Benchmarks<a href="#The-Julia-Benchmarks">¬∂</a></h2><p>The Julia benchmarks, featured on <a href="http://julialang.org/">the Julia website</a>, test components of the programming language for speed. <strong>This doesn't mean it's testing the fastest implemention</strong>. That is where a major misconception occurs. You'll have an R programmer look at the R code for the Fibonacci calculator and say "wow, that's terrible R code. You're not supposed to use recursion in R. Of course it's slow". However, the Fibonacci problem is designed to test recursion, not the fastest implementation to the the ith Fibonacci number. The other problems are the same way: testing basic components of the langauge to see how fast they are.</p>
<p>Julia is built up using multiple-dispatch on type-stable ‚Ä¶</p></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia">https://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia</a></em></p>]]>
            </description>
            <link>https://ucidatascienceinitiative.github.io/IntroToJulia/Html/WhyJulia</link>
            <guid isPermaLink="false">hacker-news-small-sites-24846033</guid>
            <pubDate>Wed, 21 Oct 2020 09:06:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What ORMs Have Taught Me: Just Learn SQL (2014)]]>
            </title>
            <description>
<![CDATA[
Score 339 | Comments 282 (<a href="https://news.ycombinator.com/item?id=24845300">thread link</a>) | @IA21
<br/>
October 20, 2020 | https://wozniak.ca/blog/2014/08/03/1/ | <a href="https://web.archive.org/web/*/https://wozniak.ca/blog/2014/08/03/1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<p>
I‚Äôve come to the conclusion that, for me, ORMs are more detriment than
benefit. In short, they can be used to nicely augment working with SQL
in a program, but they should not replace it.
</p>

<p>
Some background: For the past 30 months I‚Äôve been working with code
that has to interface with Postgres and to some extent, SQLite. Most
of that has been with <a href="http://sqlalchemy.org/">SQLAlchemy</a> (which I quite like) and <a href="http://hibernate.org/">Hibernate</a>
(which I don‚Äôt). I‚Äôve worked with existing code and data models, as
well as designing my own. Most of the data is event-based storage
(‚Äútimelines‚Äù) with a heavy emphasis on creating reports.
</p>

<p>
Much has been written about the Object/Relational Impedance
Mismatch. It‚Äôs hard to appreciate it until you live it. Neward, in his
<a href="http://blogs.tedneward.com/post/the-vietnam-of-computer-science/">well known essay</a>, lays out many cogent reasons why ORMs turn into
quagmires. In my experience, I‚Äôve had to deal directly with a fair
number of them: <i>entity identity issues</i>, <i>dual-schema problem</i>, <i>data
retrieval mechanism concern</i>, and the <i>partial-object problem</i>. I want to
talk briefly about my experiences with these issues and add one of my
own.
</p>

<div id="outline-container-orge4f50a6">
<h2 id="orge4f50a6">Partial objects, attribute creep, and foreign keys</h2>
<div id="text-orge4f50a6">
<p>
Perhaps the most subversive issue I‚Äôve had with ORMs is ‚Äúattribute
creep‚Äù or ‚Äúwide tables‚Äù, that is, tables that just keep accruing
attributes. As much as I‚Äôd like to avoid it, sometimes it becomes
necessary (although things like <a href="http://www.postgresql.org/docs/9.3/interactive/hstore.html">Postgres‚Äô hstore</a> can help). For
example, a client may be providing you with lots of data that they
want attached to reports based on various business logic. Furthermore,
you don‚Äôt have much insight into this data; you‚Äôre just schlepping it
around.
</p>

<p>
This in and of itself isn‚Äôt a terrible thing in a database. It becomes
a real pain point with an ORM. Specifically, the problem starts to
show up in any query that uses the entity directly to create the
query. You may have a Hibernate query like so early on in the project.
</p>

<pre>query(Foo.class).add(Restriction.eq("x", value))
</pre>

<p>
This may be fine when Foo has five attributes, but becomes a data fire
hose when it has a hundred. This is the equivalent of using <code>SELECT
*</code>, which is usually saying more than what is intended. ORMs, however,
encourage this use and often make writing precise projections as
tedious as they are in SQL. (I have optimized such queries by adding
the appropriate projection and reduced the run time from minutes to
seconds; all the time was spent translating the database row into a
Java object.)
</p>

<p>
Which leads to another bad experience: the pernicious use of foreign
keys. In the ORMs I‚Äôve used, links between classes are represented in
the data model as foreign keys which, if not configured carefully,
result in a large number of joins when retrieving the object. (A
recent count of one such table in my work resulted in over 600
attributes and 14 joins to access a single object, using the preferred
query methodology.)
</p>

<p>
Attribute creep and excessive use of foreign keys shows me is that in
order to use ORMs effectively, you still need to know SQL. My
contention with ORMs is that, if you need to know SQL, just use SQL
since it prevents the need to know how non-SQL gets translated to SQL.
</p>
</div>
</div>

<div id="outline-container-org2d7e33d">
<h2 id="org2d7e33d">Data retrieval</h2>
<div id="text-org2d7e33d">
<p>
Knowing how to write SQL becomes even more important when you attempt
to actually write queries using an ORM. This is especially important
when efficiency is a concern.
</p>

<p>
From what I‚Äôve seen, unless you have a really simple data model (that
is, you never do joins), you will be bending over backwards to figure
out how to get an ORM to generate SQL that runs efficiently. Most of
the time, it‚Äôs more obfuscated than actual SQL.
</p>

<p>
And if you elect to keep the query simple, you end up doing a lot of
work in the code that could be done in the database faster. <a href="https://en.wikipedia.org/wiki/Window_function_%28SQL%29#Window_function">Window
functions</a> are relatively advanced SQL that is painful to write with
ORMs. Not writing them into the query likely means you will be
transferring a lot of extra data from the database to your
application.
</p>

<p>
In these cases, I‚Äôve elected to write queries using a templating
system and describe the tables using the ORM. I get the convenience of
an application level description of the table with direct use of
SQL. It‚Äôs a lot less trouble than anything else I‚Äôve used so far.
</p>
</div>
</div>

<div id="outline-container-org05d550b">
<h2 id="org05d550b">Dual schema dangers</h2>
<div id="text-org05d550b">
<p>
This one seems to be one of those unavoidable redundancies.  If you
try to get rid of it, you only make more problems or add excessive
complexity.
</p>

<p>
The problem is that you end up having a data definition in two places:
the database and your application.  If you keep the definition
entirely in the application, you end up having to write the SQL Data
Definition Language (DDL) with the ORM code, which is the same
complication as writing advanced queries in the ORM.  If you keep it
in the database, you will probably want a representation in the
application for convenience and to prevent too much ‚Äústring typing‚Äù.
</p>

<p>
I much prefer to keep the data definition in the database and read it
into the application.  It doesn‚Äôt solve the problem, but it makes it
more manageable.  I‚Äôve found that reflection techniques to get the
data definition are not worth it and I succumb to managing the
redundancy of data definitons in two places.
</p>

<p>
But the damn migration issue is a real kick in the teeth: changing the
model is no big deal in the application, but a real pain in the
database.  After all, databases are persistent whereas application
data is not.  ORMs simply get in the way here because they don‚Äôt help
manage data migration at all.  I work on the principle that the
database‚Äôs data definitions aren‚Äôt things you should manipulate in the
application.  Instead, manipulate the results of queries.  That is,
the queries are your API to the database.  So instead of thinking
about objects, I think about functions with return types.
</p>

<p>
Thus, one is forced to ask, should you use an ORM for anything but
convenience in making queries?
</p>
</div>
</div>

<div id="outline-container-org7033826">
<h2 id="org7033826">Identities</h2>
<div id="text-org7033826">
<p>
Dealing with entity identities is one of those things that you have to
keep in mind at all times when working with ORMs, forcing you to write
for two systems while only have the expressivity of one.
</p>

<p>
When you have foreign keys, you refer to related identities with an
identifier. In your application, ‚Äúidentifier‚Äù takes on various
meanings, but usually it‚Äôs the memory location (a pointer). In the
database, it‚Äôs the state of the object itself. These two things don‚Äôt
really get along because you can really only use database identifiers
in the database (the ultimate destination of the data you‚Äôre working
with).
</p>

<p>
What this results in is having to manipulate the ORM to get a database
identifier by manually flushing the cache or doing a partial commit to
get the actual database identifier.
</p>

<p>
I can‚Äôt even call this a leaky abstraction because the work ‚Äúleak‚Äù
implies small amounts of the contents escaping relative to the source.
</p>
</div>
</div>

<div id="outline-container-orgddbdda4">
<h2 id="orgddbdda4">Transactions</h2>
<div id="text-orgddbdda4">
<p>
Something that Neward alludes to is the need for developers to handle
transactions. Transactions are dynamically scoped, which is a powerful
but mostly neglected concept in programming languages due to the
confusion they cause if overused.  This leads to a lot of boilerplate
code with exception handlers and a careful consideration of where
transaction boundaries should occur.  It also makes you pass session
objects around to any function/method that might have to communicate
with the database.
</p>

<p>
The concept of a transaction translates poorly to applications due to
their reliance on context based on time. As mentioned, dynamic scoping
is one way to use this in a program, but it is at odds with lexical
scoping, the dominant paradigm. Thus, you must take great care to know
about the ‚Äúwhen‚Äù of a transaction when writing code that works with
databases and can make modularity tricky (‚ÄúHere‚Äôs a useful function
that will only work in certain contexts‚Äù).
</p>

<p>
Where do I see myself going?
</p>

<p>
At this point, I‚Äôm starting to question the wisdom behind the outright
rejection of <a href="http://c2.com/cgi/wiki?StoredProcedures">stored procedures</a>.  It sounds <a href="http://c2.com/cgi/wiki?StoredProceduresAreEvil">heretical</a>, but it may work
for my use cases.  (And hey, with the advent of ‚Äúdevops‚Äù, the divide
between the developer and the database administrator is basically
non-existent.)
</p>

<p>
I‚Äôve found myself thinking about the database as just another data
type that has an API: the queries.  The queries return values of some
type, which are represented as some object in the program. By moving
away from thinking of the objects in my application as something to be
stored in a database (the raison d‚Äô√™tre for ORMs) and instead thinking
of the database as a (large and complex) data type, I‚Äôve found working
with a database from an application to be much simpler. And wondering
why I didn‚Äôt see it earlier.
</p>

<p>
(It should be made clear that I am not claiming this is how all
applications should deal with a database.  All I am saying is that
this fits my use case based on the data I am working with.)
</p>

<p>
Regardless of whether I find that stored procedures aren‚Äôt actually
that evil or whether I keep using templated SQL, I do know one thing:
I won‚Äôt fall into the ‚ÄúORMs make it easy‚Äù trap. They are an acceptable
way to represent a data definition, but a poor way to write queries
and a bad way to store object state. If you‚Äôre using an RDBMS, bite
the bullet and learn SQL.
</p>

<p>
August 03, 2014
</p>
</div>
</div>
</div></div>]]>
            </description>
            <link>https://wozniak.ca/blog/2014/08/03/1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24845300</guid>
            <pubDate>Wed, 21 Oct 2020 06:37:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Unconditioned Texas Garage Lab ‚Äì 1 year later]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24842218">thread link</a>) | @monstermunch
<br/>
October 20, 2020 | https://blog.networkprofile.org/unconditioned-garage-lab-1-year-later/ | <a href="https://web.archive.org/web/*/https://blog.networkprofile.org/unconditioned-garage-lab-1-year-later/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.networkprofile.org/content/images/size/w300/2020/10/2020-10-20-14.21.11.JPG 300w,
                            https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-20-14.21.11.JPG 600w,
                            https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-20-14.21.11.JPG 1000w,
                            https://blog.networkprofile.org/content/images/size/w2000/2020/10/2020-10-20-14.21.11.JPG 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.networkprofile.org/content/images/size/w2000/2020/10/2020-10-20-14.21.11.JPG" alt="Unconditioned Texas Garage Lab - 1 year later (Its all fine)">
            </figure>

            <section>
                <div>
                    <p>A little over a year ago I mounted a rack in my garage, threw a Cisco Switch in there, an APC UPS and my Flight Tracking setup, and a second ESXi host. See below for the original:</p><figure><a href="https://blog.networkprofile.org/mounting-a-network-rack-in-my-garage/"><div><p>Garage Network Rack with 10G Fiber</p><p>This details my 12u rack in my detached garage which has a 10G uplink to my mainnetwork I needed some networking in my detached garage for an access point, my flighttracking setup, 3 x PoE cameras and some future additions, and instead ofrunning multiple Cat6 cables across, I decided to just run‚Ä¶</p><p><img src="https://blog.networkprofile.org/favicon.ico"><span>NetworkProfile.org</span></p></div><p><img src="https://blog.networkprofile.org/content/images/2020/01/ndEotkV-1.jpg"></p></a></figure><p>Its been through the "cold" of a Houston Winter and the heat and humidity of the brutal Houston Summer. To date, nothing has even complained that its hot, in fact the fans are not even spun all the way up on the switch</p><p>The temperatures actually never got above what I see equipment go to in my air conditioned lab in the Houston, which proves that as long air is flowing through the hardware, its fine. No equipment failures, no battery failures, no SSD failures, no HDD failures, no Transceiver failures</p><p>I replaced the UPS batteries last week, but they still reported 1 hour of runtime, and passed the test fine. I now use those batteries in a spare test UPS, they were 7 years old...</p><p>There is however a LOT of dust... but it has not affected the systems. I plan to give them a good clean later on this year</p><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-20-14.17.48.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-20-14.17.48.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-20-14.17.48.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-20-14.17.48.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-20-14.17.48.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-20-14.19.02.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-20-14.19.02.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-20-14.19.02.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-20-14.19.02.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-20-14.19.02.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-20-14.18.50.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-20-14.18.50.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-20-14.18.50.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-20-14.18.50.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-20-14.18.50.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-20-14.17.45.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-20-14.17.45.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-20-14.17.45.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-20-14.17.45.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-20-14.17.45.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>There is a nice dust/dirt stain in the plywood behind the switch. I guess that means a lot of the dust ends up out the switch at least!</p><figure><img src="https://blog.networkprofile.org/content/images/2020/10/2020-10-20-14.17.42.JPG" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/2020-10-20-14.17.42.JPG 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/2020-10-20-14.17.42.JPG 1000w, https://blog.networkprofile.org/content/images/size/w1600/2020/10/2020-10-20-14.17.42.JPG 1600w, https://blog.networkprofile.org/content/images/size/w2400/2020/10/2020-10-20-14.17.42.JPG 2400w" sizes="(min-width: 1200px) 1200px"></figure><p>There isn't much more to say, because nothing of note happened, it all just works fine...</p><p>Apart from a power outage during electrical work I had done, its been running 24/7 with no problem</p><figure><img src="https://blog.networkprofile.org/content/images/2020/10/image-2.png" alt="" srcset="https://blog.networkprofile.org/content/images/size/w600/2020/10/image-2.png 600w, https://blog.networkprofile.org/content/images/size/w1000/2020/10/image-2.png 1000w, https://blog.networkprofile.org/content/images/2020/10/image-2.png 1332w" sizes="(min-width: 1200px) 1200px"></figure><p>Full Equipment Listing for those Wondering</p><ul><li><strong>Cisco Catalyst 2960-S</strong> PoE+ 10G 48 Port Switch (48 x 1G PoE+ and 2 x 10G SFP+) connected back to my main rack with OM3 Fiber @ 10G</li><li><strong>APC SMT1000RM2U</strong> UPS</li><li><strong>Lenovo M73 Tiny</strong> (i5, 16GB DDR3, 800GB Intel DC S3700 SSD, 5TB 2.5" SATA HDD) Running ESXi 6.7</li><li><strong>Raspberry Pi4</strong> running ADSB Flight Tracking duties (FlightAware, ADSDX and FA24)</li></ul><p>If you are wondering if you can run your lab in your garage, go for it. You have my permission </p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.networkprofile.org/unconditioned-garage-lab-1-year-later/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24842218</guid>
            <pubDate>Tue, 20 Oct 2020 20:47:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Julia GPU]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24840972">thread link</a>) | @cdsousa
<br/>
October 20, 2020 | https://notamonadtutorial.com/julia-gpu-98a461d33e21 | <a href="https://web.archive.org/web/*/https://notamonadtutorial.com/julia-gpu-98a461d33e21">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><div><div><div><p><a href="https://medium.com/@federicocarrone?source=post_page-----98a461d33e21--------------------------------" rel="noopener"><img alt="Federico Carrone" src="https://miro.medium.com/fit/c/96/96/2*p2NbnNI4sEc75QvzOZ1gaA.jpeg" width="48" height="48"></a></p></div></div></div></div><figure><div><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/2816/1*KJX3T1Y9T1Cj0aV3m-A22w.png" width="1408" height="528" srcset="https://miro.medium.com/max/552/1*KJX3T1Y9T1Cj0aV3m-A22w.png 276w, https://miro.medium.com/max/1104/1*KJX3T1Y9T1Cj0aV3m-A22w.png 552w, https://miro.medium.com/max/1280/1*KJX3T1Y9T1Cj0aV3m-A22w.png 640w, https://miro.medium.com/max/1400/1*KJX3T1Y9T1Cj0aV3m-A22w.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*KJX3T1Y9T1Cj0aV3m-A22w.png?q=20"></p></div></div></div></figure><p id="58dc">We are living in a time where more and more data is being created every day as well as new techniques and complex algorithms that try to extract the most out of it. As such, CPU capabilities are approaching a bottleneck in their computing power. GPU computing opened its way into a new paradigm for high-performance and parallel computation a long time ago, but it was not until recently that it become massively used for data science.<br>In this interview, <a href="https://twitter.com/maleadt" rel="noopener">Tim Besard</a>, one of the main contributors to the JuliaGPU project, digs into some of the details about GPU computing and the features that make Julia a language suited for such tasks, not only from a performance perspective but also from a user one.</p></div></div></section><section><div><div><p id="38e1"><em>Join the Not a Monad Tutorial Telegram </em><a href="https://t.me/notamonadtutorial" rel="noopener"><em>group</em></a><em> or </em><a href="https://t.me/channel_notamonadtutorial" rel="noopener"><em>channel</em></a><em> to talk about programming, computer science and papers. See you there!</em></p><p id="b3d6"><em>If you are looking for good engineers send me an email to mail@fcarrone.com or you can also reach me via twitter at </em><a href="https://twitter.com/federicocarrone" rel="noopener"><em>@federicocarrone</em></a><em>.</em></p></div></div></section><section><div><div><h2 id="6d6e">Please tell us a bit about yourself. What is your background? what is your current position?</h2><p id="b291">I√¢‚Ç¨‚Ñ¢ve always been interested in systems programming, and after obtaining my CS degree I got the opportunity to start a PhD at Ghent University, Belgium, right when Julia was first released around 2012. The language seemed intriguing, and since I wanted to gain some experience with LLVM, I decided to port some image processing research code from MATLAB and C++ to Julia. The goal was to match performance of the C++ version, but some of its kernels were implemented in CUDA C√¢‚Ç¨¬¶ So obviously Julia needed a GPU back-end!</p><p id="7f14">That was easier said than done, of course, and much of my PhD was about implementing that back-end and (re)structuring the existing Julia compiler to facilitate these additional back-ends. Nowadays I√¢‚Ç¨‚Ñ¢m at Julia Computing, where I still work on everything GPU-related.</p><h2 id="8f7f">What is JuliaGPU? What is the goal of the project?</h2><p id="43d9">JuliaGPU is the name we use to group GPU-related resources in Julia: There√¢‚Ç¨‚Ñ¢s a <a href="https://github.com/JuliaGPU" rel="noopener">GitHub organization</a> where most packages are hosted, a <a href="https://juliagpu.org/" rel="noopener">website</a> to point the way for new users, we have <a href="https://github.com/JuliaGPU/gitlab-ci" rel="noopener">CI infrastructure</a> for JuliaGPU projects, there√¢‚Ç¨‚Ñ¢s a Slack channel and Discourse category, etc.</p><p id="ceaa">The goal of all this is to make it easier to use GPUs for all kinds of users. Current technologies often impose significant barriers to entry: CUDA is fairly tricky to install, C and C++ are not familiar to many users, etc. With the software we develop as part of the JuliaGPU organization, we aim to make it easy to use GPUs, without hindering the ability to optimize or use low-level features that the hardware has to offer.</p><h2 id="5da7">What is GPU computing? How important is it nowadays?</h2><p id="4de0">GPU computing means using the GPU, a device originally designed for graphics processing, to perform general-purpose computations. It has grown more important now that CPU performance is not improving as steadily as it used to. Instead, specialized devices like GPUs or FPGAs are increasingly used to improve the performance of certain computations. In the case of GPUs, the architecture is a great fit to perform highly-parallel applications. Machine learning networks are a good example of such parallel applications, and their popularity is one of the reasons GPUs have become so important.</p><h2 id="f596">Do you think Julia is an appropriate language to efficiently use GPU capabilities? Why?</h2><p id="775f">Julia√¢‚Ç¨‚Ñ¢s main advantage is that the language was designed to be compiled. Even though the syntax is high-level, the generated machine code is<br>compact and has great performance characteristics (for more details, see <a href="http://janvitek.org/pubs/oopsla18b.pdf" rel="noopener">this paper</a>). This is crucial for GPU execution, where we are required to run native binaries and cannot easily (or efficiently) interpret code as is often required by other language√¢‚Ç¨‚Ñ¢s semantics.</p><p id="b90b">Because we√¢‚Ç¨‚Ñ¢re able to directly compile Julia for GPUs, we can use almost all of the language√¢‚Ç¨‚Ñ¢s features to build powerful abstractions. For example, you can define your own types, use those in GPU arrays, compose that with existing abstractions like lazy "Transpose" wrappers, access those on the GPU while benefiting from automatic bounds-checking (if needed), etc.</p><h2 id="824e">From a Python programmer perspective, how does CUDA.jl compare to PyCUDA? Are their functionalities equivalent?</h2><p id="cf3f">PyCUDA gives the programmer access to the CUDA APIs, with high-level Python functions that are much easier to use. CUDA.jl provides the same, but in Julia. The `hello world` from PyCUDA√¢‚Ç¨‚Ñ¢s home page looks almost identical in Julia:</p><pre><span id="a986">using CUDA</span><span id="03e4">function multiply_them(dest, a, b)<br> i = threadIdx().x<br> dest[i] = a[i] * b[i]<br> return<br>end</span><span id="9e7d">a = CuArray(randn(Float32, 400))<br>b = CuArray(randn(Float32, 400))</span><span id="5833">dest = similar(a)<br>@cuda threads=400 multiply_them(dest, a, b)</span><span id="1372">println(dest-a.*b)</span></pre><p id="396f">There√¢‚Ç¨‚Ñ¢s one very big difference: "multiply_them" here is a function written in Julia, whereas PyCUDA uses a kernel written in CUDA C. The reason is straightforward: Python is not simple to compile. Of course, projects like Numba prove that it is very much possible to do so, but in the end those are separate compilers that try to match the reference Python compilers as closely as possible. With CUDA.jl, we integrate with that reference compiler, so it√¢‚Ç¨‚Ñ¢s much easier to guarantee consistent semantics and follow suit when the language changes (for more details,<br>refer to <a href="https://arxiv.org/abs/1712.03112" rel="noopener">this paper</a>).</p><h2 id="5cf6">Are the packages in the JuliaGPU organization targeted to experienced programmers only?</h2><p id="6590">Not at all. CUDA.jl targets different kinds of (GPU) programmers. If you are confident writing your own kernels, you can do so, while using all of the low-level features CUDA GPUs have to offer. But if you are new to the world of GPU programming, you can use high-level array operations that use existing kernels in CUDA.jl. For example, the above element-wise multiplication could just as well be written as:</p><pre><span id="9c1f">using CUDA</span><span id="476a">a = CuArray(randn(Float32, 400))<br>b = CuArray(randn(Float32, 400))</span><span id="efdf">dest = a .* b</span></pre><h2 id="4c26">Is it necessary to know how to code in CUDA.jl to take full advantage of GPU computing in Julia?</h2><p id="208b">Not for most users. Julia has a powerful language of generic array operations ("map", "reduce", "broadcast", "accumulate", etc) which can be applied to all kinds of arrays, including GPU arrays. That means you can often re-use your codebase developed for the CPU with CUDA.jl (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0965997818310123" rel="noopener">this paper</a> shows some powerful examples). Doing so often requires minimal changes: changing the array type, making sure you use array operations instead of for loops, etc.</p><p id="672d">It√¢‚Ç¨‚Ñ¢s possible you need to go beyond this style of programming, e.g., because your application doesn√¢‚Ç¨‚Ñ¢t map cleanly onto array operations, to use specific GPU features, etc. In that case, some basic knowledge about CUDA and the GPU programming model is sufficient to write kernels in CUDA.jl.</p><h2 id="1a33">How is the experience of coding a kernel in CUDA.jl in comparison to CUDA C and how transferable is the knowledge to one another?</h2><p id="6a08">It√¢‚Ç¨‚Ñ¢s very similar, and that√¢‚Ç¨‚Ñ¢s by design: We try to keep the kernel abstractions in CUDA.jl close to their CUDA C counterparts such that the programming environment is familiar to existing GPU programmers. Of course, by using a high-level source language there√¢‚Ç¨‚Ñ¢s many quality-of-life improvements. You can allocated shared memory, for example, statically and dynamically as in CUDA C, but instead of a raw pointers we use an N-dimensional array object you can easily index. An example from the <a href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/" rel="noopener">NVIDIA developer blog</a>:</p><pre><span id="ef8a">__global__ void staticReverse(int *d, int n)<br>{<br> __shared__ int s[64];<br> int t = threadIdx.x;<br> int tr = n-t-1;<br> s[t] = d[t];<br> __syncthreads();<br> d[t] = s[tr];<br>}</span></pre><p id="9cb0">The CUDA.jl equivalent of this kernel looks very familiar, but uses array objects instead of raw pointers:</p><pre><span id="996c">function staticReverse(d)<br> s = @cuStaticSharedMem(Int, 64)<br> t = threadIdx().x<br> tr = length(d)-t+1<br> s[t] = d[t]<br> sync_threads()<br> d[t] = s[tr]<br> return<br>end</span></pre><p id="9a14">Using array objects has many advantages, e.g. multi-dimensional is greatly simplified and we can just do "d[i,j]". But it√¢‚Ç¨‚Ñ¢s also safer, because these accesses are bounds checked:</p><pre><span id="3d65">julia&gt; a = CuArray(1:64)<br>64-element CuArray{Int64,1}:<br> 1<br> 2<br> 3<br> √¢‚Äπ¬Æ<br> 62<br> 63<br> 64</span><span id="0d9c">julia&gt; @cuda threads=65 staticReverse(a)<br>ERROR: a exception was thrown during kernel execution.<br>Stacktrace:<br> [1] throw_boundserror at abstractarray.jl:541</span></pre><p id="c17a">Bounds checking isn√¢‚Ç¨‚Ñ¢t free, of course, and once we√¢‚Ç¨‚Ñ¢re certain our code is correct we can add an "@inbounds" annotation to our kernel and get the high-performance code we expect:</p><pre><span id="6846">julia&gt; @device_code_ptx @cuda threads=64 staticReverse(a)<br>.visible .entry staticReverse(.param .align 8 .b8 d[16]) {<br> .reg .b32 %r&lt;2&gt;;<br> .reg .b64 %rd&lt;15&gt;;<br> .shared .align 32 .b8 s[512];</span><span id="09c9">mov.b64 %rd1, d;<br> ld.param.u64 %rd2, [%rd1];<br> ld.param.u64 %rd3, [%rd1+8];<br> mov.u32 %r1, %tid.x;<br> cvt.u64.u32 %rd4, %r1;<br> mul.wide.u32 %rd5, %r1, 8;<br> add.s64 %rd6, %rd5, -8;<br> add.s64 %rd7, %rd3, %rd6;<br> ld.global.u64 %rd8, [%rd7+8];<br> mov.u64 %rd9, s;<br> add.s64 %rd10, %rd9, %rd6;<br> st.shared.u64 [%rd10+8], %rd8;<br> bar.sync 0;<br> sub.s64 %rd11, %rd2, %rd4;<br> shl.b64 %rd12, %rd11, 3;<br> add.s64 %rd13, %rd9, %rd12;<br> ld.shared.u64 %rd14, [%rd13+-8];<br> st.global.u64 [%rd7+8], %rd14;<br> ret;<br>}</span><span id="4489">julia&gt; a<br>64-element CuArray{Int64,1}:<br> 64<br> 63<br> 62<br> √¢‚Äπ¬Æ<br> 3<br> 2<br> 1</span></pre><p id="3cee">Tools like "@device_code_ptx" make it easy for an experienced developer to inspect generated code and ensure the compiler does what he wants.</p><h2 id="164a">Why does having a compiler have such an impact in libraries like CUDA.jl? (How was the process of integrating it to the Julia compiler?)</h2><p id="e360">Because we have a compiler at our disposal, we can rely on higher-order functions and other generic abstractions that specialize based on the arguments that users provide. That greatly simplifies our library, but also gives the user very powerful tools. As an example, we have carefully implemented a `mapreduce` function that uses shared memory, warp intrinsics, etc to perform a high-performance reduction. The implementation is generic though, and will automatically re-specialize (even at run time) based on the arguments to the function:</p><pre><span id="9da7">julia&gt; mapreduce(identity, +, CuArray([1,2,3]))<br>6</span><span id="c680">julia&gt; mapreduce(sin, *, CuArray([1.1,2.2,3.3]))<br>-0.11366175839582586</span></pre><p id="52e8">With this powerful `mapreduce` ‚Ä¶</p></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://notamonadtutorial.com/julia-gpu-98a461d33e21">https://notamonadtutorial.com/julia-gpu-98a461d33e21</a></em></p>]]>
            </description>
            <link>https://notamonadtutorial.com/julia-gpu-98a461d33e21</link>
            <guid isPermaLink="false">hacker-news-small-sites-24840972</guid>
            <pubDate>Tue, 20 Oct 2020 18:48:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Palo Alto Networks sends cease-and-desist letter to take down review videos]]>
            </title>
            <description>
<![CDATA[
Score 455 | Comments 128 (<a href="https://news.ycombinator.com/item?id=24840119">thread link</a>) | @bonfire
<br/>
October 20, 2020 | https://orca.security/cybersecurity-community-transparency/ | <a href="https://web.archive.org/web/*/https://orca.security/cybersecurity-community-transparency/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main">

				
							<div data-elementor-type="single" data-elementor-id="1240" data-elementor-settings="[]">
		<div>
			<div>
						<section data-id="665134c3" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="12af1bce" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
			<div>
					<div>
				<div data-id="19996313" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			<p><em>Abstract: A few weeks ago, Orca Security published a comparison between the Orca Cloud Security Platform and a few other cloud security tools‚Äîincluding a&nbsp;<a href="https://orca.security/prisma-cloud-security/">comparison with Palo Alto Networks Prisma</a>. In response, Palo Alto Networks sent a <a href="https://orca.security/wp-content/uploads/PANW-Legal-Letter-Cease-Desist.pdf" target="_blank" rel="noopener noreferrer">cease and desist letter</a>, demanding the comparison be removed immediately. Here is my response. I urge you to see&nbsp;<a href="https://www.youtube.com/playlist?list=PLDnqJTEi6ynvsGkSJHYeEmhz6_NfzKgbf">the videos in question</a>&nbsp;and if you, like me, believe the cybersecurity community deserves transparency and vendors shouldn‚Äôt be allowed to prevent publishing reviews or benchmarks via legal threats, then please share this post. You can also leave your own comments down below.</em></p>
<p><strong>To: Palo Alto Networks</strong></p>
<p><strong>CC: The cybersecurity community</strong></p>
<p><strong>Subject: The Cybersecurity community demands transparency, not legal threats&nbsp;</strong></p>
<p>Security has always been about transparency. The concept of security by obscurity was frowned upon as early as 1851‚Äîeven before the invention of electricity‚Äîwhen&nbsp;<a href="https://en.wikipedia.org/wiki/Alfred_Charles_Hobbs">Alfred Hobbs</a>, a Massachusetts-based locksmith, demonstrated how then state-of-the-art locks could be picked. He explained that exposing the information would make the public more secure, as rogues already knew the deficiencies. The public needed to be educated, and he‚Äôd pursue better locks. Today‚Äôs locks are more advanced, but the principle is the same.</p>
<p>The cybersecurity community preaches about many products. All come with their own advantages and disadvantages, capabilities, and limitations. I believe that the only way practitioners can choose the tools that fit their environments best is by viewing factual evidence‚Äînot by relying solely on marketing materials. This is why we launched our&nbsp;<a href="https://orca.security/cloud-security-solutions/">Cloud Security Punch-Out! Series</a>, where we deploy a few tools‚Äîincluding Orca Security‚Äîon the exact same environment and share the results with viewers who deserve to see them. I urge you to take&nbsp;<a href="https://orca.security/prisma-cloud-security/">a look at the one we did with Palo Alto Networks;</a>&nbsp;as you‚Äôll see we don‚Äôt hide those areas where Palo Alto Networks shines.</p>
<p>Unfortunately, Palo Alto Networks is now trying to use legal threats to prevent us from publishing&nbsp;these video&nbsp;reviews. In <a href="https://orca.security/wp-content/uploads/PANW-Legal-Letter-Cease-Desist.pdf" target="_blank" rel="noopener noreferrer">its letter</a>, Palo Alto Networks does not point to any factual inaccuracies in the reviews of its products‚Äô performance. Instead, it premises its threats on flimsy, boilerplate contract terms that prohibit reviews and comparisons of its products and hollow trademark allegations purporting that Palo Alto Networks is sponsoring the videos.</p>
<p>It‚Äôs outrageous that the world‚Äôs largest cybersecurity vendor (its products being used by over 65,000 organizations according to its website), believes that its users aren‚Äôt entitled to share any benchmark or performance comparison of its products. According to its boilerplate contract terms that prohibit ‚Äúdisclosing, publishing, or otherwise making publicly available any benchmark, performance, or comparison tests‚Äù of its products, you‚Äôre in violation even if you publish the results of an internal comparison of Palo Alto Networks against other products as part of your procurement process. The same goes for the hundreds of Palo Alto Networks reviews on various sites that include G2 Crowd, Capterra, and Gartner Peer Insights. It means that only benchmarks approved by Palo Alto Networks can be published.</p>
<p>Palo Alto Networks appears oblivious to the fact that the New York Attorney General‚Äôs office&nbsp;<a href="https://www.leagle.com/decision/2003579195misc2d3841519">sued and won an injunction</a>&nbsp;against McAfee from enforcing its contractual restrictions against publishing reviews or comparisons of its products without its consent more than 17 years ago. In enacting the&nbsp;<a href="https://www.law.cornell.edu/uscode/text/15/45b">Consumer Review Fairness Act</a>, Congress has also prohibited businesses from including contract terms that prohibit consumers from reviewing products or services they purchase.</p>
<blockquote><p>Palo Alto Networks, do you think your products are flawless or that the bad guys will follow along, not openly talking about products‚Äô deficiencies? If the answer is no to both, then why resort to legal threats to remove such benchmarks and comparisons? I refuse to accept a world where any vendor believes it has the right to prevent the free flow of information, and control which product reviews are made publicly available.</p></blockquote>
<p>I urge you to make your products better and focus your marketing efforts on demonstrating that, rather than throwing away money on ill-conceived gag efforts. Such action doesn‚Äôt benefit anyone. If you believe we missed something in our test, then tell us so we can make adjustments‚Äîwe‚Äôll happily integrate your comments and suggestions.</p>
<p>We could contract an objective third party to conduct additional tests. You could conduct your own tests with Palo Alto Networks and Orca Security‚Äôs products, then let the audience see and decide for themselves. All such actions would be far more beneficial to the industry, permitting both companies to learn and improve our products for the sake of customers.</p>
<p>As we all recently learned too well,&nbsp;<a href="https://www.contagionlive.com/news/sunlight-inactivates-the-airborne-virus-that-causes-covid19">sunlight is the best disinfectant</a>. The cybersecurity community deserves better than a vendor‚Äôs lack of transparency while wielding dubious legal methods. Palo Alto Networks is the worlds‚Äô largest cybersecurity vendor; with great power comes great responsibility. Your products are great‚Äîbut nothing is perfect, and the public should have free access to all of the facts.</p>
<p>Yours faithfully,<br>
Avi Shua, CEO and Co-Founder<br>
Orca Security</p>
		</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
					</div>
		</div>
		</div>
		
					
					
				
			</div></div>]]>
            </description>
            <link>https://orca.security/cybersecurity-community-transparency/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24840119</guid>
            <pubDate>Tue, 20 Oct 2020 17:31:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My platonic ideal for how engineering hiring should work]]>
            </title>
            <description>
<![CDATA[
Score 175 | Comments 288 (<a href="https://news.ycombinator.com/item?id=24840013">thread link</a>) | @leeny
<br/>
October 20, 2020 | http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/ | <a href="https://web.archive.org/web/*/http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>I‚Äôve been in and around eng hiring for the past 13 years, as an engineer, a recruiter, and a founder of a technical recruiting marketplace (interviewing.io). Over the course of those 13 years, I‚Äôve become increasingly disgruntled at the state of hiring, and now I‚Äôm mad enough to write this blog post.</p>



<p>If you‚Äôve ever been on either end of the table, you‚Äôre probably mad at the state of hiring, too. Whether you have given it a lot of thought or whether you just feel it deep down, something about the whole process feels off.</p>



<p>But we‚Äôve been doing it this way for so long that we probably take much of how hiring works as gospel, and it‚Äôs really hard to tease apart all the different components of the process and examine why they are the way they are. In this post, I‚Äôd like to challenge many of the things we assume about hiring, and, perhaps most importantly, I‚Äôd like to lay out my platonic ideal for how eng hiring should work. It‚Äôs a simple axiom, really:</p>



<p><em>It should be easy for smart people to talk to other smart people.</em></p>



<p>Or, another way to put it ‚Ä¶ if I‚Äôm a good engineer, it should be easy for me to talk to a hiring manager at a company I might be interested in, at a time of my choosing. But that‚Äôs simply not possible today. Despite the refrain that we‚Äôre in a candidate‚Äôs market and that there‚Äôs a shortage of good candidates, which should mean that candidates should have the power to call the shots, today‚Äôs hiring process couldn‚Äôt be further removed from this ideal. And it‚Äôs not just broken for a specific type of candidate. It‚Äôs broken for <em>everyone.</em></p>



<p>If you‚Äôre reading this, you might be an engineering manager, a senior engineer with stellar credentials, a recent bootcamp grad, an engineer from a background traditionally underrepresented in tech, or some combination of these. <strong>What‚Äôs truly messed up about the status quo is that, regardless of which of these groups you fall into, your journey will be unnecessarily unpleasant. Though the degree of unpleasantness will not always be the same, it‚Äôs not about race, seniority, pedigree, or gender ‚Ä¶ or even which side of the table you‚Äôre on. Hiring, in its current incarnation, is broken for everybody.</strong></p>



<p>Why? Let us go then, you and I, into the bowels of the status quo.</p>



<h2>A candidate and a hiring manager, never the twain shall meet</h2>



<p>Let‚Äôs say that I‚Äôm a competent generalist engineer who looks good on paper, and I‚Äôm thinking that it‚Äôs time to look for a new job. What happens next? The idea of having to mount a full-on job search is so daunting.&nbsp;</p>



<p>I could try some job boards to see which companies are out there. But what would I filter on? I know a lot of programming languages but am not set on having to work in a specific one. How can I tell if I‚Äôll hit it off with the team? I‚Äôm applying via a job board to a position I know next to nothing about ‚Äî will anyone even respond?</p>



<p>Suppose I find some companies where I might want to work. If I‚Äôm lucky enough to know someone there, I‚Äôll have to get them to refer me, even though that may not actually do much to speed things along. And if I don‚Äôt know anyone there, applying will be an exhausting long shot. Odds are no one will look at my application, and having to redo my resume ‚Äî or worse, write cover letters ‚Äî seems like the most tedious kind of busywork.</p>



<p>I guess I can always dig through the recruiter spam I‚Äôve gotten. But do those recruiters still work at the company? If they do, how long will it actually take to get into the process?</p>



<p>Breaking character for a moment, a friend of mine recently got this recruiting email from Google, who has elevated gaslighting to an art form: somehow the fact that it takes two months to get through their process has become a <em>selling point.</em></p>



<figure><img loading="lazy" width="750" height="319" src="http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-750x319.png" alt="" srcset="http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-750x319.png 750w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-450x191.png 450w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-768x327.png 768w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail.png 1004w" sizes="(max-width: 750px) 100vw, 750px"></figure>



<p>Once I do get into the process, why do I have to endure the same intro call ten times with different recruiters who can‚Äôt tell me anything about what I‚Äôd be working on at any level of depth?</p>



<p>Do I join some platform, create a profile that I copy-paste everywhere (with writing that was just as painful as the aforementioned resume/cover letter) and sort of hope that decent companies contact me ‚Ä¶ only to have to begin the same recruiter calls over and over again, as above?</p>



<p>Will I have to take some quizzes that drill me on obscure syntax or make me solve toy problems that have no bearing on my engineering ability before I even get to have the aforementioned inane conversation with a recruiter?</p>



<p><strong>If I‚Äôm actually good at my job, why can‚Äôt I just set up some conversations with companies I think are cool and see if it‚Äôs a fit? Why do I have to subject myself and others to an endless parade of vapid conversations and the inevitable busywork that precedes them?</strong></p>



<p>Here‚Äôs the truth. Even if I look good on paper and am well-connected, hiring still sucks because of all the noise, uncertainty, and time wasted ‚Ä¶ but at least I have options. They might not be exactly the right options for me, but at least they exist. On the other hand, if I‚Äôm an engineer without a pedigree or a network, my choices are extremely limited, no matter how good I am. Recruiters aren‚Äôt reaching out to me, I‚Äôm not well-networked enough to have friends refer me, and I <em>definitely </em>don‚Äôt hear back when I apply.</p>



<hr>



<p>Let‚Äôs take a look at the other side of the table. Let‚Äôs say I‚Äôm an eng manager who needs to hire more competent generalists for my team. Having worked as both an eng manager and a recruiter, I can tell you that what happens next isn‚Äôt particularly inspiring.</p>



<p>As an eng manager, I sit down with a recruiter and try to explain what I‚Äôm looking for. Nine times out of ten, I want a <a href="https://www.joelonsoftware.com/2007/06/05/smart-and-gets-things-done/">smart person who can get shit done</a>. But, after a farcical game of telephone, somehow those criteria get warped into years of experience with a specific technology or requirements about where the candidate went to school. I also end up with an uninspired, sterile job description that fails to capture the imagination of any candidates who might unwittingly stumble upon it.</p>



<p><strong>My recruiter then goes to any number of sourcing tools of which LinkedIn Recruiter is the ubiquitous, lackluster market leader. They type in keywords I didn‚Äôt ask for and filter on credentials I don‚Äôt care about to come up with the same homogenous list of candidates every other recruiter at every other tech company is chasing.</strong></p>



<p>They then contact these candidates en masse with generic copy about my team and the hard problems we‚Äôre solving. They celebrate single-digit response rates and spend the minimal time left over to give a cursory glance at candidates applying directly.</p>



<h2>Why is hiring broken?</h2>



<p>So therein lies the ineffectual dance. This is the process we‚Äôve come to accept. As far as I can tease out, the axioms that underlie today‚Äôs recruiting best practices go something like this (some of these were told to me verbatim when I was starting out as a recruiter, even):</p>



<ol><li><strong>Thou shalt not engage with active candidates. After all, in this market, strong candidates aren‚Äôt looking. Good recruiters build relationships so that when a good candidate does decide to enter the market, the recruiter is there, behind the next doorway, ready to spring!</strong></li><li><strong>Engineering time is expensive, so it‚Äôs critical to do as much top-of-funnel filtering as possible to make sure that it‚Äôs spent on the right candidates.</strong></li></ol>



<p>Are these axioms wrong? The sad truth is ‚Ä¶ not really. I‚Äôve written in a <a href="http://blog.alinelerner.com/the-unvarnished-unbundled-guide-to-hiring-tools/">previous post about how market forces rule everything around me</a>, and recruiting best practices are no exception. In an economy with a surplus of jobs and a shortage of talent, it follows that the best talent is going to be harder to find, engineering time will be expensive, and recruiters in their current incarnation are, dare I say it, a necessary evil. <sup><a href="#footnote_0_2455" id="identifier_0_2455" title="From what you‚Äôve read up until this point, you might think that I hate recruiters and find them useless. Not so, dear reader! I hate bad recruiters. And, unfortunately, most of them are bad. What‚Äôs sad is that the good ones, instead of spending time on tasks for which they‚Äôre uniquely qualified and well-suited, are instead stuck at the top of the funnel sourcing engineers whose qualifications they don‚Äôt have the domain expertise to evaluate and selling them on roles they don‚Äôt have the domain expertise to describe. The best recruiters I‚Äôve worked with are singularly amazing at shepherding candidates through the process, tirelessly stewarding a company‚Äôs employer brand, advising hiring managers on the best ways to close, keeping an analytical eye on the funnel to identify issues before they even arise, and much more.">1</a></sup></p>



<p>The data supports our current world view. According to Lever (one of the two application tracking systems widely used by startups, Greenhouse is the other), here‚Äôs a breakdown of how many candidates from each source it takes to make a hire. Note that here, larger numbers are bad ‚Äî for many companies, internal referrals are the best source and inbound applications are the worst.</p>



<figure><img loading="lazy" width="750" height="375" src="http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-750x375.png" alt="" srcset="http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-750x375.png 750w, http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-450x225.png 450w, http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-768x384.png 768w, http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2.png 1008w" sizes="(max-width: 750px) 100vw, 750px"><figcaption>Source: <a href="https://www.lever.co/recruiting-resources/articles/recruitment-process/" target="_blank" rel="noreferrer noopener">https://www.lever.co/recruiting-resources/articles/recruitment-process/</a></figcaption></figure>



<p>Looking at this data, you can see why recruiters simply ignore online applications. The same dynamics also apply to platforms such as AngelList ‚Äî like any jobs board, it‚Äôs noisy and probably full of candidates who don‚Äôt have much leverage (e.g., juniors/bootcamp grads and people requiring visa sponsorship).</p>



<p>As for the value of eng time, guarding it carefully isn‚Äôt exactly wrong either. In fact, if you look at what a typical hiring process looks like today, you‚Äôll see that most of the time spent is by engineers conducting interviews.</p>



<figure><table><thead><tr><th><strong>Hiring process stage</strong></th><th>Who does it?</th><th>How long does it take?</th></tr></thead><tbody><tr><td>Resume review</td><td>Recruiter</td><td>10-30 seconds</td></tr><tr><td>Recruiter screen</td><td>Recruiter</td><td>45 min</td></tr><tr><td>Technical phone screen</td><td>Engineer</td><td>1 hour</td></tr><tr><td>Onsite ‚Äì Eng portion</td><td>Engineer</td><td>6 hours</td></tr><tr><td>Onsite ‚Äì Recruiter portion</td><td>Recruiter</td><td>1 hour</td></tr><tr><td>Offer</td><td>Recruiter OR Eng mgr</td><td>1 hour</td></tr></tbody></table></figure>



<p>Engineering salaries are high, so given that most of the time spent on a single candidate is with engineers, it‚Äôs <em>rational</em> to put some recruiter gates at the top of the funnel to protect eng time. The idea is that recruiters will effectively screen out most candidates and only pass on the most promising ones to the eng team.</p>



<p>Unfortunately, when you look at an actual typical <em>funnel</em>, you‚Äôll see that despite attempts to gate the top with recruiters filtering resumes and making intro calls, it‚Äôs not really working. Below is what a typical funnel looks like.</p>



<figure><img loading="lazy" width="750" height="89" src="http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-750x89.png" alt="" srcset="http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-750x89.png 750w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-450x53.png 450w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-768x91.png 768w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42.png 1092w" sizes="(max-width: 750px) 100vw, 750px"></figure>



<p>If you <a href="https://blog.interviewing.io/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should/" target="_blank" rel="noreferrer noopener">do the math</a> and look at how many hours are spent ‚Äî not per candidate but per hire (more useful because hires are ultimately what we want) ‚Äî you‚Äôll see that despite attempts to save eng time, recruiters spend roughly 15 hours a hire <sup><a href="#footnote_1_2455" id="identifier_1_2455" title="If we add in time to review resumes, it‚Äôs an extra five hours (at most).">2</a></sup> and engineers spend about 40. In a process where you don‚Äôt make an offer 50% of the time and only convert those offers to hires 50% of ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/">http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/</a></em></p>]]>
            </description>
            <link>http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24840013</guid>
            <pubDate>Tue, 20 Oct 2020 17:22:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS and their billions in IPv4 addresses]]>
            </title>
            <description>
<![CDATA[
Score 187 | Comments 177 (<a href="https://news.ycombinator.com/item?id=24839887">thread link</a>) | @bgpdude
<br/>
October 20, 2020 | https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html | <a href="https://web.archive.org/web/*/https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div id="post-body"><p>Earlier this week, I was doing some work on AWS and wanted to know what IP addresses were being used. Luckily for me, AWS publishes this all here <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="nofollow noopener">https://ip-ranges.amazonaws.com/ip-ranges.json</a>. When you go through this list, you‚Äôll quickly see that AWS has a massive asset of IPv4 allocations. Just counting quickly I noticed a lot of big prefixes.</p><figure><blockquote data-width="550"><p lang="en" dir="ltr">Ever wondered what all of the AWS network ranges are? You can find them all here:<a href="https://t.co/NBaBF6w0la">https://t.co/NBaBF6w0la</a><br>That's *a lot* of big prefixes!<br>4x /11, 14x /12, 30x /13, 78x /14, 184x /15, 278x /16</p>‚Äî Andree Toonk, Adelante! (@atoonk) <a href="https://twitter.com/atoonk/status/1316098702260359168?ref_src=twsrc%5Etfw">October 13, 2020</a></blockquote>

</figure><p>However, the IPv4 ranges on that list are just the ranges that are in use and allocated today by AWS. Time to dig a bit deeper.</p><h3 id="ipv4-address-acquisitions-by-aws">IPv4 address acquisitions by AWS</h3><p>Over the years, AWS has acquired a lot of IPv4 address space. Most of this happens without gaining too much attention, but there were a few notable acquisitions that I‚Äôll quickly summarize below.</p><h4 id="2017-mit-selling-8-million-ipv4-addresses-to-aws">2017: MIT selling 8 million IPv4 addresses to AWS</h4><p>In 2017 <a href="https://www.internetsociety.org/blog/2017/05/mit-goes-on-ipv4-selling-spree/" rel="noopener">MIT sold half of its 18.0.0.0/8</a> allocation to AWS. This 18.128.0.0/9 range holds about 8 million IPv4 addresses.</p><h4 id="2018-ge-sells-3-0-0-0-8-to-aws">2018: GE sells 3.0.0.0/8 to AWS</h4><p>In 2018 the IPv4 prefix 3.0.0.0/8 was transferred from GE to AWS. With this, AWS became the proud owner of its first /8! That‚Äôs sixteen million new IPv4 addresses to feed us hungry AWS customers. <a href="https://news.ycombinator.com/item?id=18407173" rel="nofollow noopener">https://news.ycombinator.com/item?id=18407173</a></p><h4 id="2019-aws-buys-amprnet-44-192-0-0-10">2019: AWS buys AMPRnet 44.192.0.0/10</h4><p>In 2019 AWS bought a /10 from AMPR.org, the Amateur Radio Digital Communications (ARDC). The IPv4 range 44.0.0.0/8 was an allocation made to the Amateur Radio organization in 1981 and known as the AMPRNet. This sell caused a fair bit of discussion, check out the <a href="https://mailman.nanog.org/pipermail/nanog/2019-July/thread.html#102103" rel="noopener">nanog discussion here.</a></p><p>Just this month, it <a href="http://www.southgatearc.org/news/2020/october/sale-of-amateur-radio-amprnet-tcp-ip-addresses.htm" rel="noopener">became public knowledge</a> AWS paid $108 million for this /10. That‚Äôs $25.74 per IP address.</p><p>These are just a few examples. Obviously, AWS has way more IP addresses than the three examples I listed here. The IPv4 transfer market is very active. Check out this website to get a sense of all transfers: <a href="https://account.arin.net/public/transfer-log#NRPM-8.3IPv4" rel="noopener">https://account.arin.net/public/transfer-log</a></p><h3 id="all-aws-ipv4-addresses">All AWS IPv4 addresses</h3><p>Armed with the information above it was clear that not all of the AWS owned ranges were in the <a href="https://ip-ranges.amazonaws.com/ip-ranges.json">JSON</a> that AWS published. For example, parts of the 3.0.0.0/8 range are missing. Likely because some of it is reserved for future use.</p><p>Combining all those IPv4 prefixes, removing duplicates and overlaps by aggregating them results in the following list of unique IPv4 address owned by AWS: <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes" rel="nofollow noopener">https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes</a></p><p>The total number of IPv4 addresses in that list is just over 100 Million (100,750,168). That‚Äôs <strong>the equivalent of just over six /8‚Äôs,</strong> not bad!</p><p>If we break this down by allocation size, we see the following:</p><pre><code>1x /8     =&gt; 16,777,216 IPv4 addresses
1x /9     =&gt; 8,388,608 IPv4 addresses
4x /10    =&gt; 16,777,216 IPv4 addresses
5x /11    =&gt; 10,485,760 IPv4 addresses
11x /12   =&gt; 11,534,336 IPv4 addresses
13x /13   =&gt; 6,815,744 IPv4 addresses
34x /14   =&gt; 8,912,896 IPv4 addresses
53x /15   =&gt; 6,946,816 IPv4 addresses
182x /16  =&gt; 11,927,552 IPv4 addresses
&lt;and more&gt;</code></pre><p>A complete breakdown can be found here: <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#breakdown-by-ipv4-prefix-size" rel="nofollow noopener">https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#breakdown-by-ipv4-prefix-size</a></p><h3 id="putting-a-valuation-on-aws-ipv4-assets">Putting a valuation on AWS‚Äô IPv4 assets</h3><blockquote>Alright.. this is just for fun‚Ä¶</blockquote><p>Since AWS is (one of) the largest buyers of IPv4 addresses, they have spent a significant amount on stacking up their IPv4 resources. It‚Äôs impossible, as an outsider, to know how much AWS paid for each deal. However, we can for fun, try to put a dollar number on AWS‚Äô current IPv4 assets.</p><p>The average price for IPv4 addresses has gone up over the years. From ~$10 per IP a few years back to ~$25 per IP <a href="https://auctions.ipv4.global/" rel="noopener">nowadays</a>. <br>Note that these are market prices, so if AWS would suddenly decide to sell its IPv4 addresses and overwhelm the market with supply, prices would drop. But that won‚Äôt happen since we‚Äôre all still addicted to IPv4 ;)</p><p>Anyway, let‚Äôs stick with $25 and do the math just for fun.</p><pre><code>100,750,168 ipv4 addresses x $25 per IP = $2,518,754,200</code></pre><p>Just<strong> over $2.5 billion worth of IPv4 addresses,</strong> not bad! </p><h3 id="peeking-into-the-future">Peeking into the future</h3><p>It‚Äôs clear AWS is working hard behind the scenes to make sure we can all continue to build more on AWS. One final question we could look at is: <em>how much buffer does AWS have?</em> ie. how healthy is their IPv4 reserve?</p><p>According to their <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="noopener">published data</a>, they have allocated roughly 53 Million IPv4 addresses to existing AWS services. We found that all their IPv4 addresses combined equates to approximately 100 Million IPv4 addresses. That means they still have ~47 Million IPv4 addresses, or 47% available for future allocations. That‚Äôs pretty healthy! And on top of that, I‚Äôm sure they‚Äôll continue to source more IPv4 addresses. The IPv4 market is still hot!</p></div>
    </div></div>]]>
            </description>
            <link>https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24839887</guid>
            <pubDate>Tue, 20 Oct 2020 17:12:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Predicting Football Results with Statistical Modelling]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 30 (<a href="https://news.ycombinator.com/item?id=24838958">thread link</a>) | @henrik_w
<br/>
October 20, 2020 | https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/ | <a href="https://web.archive.org/web/*/https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      

      <section itemprop="text">
        <p>Football (or soccer to my American readers) is full of clich√©s: ‚ÄúIt‚Äôs a game of two halves‚Äù, ‚Äútaking it one game at a time‚Äù and ‚ÄúLiverpool have failed to win the Premier League‚Äù. You‚Äôre less likely to hear ‚ÄúTreating the number of goals scored by each team as independent Poisson processes, statistical modelling suggests that the home team have a 60% chance of winning today‚Äù. But this is actually a bit of clich√© too (it has been discussed <a href="https://www.pinnacle.com/en/betting-articles/soccer/how-to-calculate-poisson-distribution">here</a>, <a href="https://help.smarkets.com/hc/en-gb/articles/115001457989-How-to-calculate-Poisson-distribution-for-football-betting">here</a>, <a href="http://pena.lt/y/2014/11/02/predicting-football-using-r/">here</a>, <a href="http://opisthokonta.net/?p=296">here</a> and <a href="https://dashee87.github.io/data%20science/football/r/predicting-football-results-with-statistical-modelling/">particularly well here</a>). As we‚Äôll discover, a simple Poisson model is, well, overly simplistic. But it‚Äôs a good starting point and a nice intuitive way to learn about statistical modelling. So, if you came here looking to make money, <a href="http://www.make5000poundspermonth.co.uk/">I hear this guy makes ¬£5000 per month without leaving the house</a>.</p>

<h2 id="poisson-distribution">Poisson Distribution</h2>

<p>The model is founded on the number of goals scored/conceded by each team. Teams that have been higher scorers in the past have a greater likelihood of scoring goals in the future. We‚Äôll import all match results from the recently concluded Premier League (2016/17) season. There‚Äôs various sources for this data out there (<a href="https://www.kaggle.com/hugomathien/soccer">kaggle</a>, <a href="http://www.football-data.co.uk/englandm.php">football-data.co.uk</a>, <a href="https://github.com/jalapic/engsoccerdata">github</a>, <a href="http://api.football-data.org/index">API</a>). I built an <a href="https://github.com/dashee87/footballR">R wrapper for that API</a>, but I‚Äôll go the csv route this time around.</p>

<div><div><pre><code><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>seaborn</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>poisson</span><span>,</span><span>skellam</span>

<span>epl_1617</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>"http://www.football-data.co.uk/mmz4281/1617/E0.csv"</span><span>)</span>
<span>epl_1617</span> <span>=</span> <span>epl_1617</span><span>[[</span><span>'HomeTeam'</span><span>,</span><span>'AwayTeam'</span><span>,</span><span>'FTHG'</span><span>,</span><span>'FTAG'</span><span>]]</span>
<span>epl_1617</span> <span>=</span> <span>epl_1617</span><span>.</span><span>rename</span><span>(</span><span>columns</span><span>=</span><span>{</span><span>'FTHG'</span><span>:</span> <span>'HomeGoals'</span><span>,</span> <span>'FTAG'</span><span>:</span> <span>'AwayGoals'</span><span>})</span>
<span>epl_1617</span><span>.</span><span>head</span><span>()</span>
</code></pre></div></div>

<div>
<table>
  <thead>
    <tr>
      <th></th>
      <th>HomeTeam</th>
      <th>AwayTeam</th>
      <th>HomeGoals</th>
      <th>AwayGoals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Burnley</td>
      <td>Swansea</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Crystal Palace</td>
      <td>West Brom</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Everton</td>
      <td>Tottenham</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hull</td>
      <td>Leicester</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Man City</td>
      <td>Sunderland</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>We imported a csv as a pandas dataframe, which contains various information for each of the 380 EPL games in the 2016-17 English Premier League season. We restricted the dataframe to the columns in which we‚Äôre interested (specifically, team names and numer of goals scored by each team). I‚Äôll omit most of the code that produces the graphs in this post. But don‚Äôt worry, you can find that code on <a href="https://github.com/dashee87/blogScripts/blob/master/Jupyter/2017-06-04-predicting-football-results-with-statistical-modelling.ipynb">my github page</a>. Our task is to model the final round of fixtures in the season, so we must remove the last 10 rows (each gameweek consists of 10 matches).</p>

<div><div><pre><code><span>epl_1617</span> <span>=</span> <span>epl_1617</span><span>[:</span><span>-</span><span>10</span><span>]</span>
<span>epl_1617</span><span>.</span><span>mean</span><span>()</span>
</code></pre></div></div>

<div><div><pre><code>HomeGoals    1.591892
AwayGoals    1.183784
dtype: float64
</code></pre></div></div>

<p>You‚Äôll notice that, on average, the home team scores more goals than the away team. This is the so called ‚Äòhome (field) advantage‚Äô (discussed <a href="https://jogall.github.io/2017-05-12-home-away-pref/">here</a>) and <a href="http://bleacherreport.com/articles/1803416-is-home-field-advantage-as-important-in-baseball-as-other-major-sports">isn‚Äôt specific to soccer</a>. This is a convenient time to introduce the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a>. It‚Äôs a discrete probability distribution that describes the probability of the number of events within a specific time period (e.g 90 mins) with a known average rate of occurrence. A key assumption is that the number of events is independent of time. In our context, this means that goals don‚Äôt become more/less likely by the number of goals already scored in the match. Instead, the number of goals is expressed purely as function an average rate of goals. If that was unclear, maybe this mathematical formulation will make clearer:</p>



<p> represents the average rate (e.g. average number of goals, average number of letters you receive, etc.). So, we can treat the number of goals scored by the home and away team as two independent Poisson distributions. The plot below shows the proportion of goals scored compared to the number of goals estimated by the corresponding Poisson distributions.</p>

<p><img src="https://dashee87.github.io/images/home_away_goals_python.png" alt=""></p>

<p>We can use this statistical model to estimate the probability of specfic events.</p>



<p>The probability of a draw is simply the sum of the events where the two teams score the same amount of goals.</p>



<p>Note that we consider the number of goals scored by each team to be independent events (i.e. P(A n B) = P(A) P(B)). The difference of two Poisson distribution is actually called a <a href="https://en.wikipedia.org/wiki/Skellam_distribution">Skellam distribution</a>. So we can calculate the probability of a draw by inputting the mean goal values into this distribution.</p>

<div><div><pre><code><span># probability of draw between home and away team</span>
<span>skellam</span><span>.</span><span>pmf</span><span>(</span><span>0.0</span><span>,</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>0</span><span>],</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>1</span><span>])</span>
</code></pre></div></div>



<div><div><pre><code><span># probability of home team winning by one goal</span>
<span>skellam</span><span>.</span><span>pmf</span><span>(</span><span>1</span><span>,</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>0</span><span>],</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>1</span><span>])</span>
</code></pre></div></div>



<p><img src="https://dashee87.github.io/images/skellam_goals_python.png" alt=""></p>

<p>So, hopefully you can see how we can adapt this approach to model specific matches. We just need to know the average number of goals scored by each team and feed this data into a Poisson model. Let‚Äôs have a look at the distribution of goals scored by Chelsea and Sunderland (teams who finished 1st and last, respectively).</p>

<p><img src="https://dashee87.github.io/images/chelsea_sunderland_goals_python.png" alt=""></p>

<h2 id="building-a-model">Building A Model</h2>

<p>You should now be convinced that the number of goals scored by each team can be approximated by a Poisson distribution. Due to a relatively sample size (each team plays at most 19 home/away games), the accuracy of this approximation can vary significantly (especially earlier in the season when teams have played fewer games). Similar to before, we could now calculate the probability of various events in this Chelsea Sunderland match. But rather than treat each match separately, we‚Äôll build a more general Poisson regression model (<a href="https://en.wikipedia.org/wiki/Poisson_regression">what is that?</a>).</p>

<div><div><pre><code><span># importing the tools required for the Poisson regression model</span>
<span>import</span> <span>statsmodels.api</span> <span>as</span> <span>sm</span>
<span>import</span> <span>statsmodels.formula.api</span> <span>as</span> <span>smf</span>

<span>goal_model_data</span> <span>=</span> <span>pd</span><span>.</span><span>concat</span><span>([</span><span>epl_1617</span><span>[[</span><span>'HomeTeam'</span><span>,</span><span>'AwayTeam'</span><span>,</span><span>'HomeGoals'</span><span>]]</span><span>.</span><span>assign</span><span>(</span><span>home</span><span>=</span><span>1</span><span>)</span><span>.</span><span>rename</span><span>(</span>
            <span>columns</span><span>=</span><span>{</span><span>'HomeTeam'</span><span>:</span><span>'team'</span><span>,</span> <span>'AwayTeam'</span><span>:</span><span>'opponent'</span><span>,</span><span>'HomeGoals'</span><span>:</span><span>'goals'</span><span>}),</span>
           <span>epl_1617</span><span>[[</span><span>'AwayTeam'</span><span>,</span><span>'HomeTeam'</span><span>,</span><span>'AwayGoals'</span><span>]]</span><span>.</span><span>assign</span><span>(</span><span>home</span><span>=</span><span>0</span><span>)</span><span>.</span><span>rename</span><span>(</span>
            <span>columns</span><span>=</span><span>{</span><span>'AwayTeam'</span><span>:</span><span>'team'</span><span>,</span> <span>'HomeTeam'</span><span>:</span><span>'opponent'</span><span>,</span><span>'AwayGoals'</span><span>:</span><span>'goals'</span><span>})])</span>

<span>poisson_model</span> <span>=</span> <span>smf</span><span>.</span><span>glm</span><span>(</span><span>formula</span><span>=</span><span>"goals ~ home + team + opponent"</span><span>,</span> <span>data</span><span>=</span><span>goal_model_data</span><span>,</span> 
                        <span>family</span><span>=</span><span>sm</span><span>.</span><span>families</span><span>.</span><span>Poisson</span><span>())</span><span>.</span><span>fit</span><span>()</span>
<span>poisson_model</span><span>.</span><span>summary</span><span>()</span>
</code></pre></div></div>

<table>
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>        <td>goals</td>      <th>  No. Observations:  </th>  <td>   740</td> 
</tr>
<tr>
  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   700</td> 
</tr>
<tr>
  <th>Model Family:</th>        <td>Poisson</td>     <th>  Df Model:          </th>  <td>    39</td> 
</tr>
<tr>
  <th>Link Function:</th>         <td>log</td>       <th>  Scale:             </th>    <td>1.0</td>  
</tr>
<tr>
  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -1042.4</td>
</tr>
<tr>
  <th>Date:</th>           <td>Sat, 10 Jun 2017</td> <th>  Deviance:          </th> <td>  776.11</td>
</tr>
<tr>
  <th>Time:</th>               <td>11:17:38</td>     <th>  Pearson chi2:      </th>  <td>  659.</td> 
</tr>
<tr>
  <th>No. Iterations:</th>         <td>8</td>        <th>                     </th>     <td> </td>   
</tr>
</tbody></table>
<table>
<tbody><tr>
               <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th> <th>[95.0% Conf. Int.]</th> 
</tr>
<tr>
  <th>Intercept</th>                  <td>    0.3725</td> <td>    0.198</td> <td>    1.880</td> <td> 0.060</td> <td>   -0.016     0.761</td>
</tr>
<tr>
  <th>team[T.Bournemouth]</th>        <td>   -0.2891</td> <td>    0.179</td> <td>   -1.612</td> <td> 0.107</td> <td>   -0.641     0.062</td>
</tr>
<tr>
  <th>team[T.Burnley]</th>            <td>   -0.6458</td> <td>    0.200</td> <td>   -3.230</td> <td> 0.001</td> <td>   -1.038    -0.254</td>
</tr>
<tr>
  <th>team[T.Chelsea]</th>            <td>    0.0789</td> <td>    0.162</td> <td>    0.488</td> <td> 0.626</td> <td>   -0.238     0.396</td>
</tr>
<tr>
  <th>team[T.Crystal Palace]</th>     <td>   -0.3865</td> <td>    0.183</td> <td>   -2.107</td> <td> 0.035</td> <td>   -0.746    -0.027</td>
</tr>
<tr>
  <th>team[T.Everton]</th>            <td>   -0.2008</td> <td>    0.173</td> <td>   -1.161</td> <td> 0.246</td> <td>   -0.540     0.138</td>
</tr>
<tr>
  <th>team[T.Hull]</th>               <td>   -0.7006</td> <td>    0.204</td> <td>   -3.441</td> <td> 0.001</td> <td>   -1.100    -0.302</td>
</tr>
<tr>
  <th>team[T.Leicester]</th>          <td>   -0.4204</td> <td>    0.187</td> <td>   -2.249</td> <td> 0.025</td> <td>   -0.787    -0.054</td>
</tr>
<tr>
  <th>team[T.Liverpool]</th>          <td>    0.0162</td> <td>    0.164</td> <td>    0.099</td> <td> 0.921</td> <td>   -0.306     0.338</td>
</tr>
<tr>
  <th>team[T.Man City]</th>           <td>    0.0117</td> <td>    0.164</td> <td>    0.072</td> <td> 0.943</td> <td>   -0.310     0.334</td>
</tr>
<tr>
  <th>team[T.Man United]</th>         <td>   -0.3572</td> <td>    0.181</td> <td>   -1.971</td> <td> 0.049</td> <td>   -0.713    -0.002</td>
</tr>
<tr>
  <th>team[T.Middlesbrough]</th>      <td>   -1.0087</td> <td>    0.225</td> <td>   -4.481</td> <td> 0.000</td> <td>   -1.450    -0.568</td>
</tr>
<tr>
  <th>team[T.Southampton]</th>        <td>   -0.5804</td> <td>    0.195</td> <td>   -2.976</td> <td> 0.003</td> <td>   -0.963    -0.198</td>
</tr>
<tr>
  <th>team[T.Stoke]</th>              <td>   -0.6082</td> <td>    0.197</td> <td>   -3.094</td> <td> 0.002</td> <td>   -0.994    -0.223</td>
</tr>
<tr>
  <th>team[T.Sunderland]</th>         <td>   -0.9619</td> <td>    0.222</td> <td>   -4.329</td> <td> 0.000</td> <td>   -1.397    -0.526</td>
</tr>
<tr>
  <th>team[T.Swansea]</th>            <td>   -0.5136</td> <td>    0.192</td> <td>   -2.673</td> <td> 0.008</td> <td>   -0.890    -0.137</td>
</tr>
<tr>
  <th>team[T.Tottenham]</th>          <td>    0.0532</td> <td>    0.162</td> <td>    0.328</td> <td> 0.743</td> <td>   -0.265     0.371</td>
</tr>
<tr>
  <th>team[T.Watford]</th>            <td>   -0.5969</td> <td>    0.197</td> <td>   -3.035</td> <td> 0.002</td> <td>   -0.982    -0.211</td>
</tr>
<tr>
  <th>team[T.West Brom]</th>          <td>   -0.5567</td> <td>    0.194</td> <td>   -2.876</td> <td> 0.004</td> <td>   -0.936    -0.177</td>
</tr>
<tr>
  <th>team[T.West Ham]</th>           <td>   -0.4802</td> <td>    0.189</td> <td>   -2.535</td> <td> 0.011</td> <td>   -0.851    -0.109</td>
</tr>
<tr>
  <th>opponent[T.Bournemouth]</th>    <td>    0.4109</td> <td>    0.196</td> <td>    2.092</td> <td> 0.036</td> <td>    0.026     0.796</td>
</tr>
<tr>
  <th>opponent[T.Burnley]</th>        <td>    0.1657</td> <td>    0.206</td> <td>    0.806</td> <td> 0.420</td> <td>   -0.237     0.569</td>
</tr>
<tr>
  <th>opponent[T.Chelsea]</th>        <td>   -0.3036</td> <td>    0.234</td> <td>   -1.298</td> <td> 0.194</td> <td>   -0.762     0.155</td>
</tr>
<tr>
  <th>opponent[T.Crystal Palace]</th> <td>    0.3287</td> <td>    0.200</td> <td>    1.647</td> <td> 0.100</td> <td>   -0.062     0.720</td>
</tr>
<tr>
  <th>opponent[T.Everton]</th>        <td>   -0.0442</td> <td>    0.218</td> <td>   -0.202</td> <td> 0.840</td> <td>   -0.472     0.384</td>
</tr>
<tr>
  <th>opponent[T.Hull]</th>           <td>    0.4979</td> <td>    0.193</td> <td>    2.585</td> <td> 0.010</td> <td>    0.120     0.875</td>
</tr>
<tr>
  <th>opponent[T.Leicester]</th>      <td>    0.3369</td> <td>    0.199</td> <td>    1.694</td> <td> 0.090</td> <td>   -0.053     0.727</td>
</tr>
<tr>
  <th>opponent[T.Liverpool]</th>      <td>   -0.0374</td> <td>    0.217</td> <td>   -0.172</td> <td> 0.863</td> <td>   -0.463     0.389</td>
</tr>
<tr>
  <th>opponent[T.Man City]</th>       <td>   -0.0993</td> <td>    0.222</td> <td>   -0.448</td> <td> 0.654</td> <td>   -0.534     0.335</td>
</tr>
<tr>
  <th>opponent[T.Man United]</th>     <td>   -0.4220</td> <td>    0.241</td> <td>   -1.754</td> <td> 0.079</td> <td>   -0.894     0.050</td>
</tr>
<tr>
  <th>opponent[T.Middlesbrough]</th>  <td>    0.1196</td> <td>    0.208</td> <td>    0.574</td> <td> 0.566</td> <td>   -0.289     0.528</td>
</tr>
<tr>
  <th>opponent[T.Southampton]</th>    <td>    0.0458</td> <td>    0.211</td> <td>    0.217</td> <td> 0.828</td> <td>   -0.369     0.460</td>
</tr>
<tr>
  <th>opponent[T.Stoke]</th>          <td>    0.2266</td> <td>    0.203</td> <td>    1.115</td> <td> 0.265</td> <td>   -0.172     0.625</td>
</tr>
<tr>
  <th>opponent[T.Sunderland]</th>     <td>    0.3707</td> <td>    0.198</td> <td>    1.876</td> <td> 0.061</td> <td>   -0.017     0.758</td>
</tr>
<tr>
  <th>opponent[T.Swansea]</th>        <td>    0.4336</td> <td>    0.195</td> <td>    2.227</td> <td> 0.026</td> <td>    0.052     0.815</td>
</tr>
<tr>
  <th>opponent[T.Tottenham]</th>      <td>   -0.5431</td> <td>    0.252</td> <td>   -2.156</td> <td> 0.031</td> <td>   -1.037    -0.049</td>
</tr>
<tr>
  <th>opponent[T.Watford]</th>        <td>    0.3533</td> <td>    0.198</td> <td>    1.782</td> <td> 0.075</td> <td>   -0.035     0.742</td>
</tr>
<tr>
  <th>opponent[T.West Brom]</th>      <td>    0.0970</td> <td>    0.209</td> <td>    0.463</td> <td> 0.643</td> <td>   -0.313     0.507</td>
</tr>
<tr>
  <th>opponent[T.West Ham]</th>       <td>    0.3485</td> <td>    0.198</td> <td>    1.758</td> <td> 0.079</td> <td>   -0.040     0.737</td>
</tr>
<tr>
  <th>home</th>                       <td>    0.2969</td> <td>    0.063</td> <td>    4.702</td> <td> 0.000</td> <td>    0.173     0.421</td>
</tr>
</tbody></table>

<p>If you‚Äôre curious about the <code>smf.glm(...)</code> part, you can find more information <a href="http://www.statsmodels.org/stable/examples/notebooks/generated/glm_formula.html">here</a> (edit: earlier versions of this post had erroneously employed a Generalised Estimating Equation (GEE)- <a href="https://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models">what‚Äôs the difference?</a>). I‚Äôm more interested in the values presented in the <code>coef</code> column in the model summary table, which are analogous to the slopes in linear regression. Similar to <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, we take the <a href="http://www.lisa.stat.vt.edu/sites/default/files/Poisson.and_.Logistic.Regression.pdf">exponent of the parameter values</a>. A positive value implies more goals (), while values closer to zero represent more neutral effects (). Towards the bottom of the table you might notice that <code>home</code> has a <code>coef</code> of 0.2969. This captures the fact that home teams generally score more goals than the away team (specifically, =1.35 times more likely). But not all teams are created equal. Chelsea has a <code>coef</code> of 0.0789, while the corresponding value for Sunderland is -0.9619 (sort of saying Chelsea (Sunderland) are better (much worse!) scorers than average). Finally, the <code>opponent*</code> values ‚Ä¶</p></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/">https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/</a></em></p>]]>
            </description>
            <link>https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24838958</guid>
            <pubDate>Tue, 20 Oct 2020 16:02:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Just Write the Parser]]>
            </title>
            <description>
<![CDATA[
Score 250 | Comments 64 (<a href="https://news.ycombinator.com/item?id=24837898">thread link</a>) | @matt_d
<br/>
October 20, 2020 | https://tiarkrompf.github.io/notes/?/just-write-the-parser/ | <a href="https://web.archive.org/web/*/https://tiarkrompf.github.io/notes/?/just-write-the-parser/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://tiarkrompf.github.io/notes/?/just-write-the-parser/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24837898</guid>
            <pubDate>Tue, 20 Oct 2020 14:44:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why build another website builder?]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 87 (<a href="https://news.ycombinator.com/item?id=24837331">thread link</a>) | @apledger3
<br/>
October 20, 2020 | https://www.makeswift.com/blog/why-build-another-website-builder | <a href="https://web.archive.org/web/*/https://www.makeswift.com/blog/why-build-another-website-builder">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div width="[object Object]" data-slate-editor="true" data-key="8882" autocorrect="on" spellcheck="true" data-gramm="false"><p data-slate-object="block" data-key="8883"><span data-slate-object="text" data-key="8884"><span data-slate-leaf="true" data-offset-key="8884:0"><span><span data-slate-string="true">Name something expensive that's difficult to build and trashed after each use.</span></span></span></span></p><p data-slate-object="block" data-key="8887"><span data-slate-object="text" data-key="8888"><span data-slate-leaf="true" data-offset-key="8888:0"><span><span data-slate-string="true">Did you say rockets? No, SpaceX figured out how to reuse those in 2015. </span></span></span></span></p><p data-slate-object="block" data-key="8891"><span data-slate-object="text" data-key="8892"><span data-slate-leaf="true" data-offset-key="8892:0"><span><span data-slate-string="true">I'm talking about your company's website. Completely overhauling the website has become so routine that we've become numb to the pain of throwing it all away, over and over again.</span></span></span></span></p><p data-slate-object="block" data-key="8895"><span data-slate-object="text" data-key="8896"><span data-slate-leaf="true" data-offset-key="8896:0"><span><span data-slate-string="true">Let's explore why this happens.</span></span></span></span></p><p data-slate-object="block" data-key="8899"><span data-slate-object="text" data-key="8900"><span data-slate-leaf="true" data-offset-key="8900:0"><span><span data-slate-string="true">Imagine you're the cofounder of a new company. You need to set up a website, but your product partner is already behind on features so that leaves it solely up to you. What's your plan? Given the tight budget and schedule, the disciplined move would be to quickly use a template in an inexpensive website builder. You can graduate to something a little more custom once you have more resources and your brand, positioning, and voice figured out.</span></span></span></span></p><p data-slate-object="block" data-key="8903"><span data-slate-object="text" data-key="8904"><span data-slate-leaf="true" data-offset-key="8904:0"><span><span data-slate-string="true">Sounds like a good plan, but anyone who's gone down this path knows it's never as simple as it seems.</span></span></span></span></p><p data-slate-object="block" data-key="8907"><span data-slate-object="text" data-key="8908"><span data-slate-leaf="true" data-offset-key="8908:0"><span><span data-slate-string="true">For one, the template never lasts as long as you need it to. You realize the design isn't working, or your competitor just unexpectedly made a move. So you start trying to make changes. Swapping out the text and images comes easy, but as soon as you start adjusting the layout, frustration begins to set in. You've got a million other things to do and for some reason you can't get the page to look right on mobile. </span></span></span></span></p><p data-slate-object="block" data-key="8911"><span data-slate-object="text" data-key="8912"><span data-slate-leaf="true" data-offset-key="8912:0"><span><span data-slate-string="true">As it turns out, the same features and guard rails that made it easy to stand up the template have now become the reason it's hard to iterate. You're stuck, and whether or not you were ready, the time has come</span></span></span></span><span data-slate-object="text" data-key="8913"><span data-slate-leaf="true" data-offset-key="8913:0"><span><span data-slate-string="true"> to deal with the classic website building dilemma:</span></span></span></span></p><p data-slate-object="block" data-key="8916"><span data-slate-object="text" data-key="8917"><span data-slate-leaf="true" data-offset-key="8917:0"><span><span data-slate-string="true">Do you stay put and compromise on your vision, or invest in another solution that </span></span></span></span><span data-slate-object="text" data-key="8918"><span data-slate-leaf="true" data-offset-key="8918:0"><span><span data-slate-string="true">might</span></span></span></span><span data-slate-object="text" data-key="8919"><span data-slate-leaf="true" data-offset-key="8919:0"><span><span data-slate-string="true"> be better?</span></span></span></span></p><p data-slate-object="block" data-key="8922"><span data-slate-object="text" data-key="8923"><span data-slate-leaf="true" data-offset-key="8923:0"><span><span data-slate-string="true">After researching more advanced solutions for a few days, it starts to become apparent that you're out of your depth. There are so many different products, and every time you begin stepping away from the template you're consistently met with a giant learning curve. Frustration sets in again so you decide to call that technical friend. You're in luck! She's got her favorite tool and she's available to help.</span></span></span></span></p><p data-slate-object="block" data-key="8926"><span data-slate-object="text" data-key="8927"><span data-slate-leaf="true" data-offset-key="8927:0"><span><span data-slate-string="true">Unfortunately, all you're about to do is move your bottleneck. You may think once things are set up you can say your goodbyes, but the reality is you're never free. As your team and ideas grow you'll need more and more help, and she'll be the only one who can make the big changes. Your projects will start moving slower and slower. But on the bright side, at least you'll be able to get it done... eventually.</span></span></span></span></p><p data-slate-object="block" data-key="8930"><span data-slate-object="text" data-key="8931"><span data-slate-leaf="true" data-offset-key="8931:0"><span><span data-slate-string="true">But what happens if she leaves? Nobody likes stepping into someone else's mess, so it ends up being cheaper to just rebuild using whatever tool the new technical expert is comfortable with. </span></span></span></span><span data-slate-object="text" data-key="8932"><span data-slate-leaf="true" data-offset-key="8932:0"><span><span data-slate-string="true">Is it any mystery, then, why we're stuck in this constant build, trash, move, build cycle?</span></span></span></span><span data-slate-object="text" data-key="8933"><span data-slate-leaf="true" data-offset-key="8933:0"><span><span data-slate-string="true"> </span></span></span></span><span data-slate-object="text" data-key="8934"><span data-slate-leaf="true" data-offset-key="8934:0"><span><span data-slate-string="true">The solutions have changed, but the underlying trade-off hasn't. </span></span></span></span><span data-slate-object="text" data-key="8935"><span data-slate-leaf="true" data-offset-key="8935:0"><span><span data-slate-string="true">All options are either too basic and restrictive, or too technical and complex.</span></span></span></span></p><p data-slate-object="block" data-key="8938"><span data-slate-object="text" data-key="8939"><span data-slate-leaf="true" data-offset-key="8939:0"><span><span data-slate-string="true">Founders building out early marketing for their startups are not the only ones who experience this. At some point, all marketers feel this pain.</span></span></span></span></p><p data-slate-object="block" data-key="8942"><span data-slate-object="text" data-key="8943"><span data-slate-leaf="true" data-offset-key="8943:0"><span><span data-slate-string="true">The problem is so widespread that it's given birth to a whole category of products called landing page builders.</span></span></span></span></p><p data-slate-object="block" data-key="8946"><span data-slate-object="text" data-key="8947"><span data-slate-leaf="true" data-offset-key="8947:0"><span><span data-slate-string="true">Marketers are so fed up with being sidelined that they're willing to duct tape their site with </span></span></span></span><span data-slate-object="text" data-key="8948"><span data-slate-leaf="true" data-offset-key="8948:0"><span><span data-slate-string="true">mostly</span></span></span></span><span data-slate-object="text" data-key="8949"><span data-slate-leaf="true" data-offset-key="8949:0"><span><span data-slate-string="true"> on-brand pages, just to have some operational independence when it comes to web content. Sure, landing page builders can have extra features bolted on like A/B testing and analytics, but they aren't the real reason marketers are out shopping for a solution. Marketers are looking for a place where they can get creative and not break anything.</span></span></span></span></p><p data-slate-object="block" data-key="8952"><span data-slate-object="text" data-key="8953"><span data-slate-leaf="true" data-offset-key="8953:0"><span><span data-slate-string="true">We know this because we built and eventually sunset a landing page builder, Landing Lion. Many of our customers wished they could build full websites in it, and some actually did. Despite its many shortcomings around bulk management, a surprising number of customers were willing to put in extra manual work to build and maintain full websites. The reason? Our user experience was actually designed for them‚Äîintelligent and tech-savvy generalists who wanted to break free from the template without having to go get a degree. They could finally build exactly what they envisioned and they could do it by themselves, quickly.</span></span></span></span></p><p data-slate-object="block" data-key="8956"><span data-slate-object="text" data-key="8957"><span data-slate-leaf="true" data-offset-key="8957:0"><span><span data-slate-string="true">The real problem was that for the rest of our customers, their websites were locked down. They'd been delicately constructed by the real owners, the technical experts. But who is ultimately responsible for the </span></span></span></span><span data-slate-object="text" data-key="8958"><span data-slate-leaf="true" data-offset-key="8958:0"><span><span data-slate-string="true">entire</span></span></span></span><span data-slate-object="text" data-key="8959"><span data-slate-leaf="true" data-offset-key="8959:0"><span><span data-slate-string="true"> brand experience, website included? The marketers.</span></span></span></span></p><p data-slate-object="block" data-key="8962"><span data-slate-object="text" data-key="8963"><span data-slate-leaf="true" data-offset-key="8963:0"><span><span data-slate-string="true">That's why we're building Makeswift.</span></span></span></span></p></div></div><div><div width="[object Object]" data-slate-editor="true" data-key="8965" autocorrect="on" spellcheck="true" data-gramm="false"><p data-slate-object="block" data-key="8966"><span data-slate-object="text" data-key="8967"><span data-slate-leaf="true" data-offset-key="8967:0"><span><span data-slate-string="true">Marketers need a website builder designed just for them. Our mission is to tackle the issues holding back marketing teams from building, shipping, and iterating on </span></span></span></span><span data-slate-object="text" data-key="8968"><span data-slate-leaf="true" data-offset-key="8968:0"><span><span data-slate-string="true">all</span></span></span></span><span data-slate-object="text" data-key="8969"><span data-slate-leaf="true" data-offset-key="8969:0"><span><span data-slate-string="true"> web content, on their own schedule.</span></span></span></span></p><p data-slate-object="block" data-key="8972"><span data-slate-object="text" data-key="8973"><span data-slate-leaf="true" data-offset-key="8973:0"><span><span data-slate-string="true">To do this, we need to fix the user experience. More specifically, we need to move away from the split "template &amp; content" user experience found in nearly all advanced website solutions. This approach exposes two distinct experiences to the end user: One for a technical specialist to build a template, and another basic experience, usually a form, to plug in content. This creates a problem where the people responsible for the content can't change the template‚Äîsound familiar?</span></span></span></span></p><p data-slate-object="block" data-key="8976"><span data-slate-object="text" data-key="8977"><span data-slate-leaf="true" data-offset-key="8977:0"><span><span data-slate-string="true">To move as fast as possible, the people in charge of </span></span></span></span><span data-slate-object="text" data-key="8978"><span data-slate-leaf="true" data-offset-key="8978:0"><span><span data-slate-string="true">what</span></span></span></span><span data-slate-object="text" data-key="8979"><span data-slate-leaf="true" data-offset-key="8979:0"><span><span data-slate-string="true"> to build need to also control </span></span></span></span><span data-slate-object="text" data-key="8980"><span data-slate-leaf="true" data-offset-key="8980:0"><span><span data-slate-string="true">how</span></span></span></span><span data-slate-object="text" data-key="8981"><span data-slate-leaf="true" data-offset-key="8981:0"><span><span data-slate-string="true"> it's built.</span></span></span></span></p><p data-slate-object="block" data-key="8984"><span data-slate-object="text" data-key="8985"><span data-slate-leaf="true" data-offset-key="8985:0"><span><span data-slate-string="true">That's why we're focused on designing a single, elegant user experience that can be easily taught to an entire team of tech-savvy generalists. The challenge is to provide enough power for advanced use cases without making the product difficult to </span></span></span></span><span data-slate-object="text" data-key="8986"><span data-slate-leaf="true" data-offset-key="8986:0"><span><span data-slate-string="true">learn</span></span></span></span><span data-slate-object="text" data-key="8987"><span data-slate-leaf="true" data-offset-key="8987:0"><span><span data-slate-string="true">. Learning to build completely custom websites should be no more complicated than learning to design a slide show presentation.</span></span></span></span></p><p data-slate-object="block" data-key="8990"><span data-slate-object="text" data-key="8991"><span data-slate-leaf="true" data-offset-key="8991:0"><span><span data-slate-string="true">So why build another website builder? With what feels like a new website builder popping up everyday, it's apparent that we're all still searching for something better. If you're interested in shaping the way we build, ship, and iterate on web content, please sign up for our early access program.</span></span></span></span></p></div></div></div>]]>
            </description>
            <link>https://www.makeswift.com/blog/why-build-another-website-builder</link>
            <guid isPermaLink="false">hacker-news-small-sites-24837331</guid>
            <pubDate>Tue, 20 Oct 2020 13:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Differential Dataflow]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24837031">thread link</a>) | @timhigins
<br/>
October 20, 2020 | https://timelydataflow.github.io/differential-dataflow/introduction.html | <a href="https://web.archive.org/web/*/https://timelydataflow.github.io/differential-dataflow/introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                
                

                
                
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        
<p>In this book we will work through the motivation and technical details behind <a href="https://github.com/frankmcsherry/differential-dataflow">differential dataflow</a>, a computational framework build on top of <a href="https://github.com/frankmcsherry/timely-dataflow">timely dataflow</a> intended for efficiently performing computations on large amounts of data and <em>maintaining</em> the computations as the data change.</p>
<p>Differential dataflow programs look like many standard "big data" computations, borrowing idioms from frameworks like MapReduce and SQL. However, once you write and run your program, you can <em>change</em> the data inputs to the computation, and differential dataflow will promptly show you the corresponding changes in its output. Promptly meaning in as little as milliseconds.</p>
<p>This relatively simple set-up, write programs and then change inputs, leads to a surprising breadth of exciting and new classes of scalable computation. We will explore it in this document!</p>
<hr>
<p>Differential dataflow arose from <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/naiad_sosp2013.pdf">work at Microsoft Research</a>, where we aimed to build a high-level framework that could both compute and incrementally maintain non-trivial algorithms.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        
                            <a rel="next" href="https://timelydataflow.github.io/differential-dataflow/chapter_0/chapter_0.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>
                        

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                

                
                    <a rel="next" href="https://timelydataflow.github.io/differential-dataflow/chapter_0/chapter_0.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        
        

        

        
        
        
        
        

        
        
        

        <!-- Custom JS scripts -->
        

        

    

</div>]]>
            </description>
            <link>https://timelydataflow.github.io/differential-dataflow/introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24837031</guid>
            <pubDate>Tue, 20 Oct 2020 13:31:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The surprising impact of medium-size texts on PostgreSQL performance]]>
            </title>
            <description>
<![CDATA[
Score 177 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24836979">thread link</a>) | @haki
<br/>
October 20, 2020 | https://hakibenita.com/sql-medium-text-performance | <a href="https://web.archive.org/web/*/https://hakibenita.com/sql-medium-text-performance">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article data-progress-indicator="">
        <hr>
<p>Any database schema is likely to have plenty of text fields. In this article, I divide text fields into three categories:</p>
<ol>
<li>
<p><strong>Small texts</strong>: names, slugs, usernames, emails, etc. These are text fields that usually have some low size limit, maybe even using <code>varchar(n)</code> and not <code>text</code>.</p>
</li>
<li>
<p><strong>Large texts</strong>: blog post content, articles, HTML content etc. These are large pieces of free, unrestricted text that is stored in the database.</p>
</li>
<li>
<p><strong>Medium texts</strong>: descriptions, comments, product reviews, stack traces etc. These are any text field that is between the small and the large. These type of texts would normally be unrestricted, but naturally smaller than the large texts.</p>
</li>
</ol>
<p><strong>In this article I demonstrate the surprising impact of medium-size texts on query performance in PostgreSQL.</strong></p>
<figure><img alt="Sliced bread... it gets better<br><small>Photo by <a href=&quot;https://unsplash.com/photos/WHJTaLqonkU&quot;>Louise Lysh√∏j</a></small>" src="https://hakibenita.com/images/00-sql-medium-text-performance.jpg"><figcaption>Sliced bread... it gets better<br><small>Photo by <a href="https://unsplash.com/photos/WHJTaLqonkU">Louise Lysh√∏j</a></small></figcaption>
</figure>
<details open="">
    <summary>Table of Contents</summary>

</details>
<hr>
<h2 id="understanding-toast"><a href="#understanding-toast">Understanding TOAST</a></h2>
<p>When talking about large chunks of text, or any other field that may contain large amounts of data, we first need to understand how the database handles the data. Intuitively, you might think that the database is storing large pieces of data inline like it does smaller pieces of data, but in fact, <a href="https://www.postgresql.org/docs/current/storage-toast.html" rel="noopener">it does not</a>:</p>
<blockquote>
<p>PostgreSQL uses a fixed page size (commonly 8 kB), and does not allow tuples to span multiple pages. Therefore, it is not possible to store very large field values directly.</p>
</blockquote>
<p>As the documentation explains, PostgreSQL can't store rows (tuples) in multiple pages. So how does the database store large chunks of data?</p>
<blockquote>
<p>[...] large field values are compressed and/or broken up into multiple physical rows. [...] The technique is affectionately known as TOAST (or ‚Äúthe best thing since sliced bread‚Äù).</p>
</blockquote>
<p>OK, so how is this TOAST working exactly?</p>
<blockquote>
<p>If any of the columns of a table are TOAST-able, the table will have an associated TOAST table</p>
</blockquote>
<p>So TOAST is a separate table associated with our table. It is used to store large pieces of data of TOAST-able columns (the <code>text</code> datatype for example, is TOAST-able).</p>
<p>What constitutes a large value?</p>
<blockquote>
<p>The TOAST management code is triggered only when a row value to be stored in a table is wider than TOAST_TUPLE_THRESHOLD bytes (normally 2 kB). The TOAST code will compress and/or move field values out-of-line until the row value is shorter than TOAST_TUPLE_TARGET bytes (also normally 2 kB, adjustable) or no more gains can be had</p>
</blockquote>
<p>PostgreSQL will try to compress a the large values in the row, and if the row can't fit within the limit, the values will be stored out-of-line in the TOAST table.</p>
<h3 id="finding-the-toast"><a href="#finding-the-toast">Finding the TOAST</a></h3>
<p>Now that we have <em>some</em> understanding of what TOAST is, let's see it in action. First, create a table with a text field:</p>
<div><pre><span></span><span>db=#</span> <span>CREATE</span> <span>TABLE</span> <span>toast_test</span> <span>(</span><span>id</span> <span>SERIAL</span><span>,</span> <span>value</span> <span>TEXT</span><span>);</span>
<span>CREATE TABLE</span>
</pre></div>


<p>The table contains an id column, and a value field of type <code>TEXT</code>. Notice that we did not change any of the default storage parameters.</p>
<p>The text field we added supports TOAST, or is TOAST-able, so PostgreSQL should create a TOAST table. Let's try to locate the TOAST table associated with the table <code>toast_test</code> in <a href="https://www.postgresql.org/docs/current/catalog-pg-class.html" rel="noopener"><code>pg_class</code></a>:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>relname</span><span>,</span> <span>reltoastrelid</span> <span>FROM</span> <span>pg_class</span> <span>WHERE</span> <span>relname</span> <span>=</span> <span>'toast_test'</span><span>;</span>
<span>  relname   ‚îÇ reltoastrelid</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span> toast_test ‚îÇ        340488</span>

<span>db=#</span> <span>SELECT</span> <span>relname</span> <span>FROM</span> <span>pg_class</span> <span>WHERE</span> <span>oid</span> <span>=</span> <span>340488</span><span>;</span>
<span>     relname</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span> pg_toast_340484</span>
</pre></div>


<p>As promised, PostgreSQL created a TOAST table called <code>pg_toast_340484</code>.</p>
<h3 id="toast-in-action"><a href="#toast-in-action">TOAST in Action</a></h3>
<p>Let's see what the TOAST table looks like:</p>
<div><pre><span></span><span>db=#</span> <span>\d</span> <span>pg_toast.pg_toast_340484</span>
<span>TOAST table "pg_toast.pg_toast_340484"</span>
<span>   Column   ‚îÇ  Type</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span> chunk_id   ‚îÇ oid</span>
<span> chunk_seq  ‚îÇ integer</span>
<span> chunk_data ‚îÇ bytea</span>
</pre></div>


<p>The TOAST table contains three columns:</p>
<ul>
<li><code>chunk_id</code>: A reference to a toasted value.</li>
<li><code>chunk_seq</code>: A sequence within the chunk.</li>
<li><code>chunk_data</code>: The actual chunk data.</li>
</ul>
<p>Similar to "regular" tables, the TOAST table also has the same restrictions on inline values. To overcome this restriction, large values are split into chunks that can fit within the limit.</p>
<p>At this point the table is empty:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span><span>;</span>
<span> chunk_id ‚îÇ chunk_seq ‚îÇ chunk_data</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>(0 rows)</span>
</pre></div>


<p>This makes sense because we did not insert any data yet. So next, insert a small value into the table:</p>
<div><pre><span></span><span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>'small value'</span><span>);</span>
<span>INSERT 0 1</span>

<span>db=#</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span><span>;</span>
<span> chunk_id ‚îÇ chunk_seq ‚îÇ chunk_data</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>(0 rows)</span>
</pre></div>


<p>After inserting the small value into the table, the TOAST table remained empty. This means the small value was small enough to be stored inline, and there was no need to move it out-of-line to the TOAST table.</p>
<figure>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 176.3 111" height="10em"><path d="M9 47c62-3 116 2 153-2M13 47c49 0 93 0 150 2m-3-4c3 17 2 22 4 53m-2-52c-3 14-2 31 0 56m2-4c-37 1-78 7-150 5m149-4H13m0 4c-3-15 1-28-4-56m3 55c2-11-1-25 1-54" stroke="currentColor" fill="none"></path><text y="15" font-size="16" fill="currentColor" transform="translate(33 60)">1</text><text y="15" font-size="16" fill="currentColor" transform="translate(61 61)">"small value"</text><path d="M10 12l155-1v37L12 46" stroke-width="0" fill="#f2f2f2"></path><path d="M10 9c43-1 84 0 156 2M11 11c36 1 75 0 155-1m1-2l-2 38m1-36v39m1 0c-39 2-78 1-159 0m158-2c-39 2-77 2-155 0m-3 1c0-8 3-20 3-36m-1 37V10" stroke="currentColor" fill="none"></path><path d="M52 16l4 81m0-84c-1 19-5 36-3 88" stroke="currentColor" fill="none"></path><text y="15" font-size="16" transform="translate(23 18)">id</text><g><text x="20" y="15" font-size="16" text-anchor="middle" transform="translate(68 19)">value</text></g></svg>
<figcaption>Small text stored inline</figcaption>
</figure>

<p>Let's insert a large value and see what happens:</p>
<div><pre><span></span><span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>'n0cfPGZOCwzbHSMRaX8 ... WVIlRkylYishNyXf'</span><span>);</span>
<span>INSERT 0 1</span>
</pre></div>


<p>I shortened the value for brevity, but that's a random string with 4096 characters. Let's see what the TOAST table stores now:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span><span>;</span>
<span> chunk_id ‚îÇ chunk_seq ‚îÇ chunk_data</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>   995899 ‚îÇ         0 ‚îÇ \x30636650475a4f43...</span>
<span>   995899 ‚îÇ         1 ‚îÇ \x50714c3756303567...</span>
<span>   995899 ‚îÇ         2 ‚îÇ \x6c78426358574534...</span>
<span>(3 rows)</span>
</pre></div>


<p>The large value is stored out-of-line in the TOAST table. Because the value was too large to fit inline in a single row, PostgreSQL split it into three chunks. The <code>\x3063...</code> notation is how psql displays binary data.</p>
<figure>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 435.8 254" height="20em"><path d="M9 43c58 5 116 4 154 6M14 48c34-1 70 1 148-1m2 0c-5 34-4 68-3 92m0-94c1 34 4 69 2 96m-4 0c-27-2-54-2-143-5m145 3c-47 2-96 2-147 1m-2 3c5-29 2-60 0-94m2 89c1-36-2-72-3-90" stroke="currentColor" fill="none"></path><path d="M10 95c45-2 79 3 150 0M12 96c53 0 106-3 147-3" stroke="currentColor" fill="none"></path><text y="15" font-size="16" fill="currentColor" transform="translate(33 60)">1</text><text y="15" font-size="16" fill="currentColor" transform="translate(61 61)">"small value"</text><text y="15" font-size="16" fill="currentColor" transform="translate(34 106)">2</text><path d="M108 109l4 3 3 5-2 4-5 4h-4l-4-3v-7c0-2 1-2 3-3l6-3 1 1m-4-2l3 3 3 5 1 4c0 2-1 2-2 3l-6 3-4-5c-1-2-3-4-2-5l2-4 4-4 1 3" stroke-width="0" fill="#f41d92"></path><path d="M105 110h5l2 4 2 5c0 2-3 3-4 4l-4 2c-2 0-2-1-3-2l-4-6 2-4 4-5 1 2m-1 0l6 1 3 3-1 4-1 4-6 3-3-1-3-5v-7l6-2-2-1M115 115c30 2 64 1 141 0m-141 1c34-1 70-3 142-1" stroke="currentColor" fill="none"></path><path d="M229 125c5-1 13-4 26-11m-26 11l27-10" stroke="currentColor" fill="none"></path><path d="M229 104c5 4 13 6 26 10m-26-9c6 3 14 4 27 10" stroke="currentColor" fill="none"></path><path d="M275 104h152v137l-151 3" stroke-width="0" fill="#f41d92"></path><path d="M276 99c36 2 66 5 146 7m-148-3c47-2 87-3 150 0m2 4c-5 40-4 85-2 135m2-139c-2 41-1 78 0 140m1-3c-53 5-105-2-149 0m146 4c-45 1-90 0-146-2m1-1c-4-56-3-110 0-135m-5 135c5-48 3-100 2-138" stroke="currentColor" fill="none"></path><path d="M314 107c1 30 5 66 8 136m-3-136v137" stroke="currentColor" fill="none"></path><path d="M279 153c36 6 77 8 145 2m-145 0c50-2 102-2 146-2M271 194c46 0 91 2 151 5m-148-2c29-3 59-1 146-1" stroke="currentColor" fill="none"></path><text y="15" font-size="16" fill="currentColor" transform="translate(294 165)">2</text><text y="15" font-size="16" fill="currentColor" transform="translate(295 119)">1</text><text y="15" font-size="16" fill="currentColor" transform="translate(292 207)">3</text><text y="15" font-size="16" fill="currentColor" transform="translate(329 118)">\x.....</text><text y="15" font-size="16" fill="currentColor" transform="translate(331 164)">\x.....</text><text y="15" font-size="16" fill="currentColor" transform="translate(332 206)">\x.....</text><g><path d="M12 9l153 1-1 36-153 1" stroke-width="0" fill="#f2f2f2"></path><path d="M11 12c50 0 100-4 157-2M9 11c48-3 97-3 158-1m1 0c-3 13 0 29-2 39m0-39v39m-1-1c-41-2-80 1-154-2m156 2c-62 2-122 2-158 0m2 1c1-12-2-25-3-40m1 39c1-12 2-23 1-38" stroke="currentColor" fill="none"></path></g><g><path d="M55 11c-1 39 2 67 0 130M54 16c4 36 3 74 1 120" stroke="currentColor" fill="none"></path></g><g><text y="15" font-size="16" transform="translate(23 18)">id</text></g><g><text x="20" y="15" font-size="16" text-anchor="middle" transform="translate(68 19)">value</text></g></svg>
<figcaption>Large text stored out-of-line, in the associated TOAST table</figcaption>
</figure>

<p>Finally, execute the following query to summarize the data in the TOAST table:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>
<span> chunk_id ‚îÇ chunks ‚îÇ pg_size_pretty</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>   995899 ‚îÇ      3 ‚îÇ 4096 bytes</span>
<span>(1 row)</span>
</pre></div>


<p>As we've already seen, the text is stored in three chunks.</p>
<div>
<p>size of database objects</p>
<p>There are several ways to get the <a href="https://www.postgresql.org/docs/current/functions-admin.html#FUNCTIONS-ADMIN-DBSIZE" rel="noopener">size of database objects in PostgreSQL</a>:</p>
<ul>
<li><code>pg_table_size</code>: Get the size of the table including TOAST, but excluding indexes</li>
<li><code>pg_relation_size</code>: Get the size of just the table</li>
<li><code>pg_total_relation_size</code>: Get the size of the table, including indexes and TOAST</li>
</ul>
<p>Another useful function is <code>pg_size_pretty</code>: used to display sizes in a friendly format.</p>
</div>
<h3 id="toast-compression"><a href="#toast-compression">TOAST Compression</a></h3>
<p>So far I refrained from categorizing texts by their size. The reason for that is that the size of the text itself does not matter, what matters is its size after compression.</p>
<p>To create long strings for testing, we'll implement a function to generate random strings at a given length:</p>
<div><pre><span></span><span>CREATE</span> <span>OR</span> <span>REPLACE</span> <span>FUNCTION</span> <span>generate_random_string</span><span>(</span>
  <span>length</span> <span>INTEGER</span><span>,</span>
  <span>characters</span> <span>TEXT</span> <span>default</span> <span>'0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'</span>
<span>)</span> <span>RETURNS</span> <span>TEXT</span> <span>AS</span>
<span>$$</span>
<span>DECLARE</span>
  <span>result</span> <span>TEXT</span> <span>:=</span> <span>''</span><span>;</span>
<span>BEGIN</span>
  <span>IF</span> <span>length</span> <span>&lt;</span> <span>1</span> <span>then</span>
      <span>RAISE</span> <span>EXCEPTION</span> <span>'Invalid length'</span><span>;</span>
  <span>END</span> <span>IF</span><span>;</span>
  <span>FOR</span> <span>__</span> <span>IN</span> <span>1..</span><span>length</span> <span>LOOP</span>
    <span>result</span> <span>:=</span> <span>result</span> <span>||</span> <span>substr</span><span>(</span><span>characters</span><span>,</span> <span>floor</span><span>(</span><span>random</span><span>()</span> <span>*</span> <span>length</span><span>(</span><span>characters</span><span>))</span><span>::</span><span>int</span> <span>+</span> <span>1</span><span>,</span> <span>1</span><span>);</span>
  <span>end</span> <span>loop</span><span>;</span>
  <span>RETURN</span> <span>result</span><span>;</span>
<span>END</span><span>;</span>
<span>$$</span> <span>LANGUAGE</span> <span>plpgsql</span><span>;</span>
</pre></div>


<p>Generate a string made out of 10 random characters:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>generate_random_string</span><span>(</span><span>10</span><span>);</span>
<span> generate_random_string</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span> o0QsrMYRvp</span>
</pre></div>


<p>We can also provide a set of characters to generate the random string from. For example, generate a string made of 10 random digits:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>generate_random_string</span><span>(</span><span>10</span><span>,</span> <span>'1234567890'</span><span>);</span>
<span> generate_random_string</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span> 4519991669</span>
</pre></div>


<p>PostgreSQL TOAST uses the <a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html" rel="noopener">LZ family of compression</a> techniques. Compression algorithms usually work by identifying and eliminating repetition in the value. A long string containing fewer characters should compress very well compared to a string made of many different characters when encoded into bytes.</p>
<p>To illustrate how TOAST uses compression, we'll clean out the <code>toast_test</code> table, and insert a random string made of many possible characters:</p>
<div><pre><span></span><span>db=#</span> <span>TRUNCATE</span> <span>toast_test</span><span>;</span>
<span>TRUNCATE TABLE</span>

<span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>generate_random_string</span><span>(</span><span>1024</span> <span>*</span> <span>10</span><span>));</span>
<span>INSERT 0 1</span>
</pre></div>


<p>We inserted a 10kb value made of random characters. Let's check the TOAST table:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>

<span> chunk_id ‚îÇ chunks ‚îÇ pg_size_pretty</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>  1495960 ‚îÇ      6 ‚îÇ 10 kB</span>
</pre></div>


<p>The value is stored out-of-line in the TOAST table, and we can see it is not compressed.</p>
<p>Next, insert a value with a similar length, but made out of fewer possible characters:</p>
<div><pre><span></span><span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>generate_random_string</span><span>(</span><span>1024</span> <span>*</span> <span>10</span><span>,</span> <span>'123'</span><span>));</span>
<span>INSERT 0 1</span>

<span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>

<span> chunk_id ‚îÇ chunks ‚îÇ pg_size_pretty</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>  1495960 ‚îÇ      6 ‚îÇ 10 kB</span>
<span>  1495961 ‚îÇ      2 ‚îÇ 3067 bytes</span>
</pre></div>


<p>We inserted a 10K value, but this time it only contained 3 possible digits: <code>1</code>, <code>2</code> and <code>3</code>. This text is more likely to contain repeating binary patterns, and should compress better than the previous value. Looking at the TOAST, we can see PostgreSQL compressed the value to ~3kB, which is a third of the size of the uncompressed value. Not a bad compression rate!</p>
<p>Finally, insert a 10K long string made of a single digit:</p>
<div><pre><span></span><span>db=#</span> <span>insert</span> <span>into</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>values</span> <span>(</span><span>generate_random_string</span><span>(</span><span>1024</span> <span>*</span> <span>10</span><span>,</span> <span>'0'</span><span>));</span>
<span>INSERT 0 1</span>

<span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>

<span> chunk_id ‚îÇ chunks ‚îÇ pg_size_pretty</span>
<span>‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ</span>
<span>  1495960 ‚îÇ      6 ‚îÇ 10 kB</span>
<span>  1495961 ‚îÇ      2 ‚îÇ 3067 bytes</span>
</pre></div>


<p>The string was compressed so well, that the database was able to store it in-line.</p>
<h3 id="configuring-toast"><a href="#configuring-toast">Configuring TOAST</a></h3>
<p>If you are interested in configuring TOAST for a table you can do that by setting storage parameters at <code>CREATE TABLE</code> or <code>ALTER ‚Ä¶</code></p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://hakibenita.com/sql-medium-text-performance">https://hakibenita.com/sql-medium-text-performance</a></em></p>]]>
            </description>
            <link>https://hakibenita.com/sql-medium-text-performance</link>
            <guid isPermaLink="false">hacker-news-small-sites-24836979</guid>
            <pubDate>Tue, 20 Oct 2020 13:25:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raymarching with Fennel and L√ñVE]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24835766">thread link</a>) | @forgotpwd16
<br/>
October 20, 2020 | https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/ | <a href="https://web.archive.org/web/*/https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>Previously I‚Äôve decided to implement a rather basic <a href="https://andreyorst.gitlab.io/posts/2020-06-04-simple-ray-casting-with-clojurescript/">raycasting engine in ClojureScript</a>.
It was a lot of fun, an interesting experience, and ClojureScript was awesome.
I‚Äôve implemented small <a href="https://andreyorst.gitlab.io/posts/2020-06-04-simple-ray-casting-with-clojurescript/#labyrinth-game">labyrinth game</a>, and thought about adding more features to the engine, such as camera shake, and wall height change.
But when I‚Äôve started working on these, I quickly understood, that I‚Äôd like to move on to something more interesting, like real 3D rendering engine, that also uses rays.</p>
<p>Obviously, my first though was about writing a ray-tracer<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
This technique is wide known, and gained a lot of traction recently.
With native hardware support for ray tracing, a lot of games are using it, and there are a lot of tutorials teaching how to implement one<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.
In short, we cast a bunch of rays in 3D space, and calculate their trajectories, looking for what ray will hit and bounce off.
Different materials have different bounce properties, and by tracing rays from camera to the source of light, we can imitate illumination.
There are also a lot of different approaches how to calculate bouncing, e.g. for global illumination, and ambient light, but I‚Äôve felt that it is a rather complicated task, for a weekend post.
And unlike raycasting, most ray-tracers require polygonal information in order to work, where raycasting only need to know wall start and end points.</p>
<p>I‚Äôve wanted a similar approach for 3D rendering, where we specify an object in terms of it‚Äôs mathematical representation.
Like for sphere, we‚Äôll just specify coordinate of a center, and a radius, and our rays will find intersection points with it, providing us a sufficient data to draw this sphere on screen.
And recently, I‚Äôve read about a similar technique, that uses rays for drawing on screen, but instead of casting infinite rays as in raycasting, it marches a ray in terms of steps.
And it also uses a special trick, to make this process very optimized, therefore we can use it for rendering real 3D objects.</p>
<p>I‚Äôve decided to structure this post similarly to the one about raycasting, so this will be another long-read, often more about Fennel rather than raymarching, but at the end I promise that we‚Äôll get something that looks like this:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/end-result.png"> 
</figure>

<p>So, just as in raycasting, first we need to do is to understand how raymarching engine works <em>on paper</em>.</p>
<h2 id="raymarching-basics">Raymarching basics</h2>
<p>Raymarching can be illustrated similarly to raycaster, except it requires more steps until we could render our image.
First, we need a camera, and an object to look at:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle.svg"> 
</figure>

<p>Our first step would to cast a ray, however, unlike with raycasting, we‚Äôll cast a portion of a ray:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-short-ray.svg"> 
</figure>

<p>We then check, if the ray intersects with the sphere.
It‚Äôs not, so we do one more step:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-two-steps.svg"> 
</figure>

<p>It‚Äôs not intersecting yet, so we repeat again:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-ray-overshoot.svg"> 
</figure>

<p>Oops, ray overshoot, and is now inside the sphere.
This is not really good option for us, as we want for our rays to end directly at the object‚Äôs surface, without calculating intersection point with the object itself.
We can fix this by casting shorter ray:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-small-rays.svg"> 
</figure>

<p>However, this is very inefficient!
And besides, if we‚Äôll change the angle a bit or move the camera, we will overshoot again.
Which means that we‚Äôll either have incorrect result, or require a very small step size, which will blow up computation process.
How we can fix this?</p>
<h3 id="distance-estimation">Distance estimation</h3>
<p>The solution to this is a signed distance function, or a so called Distance Estimator.
Imagine if we knew how far we are from the object at any point of time?
This would mean that we can shoot a ray of this length in any direction and still don‚Äôt hit anything.
Let‚Äôs add another object to the scene:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/scene-with-two-objects.svg"> 
</figure>

<p>Now, let‚Äôs draw two circles, which will represent distances from the objects, to the point from where we‚Äôll cast rays:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/distances.svg"> 
</figure>

<p>We can see, that there are two circles, and one is bigger than another.
This means, that if we choose the shortest safe distance, we can safely cast ray in any direction and not overshoot anything.
For example, let‚Äôs cast a ray towards the square:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/ray-in-safe-zone.svg"> 
</figure>

<p>We can see, that we haven‚Äôt reached the square, but more importantly we did not overshoot it.
Now we need to march the ray again, but what distance should it cover?
To answer this question, we need to take another distance estimation from ray end to the objects in the scene:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/second-safe-march.svg"> 
</figure>

<p>Once again we choose shorter distance, and march towards the square, then get the distance again, and repeat the whole process:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/another-safe-match.svg"> 
</figure>

<p>You can see that with each step the distance to the object becomes smaller, and thus we will never overshoot the object.
However this also means, that we will take a lot of really small steps, until we finally fully hit the object, if we ever do.
This is not a good idea, because it is even more inefficient than using fixed distance, and produces too accurate results, which we don‚Äôt really need.
So instead of marching up until we exactly hit the object, we will march <em>enough</em> times.
E.g. until the distance to the object is small enough, then there‚Äôs no real point to continue marching, as it is clear that we will hit the object soon.
But this also means, that if the ray goes near the edge of an object, we do a lot of expensive steps of computing distance estimations.</p>
<p>Here‚Äôs a ray that is parallel to the side of the square, and marches towards the circle:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/expensive-marching.svg"> 
</figure>

<p>We do a lot of seemingly pointless measurements, and if a ray was closer to the square‚Äôs side, we would do even more steps.
However this also means, that we can use this data (since we‚Äôre already computed it) to render such things as glow, or ambient occlusion.
But more on this later.</p>
<p>Once ray hit an object we have all the data we need.
Ray represents a point on the screen, and the more rays we cast the higher resolution of our image will be.
And since we‚Äôre not using triangles to represent objects, our spheres will always be smooth, no matter how close we are to it, because there‚Äôs no polygons involved.</p>
<p>This is basically it.
Ray marching is quite simple concept, just like raycaster, although it‚Äôs a bit more complicated, as we do have to compute things in 3D space now.
So let‚Äôs begin implementing it by installing required tools, and setting up the project.</p>
<h2 id="project-structure">Project structure</h2>
<p>As you know from the title we will use two main tools to create ray-marcher, which are <a href="https://love2d.org/">L√ñVE</a>, a free game engine, and <a href="https://fennel-lang.org/">Fennel</a> the programming language.
I‚Äôve chosen Fennel, because it is a Lisp like language, that compiles to Lua, and I‚Äôm quite a fan of Lisps.
But we also needed to draw somewhere, and I know no GUI toolkit for Lua.
But there is L√ñVE - a game engine that runs Lua code, which is capable on running on all systems, thus a perfect candidate for our task.</p>
<p>Installation steps may differ per operating system, so please refer to manuals<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup><sup>, </sup><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.
At the time of writing this post I‚Äôm using Fedora GNU/Linux, so for me it means:</p>
<div><pre><code data-lang="sh">$ sudo dnf install love luarocks readline-devel
$ luarocks install --local fennel
$ luarocks install --local readline <span># requires readline-devel</span>
$ <span>export</span> <span>PATH</span>=<span>"</span><span>$PATH</span><span>:</span><span>$HOME</span><span>/.luarocks/bin"</span>
</code></pre></div><p>It‚Äôs better to permanently add <code>$HOME/luarocks/bin</code> (or another path, if your installation differs) to the <code>PATH</code> variable in your shell, in order to be able to use installed utilities without specifying full path every time.
You can test if everything is installed correctly, by running <code>fennel</code> in you command line.</p>
<div><pre><code data-lang="sh">$ fennel
Welcome to Fennel 0.5.0 on Lua 5.3!
Use (doc something) to view documentation.
&gt;&gt; (+ 1 2 3)
6
&gt;&gt;
</code></pre></div><p>For other distributions installation steps may vary, and for Windows, I think it‚Äôs safe to skip the <code>readline</code> part, which is fully optional, but makes editing in a REPL a bit more comfortable.</p>
<p>Once everything is installed, let‚Äôs create the project directory, and the <code>main.fnl</code> file, where we will write our code.</p>
<div><pre><code data-lang="sh">$ mkdir love_raymarching
$ <span>cd</span> love_raymarching
$ touch main.fnl
</code></pre></div><p>And that‚Äôs it!
We can test if everything works by adding this code to <code>main.fnl</code>:</p>
<div><pre><code data-lang="clojure">(<span>fn </span><span>love.draw</span> []
  (<span>love.graphics.print</span> <span>"It works!"</span>))
</code></pre></div><p>Now we can compile it with <code>fennel --compile main.fnl &gt; main.lua</code>, thus producing the <code>main.lua</code> file, and run <code>love .</code> (dot is intentional, it indicates current directory).</p>
<p>A window should appear, with white text <code>It works!</code> in upper left corner:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/love-works.png"> 
</figure>

<p>Now we can begin implementing our raymarcher.</p>
<h2 id="scene-setup">Scene setup</h2>
<p>Just as in raycaster, we need a camera that will shoot rays, and some objects to look at.
Let‚Äôs begin by creating a camera object, that will store coordinates and rotation information.
We can do so, by using <code>var</code> to declare a variable that is local to our file, and that we can later change with <code>set</code><sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>:</p>

<div><pre><code data-lang="clojure">(<span>var </span><span>camera</span> {<span>:pos</span> [0.0 0.0 0.0]
             <span>:x-rotate</span> 0.0
             <span>:z-rotate</span> 0.0})
</code></pre></div><blockquote>
<p>For those unfamiliar with Lisps, and especially Clojure, let me quickly explain what this syntax is.
If you know this stuff, feel free to <a href="#org6f2d291">skip this part</a>.</p>
<p>We start by using a <code>var</code> special form, that binds a value to a name like this: <code>(var name value)</code>.
So if we start the REPL, using <code>fennel</code> command in the shell, and write <code>(var a 40)</code>, a new variable <code>a</code> will be created.
We then can check, that it has the desired value by typing <code>a</code>, and pressing return:</p>
<p>We can then alter the contents of this variable by using <code>set</code> special form, which works like this <code>(set name new-value)</code>:</p>
<div><pre><code data-lang="clojure"><span>&gt;&gt;</span> (<span>set </span><span>a</span> (<span>+ </span><span>a</span> 2))
<span>&gt;&gt;</span> <span>a</span>
42
</code></pre></div><p>Now to curly and square brackets.
Everything enclosed in curly braces is a hashmap.
We can use any Lua value as our key, and the most common choice is a string, but Fennel has additional syntax for defining keys - a colon followed by a word: <code>:a</code>.
This is called a keyword, and in Fennel it is essentially the same as <code>"a"</code>, but we don‚Äôt need to write a pair of quotes.
However keywords can‚Äôt contain spaces, and some other symbols.</p>
<p>So writing this <code>{:a 0 :b 2 :c :hello}</code> in the REPL will make a new table, that holds three key value pairs, which we can later get with another syntax - the dot <code>.</code>.
Combining it with <code>var</code>, we can see that it works:</p>
<div><pre><code data-lang="clojure"><span>&gt;&gt;</span> (<span>var </span><span>m</span> {<span>:a</span> 1 <span>:b</span> 2 <span>:c</span> <span>:hello</span>})
<span>&gt;&gt;</span> (<span>. </span><span>m</span> <span>:b</span>)
2
</code></pre></div><p>There‚Äôs also a shorthand for this ‚Ä¶</p></blockquote></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/">https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/</a></em></p>]]>
            </description>
            <link>https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24835766</guid>
            <pubDate>Tue, 20 Oct 2020 10:26:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Assorted Thoughts on Zig and Rust]]>
            </title>
            <description>
<![CDATA[
Score 338 | Comments 289 (<a href="https://news.ycombinator.com/item?id=24835357">thread link</a>) | @ikskuh
<br/>
October 20, 2020 | https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/ | <a href="https://web.archive.org/web/*/https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>I've been using <a href="https://ziglang.org/">zig</a> for ~4 months worth of side projects, including a <a href="https://git.sr.ht/%7Ejamii/focus/tree">toy text editor</a> and an <a href="https://git.sr.ht/%7Ejamii/imp">interpreter for a relational language</a>. I've written ~10kloc.</p>
<p>That's not nearly enough time to form a coherent informed opinion. So instead here is an incoherent assortment of thoughts and experiences, in no particular order :)</p>
<p>This is not meant to be an introduction to zig - check out the excellent <a href="https://ziglang.org/documentation/master/">language docs</a> or the new <a href="https://ziglearn.org/">ziglearn.org</a> instead. I'll try to focus instead on things that are not immediately obvious from reading intro material.</p>
<p>The obvious point of comparison is to rust. For context, I've been using rust <a href="https://scattered-thoughts.net/writing/three-months-of-rust/">since 2015</a>. Mostly in research positions writing throwaway code, but also ~14 months working on <a href="https://materialize.io/">a commercial database</a> which is ~100kloc.</p>
<hr>
<p>Zig is dramatically simpler than rust. It took a few days before I felt proficient vs a month or more for rust.</p>
<p>Most of this difference is <strong>not</strong> related to lifetimes. Rust has patterns, traits, dyn, modules, declarative macros, procedural macros, derive, associated types, annotations, cfg, cargo features, turbofish, autoderefencing, deref coercion etc. I encountered most of these in the first week. Just understanding how they all work is a significant time investment, let alone learning when to use each and how they affect the available design space.</p>
<p>I still haven't internalized the full rule-set of rust enough to be able predict whether a design in my head will successfully compile. I don't remember the order in which methods are resolved during autoderefencing, or how module visibility works, or how the type system determines if one impl might <a href="https://github.com/Ixrec/rust-orphan-rules#what-are-the-orphan-rules">overlap another or be an orphan</a>. There are frequent moments where I know what I want the machine to do but struggle to encode it into traits and lifetimes.</p>
<p>Zig manages to provide many of the same features with a single mechanism - compile-time execution of regular zig code. This comes will all kinds of pros and cons, but one large and important pro is that I already know how to write regular code so it's easy for me to just write down the thing that I want to happen.</p>
<hr>
<p>One of the key differences between zig and rust is that when writing a generic function, rust will prove that the function is type-safe for every possible value of the generic parameters. Zig will prove that the function is type-safe only for each parameter that you actually call the function with.</p>
<p>On the one hand, this allows zig to make use of arbitrary compile-time logic where rust has to restrict itself to structured systems (traits etc) about which it can form general proofs. This in turn allows zig a great deal of expressive power and also massively simplifies the language.</p>
<p>On the other hand, we can't type-check zig libraries which contain generics. We can only type-check specific uses of those libraries.</p>
<pre><span>// This function is typesafe if there exist no odd perfect numbers
// https://en.wikipedia.org/wiki/Perfect_number#Odd_perfect_numbers
fn foo(comptime n: comptime_int, i: usize) usize {
  const j = if (comptime is_odd_perfect_number(n)) "surprise!" else 1;
  return i + j;
}
</span></pre>
<p>This means zig also doesn't get the automatic, machine-checked documentation of type constraints that rust benefits from and may face more challenges providing IDE support.</p>
<p>This might harm the zig ecosystem by making it harder to compose various libraries. But <a href="https://julialang.org/">julia</a> has a similar model and in practice it has worked very well (<a href="https://youtu.be/dmWQtI3DFFo?t=1710">eg</a>, <a href="https://www.oxinabox.net/2020/02/09/whycompositionaljulia.html">eg</a>).</p>
<hr>
<p>Zig's comptime allows expressing <a href="https://scattered-thoughts.net/writing/open-multiple-dispatch-in-zig/">open multiple dispatch</a> as a library.</p>
<p>It should be relatively trivial to implement specialization the same way, which has been a <a href="https://github.com/rust-lang/rust/issues/31844">work in progress</a> in rust for years and is critical to many optimizations in julia's math libraries.</p>
<p>Julia chose dynamic typing because it's very difficult to encode the types of various mathematical operations into a general schema (eg fortress <a href="https://youtu.be/EZD3Scuv02g?t=3011">struggled with this</a>). Zig's approach of not requiring general schemas but still type-checking individual cases may be an interesting sweet spot.</p>
<hr>
<p>I used the <a href="https://cwe.mitre.org/data/definitions/1350.html">2020 CWE Top 25 Most Dangerous Software Weaknesses</a> to get a sense of the relative frequency of different causes of memory unsafety.</p>
<p>(I'm assuming that the zig programmer is using release-safe mode instead of the unfortunately named release-fast mode which disables all runtime safety checks.)</p>
<ul>
<li>Out-of-bounds Write (787/1350)</li>
<li>Out-of-bounds Read (125/1350)</li>
<li>Improper Restriction of Operations within the Bounds of a Memory Buffer (119/1350)</li>
</ul>
<p>Both languages primarily use bounds-checked slices and relegate pointer arithmetic to a separate type (<code>*T</code> in rust, <code>[*]T</code> in zig).</p>
<ul>
<li>NULL Pointer Dereference (476/1350)</li>
</ul>
<p>Both languages require explicit annotations for nulls (<code>Option&lt;T&gt;</code> in rust, <code>?T</code> in zig) and require code to either handle the null case or safely crash on null (<code>x.unwrap()</code> in rust, <code>x.?</code> in zig).</p>
<p>Dereferencing/casting a null c pointer is undefined behavior in both languages, but is checked at runtime in zig.</p>
<ul>
<li>Integer Overflow or Wraparound (190/1350)</li>
</ul>
<p>Rust catches overflow in debug and wraps in release. Zig catches overflow in debug/release-safe and leaves behavior undefined in release-fast.</p>
<p>Both languages allow explicitly asking for wraparound (<code>x.wrapping_add(1)</code> in rust, <code>x +% 1</code> in zig).</p>
<ul>
<li>Use After Free (416/1350)</li>
</ul>
<p>As long as all unsafe code obeys the aliasing and lifetime rules, rust protects completely against UAF.</p>
<p>Zig has little protection. The recently merged
<a href="https://github.com/ziglang/zig/blob/575fbd5e3592cff70cbfc5153884d919e6bed89f/lib/std/heap/general_purpose_allocator.zig">GeneralPurposeAllocator</a> avoids reusing memory regions (which prevents freed data from being overwritten) and reusing pages (which means that UAF will eventually result in a page fault). But this comes at the cost of fragmentation and lower performance and it also won't provide protection for child allocators using the GPA as a backing allocator.</p>
<hr>
<p>Both languages will insert implicit casts between primitive types and pointers whenever it is safe to do so, and require explicit casts otherwise. (With the odd exception that rust will not implicitly upcast numbers).</p>
<p>Both languages support generics which almost entirely avoids the need to cast void pointers.</p>
<hr>
<p>In rust the Send/Sync traits flag types which are safe to move/share across threads. In the absence of unsafe code it should be impossible to cause data races.</p>
<p>Zig has no comparable protection. It's possible to implement the same logic as Send/Sync in comptime zig, but without the ability to track ownership the rules would have to be much more restrictive.</p>
<hr>
<p>Rust prevents having multiple mutable references to the same memory region at the same time.</p>
<p>This means that eg iterator invalidation is prevented at compile time, because the borrow checker won't allow mutating a data-structure while an iterator is holding a reference to the data-structure. Similarly for resizing a data-structure while holding a reference to the old allocation. Both examples are easy sources of UAF in zig.</p>
<hr>
<p>Neither language is able to produce stack traces for stack overflows at the moment (<a href="https://github.com/rust-lang/rust/issues/51405">rust</a>, <a href="https://github.com/ziglang/zig/issues/1616">zig</a>)</p>
<p>In the future zig is <a href="https://github.com/ziglang/zig/issues/1006">intended</a> to statically check the maximum stack usage of your program and force recursive code to explicitly allocate space on the heap, so that stack overflows produce a recoverable OutOfMemory error rather than a crash.</p>
<p>This is not an academic problem - I've seen real-world crashes from recursive tree transformations in compilers (<a href="https://github.com/MaterializeInc/materialize/pull/3996">eg</a>) and it's often painful to write the same logic without recursion.</p>
<hr>
<p>Undefined behavior in rust is defined <a href="https://doc.rust-lang.org/nomicon/what-unsafe-does.html">here</a>. It's worth noting that breaking the aliasing rules in unsafe rust can cause undefined behavior but these rules are not yet well-defined. So far this hasn't caused me any problems but it is a little unnerving.</p>
<p><a href="https://github.com/rust-lang/miri">Miri</a> is an interpreter for rusts Mid-level Intermediate Representation which will detect many (but not all) cases of undefined behavior in unsafe rust. It's far too slow to use for the whole materialize test suite but was useful for unit-testing an unsafe module.</p>
<p>Undefined behavior in zig is defined <a href="https://ziglang.org/documentation/master/#Undefined-Behavior">here</a>. This list is <a href="https://github.com/ziglang/zig/issues/1966">probably incomplete</a> given that the core language is still under development.</p>
<p>Zig <a href="https://github.com/ziglang/zig/issues/2301">aspires</a> to insert runtime checks for almost all undefined behavior when compiling in debug mode. So far all the easy cases are handled, which is already a dramatic improvement over c.</p>
<p>Zigs compile-time partial evaluation is done by an IR interpreter - it seems plausible that this could also be used as a miri-like tool in the future.</p>
<hr>
<p><code>@import</code> takes a path to a file and turns the whole file into a struct. So modules are just structs. And vice-versa - if you have a large struct declaration you can move it into a file to reduce the indentation.</p>
<p>Zig doesn't care at all where you put files on the filesystem.</p>
<p><code>@import</code> is part of the compile-time execution system so things like platform-specific modules and configurable features can be specified in regular code rather than rust's limited set of <code>#[cfg(...)]</code> macros.</p>
<hr>
<p>Array, struct, enum and union literals can be anonymous - <code>.{.Constant = 1.0}</code> is an anonymous union with it's own type, but can be implicitly cast to any union with a <code>Constant: f64</code> field because they share the same structure.</p>
<p>In rust my code is littered with <code>use Expr::*</code> and I'm careful to avoid name collisions between different enums that I might want to import in the same functions. In zig I just use anonymous literals everywhere and don't worry about it.</p>
<hr>
<p>Anonymous literals are also nice when using structs to simulate keyword arguments. No need to find and import the correct type:</p>
<pre><span>fn do_things(config: struct {
  max_things: usize = 1000, // default value
  flavor: Flavor,
}) void {
  ...
}

do_things(.{.flavor = .Strawberry});
</span></pre>
<hr>
<p>There is a pattern that shows up a lot in the materialize codebase:</p>
<pre><span>let</span><span> constant </span><span>= </span><span>if let </span><span>Expr</span><span>::</span><span>Constant(constant) </span><span>=</span><span> expr { constant } </span><span>else </span><span>{ </span><span>panic!</span><span>() }</span><span>;
</span></pre>
<p>It's common enough that many types have methods like <code>expr.unwrap_constant()</code>.</p>
<p>In zig:</p>
<pre><span>const constant = expr.Constant;
</span></pre>
<p>A similar pattern is:</p>
<pre><span>if</span><span> some_condition {
    </span><span>if let </span><span>Expr</span><span>::</span><span>Constant(</span><span>_</span><span>) </span><span>=</span><span> expr {
        </span><span>...
    </span><span>}
}
</span></pre>
<p>Again, many types get methods like <code>expr.is_constant()</code>.</p>
<pre><span>if</span><span> some_condition </span><span>&amp;&amp;</span><span> expr</span><span>.</span><span>is_constant</span><span>(‚Ä¶</span></pre></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/">https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/</a></em></p>]]>
            </description>
            <link>https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24835357</guid>
            <pubDate>Tue, 20 Oct 2020 09:22:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Design Stripe or Hacker News-like favicons in seconds]]>
            </title>
            <description>
<![CDATA[
Score 192 | Comments 40 (<a href="https://news.ycombinator.com/item?id=24835219">thread link</a>) | @hosshams
<br/>
October 20, 2020 | https://formito.com/tools/favicon | <a href="https://web.archive.org/web/*/https://formito.com/tools/favicon">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Copy the following code and put it inside the<!-- --> <code>&lt;head&gt;</code> tag of your website.</p><pre><code></code></pre><p>Or, download the SVG file, add the following code to<!-- --> <code>&lt;head&gt;</code> tag of your website, and replace<!-- --> <code>href</code> attribute with URL to your SVG file.</p><pre><code>&lt;link rel="icon" type="image/svg+xml" href="favicon.svg" /&gt;</code></pre></div></div>]]>
            </description>
            <link>https://formito.com/tools/favicon</link>
            <guid isPermaLink="false">hacker-news-small-sites-24835219</guid>
            <pubDate>Tue, 20 Oct 2020 08:56:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Discipline Doesn‚Äôt Scale]]>
            </title>
            <description>
<![CDATA[
Score 309 | Comments 167 (<a href="https://news.ycombinator.com/item?id=24834965">thread link</a>) | @ingve
<br/>
October 20, 2020 | https://www.sicpers.info/2020/10/discipline-doesnt-scale/ | <a href="https://web.archive.org/web/*/https://www.sicpers.info/2020/10/discipline-doesnt-scale/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>If programmers were just more disciplined, more <em>professional</em>, they‚Äôd write better software. All they need is a <a href="https://www.pearson.com/us/higher-education/program/Martin-Clean-Coder-The-A-Code-of-Conduct-for-Professional-Programmers/PGM8366.html">code of conduct</a> telling them how to work like those of us who‚Äôve worked it out.</p>
<p>The above statement is true, which is a good thing for those of us interested in improving the state of software and in helping our fellow professionals to improve their craft. However, it‚Äôs also very difficult and inefficient to apply, in addition to being entirely unnecessary. In the common parlance of our industry, ‚Äúdiscipline doesn‚Äôt scale‚Äù.</p>
<p>Consider the trajectory of object lifecycle management in the Objective-C programming language, particularly the NeXT dialect. Between 1989 and 1995, the dominant way to deal with the lifecycle of objects was to use the <tt>+new</tt> and <tt>-free</tt> methods, which work much like <tt>malloc/free</tt> in C or <tt>new/delete</tt> in C++. Of course it‚Äôs <em>possible</em> to design a complex object graph using this ownership model, it just needs discipline, that‚Äôs all. Learn the heuristics that the experts use, and the techniques to ensure correctness, and get it correct.</p>
<p>But you know what‚Äôs better? Not having to get that right. So around 1994 people introduced new tools to do it an easier way: reference counting. With NeXTSTEP Mach Kit‚Äôs <tt>NXReference</tt> protocol and OpenStep‚Äôs <tt>NSObject</tt>, developers no longer need to know when <em>everybody</em> in an app is done with an object to destroy it. They can indicate when a reference is taken and when it‚Äôs relinquished, and the object itself will see when it‚Äôs no longer used and free itself. Learn the heuristics and techniques around auto releasing and unretained references, and get it correct.</p>
<p>But you know what‚Äôs better? Not having to get that right. So a couple of other tools were introduced, so close together that they were probably developed in parallel[*]: Objective-C 2.0 garbage collection (2006) and Automatic Reference Counting (2008). ARC ‚Äúwon‚Äù in popular adoption so let‚Äôs focus there: developers no longer need to know exactly when to retain, release, or autorelease objects. Instead of describing the edges of the relationships, they describe the <em>meanings</em> of the relationships and the compiler will automatically take care of ownership tracking. Learn the heuristics and techniques around weak references and the ‚Äúweak self‚Äù dance, and get it correct.</p>
<p>[*] I‚Äôm ignoring here the significantly earlier integration of the Boehm conservative GC with Objective-C, because so did everybody else. That in itself is an important part of the technology adoption story.</p>
<p>But you know what‚Äôs better? You get the idea. You see similar things happen in other contexts: for example C++‚Äôs move from <tt>new/delete</tt> to smart pointers follows a similar trajectory over a similar time. The reliance on an entire programming community getting some difficult rules right, when faced with the alternative of using different technology <em>on the same computer</em> that follows the rules for you, is a tough sell.</p>
<p>It seems so simple: computers exist to automate repetitive information-processing tasks. Requiring programmers who have access to computers to recall and follow repetitive information processes is wasteful, when the computer can do that. So give those tasks to the computers.</p>
<p>And yet, for some people the problem with software isn‚Äôt a lack of automation but a lack of discipline. Software would be better if only people knew the rules, honoured them, and slowed themselves down so that instead of cutting corners they just chose to ignore important business milestones instead. Back in my day, everybody knew ‚Äúno Markdown around town‚Äù and ‚Äúdon‚Äôt code in an IDE after Labour Day‚Äù, but now the kids do whatever they want. The motivations seem different, and I‚Äôd like to sort them out.</p>
<p>Let‚Äôs start with hazing. A lot of the software industry suffers from ‚ÄúI had to go through this, you should too‚Äù. Look at software engineering interviews, for example. I‚Äôm not sure whether anybody actually believes ‚ÄúI had to deal with carefully ensuring NUL-termination to avoid buffer overrun errors so you should too‚Äù, but I do occasionally still hear people telling less-experienced developers that they should learn C to learn more about how their computer works. <a href="https://queue.acm.org/detail.cfm?id=3212479">Your computer is not a fast PDP-11</a>, all you will learn is how the C virtual machine works.</p>
<p>Just as Real Men Don‚Äôt Eat Quiche, so <a href="https://www.codeproject.com/articles/927/real-programmers-don-t-use-pascal">real programmers don‚Äôt use Pascal</a>. Real Programmers use FORTRAN. This motivation for sorting discipline from rabble is based on the idea that if it isn‚Äôt at least as hard as it was when <em>I</em> did this, it isn‚Äôt hard enough. And that means that the goalposts are movable, based on the orator‚Äôs experience.</p>
<p>This is often related to the <em>term</em> of their experience: you don‚Äôt need TypeScript to write good React Native code, just Javascript and some discipline. You don‚Äôt need React Native to write good front-end code, just JQuery and some discipline. You don‚Äôt need JQuery‚Ä¶</p>
<p>But along with the term of experience goes the breadth. You see, the person who learned reference counting in 1995 and thinks that you can only <em>really</em> understand programming if you manually type out your own reference-changing events, presumably didn‚Äôt go on to use garbage collection in Java in 1996. The person who thinks you can only <em>really</em> write correct software if every case is accompanied by a unit test presumably didn‚Äôt learn Eiffel. The person who thinks that you can only <em>really</em> design systems if you use the Haskell type system may not have tried OCaml. And so on.</p>
<p>The conclusion is that for this variety of disciplinarian, the appropriate character and quantity of discipline is whatever they had to deal with at some specific point in their career. Probably a high point: after they‚Äôd got over the tricky bits and got productive, and after you kids came along and ruined everything.</p>
<p>Sometimes the reason for suggesting the disciplined approach is entomological in nature, as in the case of the eusocial insect the ‚Äúperformant‚Äù which, while not a real word, exists in greater quantities in older software than in newer software, apparently. The performant is capable of making software faster, or use less memory, or more concurrent, or less dependent on I/O: the specific characteristics of the performant depend heavily on context.</p>
<p>The performant is often not talked about in the same sentences as its usual companion species, the irrelevant. Yes, there may be opportunities to shave a few percent off the runtime of that algorithm by switching from the automatic tool to the manual, disciplined approach, but does that matter (yet, or at all)? There are software-construction domains where specific performance characteristics are desirable, indeed that‚Äôs true across a lot of software. But it‚Äôs typical to focus performance-enhancing techniques on the bits where they enhance performance that needs enhancing, not to adopt them across the whole system on the basis that it was better when everyone worked this way. You might save a few hundred cycles writing native software instead of using a VM for that UI method, but if it‚Äôs going to run after a network request completes over EDGE then trigger a 1/3s animation, nobody will notice the improvement.</p>
<p>Anyway, whatever the source, the problem with calls for discipline is that there‚Äôs no strong motivation to <em>become</em> more disciplined. I can use these tools, and my customer is this much satisfied, and my employer pays me this much. Or I can learn from you how I‚Äôm <em>supposed</em> to be doing it, which will slow me down, for‚Ä¶your satisfaction? So you know I‚Äôm doing it the way it‚Äôs supposed to be done? Or so that I can tell everyone else that they‚Äôre doing it wrong, too? Sounds like a great deal.</p>
<p>Therefore discipline doesn‚Äôt scale. Whenever you ask some people to slow down and think harder about what they‚Äôre doing, some fraction of them will. Some will wonder whether there‚Äôs some other way to get what you‚Äôre peddling, and may find it. Some more will not pay any attention. The dangerous ones are the ones who thought they <em>were</em> paying attention and yet still end up not doing the disciplined thing you asked for: they either torpedo your whole idea or turn it into not doing the thing (see OOP, Agile, Functional Programming). And still more people, by far the vast majority, just weren‚Äôt listening at all, and you‚Äôll never reach them.</p>
<p>Let‚Äôs flip this around. Let‚Äôs look at where we <em>need</em> to be disciplined, and ask if there are gaps in the tool support for software engineers. Some people want us to always write a failing test and make it pass before adding any code (or want us to write a passing test and revert our changes if it accidentally fails): does that mean our tools should not let us write code for which there‚Äôs no test? Does the same apply for acceptance tests? Some want us to refactor mercilessly; does that mean our design tools should always propose more parsimonious alternatives for passing the same tests? Some say we should get into the discipline of writing code that always reveals its intent: should the tools make a crack at interpreting the intention of the code-as-prose?</p>
	</div></div>]]>
            </description>
            <link>https://www.sicpers.info/2020/10/discipline-doesnt-scale/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834965</guid>
            <pubDate>Tue, 20 Oct 2020 08:11:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Long Road to HTTP/3]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 72 (<a href="https://news.ycombinator.com/item?id=24834767">thread link</a>) | @todsacerdoti
<br/>
October 20, 2020 | https://scorpil.com/post/the-long-road-to-http3/ | <a href="https://web.archive.org/web/*/https://scorpil.com/post/the-long-road-to-http3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wrapper"><div id="container"><div><article itemscope="" itemtype="http://schema.org/BlogPosting"><div itemprop="articleBody"><p>While HTTP/3 specification is still in the draft stage, the latest version of the Chrome browser already <a href="https://blog.chromium.org/2020/10/chrome-is-deploying-http3-and-ietf-quic.html" target="_blank" rel="nofollow">supports it by default</a>
. With Chrome holding around 70% of browser market share, you could say HTTP/3 has gone mainstream.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/quic-logo.png" alt="QUIC logo"></p><p>The new revision of this foundational protocol aims to make the web more efficient, secure, and shorten the content-delivery latencies. In some ways, it‚Äôs a braver take of HTTP2: similar goals addressed by replacing the underlying TCP protocol with a new, purpose-built protocol QUIC. The best way to explain the benefits of QUIC is to illustrate where TCP falls short as a transport for HTTP requests. And to do that, we‚Äôll start at the very beginning.</p><h3 id="http-the-original">HTTP. The Original.</h3><p>When Sir Tim Berners-Lee formalized the design of a simple <a href="https://www.w3.org/Protocols/HTTP/AsImplemented.html" target="_blank" rel="nofollow">one-line hyper-text-exchange protocol</a>
in 1991, TCP was already an old, reliable protocol. The original definition document of what later became known as HTTP 0.9 specifically mentions TCP as a preferred, albeit not exclusive, transport protocol:</p><blockquote><p>Note: HTTP currently runs over TCP, but could run over any connection-oriented service.</p></blockquote><p>Of course, this proof-of-concept version of HTTP had very few similarities to HTTP we now know and love today. There were no headers and no status codes. The typical request was as simple as <code>GET /path</code>. The response contained only HTML and ended with the closing of the TCP connection.
Since browsers were not yet a thing, user was supposed to read HTML directly. It was possible to link to other resources, but none of the tags present in this early version of HTML requested additional resources asynchronously. A single HTTP request delivered a complete, self-sufficient page.</p><h3 id="emergence-of-http10">Emergence of HTTP/1.0</h3><p>In subsequent years the internet has exploded, and HTTP evolved to be an extendable and flexible general-purpose protocol, although transporting HTML remained its chief specialty. There are three critical updates to HTTP that enabled this evolution:</p><ul><li>introduction of methods allowed the client to identify the type of action it wants to perform. For example, POST was created to allow client sending data to the server to process and store</li><li>status codes provided a way for client to confirm that the server has processed the request successfully, and if not - to understand what kind of error has occured</li><li>headers added an ability to attach structured textual metadata to requests and responses that could modify the behavior of the client or server. Encoding and content-type headers, for example, allowed HTTP to transfer not just HTML, but any type of payload. ‚ÄúCompression‚Äù header allowed the client and server to negotiate supported compression formats, thus reducing the amount of data to transfer over the connection</li></ul><p>At the same time, HTML advanced to support images, styles, and other linked resources. Browsers were now forced to perfrom multiple requests to display a single web page, which the original connection-per-request architecture was not designed to handle. Establishing and ending a TCP connection involves a lot of back-and-forth packet exchange, so it is relatively expensive in terms of latency overhead. It didn‚Äôt matter much when a web-page consisted of a single text file, but as the number of requests per page increased, so did the latency.</p><p>The picture below illustrates how much overhead was involved in establishing a new TCP connection per request.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http1-tcp-overhead.png" alt="TCP connection requires three requests to establish connection and four to close it cleanly"></p><p>A ‚Äúconnection‚Äù header was created to address this problem. Client sends a request with ‚Äúconnection: keep-alive‚Äù header to signal intent to keep the TCP connection open for subsequent requests. If server understands this header and agrees to respect it, its response will also contain the ‚Äúconnection: keep-alive‚Äù header. This way, both parties maintain TCP channel open and use it for subsequent communication until either party decides to close it. This became even more important with the spread of SSL/TLS encryption, because negotianting an encryption algorithm and exchanging cryptographic keys requires an additional request/response cycle on each connection.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http1-keepalive.png" alt="A single TCP connection can be reused for multiple requests with ‚Äúconnection: keep-alive‚Äù header"></p><p>At the time, many of the HTTP improvements appeared spontaneously. When a popular browser or a server app saw a need for a new HTTP feature, they would simply implement it themselves and hoped that other parties would follow the suit. Ironically, a decentralized web needed a centralized governing body to avoid fragmentation into incompatible pieces. Tim Berners-Lee, the original creator of the protocol, recognized the danger and founded the World Wide Web Consortium (W3C) in 1994, which together with the Internet Engineering Task Force (IETF) worked on formalizing stack of internet technologies. As the initial step to bring more structure to the existing environment, they documented the most common features used in HTTP at the time and named the resulting protocol HTTP/1.0. However, because this ‚Äúspecification‚Äù described varied, often inconsistent techniques as seen ‚Äúin the wild‚Äù, it never received a status of a standard. Instead, the work on the new version of the HTTP protocol has begun.</p><h3 id="standardization-of-http11">Standardization of HTTP/1.1</h3><p>HTTP/1.1 fixed inconsistencies of HTTP/1.0 and adjusted the protocol to be more performant in the new web ecosystem. Two of the most critical changes introduced were the use of persistent TCP connections (keep-alive‚Äôs) by default and HTTP pipelining.</p><p>HTTP pipelining simply means that client does not need to wait for the server to respond to a request before sending subsequent HTTP requests. This feature resulted in even more efficient use of bandwidth and reduced latencies, but it could be improved even more. HTTP pipelining still requires from server to respond in the order of requests received, so if a single request in a pipeline is slow to fulfill, all subsequent responses to a client will be delayed accordingly. This problem is known as head-of-the-line blocking.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http11-blocking.png" alt="Since large-picture.jpg was requested first, it‚Äôs blocking the delivery of the style.css"></p><p>At this point in time, the web is gaining more and more interactive capabilities. Web 2.0 is just around the corner, some webpages include dozens or even hundreds of external resources. To work around the head-of-the-line blocking, and to decrease page loading speeds, clients establish multiple TCP connections per host. Of course, the connection overhead never went anywhere. In reality, it got worse, since more and more applications encrypt HTTP traffic with SSL/TLS. So most browsers set the limit of maximal possible simultaneous connections in an attempt to strike a delicate balance.</p><p>Many of the larger web-services have recognized that existing limitations are too restricting for their exceptionally heavy interactive web-applications, so they ‚Äúgamed the system‚Äù by distributing their app through multiple domain names. It all worked, somehow, but the solution has been far from elegant.</p><p>Despite a few shortcomings, the simplicity of HTTP/1.0 and HTTP/1.1 has made them widely successful, and for over a decade no one has made a serious attempt to change them.</p><h3 id="spdy-and-http2">SPDY and HTTP/2</h3><p>In 2008 Google released the Chrome browser, which rapidly gained popularity for being quick and innovative. It has given Google a strong vote on matters of internet technologies. In the early 2010s, Google adds support for its web protocol SPDY to Chrome.</p><p>HTTP/2 standard was based on SPDY with some improvements. HTTP/2 solved the head-of-the-line blocking problem by multiplexing the HTTP requests over a single open TCP connection. This allowed server to answer requests in any order, client could then re-assemble the responses as it received them, making the whole exchange faster within a single connection.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http2-multiplexing.png" alt="style.css was returned before the large-picture.jpg, becuase of HTTP/2 multiplexing"></p><p>In fact, with HTTP/2 server can serve the resources to a client before it even asked for it! To give an example, if the server knows that client will most likely need a stylesheet to display an HTML page, it can ‚Äúpush‚Äù the CSS to the client without waiting for a corresponding request. While beneficial in theory, this feature rarely seen in practice, since it requires a server to understand the structure of the HTML it serves, which is rarely the case.</p><p>HTTP/2 also allows compressing request headers in addition to the request body, which further reduces the amount of data transferred over the wire.</p><p>HTTP/2 solved a lot of problems for the web, but not all of them. A similar type of head-of-the-line problem is still present on the level of TCP protocol, which remains a foundational building block of the web. When a TCP packet gets lost in transit, the receiver can‚Äôt acknowledge incoming packages until the lost package is re-sent by a server. Since TCP is by design oblivious to higher-level protocols like HTTP, a single lost packet will block the stream for all in-flight HTTP requests until the missing data is re-sent. This problem is especially prominent on an unreliable connection, which is not rare in the age of ubiquitous mobile devices.</p><h3 id="http3-revolution">HTTP/3 revolution</h3><p>Since issues with HTTP/2 can not be resolved purely on the application layer a new iteration of the protocol must update the transport layer. However, creating a new transport-layer protocol is not an easy task. Transport protocols need to be supported by hardware vendors and deployed by the majority of network operators, which are reluctant to update because of the costs and efforts involved. Take IPv6 as an example: it was introduced 24 years ago and is still far from being universally supported.</p><p>Fortunately, there is another option. UDP protocol is as widely supported as TCP but is simple enough to serve as a building block for custom protocols running on top of it. UDP packets are fire-and-forget: there are no handshakes, persistent connections, or error-correction. The primary idea behind HTTP3 is to abandon TCP in favor of a UDP-based QUIC protocol. QUIC adds the necessary features (those that were previously provided by TCP, and more) in a way that makes sense for the web environment.</p><p>Unlike HTTP2, which technically allowed an unencrypted communication, QUIC strictly requires encryption to establish a connection. Additionally, encryption is applied to all data ‚Ä¶</p></div></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://scorpil.com/post/the-long-road-to-http3/">https://scorpil.com/post/the-long-road-to-http3/</a></em></p>]]>
            </description>
            <link>https://scorpil.com/post/the-long-road-to-http3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834767</guid>
            <pubDate>Tue, 20 Oct 2020 07:37:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adolphe Sax, Inventor of the Saxophone]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 38 (<a href="https://news.ycombinator.com/item?id=24834716">thread link</a>) | @bobf
<br/>
October 20, 2020 | http://www.dinant.be/en/inheritance/adolphe-sax | <a href="https://web.archive.org/web/*/http://www.dinant.be/en/inheritance/adolphe-sax">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tbody>
<tr>
<td>


<h2><span>Adolphe Sax, a Dinantais of genius</span></h2>
<blockquote>
<p>Along with Joachim Patenier (1485-1524), the creator of landscape painting; with Antoine Wiertz (1806-1865), the lyrical painter; with a plethora of sculptors, painters, musicians, brassworkers and others, Dinant can legitimately pride itself on having been the birthplace on 6 November 1814 of Antoine-Joseph, or Adolphe, Sax, a prolific and inspired inventor in the manufacture of musical instruments.</p>
<p>In 1860, the diarist, Oscar Comettant, wrote, "In the services that he has rendered to musical art, in the battles he has had to go through to bring his discoveries to the light of day and defend them from despoilment and in the rewards he has been the object of from all the industrial nations, [Sax's life] rises to the heights of a social event. Novelists will draw from this strange life mysterious and moving episodes (we would add: the legal world will find in the account of the "Sax trial" a vast domain for a case law study) and the moralists will find in it the features of self-denial, physical courage and perseverance, of which only a lifted soul and a great heart are capable.</p>
</blockquote>
<a name="Enfance"></a>
<h2><span>An Agitated Childhood</span></h2>

<blockquote>
<p>Antoine-Joseph Sax was born in the street that has borne his name since 1896, in a modest house, which was destroyed in 1914, and which was built on the present site of an important commercial building.</p>
<p>In its fa√ßade, there is a stained-glass window and an inscription chiselled into the stonework: "Adolphe Sax, 1814-1894, was born here". This window was solemnly inaugurated on 27 June 1954, on the initiative of the Tourist Information Centre, under the mayorship of Mr L√©on Sasserath. It is the work of Mr Jean Jadin, who designed the cartoon, and Miss Maggy Arz√©e. Both were taught by Miss Yvonne G√©rard and Mr Perot, teachers of graphic art and decoration at the Fine Arts Academy in Namur, which was then directed by Mr Lambeau. It was created under the direction of Mr Van de Capelle.</p>
<p>Son of Charles-Joseph Sax (1791-1865) and Marie-Joseph Masson (1813-1861), Antoine-Joseph was the eldest of eleven children (six boys and five girls, only four of whom survived, the others dying between the ages of 20 and 25).</p>
<p>His childhood was tragic. Hardly able to stand, Antoine-Joseph fell from a height of three floors, seriously bumping his head against a stone: he was believed dead. At the age of three, he swallowed a bowl of vitriolized water, and then a pin. Later, he was seriously burned in a gunpowder explosion; he fell onto a cast iron frying pan and burned himself on one side. Three times he escaped poisoning and asphyxiation in his bedroom, where varnished items were lying about during the night. Another time, he was hit on the head by a cobblestone; he fell into a river and was saved by the skin of his teeth.</p>
<p>"<em>He's a child condemned to misfortune; he won't live," his mother said. In the district, they called him "little Sax, the ghost</em>".</p>
<p>These initial serious incidents were, alas, but the prelude to an eventful existence such as only a few have known. In 1858, Adolphe Sax was miraculously saved from a cancer of the lip by a black doctor who knew the properties of certain Indian plants. What would the future have been but for this intervention?</p>
</blockquote>
<a name="Charles-Joseph"></a>
<h2><span>Charles-Joseph Sax</span></h2>

<blockquote>
<p>A joiner-cabinetmaker, Charles-Joseph Sax quickly launched himself, with success, into the manufacture of musical instruments. In the "New Street" he ran a large workshop. In this trade, he acquired such a reputation that, in 1815 (his eldest son was only one year old), he also set up a workshop in Brussels (where Antoine-Joseph's brothers and sisters were to be born), where he was summoned by William I of Orange (we were then under Dutch occupation). The latter appointed him as maker to the Court and entrusted him with the task of supplying suitable instruments to Belgium regimental music corps.</p>
<p>A self-taught man, therefore, Charles-Joseph Sax made woodwind and brass instruments, even violins and pianos. He registered a dozen patents and brought his instruments to perfection. He successfully participated in numerous exhibitions, where he was awarded flattering distinctions.</p>
<p>At the time when he could have spend the day playing, laughing and having fun, Antoine-Joseph observed the work in his father's workshop, besides being given instruction by one of his uncles, a teacher in Dinant. He was intelligent and his inventive mind was already showing itself, thanks to his love for music (whilst very young, he took singing and flute lessons). Thereafter, he was given lessons by his father, who quickly appreciated his abilities and did all he could to develop them.</p>
<p>Far from disregarding his son's aspirations, Charles-Joseph Sax made him his apprentice and, from a young age, he was conscious of the importance of his work, as though he were anticipating his destiny.</p>
<p>In 1853, after the death of seven of his eleven children, and following financial worries at his Brussels business, Charles-Joseph joined his son in Paris. The master was to become the servant, and was from then on in charge of making saxophones until his death in 1865.</p>
</blockquote>
<a name="Jeunesse"></a>
<h2><span>A productive childhood</span></h2>

<blockquote>
<p>Supported and assisted by his father, the youth worked. He created, he perfected instruments and he played them. He was 16 when he went to the Industrial Exposition in Brussels to present flutes and ivory clarinets. At the age of 20, he made an entirely new clarinet, with 24 keys, a work of imagination and a masterpiece of manual work. Then, a new bass clarinet, which incited enthusiasm in Habeneck, the leader of the orchestra at the Paris Opera House, who was passing through Brussels, and who called the other clarinets "barbarian instruments".</p>
<p>Even at that early stage, this creation provoked jealousy in the soloist at the "Great Royal Harmony" in Brussels, who refused to use it because, he said, it had come from "that weedy little pupil, Sax". "Play your clarinet, then" Sax answered, " and I shall play mine." The challenge accepted, Sax triumphed in front of four thousand people. He became a soloist. Works were written for him that, after his departure, were no longer played because they were so difficult!</p>
<p>The young genius pursued his work. He invented a sound reflector, a new double-bass clarinet, a piano-tuning process that remained the inventor's secret and who probably was unable to exploit it for want of money, a steam organ "capable of being heard throughout the province": now that just shows Sax's tendency to think big!</p>
<p>Sax's beginnings throw a very curious light on his character (we shall call him Adolphe from now on): energy, courage, dynamism, total self-confidence. He refused to go and set up a business in St Petersburg, rejected an offer to set up in London. That means that his reputation exceeded frontiers. Sax was conscious of all his possibilities and his talent; he conceived the work that he felt the call to achieve; he was full of hope and he believed he had every chance of success; he had great visions, he believed in what he saw. He suffocated in his little country.</p>
<p>In 1840, he presented nine inventions at the Belgian Exhibition. He was denied the first medal on the plea of his young age; there would be nothing left to offer him the year after. He was thwarted in his true-love, if not in his pride. He refused the vermeil medal he was awarded, replying with pride, "<em>If they think me too young to deserve the gold medal, I myself think me too old to accept this vermeil one</em>."</p>
</blockquote>
<a name="Paris"></a>
<h2><span>The Call to Paris</span></h2>

<blockquote>
<p>Europe's centre of attraction, Paris haunted him, Paris called him.</p>
<p>The composer Hal√©vy wrote to him of the hope that composers had in his inventions: "<em>Hurry and finish your new family of instruments (saxophones) and come and succour to the poor composers that are looking for something new and to the public that is demanding it, if not to the world itself.</em>"</p>
<p>Let us add to this call and the snub in Brussels the fact of his family trials, and the decision was made: Adolphe Sax left for Paris "rich in ideas and light in cash": he had thirty francs in his pocket!</p>
<p>The year 1842 formed the turning point in Sax's life, possessing as he did his new invention: the saxophone and its family.</p>
<p><img src="http://www.dinant.be/uploads/pages/286/sax_atel.jpg" alt="">Moreover, in 1841, had he not presented it anonymously in Brussels, behind a curtain, so as not to disclose it and avoid the risk of plagiarism?</p>
<p>Adolphe Sax was almost thirty, "the age at which man's creative character affirms itself, at which the human personality is drawn." At the age of 27, Napoleon won his first battle in Italy; Newton was 24 and Einstein 26 when they devised their theories. Mozart died aged 35 and Schubert at 31. Examples of precocious geniuses are manifold.</p>
<p>As one former inhabitant of Dinant once rightfully said (1) "<em>a distinction has to be drawn here between a man who draws from his own abstract thoughts the stuff that his genius will knead, him for whom symbols and signs are sufficient to bring forth a thought laden with restrained life and latent splendours; and the other man for whom a technique, slow and tenacious apprenticeship on a complicated apparatus is necessary for him to be able to physically achieve the formal idea. Count, for example, how many early-developing mathematicians there are compared with child physicists. The former exist, the latter are nowhere to be found. Sax is of the category of intellectuals that concentrated on matter and not pure form"</em>.</p>
<p>In 1842, there was Adolphe Sax living in a simple shed in Rue Saint-Georges, Paris. To set up business, he had to borrow money from a musician acquaintance.</p>

</blockquote>
<a name="Berlioz"></a>
<h2><span>Thanks to Berlioz</span></h2>
<blockquote>
<div>
<p><span><strong>About the saxophone, he said "<em>Its principal merit in my view is the varied beauty of its accent, sometimes serious, sometimes calm, sometimes impassioned, dreamy or melancholic, or vague, like the weakened echo of an echo, like the indistinct plaintiff moans of the breeze in the woods and, even better, like the mysterious vibrations of a bell, long after it has been struck; there does not exist another musical instrument ‚Ä¶</em></strong></span></p></div></blockquote></td></tr></tbody></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.dinant.be/en/inheritance/adolphe-sax">http://www.dinant.be/en/inheritance/adolphe-sax</a></em></p>]]>
            </description>
            <link>http://www.dinant.be/en/inheritance/adolphe-sax</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834716</guid>
            <pubDate>Tue, 20 Oct 2020 07:28:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My chatbot is dead ‚Äì Why yours should probably be too]]>
            </title>
            <description>
<![CDATA[
Score 174 | Comments 160 (<a href="https://news.ycombinator.com/item?id=24834552">thread link</a>) | @raphaelsaunier
<br/>
October 19, 2020 | https://azumbrunnen.me/blog/my-chatbot-is-dead-%C2%B7-why-yours-should-probably-be-too/ | <a href="https://web.archive.org/web/*/https://azumbrunnen.me/blog/my-chatbot-is-dead-%C2%B7-why-yours-should-probably-be-too/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
                
<audio controls="controls"><source src="https://azumbrunnen.me/audio/chatbot-is-dead" type="audio/mpeg"></audio>



<p>Personal websites are usually like old books in a shelf. They languish, accumulate dust, and their wrinkles and cracks become more apparent over time. About 3 years ago I embarked on a simple experiment that would end up prolonging the shelf-life of my website by an unusually large margin.</p>



<p>Back in 2017, it seemed like Conversational UI was poised to take over the world. We saw Quartz turning news into a conversation, WeChat being featured as the poster-child of a post application world, iMessage turning into an unnecessarily complex mess, and chatbots popping up like mushrooms in moist forests.</p>



<p>Of course, any trend gaining so much traction and interest needs to be taken seriously. As such, I decided to familiarize myself with the topic, and turned my website into a chatbot.</p>



<p>Instead of being greeted by the internationally standardized greeting every designer used at some point in their career, there was no bold, dramatically oversized, and deep black sans-serif reading: <em>Hi, I‚Äôm a designer.</em> (To be fair, I didn‚Äôt use Proxima Nova either)</p>



<p>Instead, a couple of chat bubbles exuberantly ushered onto the canvas to greet users as if we had all been long time friends.</p>



<figure><video autoplay="" loop="" muted="" src="https://azumbrunnen.me/wp-content/uploads/chatbot-animated.mp4" playsinline=""></video><figcaption>Conversational intro</figcaption></figure>



<p>It was witty, new, and slightly awkward. People would send messages that ranged from simple chit chat, to deep philosophical topics, to downright disturbing and ridiculous insults.</p>



<p>The experiment got featured on Hackernews, Medium, was used in psychological studies conducted by Dan Ariely‚Äôs team, and the source code was ripped and edited by various startups to fit their needs. One business in the Bay Area had an idea to use it to sell flowers in a conversational way. It looks like they went out of business.</p>



<p>The reaction and feedback was surprising to say the least. It was an idea so simple, so silly, that the outcome was in many ways unexpected. After all, the only one who really cares about your website, is usually yourself.</p>



<p>That didn‚Äôt stop me from revamping my website and kill the very thing that had turned it into a micro-celebrity before. With the death of my old chatbot, some angry emails by schools who are using it as a reference for ‚Äúcreative‚Äù web design, and a good amount of time that has passed ever since, I wanted to take a step back and set the record straight.</p>



<hr>



<h2>When chatbots matter</h2>



<p>So let‚Äôs be honest with ourselves for a moment: <em>when did you actually ever enjoy talking to a chat bot?</em> And I‚Äôm not talking about the type of bots you talk to when you‚Äôre bored, but about those that provide a deeper purpose.</p>



<p>It turns out that the answer is, at least for most of us, almost never.</p>



<p>I love you Intercom, except when I don‚Äôt. 99% of time I don‚Äôt want to talk to a silly and obtrusive avatar popping up from some corner of the screen before I even had a chance to check out what‚Äôs going on. Somehow, I can‚Äôt help but think others feel the same.</p>



<p>In fact, we do know that others feel the same. Chat heads jumping at us unasked, are the quintessential equivalent of the infamous sales clerk who eagerly talks to us upon entering a store.</p>



<p>To further add to the challenges: as soon as users go off-script, chat bot‚Äôs don‚Äôt just become awkward and unpredictable‚Äîthey turn into little sociopaths that might rub users the wrong way.</p>



<figure><img loading="lazy" width="400" height="346" src="https://azumbrunnen.me/wp-content/uploads/grandma.png" alt="" srcset="https://azumbrunnen.me/wp-content/uploads/grandma.png 400w, https://azumbrunnen.me/wp-content/uploads/grandma-300x260.png 300w" sizes="(max-width: 400px) 100vw, 400px"><figcaption>UX Chat.me ‚Äî&nbsp;Conversational UX News</figcaption></figure>



<p>The moment you create a chat bot is the moment you allow customers to have a conversation with <em>your brand</em>. Not with yourself, not with your friend, but with an uber entity‚Äîa symbol‚Äîthat represents everything you and your team stand for. That‚Äôs not a step to be taken lightly.</p>



<p>This simple conversational entity can be a fun tool to engage with people but depending on how the conversation goes, it can quickly turn into a misrepresentation of the values of your team and your company. So building a chat bot should never be the default choice, but an intentional one.</p>



<p>That‚Äôs why it‚Äôs worth asking yourself the following three questions before venturing into this space:</p>



<h3>1. Is your use case simple enough to be solved through chat?</h3>



<p>Conversation is incredibly complex and it‚Äôs challenging enough to keep it on track in the real world. If the use case isn‚Äôt simple, chances are, chat bots are not the right tool for the job.</p>



<h3>2. Is your NLP capable and sophisticated enough?</h3>



<p>There are two types of bots: pre-scripted bots with a range of default answers users can choose from, and Natural Language Processing based ones.</p>



<p>Choosing the right one is hard. While pre-scripted can feel too limiting, NLP can break at every corner. Often times, teams quickly fall into the trap of spending a huge amount of time focusing on personality and silly jokes, instead of solving the problem users hired you for in the first place.</p>



<p>Therefore, building on top of the first point above, within the conversational landscape, simple always wins.</p>



<h3>3. Are your users actually in chat based environments?</h3>



<p>Chat bots work best where users already are. If your users are primarily spending time in messaging platforms where bots and micro-apps can be seamlessly embedded, great. That can serve as an effective and natural way to engage with your audience because it matches the ‚Äúbe where users already are‚Äù principle.</p>



<p>If on the other hand, people come to your website, a medium that has made great strides to provide content in a non-linear and quick way, it often unnecessarily slows users down. </p>



<hr>



<h2>Farewell chatbot</h2>



<p>I don‚Äôt want to discredit chat bots as a paradigm. They have their use in certain industries, medium, and work well for a specific set of use cases. The important part is being deliberate, rather than jumping ship blindfolded.</p>



<p>So whereas turning my website into a chat was a fun experiment, I ultimately feel like it has slowly turned into a fad. I got fooled by the trend, and as a by-product became part of the trend itself.  Fads come and go, and as they get refined and re-interpreted, they ultimately find their true purpose. What we‚Äôre left with is the age old insight that it‚Äôs only through experimentation, that we can unlock concepts and ideas that last.</p>



<p>Rest in peace chat bot, long live chat bots.</p>
            </article></div>]]>
            </description>
            <link>https://azumbrunnen.me/blog/my-chatbot-is-dead-%C2%B7-why-yours-should-probably-be-too/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834552</guid>
            <pubDate>Tue, 20 Oct 2020 06:52:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Disney Animation data sets (2018)]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24834299">thread link</a>) | @ascorbic
<br/>
October 19, 2020 | https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html | <a href="https://web.archive.org/web/*/https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>

            
            <span>July 03, 2018 
                 | Tags: 
                     
                        Datasets
                      
                </span>

            <section>
                <p>Today at <a href="https://cg.ivd.kit.edu/egsr18/">EGSR 2018</a>, Walt Disney Animation Studios announced the release of two large, production quality/scale data sets for rendering research purposes.
The data sets are available on a new <a href="https://disneyanimation.com/data-sets/">data sets page on the official Disney Animation website</a>.
The first data set is the Cloud Data Set, which contains a large and highly detailed volumetric cloud data set that we used for our ‚Äú<a href="https://blog.yiningkarlli.com/2017/07/spectral-and-decomposition-tracking.html">Spectral and Decomposition Tracking for Rendering Heterogeneous Volumes</a>‚Äù SIGGRAPH 2017 paper, and the second data set is the Moana Island Scene, which is a full production scene from <a href="https://blog.yiningkarlli.com/2016/11/moana.html">Moana</a>.</p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/shotCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/shotCam_hyperion.jpg" alt="Figure 1: The Moana Island Data Set, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/wdas_cloud_hyperion_render.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/wdas_cloud_hyperion_render.jpg" alt="Figure 2: The Cloud Data Set, rendered using Disney's Hyperion Renderer."></a></p>

<p>In this post, I‚Äôll share some personal thoughts, observations, and notes.
The release of these data sets was announced by my teammate, Ralf Habel, at EGSR today, but this release has been in the works for a very long time now, and is the product of the collective effort of an enormous number of people across the studio.
A number of people deserve to be highlighted: Rasmus Tamstorf spearheaded the entire effort and was instrumental in getting the resources and legal approval needed for the Moana Island Scene.
Heather Pritchett is the TD that did the actual difficult work of extracting the Moana Island Scene out of Disney Animation‚Äôs production pipeline and converting it from proprietary data formats into usable, industry-standard data formats.
Sean Palmer and Jonathan Garcia also helped in resurrecting the data from Moana.
Hyperion developers Ralf Habel and Peter Kutz led the effort to get the Cloud Data Set approved and released; the cloud itself was made by artists Henrik Falt and Alex Nijmeh.
On the management side of things, technology manager Rajesh Sharma and Disney Animation CTO, <a href="https://twitter.com/ncannon?lang=en">Nick Cannon</a>, provided crucial support and encouragement.
Matt Pharr has been crucial in collaborating with us to get these data sets released.
Matt was highly accommodating in helping us get the Moana Island Scene into a PBRT scene; I‚Äôll talk a bit more about this later.
Intel‚Äôs Embree team also gave significant feedback.
My role was actually quite small; along with other members of the Hyperion development team, I just provided some consultation throughout the whole process.</p>

<p>Please note the licenses that the data sets come with.
The Cloud Data Set is licensed under a <a href="https://disney-animation.s3.amazonaws.com/uploads/production/data_set_asset/6/asset/License_Cloud.pdf">Creative Commons Attribution ShareAlike 3.0 Unported License</a>; the actual cloud is based on a photograph by Kevin Udy on his <a href="https://coclouds.com/436/cumulus/%202012-07-26/">Colorado Clouds Blog</a>, which is also licensed under the same Creative Commons license.
The Moana Island Scene is licensed under a more restrictive, custom Disney Enterprises <a href="https://disney-animation.s3.amazonaws.com/uploads/production/data_set_asset/4/asset/License_Moana.pdf">research license</a>.
This is because the Moana Island Scene is a true production scene; it was actually used to produce actual frames in the final film.
As such, the data set is being released only for pure research and development purposes; it‚Äôs not meant for use in artistic projects.
Please stick to and follow the licenses these data sets are released under; if people end up misusing these data sets, then it makes releasing more data sets into the community in the future much harder for us.</p>

<p>This entire effort was sparked two years ago at SIGGRAPH 2016, when Matt Pharr made an appeal to the industry to provide representative production-scale data sets to the research community.
I don‚Äôt know how many times I‚Äôve had conversations about how well new techniques or papers or technologies will scale to production cases, only to have further discussion stymied by the lack of any true production data sets that the research community can test against.
We decided as a studio to answer Matt‚Äôs appeal, and last year at SIGGRAPH 2017, Brent Burley and Rasmus Tamstorf announced our intention to release both the Cloud and Moana Island data sets.
It‚Äôs taken nearly a year from announcement to release because the process has been complex, and it was very important to the studio to make sure the release was done properly.</p>

<p>One of the biggest challenges was getting all of the data out of the production pipeline and our various proprietary data formats into something that the research community can actually parse and make use of.
Matt Pharr was extremely helpful here; over the past year, Matt has added support for <a href="http://ptex.us/">Ptex</a> textures and implemented the <a href="http://blog.selfshadow.com/publications/s2015-shading-course/burley/s2015_pbs_disney_bsdf_notes.pdf">Disney Bsdf</a> in <a href="https://github.com/mmp/pbrt-v3">PBRT v3</a>.
Having Ptex and the Disney Bsdf available in PBRT v3 made PBRT v3 the natural target for an initial port to a renderer other than Hyperion, since internally all of Hyperion‚Äôs shading uses the Disney Bsdf, and all of our texturing is done through Ptex.
Our texturing also relies heavily on procedural <a href="https://www.disneyanimation.com/technology/seexpr.html">SeExpr</a> expressions; all of the expression-drive texturing had to be baked down into Ptex for the final release.</p>

<p>Both the Cloud and Moana Island data sets are, quite frankly, enormous.
The Cloud data set contains a single OpenVDB cloud that weighs in at 2.93 GB; the data set also provides versions of the VDB file scaled down to half, quarter, eighth, and sixteenth scale resolutions.
The Moana Island data set comes in three parts: a base package containing raw geometry and texture data, an animation package containing animated stuff, and a PBRT package containing a PBRT scene generated from the base package.
These three packages combined, uncompressed, weigh in at well over 200 GB of disk space; the uncompressed PBRT package along weighs in at around 38 GB.</p>

<p>For the Moana Island Scene, the provided PBRT scene requires a minimum of around 90 GB if RAM to render.
This many seem enormous for consumer machines, because it is.
However, this is also what we mean by ‚Äúproduction scale‚Äù; for Disney Animation, 90 GB is actually a fairly mid-range memory footprint for a production render.
On a 24-core, dual-socket Intel Xeon Gold 6136 system, the PBRT scene took me a little over an hour and 15 minutes to render from the ‚ÄòshotCam‚Äô camera.
Hyperion renders the scene faster, but I would caution against using this data set to do performance shootouts between different renders.
I‚Äôm certain that within a short period of time, enthusiastic members of the rendering community will end up porting this scene to Renderman and Arnold and Vray and Cycles and every other production renderer out there, which will be very cool!
But keep in mind, this data set was authored very specifically around Hyperion‚Äôs various capabilities and constraints, which naturally will be very different from how one might author a complex data set for other renderers.
Every renderer works a bit differently, so the most optimal way to author a data set for every renderer will be a bit different; this data set is no exception.
So if you want to compare renderers using this data set, make sure you understand the various ways how the way this data set is structured impacts the performance of whatever renderers you are comparing.</p>

<p>For example, Hyperion subdivides/tessellates/displaces everything to as close to sub-poly-per-pixel as it can get while still fitting within computational resources.
This means our scenes are usually very heavily subdivided and tessellated.
However, the PBRT version of the scene doesn‚Äôt come with any subdivision; as a result, silhouettes in the following comparison images don‚Äôt fully match in some areas.
Similarly, PBRT‚Äôs lights and lighting model differ from Hyperion‚Äôs, and Hyperion has various artistic controls that are unique to Hyperion, meaning the renders produced by PBRT versus Hyperion differ in many ways:</p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/shotCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/shotCam_hyperion.jpg" alt="Figure 3a: 'shotCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/shotCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/shotCam_pbrt.jpg" alt="Figure 3b: 'shotCam' camera angle, rendered using PBRT v3."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/beachCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/beachCam_hyperion.jpg" alt="Figure 4a: 'beachCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/beachCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/beachCam_pbrt.jpg" alt="Figure 4b: 'beachCam' camera angle, rendered using PBRT v3."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/dunesACam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/dunesACam_hyperion.jpg" alt="Figure 5a: 'dunesACam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/dunesACam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/dunesACam_pbrt.jpg" alt="Figure 5b: 'dunesACam' camera angle, rendered using PBRT v3. Some of the plants are in slightly different locations than the Hyperion render; this was just a small change that happened in data conversion to the PBRT scene."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/flowersCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/flowersCam_hyperion.jpg" alt="Figure 6a: 'flowersCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/flowersCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/flowersCam_pbrt.jpg" alt="Figure 6b: 'flowersCam' camera angle, rendered using PBRT v3. Note that the silhouette of the flowers is different compared to the Hyperion render because the Hyperion render subdivides the flowers, whereas the PBRT render displays the base cage."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/grassCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/grassCam_hyperion.jpg" alt="Figure 7a: 'grassCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/grassCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/grassCam_pbrt.jpg" alt="Figure 7b: 'grassCam' camera angle, rendered using PBRT v3. The sand dune in the background looks particularly different from the Hyperion render due to subdivision and displacement."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/palmsCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/palmsCam_hyperion.jpg" alt="Figure 8a: 'palmsCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/palmsCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/palmsCam_pbrt.jpg" alt="Figure 8b: 'palmsCam' camera angle, rendered using PBRT v3. The palm leaves look especially different due to differences in artistic lighting shaping and curve shading differences. Most notably, the look in Hyperion depends heavily on attributes that vary along the length of the curve, which is something PBRT doesn't support yet. Some more work is needed here to get the palm leaves to look more similar between the two renders."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/rootsCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/rootsCam_hyperion.jpg" alt="Figure 9a: 'rootsCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/rootsCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/rootsCam_pbrt.jpg" alt="Figure 9b: 'rootsCam' camera angle, rendered using PBRT v3. Again, the significant difference in appearance in the rocks is probably just due to subdivision/tesselation/displacement."></a></p>

<p>Another example of a major difference between the Hyperion renders and the PBRT renders is in the water, which Hyperion renders using photon mapping to get the caustics.
The provided PBRT scenes use unidirectional pathtracing for everything including the water, hence the very different caustics.
Similarly, the palm trees in the ‚ÄòpalmsCam‚Äô camera angle look very different between PBRT and Hyperion because Hyperion‚Äôs lighting controls are very different from PBRT; Hyperion‚Äôs lights include various artistic controls for custom shaping and whatnot, which aren‚Äôt necessarily fully physical.
Also, the palm leaves are modeled using curves, and the shading depends on varying colors and attributes along the length and width of the curve, which PBRT doesn‚Äôt support yet (getting the palm leaves is actually the top priority for if more resources are freed up to improve the data set release).
These difference between renderers don‚Äôt necessarily mean that one renderer is better than the other; they simply mean that the renderers are different.
This will be true for any pair of renderers that one wants to compare.</p>

<p>The Cloud Data Set includes an example render from Hyperion, which implements our Spectral and Decomposition Tracking paper in its volumetric rendering system to efficiently render the cloud with thousands of bounces.
This render contains no post-processing; what you see in the provided image is exactly what Hyperion outputs.
The VDB file expresses the cloud as a field of heterogeneous densities.
Also provided is an example <a href="https://www.mitsuba-renderer.org/">Mitsuba</a> scene, renderable using the <a href="https://github.com/zhoub/mitsuba-vdb">Mitsuba-VDB plugin that can be found on Github</a>.
Please consult the README file for some modifications in Mitsuba that are necessary to render the cloud.
Also, please note that the Mitsuba example will take an extremely long time to render, since Mitsuba isn‚Äôt really meant to render high-albedo heterogeneous volumes.
With proper acceleration structures and algorithms, rendering the cloud only takes us a few minutes using Hyperion, and should be similarly fast in any modern production renderer.</p>

<p>One might wonder just why production data sets in general are so large.
This is an interesting question; the short answer across the industry basically boils down to ‚Äúartist time is more expensive and valuable than computer hardware‚Äù.
We could get these scenes to fit into much smaller ‚Ä¶</p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html">https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html</a></em></p>]]>
            </description>
            <link>https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834299</guid>
            <pubDate>Tue, 20 Oct 2020 05:54:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Moana Motunui Renderer on GPU]]>
            </title>
            <description>
<![CDATA[
Score 328 | Comments 49 (<a href="https://news.ycombinator.com/item?id=24833218">thread link</a>) | @Impossible
<br/>
October 19, 2020 | https://www.render-blog.com/2020/10/03/gpu-motunui/ | <a href="https://web.archive.org/web/*/https://www.render-blog.com/2020/10/03/gpu-motunui/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
  
  <p><span>03 Oct 2020</span></p><p>Disney Animation‚Äôs Moana island dataset is a production-scale scene with memory requirements that make it challenging to render. This post summarizes some of those challenges, and describes how the <a href="https://github.com/chellmuth/gpu-motunui">GPU-Motunui</a> project is able to efficiently render the scene on a consumer-grade GPU with less than 8GB of memory. <a href="#renders">Click here</a> to skip ahead to the results.</p>

<h2 id="the-moana-island">The Moana island</h2>

<p>In 2018, Disney Animation released the Moana island dataset to the rendering research community. Compared to traditional research scenes, the scale of the Moana island scene is massive: the scene contains 90 million quad primitives, 5 million curves, and more than 28 million instances. All told, the island consists of over 15 billion primitives, weighing in at just under 30GB of geometry files.</p>

<p>The shots included with the dataset are beautiful, and showcase the amazing imagery that can be created by combining the best artists in the world with path tracing techniques and modern hardware. Here are two reference images, rendered with Disney‚Äôs proprietary Hyperion renderer:</p>

<div>
  <p><img src="https://www.render-blog.com/assets/hyperion-reference-shotCam.png" alt="Hyperion shotCam reference"></p><p>Hyperion shotCam reference</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/hyperion-reference-beachCam.png" alt="Hyperion beachCam reference"></p><p>Hyperion beachCam reference</p>
</div>

<h2 id="gpu-motunui-project">GPU-Motunui Project</h2>

<p>The goal of the GPU-Motunui project is to render all the Moana shots efficiently and accurately on a consumer-grade graphics card. There are two main challenges to accomplishing this with the Moana dataset. First, with a typical graphics card having only 8GB of memory, an out-of-core rendering solution is required to handle the large amounts of geometry. Second, the scene‚Äôs textures are provided in the Ptex format, and Ptex doesn‚Äôt have a publicly available CUDA implementation. This project currently only solves the first problem, and Ptex texture lookup is done on the CPU (although conveniently its cost is fully hidden by being computed concurrently with GPU shadow ray tracing).</p>

<p>The Hyperion reference images are impossible to match exactly; for example the varying brown and green colors along the palm tree fronds in the palmsCam shot are not provided in the dataset. Other features of the scene are possible to render but out of my initial scope, notably subdivision surfaces and their displacement maps, and a full Disney BSDF implementation.</p>

<div>
  <p><img src="https://www.render-blog.com/assets/hyperion-unique-palmsCam.png" alt="Example of an unreproducible material variation on the palm tree frond"></p><p>Example of an unreproducible material variation on the palm tree frond</p>
</div>

<p>All ray tracing operations are run through Nvidia‚Äôs OptiX 7 API. This means GPU-Motunui gets the full benefits of available RT cores and a world-class BVH implementation. The following sections describe how GPU-Motunui maps dataset assets to OptiX data structures, and how GPU-Motunui‚Äôs out-of-core rendering solution works.</p>

<h3 id="scene-representation">Scene representation</h3>

<p>The Moana scene makes widespread use of multi-level instancing. In OptiX, this requires a three-level hierarchy of acceleration structures to manage: two levels of IASs, and a base level of GASs (Instance Acceleration Structures and Geometry Acceleration Structures, respectively). GPU-Motunui makes use of OptiX‚Äôs AS compaction and relocation APIs to further reduce memory usage.</p>

<p>The isHibiscus element makes a good example of how a typical element in the scene is organized and built. The tree is assembled from a base model in one Wavefront .obj file (containing the trunk and branches), and four primitives: one flower and three leaf models (each with their own .obj file).</p>

<div>

<p>Left: The four simple primitives that will be instanced to fill out the hibiscus tree <br>Right: The base trunk and branches model </p>
</div>

<p>In OptiX, each of these models has an associated GAS, and each GAS can be subdivided into multiple build inputs. Build inputs are used to map sections of the model to information needed at shading time by indexing into OptiX‚Äôs shader binding table. These GASs form the bottom level of the hierarchy.</p>

<p>Next, an IAS is used to build the full isHibiscus element. This IAS is in the middle level of the hierarchy. The figure below shows each primitive‚Äôs instances in isolation, and combined to make the full element:</p>

<div>

<p>Left: Isolated instances for each primitive<br>Right: Full isHibiscus element</p>
</div>

<p>Finally, a second IAS is built to track all of the element‚Äôs instances present in the scene. This second IAS is the top level of the instance hierarchy.</p>

<div>
  <p><img src="https://www.render-blog.com/assets/isHibiscus-instanced-elements.png" alt="The shotCam view rendered with only isHibiscus instances"></p><p>The shotCam view rendered with only isHibiscus instances</p>
</div>

<p>Although the isHibiscus element has a typical structure, there are some more complicated elements included in the dataset. The isCoral element, for example, has different base geometry and instanced primitives for each of its element instances, but the underlying primitive geometries are shared across all the element instances.</p>

<p>The Moana GAS and IASs alone require 18.5 GB, well past the memory budget of my 8GB RTX 2070. Because OptiX has no native support for out-of-core rendering, the traditional OptiX pipeline had to be put aside for a custom-made solution.</p>

<h3 id="out-of-core-rendering">Out-of-core rendering</h3>

<p>To solve the out-of-core rendering problem, GPU-Motunui divides the scene‚Äôs geometry into different sections, and ray traces each separately, while tracking the closest hit. Replacing a traditional device trace call with a host loop comes with many design consequences to the renderer, from asset loading to the core path tracing loop that sends rays through the scene.</p>

<p>Before rendering, the asset loading process allocates a large chunk of GPU memory (currently 6.7GB). A custom allocator is implemented that manages this block of memory. It is responsible for allocating two types of memory: output and temporary. Output memory is allocated from the left of the block, and is used for OptiX structures. Temporary memory is managed on a stack from the right end of the memory block. Managing the temporary memory this way ensures that the output structures are always tightly packed.</p>

<p>After elements are processed into their accelerator structures on the GPU, their used memory is snapshotted onto the host, and the allocator is cleared. The process is repeated until all of the scene‚Äôs geometry is processed, resulting in the host managing a list of GPU memory snapshots. The figure below shows an example layout of GPU memory that could be snapshotted:</p>

<div>
  <p>GPU memory layout after loading the isHibiscus element.<br>(Dotted arrows show that an IAS holds instances of the pointed-at AS)</p>

</div>

<p>As mentioned above, when it comes time to ray trace, each snapshot is processed in a loop. This means a call to <code>cudaMemcpy</code> and <code>optixLaunch</code> for each snapshot. A global buffer is maintained that indicates the depth of the current closest intersection. This value is used as the <code>tmax</code> parameter for the CUDA kernel‚Äôs call to <code>optixTrace</code>, and a successful intersection will update the depth buffer for the next launch.</p>

<p>In a traditional OptiX path tracer, the entire render loop can run in device code inside a single call to <code>optixLaunch</code>; i.e., a successful intersection will lead to more BSDF and shadow rays being traced in the same kernel launch. Because GPU-Motunui‚Äôs design mandates multiple launches for tracing each path segment, the render loop is pulled out into host code. While this potentially diminishes OptiX‚Äôs ability to efficiently schedule program execution, it also opens up opportunties for optimization, such as running Ptex texture lookups on the CPU concurrently with GPU kernels and I/O.</p>

<h3 id="shading">Shading</h3>
<p>As with any OptiX application, GPU-Motunui makes use of the shader binding table (SBT). SBT records contain pointers to normal buffers and material attributes. The underlying data for the normal buffers is stored alongside OptiX acceleration structures and included in geometry snapshots. This ensures that GPU memory is never wasted on unreachable normal buffer data.</p>

<h2 id="renders">Renders</h2>
<p>Included below are GPU-Motunui renders of the six scenes included in the dataset. shotCam is the slowest to render at 18.2 seconds per sample at 1024x429 resolution, and took just over five hours total for the final image. All shots are 1024spp, capped at a maximum of five bounces, and were run on an Nvidia RTX 2070.</p>
<div>
  <p><img src="https://www.render-blog.com/assets/ours-shotCam.png" alt="shotCam"></p><p>shotCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-beachCam.png" alt="beachCam"></p><p>beachCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-dunesACam.png" alt="dunesACam"></p><p>dunesACam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-palmsCam.png" alt="palmsCam"></p><p>palmsCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-birdseyeCam.png" alt="birdseyeCam"></p><p>birdseyeCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-rootsCam.png" alt="rootsCam"></p><p>rootsCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-grassCam.png" alt="grassCam"></p><p>grassCam</p>
</div>

<h2 id="optimization">Optimization</h2>
<p>The initial implementation of the renderer required 42.6 seconds per 1spp on the shotCam scene. A few optimizations combined to make significant reductions in rendering time, cutting each pass down to 18.2 seconds (a 57.3% reduction).</p>

<h4 id="cpugpu-concurrency">CPU/GPU concurrency</h4>
<p>Tracing shadow rays on the GPU in parallel with Ptex lookups on the CPU cut rendering time by 23.4%. It was disappointing to be forced to do texture lookups on the CPU, but the time savings make up for it.</p>

<h4 id="multiple-ptex-caches">Multiple Ptex caches</h4>
<p>Parallelizing the Ptex lookups and using multiple Ptex caches eliminated texture lookups as a bottleneck to the system; shadow ray casting time fully dominates the texture lookup. Empirically, spawning two threads per core (totaling 12 on an Intel i7-8700K) and sharing three Ptex caches comfortably reduced the texture lookup time beneath the shadow ray budget. This improved the time savings to a 33.9% reduction over the baseline.</p>

<h4 id="pinned-memory">Pinned memory</h4>
<p>The acceleration structure snapshots are all saved to pinned host memory. Switching from normal to pinned host memory increased the transfer throughput from 7.73 GB/s to 11.84 GB/s, cutting the baseline render time by 19.5%.</p>

<h2 id="future-steps">Future Steps</h2>
<p>Getting this scene running on my RTX 2070 card was a very fun and rewarding project, but there are still many improvements to be made:</p>
<ul>
  <li>Implementing the Disney BSDF</li>
  <li>Rendering subdivision surfaces along with displacement mapping</li>
  <li>More efficiently packing the acceleration structures, and optimizing ray tracing throughput</li>
  <li>Experimenting with how various research results hold up on production scenes (e.g., testing select path guiding techniques)</li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://github.com/chellmuth/gpu-motunui/">GPU-Motunui</a></li>
  <li><a href="https://technology.disneyanimation.com/islandscene/">Moana Island Scene</a></li>
  <li><a href="https://pharr.org/matt/blog/2018/07/16/moana-island-pbrt-all.html">Swallowing the elephant</a> - Matt Pharr</li>
  <li><a href="https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html">Disney Animation Data Sets</a> - Yining Karl Li</li>
  <li><a href="https://schuttejoe.github.io/post/disneypostmortem/">Rendering the Moana Island Scene Part 2: A production scene from a hobby renderer</a> - Joe Schutte</li>
  <li><a href="https://ingowald.blog/2020/01/09/digesting-the-elephant/">Digesting the elephant</a> - Ingo Wald</li>
  <li>Brent Burley and Dylan Lacewell. <a href="http://ptex.us/ptexpaper.html">Ptex: ‚Ä¶</a></li></ul></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.render-blog.com/2020/10/03/gpu-motunui/">https://www.render-blog.com/2020/10/03/gpu-motunui/</a></em></p>]]>
            </description>
            <link>https://www.render-blog.com/2020/10/03/gpu-motunui/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24833218</guid>
            <pubDate>Tue, 20 Oct 2020 01:45:55 GMT</pubDate>
        </item>
    </channel>
</rss>
