<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 06 Aug 2020 08:20:02 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 06 Aug 2020 08:20:02 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[The best Parts of Visual Studio Code are proprietary]]>
            </title>
            <description>
<![CDATA[
Score 148 | Comments 120 (<a href="https://news.ycombinator.com/item?id=24047638">thread link</a>) | @ingve
<br/>
August 4, 2020 | https://underjord.io/the-best-parts-of-visual-studio-code-are-proprietary.html | <a href="https://web.archive.org/web/*/https://underjord.io/the-best-parts-of-visual-studio-code-are-proprietary.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<small>2020-08-03</small><!-- RSS:2020-08-03T07:25:00Z -->
<p>I've been very surprised and delighted over a number of years now by Microsoft's strong efforts in open
    source. I understand the skeptics, I was on Slashdot when they tried to sue Linux out of existence and I think only
    time will tell. I figure MS contributing is better than them hunting Linux distributions for sport. So I was mostly
    onboard for Microsofts efforts and I've especially found Visual Studio Code useful.</p>
<p>To settle a few things. When I tweeted on this subject I only got the response that I should use vim. Thanks. Great.
    I can and do on use vim. That misses a number of points. Visual Studio Code is an immensely popular editor and
    likely the most common recommendations to new developers. The primary reason I've used Visual Studio Code is
    that it has an incredibly compelling solution for remote pairing in the form of LiveShare. I've used that for a
    while with great success mentoring, coaching and generally working with other developers of varying experience and
    editor preferences. Most programmers can handle a "normal" editor like VS Code while something like Emacs
    or Vim depends a lot more on what they've learned.</p>
<p>I also ended up enjoying the Remote series of extensions for developing effectively inside remote servers or local
    containers.</p>
<h2>These things are proprietary</h2>
<p>At some point I read a piece of license that said that LiveShare could only be used with the Visual Studio family of
    products. "Huh, that sounds weird, VS Code is open source right?"</p>
<p>Sure enough. VS Code is fully MIT. The binary distribution has a separate license to allow telemetry and protect
    Microsoft trademarks and stuff. Nothing particularly weird, I can't really get worked up about telemetry, I know
    some can. But the extensions.. These extensions are in my book core differentiators that makes VS Code compelling.
    For me it is definitely part of what pushes it beyond the much leaner Sublime (paid, closed source) I was using
    before.</p>
<p>These extensions have a license limiting them and their online service parts to only be used with the Visual Studio
    family of products. This is the <a href="https://microsoftdocs.github.io/live-share/license/eula.html" title="License for LiveShare">license for LiveShare</a> and this is the <a href="https://code.visualstudio.com/preview-license" title="License for Remote">license for Remote</a>.</p>
<p>For me LiveShare is the most important thing. Google Docs style collaborative code editing, terminal sharing, port
    sharing and a bunch more features. I know Atom had an extension like this, I haven't checked the licensing there
    or tried it recently.</p>
<p>Remote is a very strong extension as well for anyone working on a server over SSH or in a container. It helps by
    installing extensions on the destination to allow language servers and such. I've seen it do terrible things to
    servers sometimes but it is very useful and generally works well.</p>
<p>It makes me uneasy to accept VS Code as an "open" project in any wider meaning of the word when compelling
    features are legally locked to only work inside the family of Visual Studio products. It makes me less certain that
    this isn't the Extend in Embrace, Extend, Extinguish. It also frustrates me that this prevents someone from
    building a compatible plugin for VIM or any other editor. This would be much more powerful if it could be in all the
    IntelliJs as well.</p>
<p>You'll find a repo for <a href="https://github.com/MicrosoftDocs/live-share" title="LiveShare on GitHub">LiveShare on GitHub</a> but it is only for documentation and issue tracking. There
    is no code. Same for <a href="https://github.com/microsoft/vscode-remote-release" title="Remote on GitHub">Remote</a>.</p>
<h2>The entire marketplace is proprietary</h2>
<p>Some additional salt in this particular wound is that the use of the Marketplace of VS Code extensions is also
    proprietarily licensed. So all these open source developers are shoving their extensions into a competitive
    advantage for one of the world's largest tech firms. And they disallow other uses of the marketplace. Even if
    the letter of open source is followed there is none of the openness, collaborative or community essence that I think
    exemplifies open source and free software projects.</p>
<p>What does this mean in practice? I guess it protects from the competition. Such as the <a href="https://github.com/VSCodium/vscodium" title="VS Codium">VS Codium</a> project which provides VS Code
    binaries without the proprietary parts. But also, as a consequence of this, without the Marketplace of extensions.
    There is an open source alternative called <a href="https://open-vsx.org/" title="Open VSX">Open VSX</a>, but since
    it isn't the canonical one it is missing a bunch of extensions and the big Liveshare and Remote ones are still
    not allowed.</p>
<p>This also blocks the <a href="https://github.com/cdr/code-server" title="code-server">code-server editor</a> that
    allows running VS Code in the browser from using it which otherwise would have been perfect for me to do development
    on an iPad Pro. I can still use that but a lot of packages are not in Open VSX.</p>
<h2>What about lock-in?</h2>
<p>Visual Studio Code is marketed with LiveShare and Remote as powerful extensions. VS Code is also marketed as open
    source. It is easy to use the editor, install the extensions and be under the impression that you are using an open
    source software suite where Microsoft simply hosts the peering service for identifying and connecting you and your
    collaborator.</p>
<p>But the peering service is not the only closed part. The extensions are not open source projects as far as I can find
    and they are licensed during distribution in a way that disallows using them with anything but Visual Studio
    products.</p>
<p>This leaves me with a sour taste in my mouth. I wasn't sold on having an Electron-based editor to begin with but
    VS Code was substantially leaner than Atom so I've been mostly accepting it.</p>
<p>If you have good suggestions for strong collaborative development tools that are open source, please let me know at
    <a href="mailto:lars@underjord.io">lars@underjord.io</a> or on Twitter <a href="https://twitter.com/lawik">@lawik</a>. If you want to follow my writing the RSS feed is
    right below. If you want more of my writing I have a tracking-free newsletter that I'd love for you to sign up for,
    also below.</p></div></div>]]>
            </description>
            <link>https://underjord.io/the-best-parts-of-visual-studio-code-are-proprietary.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24047638</guid>
            <pubDate>Tue, 04 Aug 2020 07:42:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Physical attractiveness bias in the legal system (2017)]]>
            </title>
            <description>
<![CDATA[
Score 360 | Comments 322 (<a href="https://news.ycombinator.com/item?id=24044409">thread link</a>) | @simonebrunozzi
<br/>
August 3, 2020 | https://www.thelawproject.com.au/insights/attractiveness-bias-in-the-legal-system | <a href="https://web.archive.org/web/*/https://www.thelawproject.com.au/insights/attractiveness-bias-in-the-legal-system">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="siteWrapper">

      

      

      
        
          
            
              
                
              
            
          
        
      


      
      
      

      <main id="page" role="main">
        
        <!--
        --><!--
        --><div id="content" data-content-field="main-content" data-collection-id="5817e1ff3e00be2eafd0dec4" data-edit-main-image="">
         <div>

  
  <article id="article-58c757b5e4fcb5bd2d9613d5" data-item-id="58c757b5e4fcb5bd2d9613d5">

    

    <div>
      
        <div data-layout-label="Post Body" data-type="item" data-updated-on="1489459192059" id="item-58c757b5e4fcb5bd2d9613d5"><div><div><div data-block-type="2" id="block-yui_3_17_2_3_1489466160916_36894"><div><blockquote><p><a href="http://bit.ly/2m8beq4" target="_blank">[Download for PDF/printable version of this article]</a><br></p></blockquote><p>When I started looking into this subject, I predicted a person’s physical attractiveness would only have minor advantages. I was wrong.</p><p>In fact, I was so wrong, that in one study, the effects of physical attractiveness on judges were so influential, they fined unattractive criminals 304.88% higher than attractive criminals.</p><p>Surprising, I know.</p><p>Before we proceed, I want to address a few concerns of mine. Firstly, the information that you will read may cause some readers to feel unsettled. This is not my intention. Yes, it is disheartening. But the purpose of this article is to inform lawyers and other decision makers so that they can use the attractiveness bias to their advantage or to counter it.</p><p>A second concern of mine is that I don’t want to over-emphasise the attractiveness bias. Judges and jurors are affected by all kinds of cognitive distortions, such as emotive evidence, time of day, remorse of the defendant, socioeconomic status, race, gender, anchoring effect, and the contrast bias.</p><p>In the first section of this article, I give a ‘straight-to-the-point’ summary of the research conducted by 27 studies. Next, I enter into greater depth on the attractiveness bias and its effects on judges, jurors, and lawyers. Lastly, I provide research on the attractiveness bias in everyday life. Arguably, the last section is the most interesting.</p><p>Enjoy!</p><p>* * *</p><ol data-rte-list="default"><li><p>Physical Attractiveness had a significant influence on judges sentencing. The more unattractive the criminal, the higher the sentence. Or conversely, the more attractive the criminal, the lower the sentence. The results of three studies show a minimum increase of 119.25% and a maximum increase of 304.88%.<br></p></li><li><p>Attractiveness had little to no effect on a judge’s verdict of guilt. Attractive and unattractive criminals were convicted equally.<br></p></li><li><p>Mock jurors generally sentenced unattractive criminals significantly higher than attractive criminals. However, as jurors do not determine sentencing in real court cases, these results are not directly applicable.<br></p></li><li><p>Attractiveness had minor effects on mock juror’s verdicts. Some studies reported minor effects and some studies reported no effects.<br></p></li><li><p>Generally, attractive people are perceived as more intelligent, more socially skilled, more appealing personalities, more moral, more altruistic, more likely to succeed, more hirable as managers, and more competent. Attractive people tend to have better physical health, better mental health, better dating experiences, earn more money, obtain higher career positions, chosen for jobs more often, promoted more often, receive better job evaluations, and chosen as business partners more often, than unattractive people.<br></p></li><li><p>I believe that the attractiveness bias is rarely conscious. I do not think people are consciously disfavouring unattractive people. I also do not place moral blame on the typical person for their unconscious bias.</p></li></ol><p>* * *</p><h2><strong>REAL JUDGES: SENTENCING</strong></h2><p><strong>THE MISDEMEANOUR STUDY </strong><em>[1]</em></p><p>The first study we will observe is the research conducted by Downs and Lyons.</p><p>The purpose of this study was to find a link between a criminal’s attractiveness and sentencing outcomes.</p><p>They gathered a group of police officers and students to rate the attractiveness of over 2000 criminals. A scale of 1 - 5 was used and their ratings were mostly similar.</p><p>Then, the judges sentencing decisions were divided into two main categories: misdemeanors and felonies. Misdemeanors were separated into to 3 classes, related to the severity of the crime.</p><p><em>The Results &amp; Key Takeaways</em></p><p>For misdemeanours, the judges fined unattractive criminals significantly more than attractive criminals. The fine incrementally increased as the attractiveness decreased.</p><p>1.&nbsp;&nbsp;&nbsp;&nbsp; Minor Misdemeanours = +224.87%</p><p>2.&nbsp;&nbsp;&nbsp;&nbsp; Moderate Misdemeanours = +304.88%</p><p>3.&nbsp;&nbsp;&nbsp;&nbsp; Serious Misdemeanours = + 174.78%</p><p>The results are graphed below.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_6_1489459079955_48500"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489539944610-FD8SUJ05P7WIZRNN1AYU/ke17ZwdGBToddI8pDm48kJTwxz64trr3drbHr6lHJk8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcrARKSnFMgvt0aSzcObof0G5Q2td9bvxksMtgDYlbDbROWDvDtu6PEW58QOEcShy6/Image+test+1.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489539944610-FD8SUJ05P7WIZRNN1AYU/ke17ZwdGBToddI8pDm48kJTwxz64trr3drbHr6lHJk8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcrARKSnFMgvt0aSzcObof0G5Q2td9bvxksMtgDYlbDbROWDvDtu6PEW58QOEcShy6/Image+test+1.jpg" data-image-dimensions="1047x619" data-image-focal-point="0.5,0.5" alt="Image test 1.jpg" data-load="false" data-image-id="58c89366f7e0ab29642796d9" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489539944610-FD8SUJ05P7WIZRNN1AYU/ke17ZwdGBToddI8pDm48kJTwxz64trr3drbHr6lHJk8UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcrARKSnFMgvt0aSzcObof0G5Q2td9bvxksMtgDYlbDbROWDvDtu6PEW58QOEcShy6/Image+test+1.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_6_1489459079955_48699"><div><p>Curiously, felony fines had no correlation with the attractiveness of the criminal. The study does not make it clear why this is the case.</p><p><em>Answers to Possible Objections</em></p><ul data-rte-list="default"><li><p>The judges varied in gender and race.</p></li><li><p>There was no correlation between sentencing outcomes and age, gender, and race.</p></li></ul><p><em>Weaknesses</em></p><p>For privacy reasons, the specific crime was not documented.</p><p>The direction of causation is not known. I enter into more depth in the section entitled ‘causation’.</p><p><strong>THE PENNSYLVANIAN STUDY<em> </em></strong><em>[2]</em></p><p>In Pennsylvanian and Philadelphian courts, the researcher’s gathered data on 67 defendants. The defendants were a mix of black, Hispanic, and white and there were 15 real judges in total.</p><p><em>Results &amp; Key Takeaways</em></p><p>On average (mean), criminals of low attractiveness were sentenced to 4.10 years in prison and criminals of high attractiveness were sentenced to 1.87 years in prison. This equals a 119.25% increase.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_6_1489459079955_71526"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489459602430-G7OV3RYZPYB7CFDC7Y1E/ke17ZwdGBToddI8pDm48kB9PP1roaCoWCcB5PDMLLBcUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcJDPzuPFQMkDrmP7vOugYmF8e3iX03QqVmRfNUMP65OVoNHPxvCYY7HSGytwWF1ph/image-asset.png" data-image="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489459602430-G7OV3RYZPYB7CFDC7Y1E/ke17ZwdGBToddI8pDm48kB9PP1roaCoWCcB5PDMLLBcUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcJDPzuPFQMkDrmP7vOugYmF8e3iX03QqVmRfNUMP65OVoNHPxvCYY7HSGytwWF1ph/image-asset.png" data-image-dimensions="1095x642" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="58c75991e6f2e16d0cb71613" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489459602430-G7OV3RYZPYB7CFDC7Y1E/ke17ZwdGBToddI8pDm48kB9PP1roaCoWCcB5PDMLLBcUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcJDPzuPFQMkDrmP7vOugYmF8e3iX03QqVmRfNUMP65OVoNHPxvCYY7HSGytwWF1ph/image-asset.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_6_1489459079955_71724"><div><p><em>Weaknesses</em></p><p>All observers were white.</p><p><strong>THE SECOND PENNSYLVANIAN STUDY</strong> <em>[3]</em></p><p>This study was similar to the previous study. The researchers recorded data from real court cases in Pennsylvania. They detailed the physical attractiveness of 60 defendants and their neatness, cleanliness, and quality of clothing. Then, they recorded the judge’s decisions.</p><p>The criminals were charged with a range of felonies, including ‘murder; manslaughter; rape; kidnapping; armed robbery; aggravated assault; indecent assault; arson; burglary; conspiracy to sell/delver heroin, cocaine, hashish, and other elicit drugs; extortion; fraud; theft; and firearms violation.’</p><p>They were also a mix of white, Hispanic and black.&nbsp;</p><p><em>Results &amp; Key Takeaways</em></p><p>The unattractive defendants were punished higher than the attractive defendants.</p><p><em>Weaknesses</em></p><p>The study did not give specific results. This is a major disappointment.</p><p><strong>CONCLUSIONS</strong></p><p>Unattractive criminals were punished higher than attractive criminals in three studies. The lowest increase was at 119.25% and the highest increase was at 304.88%.</p><h2><strong>REAL JUDGES: VERDICT, GUILTY OR NOT-GUILTY</strong></h2><p>There was no association between the defendant’s physical attractiveness and the judge’s verdict. Attractive and unattractive criminals were found guilty at equal rates. Zebrowitz and McDonald [4]&nbsp;also found that the plaintiff’s attractiveness had little to no effects on a judge’s verdict.</p><p><strong>THE BABY-FACED STUDY </strong><em>[5]</em></p><p>The following study is not directly related to physical attractiveness but it is related to physical appearance.</p><p>Zebrowitz and McDonald measured the effects of defendants with a ‘baby-face’ and the judge’s verdict decisions. This is a strange characteristic to measure, however, the results were significant enough to warrant attention.</p><p>‘Baby-faced adults tend to have larger eyes, thinner, higher eyebrows, a large forehead and a small chin, and a curved rather than an angular face.’<em>[6]</em>&nbsp;A team of participants sat in 421 cases in ‘6 branches of the Commonwealth of Massachusetts small claims courts. 3 judges heard 51% of the cases and the remaining 49% of the cases were presided over by 22 additional judges.’ ‘62% of the plaintiffs and 78% of the defendants were male. 96% of both plaintiffs and defendants were white, and 81% were between the ages of 21 and 50.’</p><p><em>Results &amp; Key Takeaways</em></p><p>The more baby-faced an adult was, the less likely he/she was found to be guilty for ‘intentional actions’ in civil claims. Observe the graph below.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_6_1489459079955_91414"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489459840849-7GP7F5ASMZM4ZYP8VK7K/ke17ZwdGBToddI8pDm48kND1NDuHF9nqrgeclEdLoeR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWYMQ3pkjXJ5SX4aMqPMuK4PimCRlyu3R6yKl-KltrlZA/image-asset.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489459840849-7GP7F5ASMZM4ZYP8VK7K/ke17ZwdGBToddI8pDm48kND1NDuHF9nqrgeclEdLoeR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWYMQ3pkjXJ5SX4aMqPMuK4PimCRlyu3R6yKl-KltrlZA/image-asset.jpeg" data-image-dimensions="2500x2137" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="58c75a7ed482e9a66b47537e" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489459840849-7GP7F5ASMZM4ZYP8VK7K/ke17ZwdGBToddI8pDm48kND1NDuHF9nqrgeclEdLoeR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWYMQ3pkjXJ5SX4aMqPMuK4PimCRlyu3R6yKl-KltrlZA/image-asset.jpeg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_6_1489459079955_91612"><div><p>Interestingly, baby-faced adults had no effects in claims of negligent actions.</p><h2><strong>MOCK JURY: SENTENCING</strong></h2><p>Before I present the following research, I need to address a major limitation. Jurors do not decide upon sentencing, thus, the following results may not have direct application.</p><p><strong>THE META-ANALYSIS STUDY </strong><em>[7]</em></p><p>A meta-analysis examined 25 studies on the effects of physical attractiveness on mock jurors. They found that mock jurors gave higher sentences to unattractive criminals than attractive criminals. This was only for crimes of rape, robbery, and negligent homicide. For swindle, the punishment was equal. The physical attractiveness of the victim also had no effects on the jurors.</p><p><strong>THE BURGLARY STUDY </strong><em>[8]</em></p><p>In this study, the participants were given a burglary scenario along with an image of the criminal. Some received the unattractive criminal and others received the attractive criminal. 10 psychology students rated the attractiveness of the criminals prior to the study to determine attractiveness.</p><p>Then, they were asked to suggest a 1, 5, 10, 15, or 20 years imprisonment.</p><p>‘[The] participants consisted of 40 Euro-American men, 40 Euro-American women, 40 African- American men, and 40 African-American women.’ A strength of this study is the participants ranged in race, gender, and age.</p><p><em>Results &amp; Key Takeaways</em></p><p>The attractive criminal was given an average sentence of 9.7 years, and the unattractive criminal was given 14.7 years. That’s an increase of 51.55%.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_8_1489459079955_11521"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489460099397-DVHCYTA21L18OEBFO2Z5/ke17ZwdGBToddI8pDm48kAd5GqBagiRmFFXdPSaeu6AUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcxXf4uRSJSWTC3Ffy6OHbvGAonPHpqoLXAI0RDTapQcSqK99-n2msqdbGF6fjDgYG/image-asset.png" data-image="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489460099397-DVHCYTA21L18OEBFO2Z5/ke17ZwdGBToddI8pDm48kAd5GqBagiRmFFXdPSaeu6AUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcxXf4uRSJSWTC3Ffy6OHbvGAonPHpqoLXAI0RDTapQcSqK99-n2msqdbGF6fjDgYG/image-asset.png" data-image-dimensions="1098x643" data-image-focal-point="0.5,0.5" alt="" data-load="false" data-image-id="58c75b829de4bb5cb740f698" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5817bb2746c3c4a605334446/1489460099397-DVHCYTA21L18OEBFO2Z5/ke17ZwdGBToddI8pDm48kAd5GqBagiRmFFXdPSaeu6AUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcxXf4uRSJSWTC3Ffy6OHbvGAonPHpqoLXAI0RDTapQcSqK99-n2msqdbGF6fjDgYG/image-asset.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_8_1489459079955_11721"><div><p><em>Weaknesses</em></p><p>The researchers measured more items than simply attractiveness. This means that the 160 participants were not all measured on attractiveness. As they measured 8 different items and only two of them on attractiveness, I infer …</p></div></div></div></div></div></div></article></div></div></main></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.thelawproject.com.au/insights/attractiveness-bias-in-the-legal-system">https://www.thelawproject.com.au/insights/attractiveness-bias-in-the-legal-system</a></em></p>]]>
            </description>
            <link>https://www.thelawproject.com.au/insights/attractiveness-bias-in-the-legal-system</link>
            <guid isPermaLink="false">hacker-news-small-sites-24044409</guid>
            <pubDate>Mon, 03 Aug 2020 22:50:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Go vs. Rust: Writing a CLI Tool]]>
            </title>
            <description>
<![CDATA[
Score 60 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24044043">thread link</a>) | @JeremyMorgan
<br/>
August 3, 2020 | https://cuchi.me/posts/go-vs-rust | <a href="https://web.archive.org/web/*/https://cuchi.me/posts/go-vs-rust">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<p><img src="https://gist.githubusercontent.com/cuchi/59255d61717e2d469263eb86cf083067/raw/6ef1a42f335022adf481fb84cabc32ac47f18679/go-vs-rust.png" alt="Go vs. Rust"></p>
<p>This text is about my adventure writing a small CLI application (twice) using
two languages I had little experience with.</p>
<p>If you are eager to jump right into the code and compare it yourself, check it
out the <a href="https://github.com/cuchi/hashtrack/tree/master/cli-go">Go source</a> and
the <a href="https://github.com/cuchi/hashtrack/tree/master/cli-rust">Rust source</a>.</p>
<hr>
<h2>About the Project</h2>
<p>I have a <em>pet project</em> called Hashtrack, which is a full-stack web application I
wrote for a technical interview. This project is rather small and it is simple
to use:</p>
<ol>
<li>You authenticate - considering you already created your account</li>
<li>You input hashtags you want to track</li>
<li>You wait for the <em>captured</em> tweets to show on your screen</li>
</ol>
<p><a href="https://hashtrack.herokuapp.com/">Check it out here.</a></p>
<p>After my interview, I kept improving this project just for fun, and I noticed
that it could be a perfect place to test my skills by implementing a CLI tool. I
already had the server, so I just needed to pick a language to implement a small
set of features under my project's API.</p>
<h2>Features</h2>
<ul>
<li><code>hashtrack login</code> - Creates a session token and store it in the local
filesystem in a config file.</li>
<li><code>hashtrack logout</code> - Remove the locally stored session token.</li>
<li><code>hashtrack track &lt;hashtag&gt; [...]</code> - Tracks one or more hashtags.</li>
<li><code>hashtrack untrack &lt;hashtag&gt; [...]</code> - Untracks one or more previously tracked
hashtags.</li>
<li><code>hashtrack tracks</code> - Displays the hashtags you are tracking.</li>
<li><code>hashtrack list</code> - Displays the latest 50 captured tweets.</li>
<li><code>hashtrack watch</code> - Stream and display the captured tweets in real-time.</li>
<li><code>hashtrack status</code> - Displays who you are, if logged in.</li>
<li>Should have an <code>--endpoint</code> option to point the CLI to another server.</li>
<li>Should have a <code>--config</code> option to load a custom config file.</li>
<li>This config file could also share the <code>endpoint</code> property.</li>
</ul>
<p>What we have to know beforehand:</p>
<ul>
<li>The CLI should use the project's API, which is GraphQL under HTTP +
WebSockets.</li>
<li>The CLI should use the filesystem to store a config file.</li>
<li>The CLI should parse positional arguments and flags.</li>
</ul>
<h2>How did I end up using Go and Rust?</h2>
<p>There is a large set of languages you can use to write CLI tools.</p>
<p>In this case, I wanted a language I had little or no prior experience with, I
also wanted one that could easily compile to a native executable, which is a
nice perk to have on a CLI tool.</p>
<p>My first obvious choice was Go, maybe because a lot of CLI tools I use are
implemented using it. But I also had little experience with Rust, and I saw it
could also be a good fit for this project.</p>
<p>So... why not both? Since my main objective here is to learn, could be a great
opportunity to implement this project twice and find what are the <em>pros and
cons</em> of each one from my point of view.</p>
<blockquote>
<p>Honorable mentions to <a href="https://crystal-lang.org/">Crystal</a> and
<a href="https://nim-lang.org/">Nim</a>, those were very promising options too. I'm looking
forward to learn about them in another pet project.</p>
</blockquote>
<h2>Local environment</h2>
<p>The first thing I look when using a new toolset is whether it has an easy way to
make it available for my user, without using the distribution package manager to
install it system-wide. We are talking about version managers, they make our
life easier by installing the tools in a user-wide manner instead of
system-wide. <a href="https://github.com/nvm-sh/nvm">NVM</a> for Node.js does it very well.</p>
<p>When using Go, there is the <a href="https://github.com/moovweb/gvm">GVM</a> project which
handles the local install &amp; version management, and it is easy to setup:</p>
<pre><code>gvm install go1.14 -B
gvm use go1.14
</code></pre>
<p>There are also two environment variables we need to know, they are <code>GOROOT</code> and
<code>GOPATH</code> -- You can read more about them
<a href="https://www.jetbrains.com/help/go/configuring-goroot-and-gopath.html">here</a>.</p>
<p>The first <em>problem</em> I found using Go, was when I was figuring out how the module
resolution worked along with the <code>GOPATH</code>, it became quite frustrating to
set up a project structure with a functional local development environment.</p>
<p>In the end, I just used <code>GOPATH=$(pwd)</code> in my project's directory, the main perk
was to have a per-project dependency setup, like a <code>node_modules</code>. It worked
well.</p>
<blockquote>
<p>After finishing my project, I found out that
<a href="https://github.com/GetStream/vg">virtualgo</a> existed and would solve my problems
with <code>GOPATH</code>.</p>
</blockquote>
<p>Rust has an official project called <a href="https://rustup.rs/">rustup</a>, which manages
the Rust installation, also known as <em>toolchain</em>. It can be easily set up with a
one-liner. Also, there is a set of optional components using <code>rustup</code>,
such as the <a href="https://github.com/rust-lang/rls">rls</a> and
<a href="https://github.com/rust-lang/rustfmt">rustfmt</a>.
Many projects require a <em>nightly</em> version of the Rust toolchain, with <code>rustup</code>
there was no problem switching between the versions.</p>
<h3>Editor Support</h3>
<p>For both of the languages, editor tooling was flawless, as a VSCode user, I can
find extensions for both Go and Rust in the marketplace.</p>
<p>When debugging with Rust, I had to install the
<a href="https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb">CodeLLDB</a>
extension after following
<a href="https://www.forrestthewoods.com/blog/how-to-debug-rust-with-visual-studio-code/">this tutorial</a>.</p>
<h2>Package management</h2>
<p>Go doesn't have a package manager or even an official registry. Instead, its
module resolution works in a way you can import them from external URLs.</p>
<p>For dependency management, Rust uses the Cargo, which downloads and compiles
dependencies from <a href="https://crates.io/">crates.io</a>, which is the official
registry for Rust packages. Packages inside the Crates ecosystem can also have
their documentation available in <a href="https://docs.rs/">docs.rs</a></p>
<h2>Libraries</h2>
<p>My first objective was to see how easy could be to implement a simple GraphQL
query/mutation over HTTP.</p>
<p>For the Go language, I found some libraries, like
<a href="https://github.com/machinebox/graphql">machinebox/graphql</a> and
<a href="https://github.com/shurcooL/graphql">shurcooL/graphql</a>, the second one uses
structs for (un) marshaling the data, that is what made me stick to it.</p>
<blockquote>
<p>I used a fork of shurcooL/graphql, because I needed to set the
<code>Authorization</code> header in the client, the changes are in
<a href="https://github.com/shurcooL/graphql/pull/48">this pull request</a>.</p>
</blockquote>
<p>This is the Go example of an GraphQL mutation call:</p>
<pre><code><span>type</span> creationMutation <span>struct</span> {
    CreateSession <span>struct</span> {
        Token graphql.String
    } <span>`graphql:"createSession(email: $email, password: $password)"`</span>
}

<span>type</span> CreationPayload <span>struct</span> {
    Email    <span>string</span>
    Password <span>string</span>
}

<span><span>func</span> <span>Create</span><span>(client *graphql.Client, payload CreationPayload)</span> <span>(<span>string</span>, error)</span></span> {
    <span>var</span> mutation creationMutation
    variables := <span>map</span>[<span>string</span>]<span>interface</span>{}{
        <span>"email"</span>:    graphql.String(payload.Email),
        <span>"password"</span>: graphql.String(payload.Password),
    }
    err := client.Mutate(context.Background(), &amp;mutation, variables)

    <span>return</span> <span>string</span>(mutation.CreateSession.Token), err
}

</code></pre>
<p>In Rust, I had to use two libraries to make GraphQL calls. That is because
<code>graphql_client</code> is protocol-agnostic, it only focuses on code generation for
serializing and deserializing data. So I needed a second library (<code>reqwest</code>) to
take care of the HTTP requests.</p>
<pre><code><span>#[derive(GraphQLQuery)]</span>
<span>#[graphql(
    schema_path = <span>"graphql/schema.graphql"</span>,
    query_path = <span>"graphql/createSession.graphql"</span>
)]</span>
<span><span>struct</span> <span>CreateSession</span></span>;

<span>pub</span> <span><span>struct</span> <span>Session</span></span> {
    <span>pub</span> token: <span>String</span>,
}

<span>pub</span> <span><span>type</span> <span>Creation</span></span> = create_session::Variables;

<span>pub</span> <span>async</span> <span><span>fn</span> <span>create</span></span>(context: &amp;Context, creation: Creation) -&gt; <span>Result</span>&lt;Session, api::Error&gt; {
    <span>let</span> res = api::build_base_request(context)
        .json(&amp;CreateSession::build_query(creation))
        .send()
        .<span>await</span>?
        .json::&lt;Response&lt;create_session::ResponseData&gt;&gt;()
        .<span>await</span>?;
    <span>match</span> res.data {
        <span>Some</span>(data) =&gt; <span>Ok</span>(Session {
            token: data.create_session.token,
        }),
        _ =&gt; <span>Err</span>(api::Error(api::get_error_message(res).to_string())),
    }
}
</code></pre>
<p>Neither of the libraries for Go and Rust had any implementation for GraphQL via
WebSocket protocol.</p>
<p>In fact, <code>graphql_client</code> for Rust supports <em>Subscriptions</em>, but since it is
protocol-agnostic, I had to implement the whole GraphQL WebSocket communication
on my own,
<a href="https://github.com/cuchi/hashtrack/blob/b5a75f4368837cd51c621b6560a03e1835ec4e5b/cli-rust/src/tweet.rs#L90">check it out</a>.</p>
<p>To use WebSockets in the Go version, the library should be modified to support
the protocol. Since I was already using a fork of the library, I didn't feel
like doing it. Instead, I used a poor man's way of "watching" the new tweets,
which was to request the API every 5 seconds to retrieve them,
<a href="https://github.com/cuchi/hashtrack/blob/b5a75f4368837cd51c621b6560a03e1835ec4e5b/cli-go/src/hashtrack/tweets/tweets.go#L65">I'm not proud of it</a>.</p>
<p>Using Go, there is the <code>go</code> keyword to spawn a lightweight thread, also called
<em>goroutine</em>. In contrast, Rust uses operating system threads by calling a
<code>Thread::spawn</code>. Besides that, both implementations use channels to transfer
objects between their threads.</p>
<h2>Error handling</h2>
<p>In Go, errors are treated just like any other value. The common way to handle
errors in Go is to just check if they are present.</p>
<pre><code><span><span>func</span> <span>(config *Config)</span> <span>Save</span><span>()</span> <span>error</span></span> {
	contents, err := json.MarshalIndent(config, <span>""</span>, <span>"    "</span>)
	<span>if</span> err != <span>nil</span> {
		<span>return</span> err
	}

	err = ioutil.WriteFile(config.path, contents, <span>0</span>o644)
	<span>if</span> err != <span>nil</span> {
		<span>return</span> err
	}

	<span>return</span> <span>nil</span>
}
</code></pre>
<p>Rust has the <code>Result&lt;T, E&gt;</code> enum, which can encapsulate an <code>Ok(T)</code> for success,
or an <code>Err(E)</code> for errors. It also has the <code>Option&lt;T&gt;</code> enum, with <code>Some(T)</code> or
<code>None</code>. If you are familiar with Haskell, you may recognize
those as the <code>Either</code> and the <code>Maybe</code> monads.</p>
<p>There is also a syntactic sugar for error propagation (the <code>?</code> operator) that
resolves the value from the <code>Result</code> or <code>Option</code> structure, automatically
returning <code>Err(...)</code> or <code>None</code> when something goes bad.</p>
<pre><code><span>pub</span> <span><span>fn</span> <span>save</span></span>(&amp;<span>mut</span> <span>self</span>) -&gt; io::<span>Result</span>&lt;()&gt; {
    <span>let</span> json = serde_json::to_string(&amp;<span>self</span>.contents)?;
    <span>let</span> <span>mut</span> file = File::create(&amp;<span>self</span>.path)?;
    file.write_all(json.as_bytes())
}
</code></pre>
<p>The code above is the equivalent of</p>
<pre><code><span>pub</span> <span><span>fn</span> <span>save</span></span>(&amp;<span>mut</span> <span>self</span>) -&gt; io::<span>Result</span>&lt;()&gt; {
    <span>let</span> json = <span>match</span> serde_json::to_string(&amp;<span>self</span>.contents) {
        <span>Ok</span>(json) =&gt; json,
        <span>Err</span>(e) =&gt; <span>return</span> <span>Err</span>(e.into())
    };
    <span>let</span> <span>mut</span> file = <span>match</span> File::create(&amp;<span>self</span>.path) {
        <span>Ok</span>(file) =&gt; file,
        <span>Err</span>(e) =&gt; <span>return</span> <span>Err</span>(e.into())
    };
    file.write_all(json.as_bytes())
}
</code></pre>
<p>Rust has:</p>
<ul>
<li>monadic constructs (<code>Option</code> &amp; <code>Result</code>)</li>
<li>the error propagation operator</li>
<li>the <code>From</code> trait, to automatically convert errors on propagation</li>
</ul>
<p>The combination of the three features above makes up the best error handling
solution I saw in a language, being simple, sound, and maintainable at the same
time.</p>
<h2>Compilation time</h2>
<p>Go is built with fast compilation time as a critical requirement, let's see:</p>
<pre><code>&gt; time go get hashtrack 
go get hashtrack  1,39s user 0,41s system 43% cpu 4,122 total

&gt; time go build -o hashtrack hashtrack 
go build -o hashtrack hashtrack  0,80s user 0,12s system 152% cpu 0,603 total

&gt; time go build -o hashtrack hashtrack 
go build -o hashtrack hashtrack  0,19s user 0,07s system 400% cpu 0,065 total

&gt; time go build -o hashtrack hashtrack 
go build -o hashtrack hashtrack  0,94s user 0,13s …</code></pre></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cuchi.me/posts/go-vs-rust">https://cuchi.me/posts/go-vs-rust</a></em></p>]]>
            </description>
            <link>https://cuchi.me/posts/go-vs-rust</link>
            <guid isPermaLink="false">hacker-news-small-sites-24044043</guid>
            <pubDate>Mon, 03 Aug 2020 22:08:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PP-YOLO Surpasses YOLOv4 – State-of-the-art object detection techniques]]>
            </title>
            <description>
<![CDATA[
Score 102 | Comments 37 (<a href="https://news.ycombinator.com/item?id=24043812">thread link</a>) | @rocauc
<br/>
August 3, 2020 | https://blog.roboflow.ai/pp-yolo-beats-yolov4-object-detection/ | <a href="https://web.archive.org/web/*/https://blog.roboflow.ai/pp-yolo-beats-yolov4-object-detection/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <div>
              <div><p>Baidu publishes PP-YOLO and pushes the state of the art in object detection research by building on top of YOLOv3, the PaddlePaddle deep learning framework, and cutting edge computer vision research.</p><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-1.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-1.png 600w, https://blog.roboflow.ai/content/images/2020/08/image-1.png 692w"><figcaption><a href="https://arxiv.org/pdf/2007.12099.pdf">PP-YOLO</a> evaluation shows faster inference (x-axis) with better accuracy (y-axis)</figcaption></figure><p>PP-YOLO evaluation metrics show improved performance over <a href="https://blog.roboflow.ai/a-thorough-breakdown-of-yolov4/">YOLOv4</a>, the incumbent state of the art object detection model. Yet, the Baidu authors write:</p><figure><pre><code>This paper is not intended to introduce a novel object detecotor. 
It is more like a recipe, which tell you how to build a better detector step by step.</code></pre><figcaption>Mysterious introduction in the <a href="https://arxiv.org/pdf/2007.12099.pdf">PP-YOLO paper</a></figcaption></figure><p>Let's unpack that.</p><h2 id="yolo-development-history">YOLO Development History</h2><p>YOLO was originally authored by Joseph Redmon to detect objects. Object detection is a computer vision technique that localizes and tags objects by drawing a bounding box around them and identifying the class label that a given box belongs too. Unlike massive NLP transformers, YOLO is designed to be tiny, enabling realtime inference speeds for deployment on device. &nbsp;</p><p>YOLO-9000 was the second "YOLOv2" object detector published by Joseph Redmon, improving the detector and emphasizing the detectors ability to generalize to any object in the world.</p><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-2.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-2.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-2.png 1000w, https://blog.roboflow.ai/content/images/size/w1600/2020/08/image-2.png 1600w, https://blog.roboflow.ai/content/images/2020/08/image-2.png 1810w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://github.com/PaddlePaddle/PaddleDetection/blob/release/0.4/docs/tutorials/Custom_DataSet.md">PP-YOLO</a> is being trained to identify different fruit flies in this photo.</figcaption></figure><p>YOLOv3 made further improvements to the detection network and began to mainstream the object detection process. We began to publish tutorials on <a href="https://blog.roboflow.ai/releasing-a-new-yolov3-implementation/">how to train YOLOv3 in PyTorch</a>, <a href="https://blog.roboflow.ai/training-a-yolov3-object-detection-model-with-a-custom-dataset/">how to train YOLOv3 in Keras</a>, and <a href="https://blog.roboflow.ai/yolov3-versus-efficientdet-for-state-of-the-art-object-detection/">compared YOLOv3 performance to EfficientDet </a>(another state of the art detector).</p><p>Then Joseph Redmon stepped out of the object detection game due to ethical concerns. </p><p>Naturally, the open source community picked up the baton and continues to move YOLO technology forward. </p><p>YOLOv4 was published recently this spring by Alexey AB in his for of the YOLO Darknet repository. YOLOv4 was primarily an ensemble of other known computer vision technologies, combined and validated through the research process. See here for a <a href="https://blog.roboflow.ai/a-thorough-breakdown-of-yolov4/">deep dive on YOLOv4</a>. The YOLOv4 paper reads similarly to the PP-YOLO paper, as we will see below. We put together some great training tutorials on <a href="https://blog.roboflow.ai/training-yolov4-on-a-custom-dataset/">how to train YOLOv4 in Darknet</a>.</p><p>Then, just a few months ago <a href="https://blog.roboflow.ai/yolov5-is-here/">YOLOv5 was released</a>. YOLOv5 took the Darknet (C based) training environment and converted the network to PyTorch. Improved training techniques pushed performance of the model even further and created a great, easy to use, out of the box object detection model. Ever since, we have been encouraging developers using Roboflow to direct their attention to YOLOv5 for the formation of their custom object detectors via this <a href="https://blog.roboflow.ai/how-to-train-yolov5-on-a-custom-dataset/">YOLOv5 training tutorial</a>.</p><p>Enter PP-YOLO.</p><h2 id="what-does-pp-stand-for">What Does PP Stand For?</h2><p>PP is short for PaddlePaddle, a deep learning framework written by Baidu. </p><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-9.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-9.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-9.png 1000w, https://blog.roboflow.ai/content/images/size/w1600/2020/08/image-9.png 1600w, https://blog.roboflow.ai/content/images/2020/08/image-9.png 1634w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://www.paddlepaddle.org.cn/">PaddlePaddle</a> distributions provided on their website.</figcaption></figure><p>If PaddlePaddle is new to you, then we are in the same boat. Primarily written in Python, PaddlePaddle seems akin to PyTorch and TensorFlow. A deep dive into the PaddlePaddle framework is intriguing, but beyond the scope of this article. </p><h2 id="pp-yolo-contributions">PP-YOLO Contributions</h2><p>The PP-YOLO paper reads much like the YOLOv4 paper in that it is a compilation of techniques that are known to work in computer vision. The novel contribution is to prove that the ensemble of these technologies improves performance, and to provide an ablation study of how much each step helps the model along the way.</p><p>Before we dive into the contributions of PP-YOLO, it will be useful to review the YOLO detector architecture.</p><h3 id="anatomy-of-the-yolo-detector">Anatomy of the YOLO Detector</h3><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image.png 1000w, https://blog.roboflow.ai/content/images/2020/08/image.png 1376w" sizes="(min-width: 720px) 720px"><figcaption>A graphical depiction of the <a href="https://arxiv.org/pdf/2007.12099.pdf">PP-YOLO</a> object detection network</figcaption></figure><p>The YOLO detector is broken into three main pieces.</p><p><strong>YOLO Backbone</strong> - The YOLO backbone is a convolutional neural network that pools image pixels to form features at different granularities. The Backbone is typically pretrained on a classification dataset, typically ImageNet.</p><p><strong>YOLO Neck - </strong>The YOLO neck (FPN is chosen above) combines and mixes the ConvNet layer representations before passing on to the prediction head.</p><p><strong>YOLO Head</strong> - This is the part of the network that makes the bounding box and class prediction. It is guided by the three YOLO loss functions for class, box, and objectness. </p><h2 id="now-let-s-dive-into-the-pp-yolo-contributions-">Now let's dive into the PP YOLO Contributions.</h2><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-10.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-10.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-10.png 1000w, https://blog.roboflow.ai/content/images/2020/08/image-10.png 1136w" sizes="(min-width: 720px) 720px"><figcaption>Marginal mAP accuracy performance increase from each technique in PP-YOLO</figcaption></figure><h3 id="replace-backbone">Replace Backbone</h3><p>The first PP YOLO technique is to replace the YOLOv3 Darknet53 backbone with the Resnet50-vd-dcn ConvNet backbone. Resnet is a more popular backbone, more frameworks are optimized for its execution, and it has fewer parameters than Darknet53. Seeing a mAP improvement by swapping this backbone is a huge win for PP YOLO. </p><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-12.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-12.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-12.png 1000w, https://blog.roboflow.ai/content/images/2020/08/image-12.png 1422w" sizes="(min-width: 720px) 720px"><figcaption>Graphical depiction in <a href="https://arxiv.org/pdf/1603.05027.pdf">ResNet</a></figcaption></figure><h3 id="ema-of-model-parameters">EMA of Model Parameters</h3><p>PP YOLO tracks the Exponential Moving Average of network parameters to maintain a shadow of the models weights for prediction time. This has been shown to improve inference accuracy.</p><h3 id="larger-batch-size">Larger Batch Size</h3><p>PP-YOLO bumps the batch size up from 64 to 192. Of course, this is hard to implement if you have GPU memory constraints.</p><h3 id="dropblock-regularization">DropBlock Regularization</h3><p>PP YOLO implements DropBlock regularization in the FPN neck (in the past, this has usually occurred in the backbone). DropBlock randomly removes a block of the training features at a given step in the network to teach the model to not rely on key features for detection.</p><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-16.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-16.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-16.png 1000w, https://blog.roboflow.ai/content/images/2020/08/image-16.png 1050w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://arxiv.org/pdf/1810.12890.pdf">Drop Block</a> regularization technique - features are hidden in blocks (c) not randomly (b)</figcaption></figure><h3 id="iou-loss">IoU Loss</h3><p>The YOLO loss function does not translate well to the <a href="https://blog.roboflow.ai/what-is-mean-average-precision-object-detection/">mAP metric</a>, which uses the Intersection over Union heavily in its calculation. Therefore, it is useful to edit the training loss function with this end prediction in mind. This edit was also present in YOLOv4.</p><h3 id="iou-aware">IoU Aware</h3><p>The PP-YOLO network adds a prediction branch to predict the model's estimated IOU with a given object. Including this IoU awareness when making the decision to predict an object or not improves performance.</p><h3 id="grid-sensitivity">Grid Sensitivity</h3><p>The old YOLO models do not do a good job of making predictions right around the boundaries of anchor box regions. It is useful to define box coordinates slightly differently to avoid this problem. This technique is also present in YOLOv4.</p><h3 id="matrix-nms">Matrix NMS </h3><p>Non-Maximum Suppression is a technique to remove over proposals of candidate objects for classification. Matrix NMS is a technique to sort through these candidate predictions in parallel, speeding up the calculation. </p><h3 id="coordconv">CoordConv</h3><p>CoordConv was motivated by the problems ConvNets were having with simply mapping (x,y) coordinates to a one-hot pixel space. The CoordConv solution gives the convolution network access to its own input coordinates. CoordConv interventions are marked with yellow diamonds above. More details are available in <a href="https://arxiv.org/pdf/1807.03247.pdf">the CordConv paper</a>.</p><h3 id="spp">SPP</h3><p>Spatial Pyramid Pooling is an extra block after the backbone layer to mix and pool spatial features. Also implemented in YOLOv4 and YOLOv5.</p><h3 id="better-pretrained-backbone">Better Pretrained Backbone</h3><p>The PP YOLO authors distilled down a larger ResNet model to serve as the backbone. A better pretrained model shows to improve downstream transfer learning as well. </p><h2 id="is-pp-yolo-state-of-the-art"><br>Is PP-YOLO State of the Art?</h2><p>PP-YOLO outperforms the results <a href="https://arxiv.org/pdf/2004.10934.pdf">YOLOv4 published</a> on April 23, 2020.</p><p>In fairness, the authors note this may be the wrong question to be asking. The authors' intent appears to not simply "introduce a new novel detector," rather to show the process of carefully tuning an object detector to maximize performance. Quoting the paper's introduction here:</p><blockquote>The focus of this paper is how to stack some effective tricks that hardly affect efficiency to get better performance... This paper is not intended to introduce a novel object detector. It is more like a recipe, which tell you how to build a better detector step by step. We have found some tricks that are effective for the YOLOv3 detector, which can save developers’ time of trial and error. <strong>The final PP-YOLO model improves the mAP on COCO from 43.5% to 45.2% at a speed faster than YOLOv4</strong></blockquote><p><em>(emphasis ours)</em></p><p>The PP-YOLO contributions reference above took the YOLOv3 model from 38.9 to 44.6 mAP on the COCO object detection task and increased inference FPS from 58 to 73. These metrics are shown in the paper to beat the currently published results for YOLOv4 and EfficientDet. </p><p>In benchmarking PP-YOLO against <a href="https://blog.roboflow.ai/yolov5-improvements-and-evaluation/">YOLOv5</a>, it appears <a href="https://blog.roboflow.ai/yolov5-improvements-and-evaluation/">YOLOv5</a> still has the fastest inference time-to-accuracy performance (AP vs FPS) tradeoff on a V100. However, a YOLOv5 paper still remains to be released. Furthermore, it has been shown that training the YOLOv4 architecture on the YOLOv5 Ultralytics repository outperforms YOLOv5 and, transitively, YOLOv4 trained using YOLOv5 contributions would outperform the PP-YOLO results posted here. These results are still to be formally published but can be traced to <a href="https://github.com/ultralytics/yolov5/issues/6">this GitHub discussion</a>.</p><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-14.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-14.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-14.png 1000w, https://blog.roboflow.ai/content/images/2020/08/image-14.png 1354w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://arxiv.org/pdf/2007.12099.pdf">PP-YOLO evaluation</a> on COCO dataset on V100 GPU (note AP_50 column)</figcaption></figure><figure><img src="https://blog.roboflow.ai/content/images/2020/08/image-15.png" alt="" srcset="https://blog.roboflow.ai/content/images/size/w600/2020/08/image-15.png 600w, https://blog.roboflow.ai/content/images/size/w1000/2020/08/image-15.png 1000w, https://blog.roboflow.ai/content/images/2020/08/image-15.png 1304w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://blog.roboflow.ai/yolov5-improvements-and-evaluation/">YOLOv5 evaluation</a> on COCO dataset on V100 GPU (note AP_50 column)</figcaption></figure><p>It is worth noting that many of the techniques (such as architecture search and data augmentation) that were used in YOLOv4 were not used in PP YOLO. This means that there is still room for the state of the art in object detection to grow as more of these techniques are combined and integrated together. </p><p>Needless to say, is an exciting time to be implementing computer vision technologies.</p><h2 id="should-i-switch-from-yolov4-or-yolov5-to-pp-yolo">Should I Switch from YOLOv4 or YOLOv5 to PP-YOLO?</h2><p>The PP-YOLO model shows the promise of state of the art object detection, but the improvements are incremental over other object detectors and it is written in a new framework. At this stage, the best thing to do is to develop your own empirical result by training PP-YOLO on your own dataset. (To be notified when you can easily use PP-YOLO on your dataset, <a href="https://roboflow.us5.list-manage.com/subscribe?u=26126ade12b1dd890dbd7b07e&amp;id=3e926cf19a">subscribe to our newsletter</a>.)</p><p>In the meantime, I recommend checking out the following YOLO tutorials to get your object detector off the ground:</p><ul><li><a href="https://blog.roboflow.ai/training-yolov4-on-a-custom-dataset/">How to Train YOLOv4 in Darknet</a></li><li><a href="https://blog.roboflow.ai/how-to-train-yolov5-on-a-custom-dataset/">How to Train YOLOv5 in PyTorch</a></li></ul><p>As always - happy training! </p></div>
              
            </div>
          </div></div>]]>
            </description>
            <link>https://blog.roboflow.ai/pp-yolo-beats-yolov4-object-detection/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24043812</guid>
            <pubDate>Mon, 03 Aug 2020 21:43:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[RISC-V OS using Rust: Graphics]]>
            </title>
            <description>
<![CDATA[
Score 92 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24041869">thread link</a>) | @azhenley
<br/>
August 3, 2020 | https://blog.stephenmarz.com/2020/07/24/risc-v-os-using-rust-graphics/ | <a href="https://web.archive.org/web/*/https://blog.stephenmarz.com/2020/07/24/risc-v-os-using-rust-graphics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
								
								<div>
									
<p>An operating system is used to make our job easier when using graphics. In our instance, in addition to everything else. In this post, we will be writing a GPU (graphics processing unit) driver using the VirtIO specification. In here, we will allow user applications to have a portion of the screen as RAM–with what is commonly known as a <em>framebuffer</em>.</p>



<hr>



<h2>Contents</h2>



<ol><li><a href="#overview">Overview</a></li><li><a href="#pixels">Pixels and Resolution</a></li><li><a href="#virtio">The GPU VirtIO Device</a></li><li><a href="#init">Initialization</a></li><li><a href="#invalid">Invalidation and Transfer</a></li><li><a href="#response">Device Responses</a></li><li><a href="#user">User Space</a></li><li><a href="#api">Simple Graphics API</a></li><li><a href="#conclusion">Conclusions and Further Reading</a></li></ol>



<hr>



<h2 id="overview">Overview</h2>



<p>We command the virtual GPU (virtio-gpu) by sending certain commands to the host (the device). The guest (the OS driver) has an allocation of RAM that becomes the framebuffer. The driver then tells the device, “hey, here’s the RAM that we’re going to use to store pixel information.”</p>



<p>The RAM is contiguous in our OS, but according to the specification, this isn’t strictly required. We will give the driver a rectangle. Everything that falls within that rectangle will be copied to the host. We don’t want to keep copying the entire buffer over and over again.</p>



<p>We will be using the virtio protocol that we used for the block driver here, so I won’t rehash the general virtio protocol. However, the device-specific structures are a bit different, so we’ll cover that part more in depth.</p>



<hr>



<h2 id="pixels">Pixels and Resolution</h2>



<p>A framebuffer must be large enough to store \(\text{width}\times\text{height}\times\text{pixel size}\) number of bytes. There are \(\text{width}\times\text{height}\) number of pixels. Each pixel has a 1-byte red, green, blue, and alpha channels. So, each pixel is exactly 4 bytes with the configuration we’re going to specify.</p>



<p>The framebuffer for our junior GPU driver is going to support a fixed resolution of \(640\times 480\). If you’re a child of the 90s, you saw this resolution a lot. In fact, my first computer, a Laser Pal 386, had a 16-color monitor with a resolution of 640 pixels wide with 480 pixels tall.</p>



<div><figure><a href="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-13.png"><img src="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-13.png" alt="" width="458" height="281" srcset="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-13.png 611w, https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-13-300x184.png 300w" sizes="(max-width: 458px) 100vw, 458px"></a></figure></div>



<p>There are red, green, and blue pixels so close together that by varying the intensity of these three channels, we can change the color. The closer we get to our monitors, the easier a pixel is to see.</p>



<div><figure><a href="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-14.png"><img src="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-14.png" alt="" width="285" height="288" srcset="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-14.png 380w, https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-14-297x300.png 297w" sizes="(max-width: 285px) 100vw, 285px"></a><figcaption>Pixels on a Viewsonic VX2770SMH-LED monitor.</figcaption></figure></div>



<p>You can see these little squares. If you squint enough, you can see that they aren’t pure white. Instead, you can see bits of red, blue, and green. That’s because each one of these little squares is subdivided into three colors: yep, red, green, and blue! To make white, these pixels are turned up to 11 (get the joke?). To make black, we turn off all three channels of that pixel.</p>



<p>The resolution refers to how many of these squares are on our monitor. This is a 1920×1080 monitor. That means that there are 1920 of these squares going left to right, and there are 1080 of these squares from top to bottom. All in all, we have \(1920\times 1080=2,073,600\) number of pixels. Each one of these pixels is expressed using 4 bytes in the framebuffer, meaning we need \(2,073,600\times 4=8,294,400\) bytes in RAM to store the pixel information.</p>



<p>You can see why I limited our resolution to 640×480, which only requires \(640\times 480\times 4=1,228,800\) bytes–a bit over a megabyte.</p>



<hr>



<h2 id="virtio">The GPU VirtIO Device</h2>



<p>The GPU device requires us to read a more up-to-date VirtIO specification. I’ll be reading from version 1.1, which you can get a copy here: <a href="https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html">https://docs.oasis-open.org/virtio/virtio/v1.1/virtio-v1.1.html</a>. Specifically, chapter 5.7 “GPU Device”. This is an <em>unaccelerated</em> 2D device, meaning that we must use the CPU to actually form the framebuffer, then we transfer our CPU formulated memory location to the host GPU, which is then responsible for drawing it to the screen.</p>



<p>The device uses a request/response system, where we the driver make a command to request something from the host (the GPU).  We add a bit of extra memory into our request so that the host can formulate its response. When the GPU interrupts us, we can take a look at this response memory location to see what the GPU told us. This is much like the <em>status</em> field on the block driver, where the block device tells us the status of our last request.</p>



<p>Each request starts with a <em>Command Header</em>, which in Rust looks as follows:</p>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#[repr(C)]
struct CtrlHeader {
	ctrl_type: CtrlType,
	flags: u32,
	fence_id: u64,
	ctx_id: u32,
	padding: u32
}</pre>



<p>The header is common for all requests and all responses. We can differentiate by the CtrlType enumeration, which is:</p>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#[repr(u32)]
enum CtrlType {
	/* 2d commands */
	CmdGetDisplayInfo = 0x0100,
	CmdResourceCreate2d,
	CmdResourceUref,
	CmdSetScanout,
	CmdResourceFlush,
	CmdTransferToHost2d,
	CmdResourceAttachBacking,
	CmdResourceDetachBacking,
	CmdGetCapsetInfo,
	CmdGetCapset,
	CmdGetEdid,
	/* cursor commands */
	CmdUpdateCursor = 0x0300,
	CmdMoveCursor,
	/* success responses */
	RespOkNoData = 0x1100,
	RespOkDisplayInfo,
	RespOkCapsetInfo,
	RespOkCapset,
	RespOkEdid,
	/* error responses */
	RespErrUnspec = 0x1200,
	RespErrOutOfMemory,
	RespErrInvalidScanoutId,
	RespErrInvalidResourceId,
	RespErrInvalidContextId,
	RespErrInvalidParameter,
}</pre>



<p>I took this directly from the specification, but Rust-ified the names to avoid getting yelled at by the linter.</p>



<h3>Pixel Formats</h3>



<p>Recall that the framebuffer is just a bunch of bytes in memory. We need to put a structure behind the framebuffer so the host (the GPU) knows how to interpret your sequence of bytes. There are several formats, but all-in-all, they just re-arrange the red, green, blue, and alpha channels. All are exactly 4 bytes, which makes the <em>stride</em> the same. The stride is the spacing from one pixel to another–4 bytes.</p>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">#[repr(u32)]
enum Formats {
	B8G8R8A8Unorm = 1,
	B8G8R8X8Unorm = 2,
	A8R8G8B8Unorm = 3,
	X8R8G8B8Unorm = 4,
	R8G8B8A8Unorm = 67,
	X8B8G8R8Unorm = 68,
	A8B8G8R8Unorm = 121,
	R8G8B8X8Unorm = 134,
}</pre>



<p>The type, <em>unorm</em>, is an 8-bit (1-byte) unsigned value from 0 through 255, where 0 represents no intensity and 255 represents full intensity, and a number in between is a linear-interpolation between no and full intensity. Since there are three color (and one alpha), that gives us \(256\times 256\times 256=16,776,216\) different colors or levels of colors.</p>



<p>For this tutorial, I selected <code>R8G8B8A8Unorm = 67</code>, which has red first, green second, blue third, and alpha fourth. This is a common ordering, so I’ll select it to make it easy to follow along.</p>



<p>Our selected format makes the pixel structure look as follows:</p>



<div><figure><a href="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21.png"><img src="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21-1024x614.png" alt="" width="512" height="307" srcset="https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21-1024x614.png 1024w, https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21-300x180.png 300w, https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21-768x461.png 768w, https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21-1536x921.png 1536w, https://blog.stephenmarz.com/wp-content/uploads/2020/05/image-21-2048x1228.png 2048w" sizes="(max-width: 512px) 100vw, 512px"></a></figure></div>



<p>Recall that each individual component R, G, B, and A are each one byte a piece, so each Pixel referred to by (x, y) is 4 bytes. This is why our memory pointer is a Pixel structure instead of a byte.</p>



<hr>



<h2 id="init">Initialization</h2>



<p>Just like all other virtio devices, we set up the virtqueues first and then we work on device-specific initialization. In my code, I just directly copied-and-pasted from the block driver into the gpu driver. The only thing I added to the Device structure was the framebuffer and dimensions of the framebuffer.</p>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">pub struct Device {
	queue:        *mut Queue,
	dev:          *mut u32,
	idx:          u16,
	ack_used_idx: u16,
	framebuffer:  *mut Pixel,
	width:        u32,
	height:       u32,
}</pre>



<p>The specification tells us to do the following in order to initialize the device and get things ready to draw. I Rust-ified some of the content to match our enumerations.</p>



<h4>Create a framebuffer and configure scanout</h4>



<ol><li>Create a host resource using <code>CmdResourceCreate2d</code>.</li><li>Allocate a framebuffer from guest ram, and attach it as backing storage to the resource just created, using <code>CmdResourceAttachBacking</code>.</li><li>Use <code>CmdSetScanout</code> to link the framebuffer to a display scanout.</li></ol>



<h3>A Request Structure</h3>



<p>Recall that our request and response come packaged together. We will put them in separate descriptors, but whenever we get a response back from the device, it is going to be easier if we free just once to free both the request and response. So, in Rust, I created the Request structure to support doing this.</p>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">struct Request&lt;RqT, RpT&gt; {
	request: RqT,
	response: RpT,
}
impl&lt;RqT, RpT&gt; Request&lt;RqT, RpT&gt; {
	pub fn new(request: RqT) -&gt; *mut Self {
		let sz = size_of::&lt;RqT&gt;() + size_of::&lt;RpT&gt;();
		let ptr = kmalloc(sz) as *mut Self;
		unsafe {
			(*ptr).request = request;
		}
		ptr
	}
}</pre>



<h4>Step 1: Create host resource</h4>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">let rq = Request::new(ResourceCreate2d {
	hdr: CtrlHeader {
		ctrl_type: CtrlType::CmdResourceCreate2d,
		flags: 0,
		fence_id: 0,
		ctx_id: 0,
		padding: 0,
	},
	resource_id: 1,
	format: Formats::R8G8B8A8Unorm,
	width: dev.width,
	height: dev.height,
});
let desc_c2d = Descriptor {
	addr: unsafe { &amp;(*rq).request as *const ResourceCreate2d as u64 },
	len: size_of::&lt;ResourceCreate2d&gt;() as u32,
	flags: VIRTIO_DESC_F_NEXT,
	next: (dev.idx + 1) % VIRTIO_RING_SIZE as u16,
};
let desc_c2d_resp = Descriptor {
	addr: unsafe { &amp;(*rq).response as *const CtrlHeader as u64 },
	len: size_of::&lt;CtrlHeader&gt;() as u32,
	flags: VIRTIO_DESC_F_WRITE,
	next: 0,
};
unsafe {
	let head = dev.idx;
	(*dev.queue).desc[dev.idx as usize] = desc_c2d;
	dev.idx = (dev.idx + 1) % VIRTIO_RING_SIZE as u16;
	(*dev.queue).desc[dev.idx as usize] = desc_c2d_resp;
	dev.idx = (dev.idx + 1) % VIRTIO_RING_SIZE as u16;
	(*dev.queue).avail.ring[(*dev.queue).avail.idx as usize % VIRTIO_RING_SIZE] = head;
	(*dev.queue).avail.idx = (*dev.queue).avail.idx.wrapping_add(1);
}</pre>



<p>All we’re really telling the GPU here is our resolution and the format of the framebuffer. When we create this, the host gets to configure itself, such as allocating an identical buffer to make transfers from our OS.</p>



<h4>Step 2: Attach framebuffer backing.</h4>



<pre data-enlighter-language="rust" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">let rq = Request3::new(AttachBacking {
	hdr: CtrlHeader {
		ctrl_type: CtrlType::CmdResourceAttachBacking,
		flags: 0,
		fence_id: 0,
		ctx_id: 0,
		padding: 0,
	},
	resource_id: 1,
	nr_entries: 1,
},
MemEntry {
	addr: dev.framebuffer as u64,
	length: dev.width * dev.height * size_of::&lt;Pixel&gt;() as u32,
	…</pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.stephenmarz.com/2020/07/24/risc-v-os-using-rust-graphics/">https://blog.stephenmarz.com/2020/07/24/risc-v-os-using-rust-graphics/</a></em></p>]]>
            </description>
            <link>https://blog.stephenmarz.com/2020/07/24/risc-v-os-using-rust-graphics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24041869</guid>
            <pubDate>Mon, 03 Aug 2020 18:58:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A gentle intro to assembly with Rust]]>
            </title>
            <description>
<![CDATA[
Score 121 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24041675">thread link</a>) | @lfn3
<br/>
August 3, 2020 | https://lfn3.net/2020/08/03/a-gentle-intro-to-assembly-with-rust/ | <a href="https://web.archive.org/web/*/https://lfn3.net/2020/08/03/a-gentle-intro-to-assembly-with-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


<div>
  
  <p><span>Aug 3 2020</span></p><p>One of the things I’ve wanted to do for a while is really dig into
assembly and get into the weeds of how programs actually run.
A rework of the <code>asm</code> macro has <a href="https://blog.rust-lang.org/inside-rust/2020/06/08/new-inline-asm.html">recently landed</a> in nightly rust
so it seemed like a good time.</p>

<p>And compared to some other ways I’ve tried to approach this there’s a lot less
setup we need to do if we just use the <a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018">rust playground</a> to
do all the heavy lifting.</p>

<p>My process for figuring things out has been pretty simple.
I write a tiny bit of rust code, look at the assembly output
and try to figure out what’s going on (with lots of googling).
I’m going to walk you through what I did, and what I figured
out.</p>

<p>Let’s start with the simplest possible thing I can think of:</p>

<pre><code>fn main() {
    1 + 2;
}
</code></pre>

<p><a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=9500bb2bc3f638a4dd89e81fecafac0e">playground link</a></p>

<p>You can get the assembly output for this by clicking the three dots next to
<code>run</code> and selecting <code>asm</code> from the dropdown. You will probably also want
to change the flavour (often referred to as syntax elsewhere) of assembly to intel (rather than at&amp;t) <sup id="fnref:1"><a href="#fn:1">1</a></sup>
if it isn’t already, by clicking the toggle under the <code>config</code> menu.</p>

<p>The assembly output from this in debug mode is far more massive than you’d expect -
I get 157 lines. And most of it isn’t our program. The code we’ve written should
be fairly easy to find though, as the compiler helpfully labels all of the functions
with their crate and function names. In this case since we’re in the playground,
the create is implicitly <code>playground</code>, so we can find our code by searching with
<code>ctrl-f</code> for <code>playground::main</code>. Doing this gets me to:</p>

<pre><code>playground::main: # @playground::main
# %bb.0:
    ret
                                        # -- End function
</code></pre>

<p>So even though this is a debug build, evidently there’s still some optimization going on,
since there’s no numbers or anything that looks like it’s adding them together.
All that’s happening here is we’re returning (<code>ret</code>) back to the function that called <code>playground::main</code>.
Everything prefixed with <code>#</code> is a comment, and therefore ignored when we run this code.</p>

<p>The only other point of interest is the label <code>playground::main:</code> - anything suffixed with <code>:</code>
is a label we can jump to with various commands, and indeed if we continue searching for <code>playground::main</code>
we can find a rather indirected call to it in <code>main</code>. Hopefully by the end of this we’ll be understand that!</p>

<h3 id="avoiding-optimizations">Avoiding optimizations</h3>

<p>For now, let’s try and evade whatever’s doing the optimization:</p>

<pre><code>fn add() -&gt; usize {
    1 + 2
}

fn main() {
    add();
}
</code></pre>

<p><a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=e06e9c1a6771d850be5e06abc6f70243">playground link</a></p>

<p>Again, searching for <code>playground::main</code> get us to:</p>

<pre><code>playground::add: # @playground::add
# %bb.0:
    mov eax, 3
    ret
                                        # -- End function

playground::main: # @playground::main
# %bb.0:
    push    rax
    call    playground::add
# %bb.1:
    pop rax
    ret
                                        # -- End function
</code></pre>

<p>So we’ve got a bit more progress here. Still some optimization going on, since we don’t see 1 or 2 in the code,
just 3. We can see that being moved (<code>mov</code>) into the <code>eax</code> register in <code>playground::add</code>.
This must be how we’re returning the value back up to <code>main</code>.</p>

<p>And indeed, inside <code>main</code> we can see <code>push rax</code> - saving the value in the register <code>rax</code> to the stack, then a
call to our <code>add</code> function, then we <code>pop rax</code> off the stack. The <code>push call pop</code> sequence is to preserve
whatever values are in the registers used in <code>add</code>. It also just throws away the value we saved in <code>eax</code> in <code>add</code>,
because <code>eax</code> and <code>rax</code> are the same register. The table <a href="https://en.wikibooks.org/wiki/X86_Assembly/X86_Architecture#General-Purpose_Registers_(GPR)_-_16-bit_naming_conventions">here</a>
shows how ‘skinnier’ registers overlap with their ‘wider’ counterparts.</p>

<h3 id="avoiding-optimizations-take-2">Avoiding optimizations, take 2</h3>

<p>So how can we make this actually do some math? Let’s try again:</p>

<pre><code>fn add(i: usize) -&gt; usize {
    1 + i
}

fn main() {
    add(2);
}
</code></pre>

<p><a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=0d821a33f2375ecaf3671c825a415c83">playground link</a></p>

<p>So we’ve got a lot more going on this time:</p>

<pre><code>playground::add: # @playground::add
# %bb.0:
    sub rsp, 24
    mov qword ptr [rsp + 16], rdi
    add rdi, 1
    setb al
    test al, 1
    mov qword ptr [rsp + 8], rdi # 8-byte Spill
    jne .LBB8_2
# %bb.1:
    mov rax, qword ptr [rsp + 8] # 8-byte Reload
    add rsp, 24
    ret

.LBB8_2:
    lea rdi, [rip + str.0]
    lea rdx, [rip + .L__unnamed_2]
    mov rax, qword ptr [rip + core::panicking::panic@GOTPCREL]
    mov esi, 28
    call rax
    ud2
                                        # -- End function

playground::main: # @playground::main
# %bb.0:
    push rax
    mov edi, 2
    call playground::add
# %bb.1:
    pop rax
    ret
                                        # -- End function
</code></pre>

<p>The thing we were actually trying to produce is finally in there!
We can see <code>add rdi, 1</code> in the output, surrounded by a pile of other
stuff. So what is all this other code?</p>

<p>Let’s start from the top of the call stack in <code>main</code>.
First we can see <code>2</code> is stored in the <code>edi</code> register
before we call <code>playground::add</code>, so we know our argument must be in
the <code>edi</code> register. Again, we can see the <code>push</code>, <code>pop</code> on <code>rax</code>, so that
must be the return value.</p>

<h3 id="looking-inside-the-function">Looking inside the function</h3>

<p>Now, looking into <code>playground::add</code> we first see <code>sub rsp, 24</code>. <code>rsp</code> is
the register that holds the stack pointer, so this is growing the stack
(since the stack grows downwards in x86<sup id="fnref:2"><a href="#fn:2">2</a></sup>). Further down we can see
we shrink the stack by the corresponding amount with <code>add rsp, 24</code>.</p>

<p>Then we have <code>mov qword ptr [rsp + 16], rdi</code>. This is copying the
value from <code>rdi</code> onto the stack at <code>rsp + 16</code> - the top of the region we just grew the stack by.
The <code>qword ptr</code> (quadword (i.e. 64bit) pointer) bit is a hint to disambiguate the argument.
Why is that pushed that onto the stack? I <em>think</em> this is just to make it easier to debug,
since we don’t ever access that value again.</p>

<p>In any case, we then proceed on to actually adding 1 to <code>rdi</code>.
The value is stored back in <code>rdi</code>, and importantly for what comes next,
we may set some of the <a href="https://en.wikibooks.org/wiki/X86_Assembly/X86_Architecture#EFLAGS_Register">flags</a>.</p>

<p>Then it gets complicated again - we’ve got <code>setb al</code>. All of the <code>set*</code>
<a href="https://github.com/HJLebbink/asm-dude/wiki/SETcc">instructions</a>
deal with the flag register. The flag register is possibly the most magical
of registers, since it’s manipulated by a bunch of instructions as a side effect.</p>

<p>The last instruction we ran was <code>add</code>, which sets 6 of the the flags:
<a href="https://en.wikipedia.org/wiki/Carry_flag">carry</a>, <a href="https://en.wikipedia.org/wiki/Parity_flag">parity</a>,
<a href="https://en.wikipedia.org/wiki/Adjust_flag">adjust (aka auxiliary carry)</a>, <a href="https://en.wikipedia.org/wiki/Zero_flag">zero</a>,
<a href="https://en.wikipedia.org/wiki/Sign_flag">sign</a> and <a href="https://en.wikipedia.org/wiki/Overflow_flag">overflow</a></p>

<p>In this case we’re checking if the carry bit is set, and then setting the <code>al</code>
register to 1 if that’s the case. What is this actually doing though?
The carry bit gets set to 1 if there is a <code>carry</code> from the two numbers we add,
meaning the resulting number is too big to be stored in the register.
What should we do in that case? Let’s read on to find out.</p>

<p>Then in the next line (<code>test al, 1</code>) we’re checking if the value in <code>al</code> is equal to one.
(<code>test</code> does a a bitwise and operation on the two arguments - like <code>&amp;</code> in rust.)
This sets some more flags, notably the <code>zero</code> flag, which is then read by the following <code>jne</code> instruction.</p>

<p><code>jne</code> stands for jump if not equal (and again there’s a series of
<a href="https://en.wikibooks.org/wiki/X86_Assembly/Control_Flow#Jump_Instructions">other</a>
<code>j*</code> instructions). Since it uses flags, it just takes a single argument: where to jump to.</p>

<p>Looking at where that jumps to gives us a big hint about the intent of the
logic above: <code>core::panicking::panic@GOTPCREL</code> really gives it away.
Basically all of this chunk of assembly from <code>setb</code> to <code>jne</code> is checking if we’ve overflowed
the register and panicking if we have.</p>

<p>The one bit we didn’t discuss is <code>mov qword ptr [rsp + 8], rdi # 8-byte Spill</code>.
As the comment implies this is “spilling” the value from the <code>rdi</code> register
onto the stack, since the code we’re possibly about to jump to might
overwrite that register - immediately after the <code>jne</code> we load the value back off
the stack.</p>

<p>Finally we shuffle the stack pointer back to it’s starting point, and <code>ret</code>
back to the caller. <code>ret</code> uses the last value on the stack (which is pushed by <code>call</code>)
to figure out where to jump back to, so moving the stack pointer back is <em>very</em> important.</p>

<p>So maybe at this point we’ve seen enough to take a stab at replacing the guts of the <code>add</code>
function with the <code>asm!</code> macro. Since we’re interested in performance,
we’ll ignore those pesky overflow checks, and just assume that we’re within the bounds of <code>u64</code>.</p>

<p>The biggest new thing we’ll have to deal with here is specifying the <code>in</code> and <code>out</code> registers.
The <a href="https://github.com/Amanieu/rfcs/blob/inline-asm/text/0000-inline-asm.md#guide-level-explanation">rfc</a>
has a very approachable explaination of these, so I’d recommend reading that.
There’s a skeleton you can start with <a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=d511cf5e95ba5cdfbcffaebaf5f72300">here</a>,
if you want to have a go yourself.</p>

<p>The version I’ve cooked up looks like <a href="https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=669b4155a1d818cc5c73b117b9454d48">this</a>.
This is probably the “fanciest” possible version of this, since we’re using as many features of the asm macro as possible:</p>

<ul>
<li>we’re letting the rust compiler pick the register we use, and then writing it in using the
<a href="https://github.com/Amanieu/rfcs/blob/inline-asm/text/0000-inline-asm.md#inputs-and-outputs"><code>format</code> string behaviour</a> of the <code>asm</code> macro.</li>
<li>we’re also using <a href="https://github.com/Amanieu/rfcs/blob/inline-asm/text/0000-inline-asm.md#late-output-operands"><code>inlateout</code></a> to
hint that we can just use a single register.</li>
</ul>

<p>This seems like a reasonable point at which to break. We’ve covered a reasonable chunk of the instruction set in x64 assembly,
and seen examples of most of the classes of instructions. There’s tons more we can explore, like:</p>

<ul>
<li>How do loops work?</li>
<li>What happens when we use values that don’t just fit in registers?</li>
<li>How do we make a syscall?</li>
</ul>

<p>Hopefully the resources I’ve linked to from here are sufficent for you to continue digging in if you want,
and maybe I’ll manage to follow this up.</p>

</div>

      </div></div>]]>
            </description>
            <link>https://lfn3.net/2020/08/03/a-gentle-intro-to-assembly-with-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24041675</guid>
            <pubDate>Mon, 03 Aug 2020 18:44:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Art of Not Thinking]]>
            </title>
            <description>
<![CDATA[
Score 452 | Comments 146 (<a href="https://news.ycombinator.com/item?id=24039887">thread link</a>) | @tmatthe
<br/>
August 3, 2020 | http://tiffanymatthe.com/not-thinking | <a href="https://web.archive.org/web/*/http://tiffanymatthe.com/not-thinking">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="skip-nav"><p><time>03.08.2020</time> — <a href="http://tiffanymatthe.com/tags/productivity">Productivity</a> — <span>5<!-- --> min read</span></p><section><img src="http://tiffanymatthe.com/static/d758a9afd33cefb9ce3dc7bb83cc213d/a6c62/not-thinking.jpg"><p><em>After years of feeling guilty about not wanting to do everything, I realized I don't need motivation to get things done. Below, I describe how I use the concept of not thinking instead.</em></p><hr><p>It took me five years to get in the habit of exercising. I just didn't want to do it. I followed Youtube workouts, hopeful that the energetic trainer on the screen would help me get fitter. I swam laps in my pool. I followed my brother on 3K runs. And afterwards, I felt great! On top of the world. And then the next day came, and I remembered I had to do it all over again. I had to be sweaty, push through the pain, and breathe like I had an asthma attack.</p><p>So every morning, I woke up and inevitably started dreading my exercise. It would slink around in my thoughts, casting a dark mood until I got it done. At one point, I would dread exercising enough to stop, and a wave of relief would wash over me. This feeling of calm usually lasted a few months, and then my disappointment in my poor levels of fitness would take over. And the cycle would restart.</p><p><strong>Everyone has things they don't want to do.</strong> It's not limited to exercising. It can be anything from studying everyday for the entire school year to vacuuming the floor. Unless you can avoid that activity with no guilt or regrets, you usually have to do it. You know it will help in the long run, to study to prepare for finals and to have clean floors, but even with that in mind, it can still be incredibly hard to do those activities.</p><p>I realized that the hardest part of doing things I don't want to do is usually not the activity itself, but getting started. Once I get started, I get into a flow and rationalize that since I'm already doing it, I might as well finish.</p><h3>How much motivation do we need?</h3><p>I like to describe the amount of energy I need for a task I don't want to do as an exothermic reaction. In this reaction, the reactants (me) need a minimum activation energy (motivation) for the reaction (task) to occur. After the reaction is complete, the products then settle down into a lower energy state (since no more energy is needed to do the task or worry about it).</p><p><span>
      <span></span>
  <img alt="Motivation Energy Reaction" title="Motivation Energy Reaction" src="http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/18e3b/exothermic.jpg" srcset="http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/46946/exothermic.jpg 240w,http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/55489/exothermic.jpg 480w,http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/18e3b/exothermic.jpg 960w,http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/60e21/exothermic.jpg 1440w,http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/69b48/exothermic.jpg 1920w,http://tiffanymatthe.com/static/45c003a7d8906347afa290a5d3bc064d/e1761/exothermic.jpg 3273w" sizes="(max-width: 960px) 100vw, 960px" loading="lazy">
    </span></p><p>So how can we get this minimum activation energy? Well, if we don't want to do the activity, it is nearly impossible to gain enough motivation to do it. The good news is that we can avoid the need for such a high activation energy.</p><p>How is this possible? A simple answer: <strong>don't try to find motivation</strong>.</p><p>When you look for motivation, you usually start by reminding yourself about the advantages of getting the task done. But your brain is a stubborn toddler. If you strongly drag it towards one direction, it will fiercely pull you to the other side. The brain thinks there's a choice, and thus a possibility to argue. It will start pointing out all the disadvantages and instant gratification alternatives.</p><p>Since humans instinctively reach for easier things, now you have not only dredged up all the negative points about your task, but also discovered easier alternatives that require an additional amount of energy to resist. In short, you have increased the minimum activation energy required to start the task.</p><p>You will also remember this awful internal debate, and associate these negative feelings with the task itself. Naturally, this does not bode well in the long run.</p><p>On the other hand, if you don't think about the task, you can avoid the entire process of arguing with yourself and making decisions that you will feel guilty about. Instead, just do it. <strong>Become a mindless robot</strong> and don't think twice<sup id="fnref-1"><a href="#fn-1">1</a></sup>.</p><p>This is, of course, easy to say and a bit more difficult to do. It's hard to think about not thinking, because you'll inadvertently wonder what it is you were trying to not think about, and bam, you've failed. Not thinking is a process, and just like any other skill you learn, it improves with time and practice. Here are a few tips.</p><h3>Make the decision in advance</h3><p>If you are temporally removed from the thing you don't want to do, it's easier to make a rational decision. By making the decision beforehand, you remove the effort needed to choose before doing your task. This reduces friction and removes one factor that could have led you to think about your task when you start it.</p><p>There are a few ways of making decisions in advance. There's the two-minute rule, where you decide that for anything that takes less than two minutes, you do it. No thinking, no arguing, just swift action. For example, you see a pile of clothes on your bed. It takes less than two minutes to organize then in your drawer, so you do it. Here, you just avoided the trap of thinking about your clothes, feeling unmotivated to put them in order, and giving yourself the terrible alternative of doing it later.</p><p>Another method is planning out your days in advance. This does not always work, but it's a good idea to try it out. The night before, you plan out all of your activities to the minute. And, of course, as you're temporally distanced from these activities, you make rational decisions. Then when the morning comes, you can mindlessly follow the schedule you have made for yourself.</p><h3>Do a small part first</h3><p>Quickly pick a random small part of the activity you were dreading. And commit to only doing that one part. This helps you avoid overthinking by giving your brain a smaller task to easily execute<sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p><p>For example, if you need to complete a scholarship application and hate writing about yourself, tell yourself to just write bullet points of topics you might include in the application. Most of the time, after you have invested those first five minutes into the activity, you enter a flow and continue working.</p><p>After implementing these strategies, where I tell myself that I have to exercise every other day for a mere 5 minutes, I now consistently exercise for at least 15 minutes without overthinking it.</p><p>So next time you find yourself not wanting to do something, make yourself a clear rule of when to do it and do the easiest part first. That way, you can avoid making too many decisions and associating the internal turmoil that stems from that process to the activity itself.</p><p>Note, not thinking works wonderfully if your sole purpose is doing an activity you don't want to do. However, unless you don't have any goals to pursue, this is not the best way to go about everything in life. Make sure to take the time to reflect on the overall purpose of the activity and if it brings you closer to where you want to be. If the answer is yes, then feel free to become a mindless robot for any activities that have passed the reflection stage.</p><p>At the small risk of being sued by Nike, just do it.</p></section></div></div>]]>
            </description>
            <link>http://tiffanymatthe.com/not-thinking</link>
            <guid isPermaLink="false">hacker-news-small-sites-24039887</guid>
            <pubDate>Mon, 03 Aug 2020 16:34:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Email got us $5k in AWS Credits]]>
            </title>
            <description>
<![CDATA[
Score 97 | Comments 58 (<a href="https://news.ycombinator.com/item?id=24037963">thread link</a>) | @CoreSet
<br/>
August 3, 2020 | https://formcake.com/blog/how-an-email-got-us-five-thousand-aws-credits | <a href="https://web.archive.org/web/*/https://formcake.com/blog/how-an-email-got-us-five-thousand-aws-credits">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>We recently went through the <a href="https://stripe.com/atlas">Stripe Atlas</a> program and <a href="https://formcake.com/blog/our-experience-with-stripe-atlas">loved it</a>.</p>
<p>But the one sour note was that we only got $1,000 AWS credits when <a href="https://formcake.com/blog/stripe-atlas-includes-five-thousand-credits-this-is-what-we-got">the copy seemed to promise $5,000</a>. Our <a href="https://formcake.com/blog/why-we-chose-a-marketing-and-app-monorepo">entire application infrastructure is on AWS</a> so those credits are pretty much straight-up cash to us. We don't have much of a digital footprint either, so $5,000 goes a long way.</p>
<p>That's why this weekend was such a surprise.</p>
<h3 id="the-email">The Email</h3>
<p>Here's how the timeline went.</p>
<ol>
<li><p>First <a href="https://formcake.com/blog/stripe-atlas-includes-five-thousand-credits-this-is-what-we-got">we posted an article</a> politely noticing that we hadn't been able to secure the full $5,000 and kinda wondering out loud why.</p>
</li>
<li><p><a href="https://formcake.com/blog/the-founders-guide-to-stripe-atlas">Stripe contacted us</a> and explained that the credit amount was more of a lifetime cap and that it was ultimately up to AWS what we got.</p>
</li>
<li><p>We send <em>one more</em> email just 'cause, even though it seems clear AWS only offers $1,000 for bootstrapped startups. The email is simple and amounts to: "Is there any way we could get the full five thousand in credits?"</p>
</li>
</ol>
<p>Stripe responds to this chain of events with yet another email, this one even kinder and more politely worded.</p>
<section>
Hey David,

<p>Thank you for sharing the details here. It sounds like we should be able to get this sorted out so that you receive the $5K in AWS credits for Stripe Atlas users.  </p>
<p>I believe it may be the case that you applied for AWS credits with a different link than the one shared for Stripe Atlas users to activate. We do not typically see users needing to answer the question regarding funding sources. I did check in with AWS on this and it sounds like they're currently processing your most recent application that you submitted through the Stripe Atlas link. They've let me know that they will email you directly with next steps, which may take up to 4 weeks. </p>
<p>Hopefully this will all soon be sorted! If you do have any questions on this or anything else, please feel free to reach out! I'll check back in a couple weeks on the AWS front, or feel free to share any updates as well!</p>
<p>Warm regards,
Taylor</p>
</section> 

<p>It looks like the funded/bootstrapped question indicated the process had gotten miffed somewhere and <em>Stripe reached out to AWS on our behalf</em> to make sure things got cleared up.</p>
<p>This weekend we saw $5,000 in credits enter our AWS account. Keep in mind that's actually <strong>in addition</strong> to the $1,000 we've already received, bringing out total up to $6,000 in AWS credits through the Stripe Atlas program.</p>
<p>For us this is a big injection. It basically funds our full application operations and lets us play around with putting money elsewhere - or just continuing to build features and mature without any kind of pressure.</p>
<h3 id="conclusion">Conclusion</h3>
<p>A couple of takeaways from the entire affair.</p>
<h4 id="1-send-that-one-last-email">1. Send that one last email</h4>
<p>Even if it seems like a bit much, or things are settled, just be sure you ask and give the other person a chance to help you in ways you can't predict. A hail mary "Is there anything I'm missing?" can sometimes land.</p>
<h4 id="2-stripe-is-truly-wonderful">2. Stripe is truly wonderful</h4>
<p>Stripe monitored the developer community enough to see our initial posts, proactively reached out to us, and then worked with AWS to ultimately get us a greater-than-even-promised payout. When people say Stripe is a company for developers, they often mean its great API or clear documentation, but this is I think one of the greatest testaments to their dev-first culture.</p>
</div></div>]]>
            </description>
            <link>https://formcake.com/blog/how-an-email-got-us-five-thousand-aws-credits</link>
            <guid isPermaLink="false">hacker-news-small-sites-24037963</guid>
            <pubDate>Mon, 03 Aug 2020 14:10:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dan Ariely and Irrational Comparison]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 19 (<a href="https://news.ycombinator.com/item?id=24036403">thread link</a>) | @brendancahill
<br/>
August 3, 2020 | https://brendancahill.io/brensblog/danariely | <a href="https://web.archive.org/web/*/https://brendancahill.io/brensblog/danariely">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-124866fbd20ff6e682de"><div><h4>Who is Dan Ariely?</h4><p>Dan Ariely is a Professor of Psychology and Behavioral Economics at Duke University whose writing merges psychology with economics. His book <a href="https://amzn.to/33pw3lh"><em>Predictably Irrational</em></a> explores why most of the smart decisions we think we make are actually quite irrational. Modern economics is based on the theory that the average person makes logical decisions. Ariely happily points out how wrong that notion is. </p><h4>Why Now?</h4><p>Comparison has always fascinated me. I grew up in Airmont, NY two minutes over the state line with Upper Saddle River, NJ. Since I didn’t like paying .50 more cents for gas, nor pumping it, I always got gas in NJ. Upper Saddle River is one of the wealthiest towns not only in America, but on the planet. The second you crossed the NY/NJ line it was like you were transported into an alternate reality where the worst car anyone drove was a BMW, every home a mansion and even the air smelled better (just kidding but I did wonder, if any town had the money to endlessly pump fabreeze into the air, they did). </p><p>When I crossed the NY/NJ state line back to my neighborhood of 3 bedroom raised ranches I felt poor by comparison. Then as a US Peace Corps Volunteer living in Ukraine in an ex-Soviet country, ridden with corruption, and the average salary being less than $100/mo I suddenly felt rich. Ukrainians were shocked that we owned two cars. That we had roads that worked. And that everyone was rich <em>to them. </em></p><h4>Why Comparison?</h4></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1596450590223_8104"><div><p>Ariely uses this illustration to make his point about comparison. Which orange circle <em>appears</em> bigger? You’d likely say the one on the right, at least at first. But, both circles are the same size. The only thing that’s changed is what you’re comparing them to. Comparison depends on context. </p><blockquote><p>“…we are always looking at the things around us in relation to others…We always compare jobs with jobs, vacations with vacations, lovers with lovers and wines with wines.”</p></blockquote><p>Comparison is a survival skill that’s served us well our first 50 million years as a species: You need to know your status in the tribe compared to others’ to ensure your own safety. You need to compare how deadly a particular tiger looks compared to other live tigers you’ve seen in wild to assess your threat level. You need to compare how someone looks to other angry people you’ve seen to avoid a bar fight. You need to compare a date to other people you’ve dated in the past to evaluate how they might be as a future spouse for you. You need to know how to compare how fast your car is going relative to all the other cars on the street to stay within the speed limit. </p><p><strong>Comparison does hijack our rationality, however. </strong></p><h4>The Economist</h4><p>Ariely was trying to figure out what decision people made when given these three options for a subscription to <em>The Economist</em>: </p><ul data-rte-list="default"><li><p>59/yr online only</p></li><li><p>125/yr print only</p></li><li><p>125/yr print and online together</p></li></ul><p>Out of 100 people here is how many chose which options:</p><ul data-rte-list="default"><li><p>59/yr online only (16)</p></li><li><p>125/yr print only (0)</p></li><li><p>125/yr print and online together (84)</p></li></ul><p>But, when Ariely removed the print only option, here is what people chose:</p><ul data-rte-list="default"><li><p>59/yr online only (68)</p></li><li><p>125/yr print and online together (32)</p></li></ul><p><em>As a business, you want to nudge as many people to the high-end of your sales as possible. So why were people opting for the online only? </em></p><p>It turns out it is easier to compare things that are alike than are not alike. By presenting people with not one but <em>two</em> print options, you’ve now made it easier for them to compare two options that you, as a business, want them to focus on. The mind has now written-off the online only option because that is the hardest option to compare to the other two. </p><p>Inside the mind of your potential customer is now a simple question: What’s better? Print only or print AND an online subscription? Everyone in the first experiment opted for the “print + online” because isn’t print plus a little something extra better? </p><p><strong>We Can’t Make Decisions In A Vacuum</strong></p><p>The second case study Ariely looks at is <a href="https://journals.sagepub.com/doi/pdf/10.2307/41166755">Williams-Sonoma’s inability to sell a $275.00 bread baker</a>. Their marketing team finally figured out that if they created a second, bigger and even more expensive bread baker placed <em>next to</em> the $275.00 dollar one, their sales would take off. And they did. </p><blockquote><p>“…people didn’t have to make their decision in a vacuum. They could say: “Well, I don’t know much about bread makers, but I do know that if I were to buy one, I’d rather have the smaller one for less money.”</p></blockquote><p>Ariely then goes on to joke that if you want to have better luck socially finding someone to date you, you should find a decoy friend of similar physical characteristics but who is slightly less attractive than you. Although the morality of this is questionable. </p><p>If you are a business, the takeaway is simple: to make more profit, create a favorable context for the product you want your customer to buy. Create an ultra-premium ridiculously high-priced product that when placed next to the product you really want to sell, makes its price not seem so bad. </p><p>Tesla knows not everyone is paying $100K + for their Cyber Truck, but they do know that by having their ridiculous premium products, it makes the $60K price tag for their “lower models” not seem so crazy. </p><p><strong>Place vs. Person</strong></p><p>I have some of the best friendships I’ve had from my former football teammates. We go through experiences, games, high and lows like few others friend groups might. But would we all have been friends were it not for football? How about military veterans who are bonded together in combat - were it not for that experience, would they have chosen to be friends? </p><p>Ariely uses the example of meeting a fellow American in a foreign place and finding an uncanny connection in an airport. He met one fellow American overseas:</p><blockquote><p>…as cultural outsiders we were each other’s best alternative for companionships. But once we returned home to our beloved American families and friends, thebasis for comparison switched back to “normal” mode. </p></blockquote><p>Would that foreign exchange student you dates in high school have been as “cool” or “exotic” had they been from Germantown, PA instead of Germany? Was that experience teaching overseas in Peace Corps really <em>that</em> cool or am I just assigning extra romanticism to it because I was an “outsider” in a foreign land?</p><h4>Takeaways</h4><p>We’re very smart, but we’re also very irrational as people. We’d like to think logic and rationality governs most of our lives but Ariely has a knack for showing us just how reliant we are upon split second and irrational comparisons to the available yet limited information we have around us. </p><p>Morally speaking, as a business owner or person you have a duty to make decisions now with this comparison fallability in mind. While yes, you can structure your products in a way to extract <em>more</em> profit than normal from your customers, you need to do this only when you truly feel like that high-end product you’re directing them to is actually giving them more value. </p><p>Personally, don’t find the most repulsive personality to take out on a double date hoping that it makes you look good. This might even back fire since your double date might end up <em>comparing you</em> to the company you hang out with and irrationally judge you just as repulsive. </p><p>Comparison is a force of nature that can’t always be controlled, but it can be guided. Ariely shows us how. </p><p>Bren</p><p>P.S. Here is a fun video by Dan at a Ted Talk</p></div></div></div>]]>
            </description>
            <link>https://brendancahill.io/brensblog/danariely</link>
            <guid isPermaLink="false">hacker-news-small-sites-24036403</guid>
            <pubDate>Mon, 03 Aug 2020 11:29:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PostgreSQL beginner guide]]>
            </title>
            <description>
<![CDATA[
Score 232 | Comments 64 (<a href="https://news.ycombinator.com/item?id=24036132">thread link</a>) | @lukasbar
<br/>
August 3, 2020 | https://knowledgepill.it/posts/postgresql-basics-guide/ | <a href="https://web.archive.org/web/*/https://knowledgepill.it/posts/postgresql-basics-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <h2 id="configure-remote-access---listen-address">Configure remote access - listen address</h2>
<p>By default after installation and creating database cluster PostgreSQL will listen only on localhost. No remote access will be allowed.</p>
<hr>
<p><a href="https://knowledgepill.it/posts/postgresql_installation/">PostgreSQL installation on Linux - with database creation</a></p>
<hr>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ netstat -lptnu | grep post
<span>(</span>Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.<span>)</span>
tcp        <span>0</span>      <span>0</span> 127.0.0.1:5432          0.0.0.0:*               LISTEN      1977/postmaster     
tcp6       <span>0</span>      <span>0</span> ::1:5432                :::*                    LISTEN      1977/postmaster     
</code></pre></div><p>To change listen address we have to configure parameter in <code>postgresql.conf</code></p>
<p>Check <code>PGDATA</code> - after <code>-D</code> parameter:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ ps aux | grep postgres
postgres  <span>1977</span>  0.0  2.5 <span>286388</span> <span>14864</span> ?        Ss   Jun28   0:02 /usr/pgsql-12/bin/postmaster -D /postgresql/data
postgres  <span>1979</span>  0.0  0.2 <span>140768</span>  <span>1360</span> ?        Ss   Jun28   0:00 postgres: logger   
postgres  <span>1981</span>  0.0  0.5 <span>286504</span>  <span>3028</span> ?        Ss   Jun28   0:00 postgres: checkpointer   
postgres  <span>1982</span>  0.0  0.2 <span>286388</span>  <span>1696</span> ?        Ss   Jun28   0:03 postgres: background writer   
postgres  <span>1983</span>  0.0  0.9 <span>286388</span>  <span>5676</span> ?        Ss   Jun28   0:03 postgres: walwriter   
postgres  <span>1984</span>  0.0  0.4 <span>286924</span>  <span>2688</span> ?        Ss   Jun28   0:02 postgres: autovacuum launcher  
</code></pre></div><p>Locate the file:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ cd /postgresql/data/
<span>[</span>postgres@postgres-lab data<span>]</span>$ ls -lah postgresql.conf
-rw-------. <span>1</span> postgres postgres 26K Jun <span>28</span> 21:44 postgresql.conf
</code></pre></div><p>Change in <code>postgresql.conf</code> parameter <code>listen_addresses</code> to your server IP or <code>*</code> to listen on all IP’s available on server:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab data<span>]</span>$ vi postgresql.conf

<span>##------------------------------------------------------------------------------</span>
<span>## CONNECTIONS AND AUTHENTICATION</span>
<span>##------------------------------------------------------------------------------</span>

<span>## - Connection Settings -</span>

listen_addresses <span>=</span> <span>'*'</span>          <span>## what IP address(es) to listen on;</span>
                                        <span>## comma-separated list of addresses;</span>
                                        <span>## defaults to 'localhost'; use '*' for all</span>
</code></pre></div><p>Restart PostgreSQL to apply changes - you can do that with <code>systemctl</code> from <code>root</code> os user  service or with <code>pg_ctl -D PGDATA restart</code> from <code>postgres</code> os user:</p>
<div><pre><code data-lang="bash"><span>[</span>root@postgres-lab ~<span>]</span><span>## systemctl restart postgresql-12.service</span>
</code></pre></div><p>Check whre PostgreSQL is listening now:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ netstat -lptnu | grep post
<span>(</span>Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.<span>)</span>
tcp        <span>0</span>      <span>0</span> 0.0.0.0:5432            0.0.0.0:*               LISTEN      30161/postmaster    
tcp6       <span>0</span>      <span>0</span> :::5432                 :::*                    LISTEN      30161/postmaster  
</code></pre></div><h2 id="configure-remote-access---pg_hbaconf">Configure remote access - pg_hba.conf</h2>
<p>PostgreSQL instance has got restricted access by <code>pg_hba.conf</code> file(host based authentication file).</p>
<p>We can provide in it information from which <code>ADDRESS</code> to which <code>DATABASE</code> on which <code>USER</code> by what <code>METHOD</code> we allow connecting. Additionaly we have to provide <code>TYPE</code> of connection.</p>
<p>This file resides in same place where <code>postgresql.conf</code>(we can alter this behavior by setting <code>pg_hba</code> parameter in <code>postgresql.conf</code>):</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ cd /postgresql/data/

<span>[</span>postgres@postgres-lab data<span>]</span>$ vi pg_hba.conf
<span>## TYPE  DATABASE        USER            ADDRESS                 METHOD</span>

<span>## "local" is for Unix domain socket connections only</span>
local   all             all                                     trust
<span>## IPv4 local connections:</span>
host    all             all             127.0.0.1/32            trust
<span>## IPv6 local connections:</span>
host    all             all             ::1/128                 trust
</code></pre></div><p>Allowed <code>TYPE</code>'s:</p>
<ul>
<li><code>local</code> - socket connection - needed to connect from shell on database server</li>
<li><code>host</code> - standard TCP/IP connection over the network - bnost SSL and no SSL</li>
<li><code>hostssl</code> - TCP/IP connection but only with SSL</li>
<li><code>hostnossl</code> - TCP/IP only without SSL</li>
<li><code>hostgssenc</code> - TCP/IP only GSSAPI</li>
<li><code>hostnogssenc</code> - TCP/IP only without GSSAPI</li>
</ul>
<p>With <code>DATABASE</code> we can specify database name or use special value <code>sameuser</code> if database name should be same as name of user that is connecting.</p>
<p>With <code>USER</code> we can specify user or role - role name should be preceded by <code>+</code> sign.</p>
<p><code>ADDRESS</code> field could be - hostname, IP range in CIDR format or special words:</p>
<ul>
<li><code>samehost</code> - which correspond to all IP adresses of database server</li>
<li><code>samenet</code> - which correspond to all IP in database server subnet</li>
</ul>
<p>With <code>METHOD</code> field we can set one of authentication methods - most important ones are:</p>
<ul>
<li><code>trust</code> - allow connection without password - moslty set for local connections from database server itself</li>
<li><code>reject</code> - reject connections</li>
<li><code>md5</code> - allow connections after getting from user password - encrypted</li>
<li><code>password</code> - allow connection after getting plain password - DO NOT USE in untrusted networks - better -&gt; never use this option</li>
<li><code>ldap</code> - getting account authorization data from LDAP server</li>
</ul>
<p>In <code>DATABASE</code> and <code>USER</code> fields you can specify special word <code>all</code> if you don’t want to create any restrictions here.</p>
<p>There can be situation when we must use additional field named <code>auth-options</code> for specyfying details for example for <code>hostssl</code> connection type. This topic will be covered in another post.</p>
<h3 id="sample-pg_hba-record---allow-all-users-connect-to-any-db-from-all-ip-addresses---only-with-password">Sample pg_hba record - allow all users connect to any DB from all IP addresses - only with password</h3>
<p>Add in <code>pg_hba.conf</code>:</p>
<div><pre><code data-lang="bash"><span>## Network access</span>
host    all             all             0.0.0.0/0               md5
</code></pre></div><p>Reload(online operation) PostgreSQL that it can use <code>pg_hba.conf</code> changes:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab data<span>]</span>$ /usr/pgsql-12/bin/pg_ctl -D /postgresql/data reload
server signaled
</code></pre></div><h2 id="connecting-to-postgresql">Connecting to PostgreSQL</h2>
<h3 id="local-from-server">Local from server</h3>
<p>It will work without password because we have <code>trust</code> in <code>pg_hba.conf</code> for <code>local</code> connections:</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql
psql <span>(</span>12.3<span>)</span>
Type <span>"help"</span> <span>for</span> help.
</code></pre></div><h3 id="remote-machine">Remote machine</h3>
<p>Default URI syntax - you can connect like this:<br>
<code>psql postgresql://user:passwd@host:5432/dbame</code><br>
or by more common method:<br>
Connect to remote database from <code>psql</code> with connections details provided in parameters(it will ask for password because of <code>md5</code> method in <code>pg_hba.conf</code> for connections from <code>0.0.0.0/0</code>):</p>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab data<span>]</span>$ psql -h 10.128.0.2 -p <span>5432</span>
Password <span>for</span> user postgres:
psql <span>(</span>12.3<span>)</span>
Type <span>"help"</span> <span>for</span> help.

postgres<span>=</span><span>##</span>
</code></pre></div><p>We can also use parameter <code>-U</code> to specify username different than OS username we currently are using.</p>
<p>Also all this parameters can be taken from shell variables which names are self descriptive - if we set all of them we can just use plain <code>psql</code> command to connect:</p>
<ul>
<li><code>PGHOST</code></li>
<li><code>PGPORT</code></li>
<li><code>PGDATABASE</code></li>
<li><code>PGUSER</code></li>
<li><code>PGPASSWORD</code></li>
</ul>
<h3 id="check-connected-database">Check connected database</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select current_database();</span>
 current_database
------------------
 postgres
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-current-user">Check current user</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select current_user;</span>
 current_user
--------------
 postgres
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-ip-and-port-used-for-connection">Check IP and port used for connection</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select inet_server_addr(), inet_server_port();</span>
 inet_server_addr | inet_server_port
------------------+------------------
 10.128.0.2       |             <span>5432</span>
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-postgresql-version">Check PostgreSQL version</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## select version();</span>
                                                version                                                 
--------------------------------------------------------------------------------------------------------
 PostgreSQL 12.3 on x86_64-pc-linux-gnu, compiled by gcc <span>(</span>GCC<span>)</span> 8.3.1 <span>20191121</span> <span>(</span>Red Hat 8.3.1-5<span>)</span>, 64-bit
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="check-connection-info">Check connection info</h3>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## \conninfo</span>
You are connected to database <span>"postgres"</span> as user <span>"postgres"</span> on host <span>"10.128.0.2"</span> at port <span>"5432"</span>.
</code></pre></div><h2 id="executing-commands-from-shell">Executing commands from shell</h2>
<h3 id="execute-single-command-from-shell">Execute single command from shell</h3>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql -c <span>"select current_time"</span>
    current_time    
--------------------
 14:09:19.854598+00
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div><h3 id="exacute-sql-script-from-shell">Exacute sql script from shell</h3>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql -f create_user.sql
CREATE ROLE
CREATE ROLE
CREATE ROLE
</code></pre></div><h3 id="combine-single-command-with-sql-script-from-shell">Combine single command with sql script from shell</h3>
<div><pre><code data-lang="bash"><span>[</span>postgres@postgres-lab ~<span>]</span>$ psql -c <span>"select current_time"</span> -f create_user.sql -c <span>"select current_time"</span>
    current_time    
--------------------
 14:14:26.922453+00
<span>(</span><span>1</span> row<span>)</span>

CREATE ROLE
CREATE ROLE
CREATE ROLE
    current_time    
--------------------
 14:14:26.926545+00
<span>(</span><span>1</span> row<span>)</span>
</code></pre></div>
<h3 id="check-all-available-metacommands">Check all available metacommands</h3>
<p>Do it yourself to see all available commands - output trimmed to important ones!</p>
<div><pre><code data-lang="bash">postgres<span>=</span><span>## \?</span>
General
  <span>\c</span>opyright             show PostgreSQL usage and distribution terms
  <span>\c</span>rosstabview <span>[</span>COLUMNS<span>]</span> execute query and display results in crosstab
  <span>\e</span>rrverbose            show most recent error message at maximum verbosity
  <span>\g</span> <span>[</span>FILE<span>]</span> or ;         execute query <span>(</span>and send results to file or |pipe<span>)</span>
  <span>\g</span>desc                 describe result of query, without executing it
  <span>\g</span>exec                 execute query, <span>then</span> execute each value in its result
  <span>\g</span>set <span>[</span>PREFIX<span>]</span>         execute query and store results in psql variables
  <span>\g</span>x <span>[</span>FILE<span>]</span>             as <span>\g</span>, but forces expanded output mode
  <span>\q</span>                     quit psql
  <span>\w</span>atch <span>[</span>SEC<span>]</span>           execute query every SEC seconds

  Query Buffer
    <span>\e</span> <span>[</span>FILE<span>]</span> <span>[</span>LINE<span>]</span>       edit the query buffer <span>(</span>or file<span>)</span> with external editor
    <span>\e</span>f <span>[</span>FUNCNAME <span>[</span>LINE<span>]</span><span>]</span>  edit <span>function</span> definition with external editor
    <span>\e</span>v <span>[</span>VIEWNAME <span>[</span>LINE<span>]</span><span>]</span>  edit view definition with external editor
    <span>\p</span>                     show the contents of the query buffer
    <span>\r</span>                     reset <span>(</span>clear<span>)</span> the query buffer
    <span>\s</span> <span>[</span>FILE<span>]</span>              display history or save it to file
    <span>\w</span> FILE                write query buffer to file

</code></pre></div><h3 id="list-objects-in-psql">List objects in psql</h3>
<ul>
<li>\d[S+]          -       list tables, views, and sequences</li>
<li>\d[S+]  NAME     -      describe table, view, sequence, or index</li>
<li>\da[S]  [PATTERN] -     list aggregates</li>
<li>\dA[+]  [PATTERN]  -    list access methods</li>
<li>\db[+]  [PATTERN]   -   list tablespaces</li>
<li>\dc[S+] [PATTERN]    -  list conversions</li>
<li>\dC[+]  [PATTERN]     - list casts</li>
<li>\dd[S]  [PATTERN]     - show object descriptions not displayed elsewhere</li>
<li>\dD[S+] [PATTERN]     - list domains</li>
<li>\ddp    [PATTERN]     - list default privileges</li>
<li>\dE[S+] [PATTERN]     - list foreign tables</li>
<li>\det[+] [PATTERN]     - list …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://knowledgepill.it/posts/postgresql-basics-guide/">https://knowledgepill.it/posts/postgresql-basics-guide/</a></em></p>]]>
            </description>
            <link>https://knowledgepill.it/posts/postgresql-basics-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24036132</guid>
            <pubDate>Mon, 03 Aug 2020 10:40:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The UX of Lego Interface Panels]]>
            </title>
            <description>
<![CDATA[
Score 334 | Comments 31 (<a href="https://news.ycombinator.com/item?id=24035866">thread link</a>) | @george_cave
<br/>
August 3, 2020 | https://www.designedbycave.co.uk/2020/LEGO-Interface-UX/ | <a href="https://web.archive.org/web/*/https://www.designedbycave.co.uk/2020/LEGO-Interface-UX/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

   
   <time>August 2020</time>

   

   

   <p>Piloting an <a href="https://www.lego.com/en-de/product/ocean-exploration-ship-60266">ocean exploration ship</a> or <a href="https://www.lego.com/en-de/product/mars-research-shuttle-60226">Martian research shuttle</a> is serious business. Let’s hope the control panel is up to scratch. Two studs wide and angled at 45°, the ubiquitous “2x2 decorated slope” is a LEGO minifigure’s interface to the world.</p>

<p>These iconic, low-resolution designs are the perfect tool to learn the basics of physical interface design. Armed with 52 different bricks, let’s see what they can teach us about the design, layout and organisation of complex interfaces.</p>

<p>Welcome to the world of LEGO UX design.</p>

<p><img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/interfaces.jpg" alt="LEGO interfaces"></p>

<h2 id="organised-chaos">Organised chaos</h2>

<p>At a glance, the variety of these designs can be overwhelming, but it’s clear that some of these interfaces look far more chaotic than others. Most interfaces in our world contain a blend of digital screens and analog inputs like switches and dials. These LEGO panels are no different.</p>

<p>Plotting the panels across these two axes reveals a few different clusters. Screens with an accompanying row of buttons sit in the top left. A small cluster of very organised switch panels lies to the far right. The centre bottom is occupied by some wild concepts that are hard to understand, even after several glances.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/positioning.jpg" alt="Design positioning with LEGO, in LEGO">
    <figcaption>Design positioning with LEGO, in LEGO</figcaption>
  </figure>

<p>Designing a complex machine interface is a juggling act of many different factors from ergonomics to engineering. But we can break down the problem into two key questions:</p>

<ol>
  <li>How can we <em>differentiate</em> between the function of different inputs?</li>
  <li>How can we <em>organise</em> the many inputs and outputs so that we understand how they relate to each other?</li>
</ol>

<p>Let’s take a deeper look at tackling these two challenges in LEGO.</p>

<h2 id="differentiating-inputs">Differentiating inputs</h2>

<p>What could cause 400 WWII pilots to raise the landing gear on their B-17 bomber just before touchdown? Catastrophic pilot error, or something more fundamental?</p>

<p>It was the psychologist Alphonsis Chapanis who first suggested that the high rate of crash landings might be the fault of poor interface design. The adjacent landing gear and flap control knobs were identically shaped. The pilots never stood a chance.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/landing.jpg" alt="B-17 belly landing, and the shape coding that helped to irradiate the problem. Source: Wikipedia">
    <figcaption>B-17 belly landing, and the shape coding that helped to irradiate the problem. Source: Wikipedia</figcaption>
  </figure>

<p>His temporary solution was to glue differently shaped strips of rubber to each switch, enabling blind operation by touch alone. This gave rise to the idea of shape coding and a system of differentiation still being followed in aircraft cockpits today.</p>

<p>We can compare the three interfaces below to see this in action. Ignore the overall layout, it’s the differences between individual switches that matter here. Imagine trying to feel for one of these buttons without looking. The left panel (“Slope 45 2 x 2 with 12 Buttons”) would require careful hand-eye co-ordination. The right panel (“Aircraft Multiple Flight Controls”) clearly distinguishes between the throttle (large, linear vertical movement), toggle switches (round vertical flick) and the push buttons (square push-in).</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/differentiation.jpg" alt="Left to right: terrible, poor and better input differentiation">
    <figcaption>Left to right: terrible, poor and better input differentiation</figcaption>
  </figure>

<p>Differentiation like this is a still a very real problem today. In 2015, <a href="https://money.cnn.com/2015/01/06/autos/ford-push-button-ignition-recall/index.html">Ford recalled 13,500 Lincoln SUVs</a> because drivers speeding down the motorway were mistakenly shutting off the engine when they tried to activate sport mode. See if you can spot why:</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/ford-lincoln.jpg" alt="Ford Lincoln MKC before the Engine start/stop button was moved. Source: CNN">
    <figcaption>Ford Lincoln MKC before the Engine start/stop button was moved. Source: CNN</figcaption>
  </figure>

<p>Shape coding is one approach to differentiation, but there are many others. Colour coding is perhaps the only one to break into our everyday vocabulary, but we can add four more: size, texture, position and operation coding. Together these six are our allies in the design of error-proof interfaces.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/codings.jpg" alt="The 6 basic codings. Notice that many of these examples actually combine multiple codings in one.">
    <figcaption>The 6 basic codings. Notice that many of these examples actually combine multiple codings in one.</figcaption>
  </figure>

<p>Size, shape and colour-coding are the fundamentals: quick-wins that can fix a lot of interface problems. Texture is also a great differentiator for blind operation, particularly on small dials requiring precise control.</p>

<p>Position-coding is seemingly straightforward but is often under used. Products with a clear default ergonomic position (like binoculars or a gaming console) can exploit the natural position of the hand to differentiate between primary and secondary actions.</p>

<p>Finally, operation-coding ascribes different types of movements (like a twist or vertical slide) to different inputs. This can be immensely powerful when the switch motion reinforces the operation behind it, e.g. a crane lever which raises the crane when the lever is raised.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/lego-codings.jpg" alt="The six different codings in use in the LEGO interfaces: size, shape, colour, texture, position, operation">
    <figcaption>The six different codings in use in the LEGO interfaces: size, shape, colour, texture, position, operation</figcaption>
  </figure>

<p>Differentiation is a good first step that will avoid confusion between adjacent switches. But its only with organisation that we can create a clear and accurate mental model of the interface for the user.</p>

<h2 id="organising-inputs">Organising inputs</h2>

<p>Compare the three panels below. Identical layouts, but the blue one is much clearer than the white. This is the <a href="https://www.usertesting.com/blog/gestalt-principles">gestalt principles</a> at work, identifying related items with a common region.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/gestalt.jpg" alt="Basic differentiation by clustering">
    <figcaption>Basic differentiation by clustering</figcaption>
  </figure>

<p>Easy. But how are you going to decide which inputs to cluster together?</p>

<p>I like to use <a href="http://blog.presentandcorrect.com/27986-2">Soviet control panels</a> as a starting point. These beautiful walls of nonsensical dials and levers are brought to life when arranged in a giant factory schematic.  It would be hard to find a more literal organisation of the information.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/soviet-control-panels.jpg" alt="Soviet control panels in action. Source: Present and Correct">
    <figcaption>Soviet control panels in action. Source: Present and Correct</figcaption>
  </figure>

<p>These panels are what I’d called a consolidated interface. Every piece of input and feedback has been moved onto the same panel. This is the approach that <a href="https://www.thedrive.com/news/33847/the-defunct-dyson-evs-steering-wheel-looks-like-it-was-made-by-vacuum-cleaner-people">Dyson took with their car</a>. Now imagine the opposite, moving each of those lights and switches to the actual location of that valve in the factory. Sounds ludicrous, but these <a href="https://deeptread.com/blog/2016/11/21/audi-tt-air-vent-design">air vents in the Audi TT</a> show that this distributed approach can also be a great win for user experience. I wrote a lot more about these <a href="https://www.designedbycave.co.uk/2018/Interfaces/">distributed interfaces last year</a>.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/distributed.jpg" alt="Lego vehicle dashboard: distributed (left) vs. consolidated (right)">
    <figcaption>Lego vehicle dashboard: distributed (left) vs. consolidated (right)</figcaption>
  </figure>

<p>Back to the Soviet factories. Those interface panels were great for answering the question “does this valve let water into tank Б?”. But they’re very poor for answering “are all water valves closed?” or “where are all the switches I need to prepare for the shift changeover?”.</p>

<p>LEGO use the Soviet schematic approach for their <a href="https://www.bricklink.com/catalogItemIn.asp?P=3298pb005&amp;in=S">fantasy</a> <a href="https://www.bricklink.com/catalogItemIn.asp?P=3298px10&amp;in=S">orientated</a> designs, because schematics are superb at providing a mental model of the inner workings of an alien system. However for everyday use, there are some other approaches that work better.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/alien-interfaces.jpg" alt="LEGO Insectoid and UFO interfaces. I wonder what these buttons actually do?">
    <figcaption>LEGO Insectoid and UFO interfaces. I wonder what these buttons actually do?</figcaption>
  </figure>

<p><strong>Feature</strong> based organisation is the most common, perhaps even the “default” design philosophy. Group together all the inputs and outputs for each product feature. This <a href="https://www.cambridgeconsultants.com/press-releases/building-life-saving-ventilator-lightning-speed">COVID-19 ventilator from Cambridge Consultants</a> is a wonderful example but we also see this a lot in cars, with a cluster of switches for the airflow control and all of the lights on one control stick.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/covid-ventilator.jpg" alt="COVID-19 ventilator by Cambridge Consultants with clear feature-based organisation. Source: Cambridge Consultants">
    <figcaption>COVID-19 ventilator by Cambridge Consultants with clear feature-based organisation. Source: Cambridge Consultants</figcaption>
  </figure>

<p>Organising by <strong>operation</strong> means putting all the switches that function in a certain way in the same place. I’ve no idea what all the valves in the picture below do, but I bet they don’t all open things that relate to each other. Anytime you see a row of switches that look and function the same, but control disparate parts of the system, you’ve come across organisation by operation.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/valves.jpg" alt="Source: Twitter @aglushko">
    <figcaption>Source: Twitter @aglushko</figcaption>
  </figure>

<p>Today most interfaces are effectively <a href="https://en.wikipedia.org/wiki/Fly-by-wire">fly-by-wire</a>, but historically the levers that you pulled in, say, a tractor cabin would literally move the hydraulic pistons beneath the seat to a new position. Routing all these different electrical, mechanical and hydraulic systems efficiently can severely compromise your interface clustering, leading to organisation by <strong>technology</strong>.</p>

<p>The modern equivalent of this is surprisingly common. Any touchscreen with buttons by the side exhibits this technology-based split. In a <a href="https://www.chrisharrison.net/index.php/Research/PneumaticDisplays">future</a> <a href="https://vimeo.com/343640141">world</a>, SpaceX might embed <a href="https://www.space.com/spacex-crew-dragon-touchscreen-astronaut-thoughts.html">these physical controls</a> right inside the screen next to the information they affect, but for now they sit awkwardly by the side as if nothing is wrong.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/spacex-dragon.jpg" alt="Bob and Doug in the SpaceX Dragon capsule. Source: SpaceX">
    <figcaption>Bob and Doug in the SpaceX Dragon capsule. Source: SpaceX</figcaption>
  </figure>

<p>In LEGO we find the feature based organisation in the “Monitor with -19° pattern”. Two clear clusters, perhaps one for temperature control and another for vital signs monitoring. In the second panel below, I don’t know what all those switches do, but they seem to be clustered based on their operation, not because of what they will operate.</p>

<p>There are many LEGO panels with a technology split like the SpaceX Dragon capsule, but I like to imagine that this early 90s police control unit was forced to divide the audio and video playback because the newer tape reel technology was incompatible with the older analog phone line system. This is organisation by technology in action.</p>

<figure>
    <img src="https://www.designedbycave.co.uk/images/posts/2020-08-LEGO-Interface-UX/organisation.jpg" alt="L-to-R: organisation by feature, operation, technology and use case">
    <figcaption>L-to-R: organisation by feature, operation, technology and use case</figcaption>
  </figure>

<p>All of our approaches so far: organisation by features, operation or technology, have been grounded in properties of the system, not of the user. Organisation by <strong>use-case</strong> is the antidote to this, a clustering based on the daily routines and tasks of the user.</p>

<p>Imagine arriving for work each day at the LEGO body scanner factory. Grouping the switches by task (prepare machine, load body, process scan…) would mean splitting up the radiation and scanner buttons into many different regions. More complex for the computer, but more streamlined for the operator. As the designer, only you and your users will be the judge of what works best.</p>

<h2 id="but-george-which-is-the-best-interface">But George, which is the best interface?</h2>

<p>I often say there’s no such thing as the best interface, but …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.designedbycave.co.uk/2020/LEGO-Interface-UX/">https://www.designedbycave.co.uk/2020/LEGO-Interface-UX/</a></em></p>]]>
            </description>
            <link>https://www.designedbycave.co.uk/2020/LEGO-Interface-UX/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24035866</guid>
            <pubDate>Mon, 03 Aug 2020 09:59:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[KLM to Lay Off 5k Employees Due to Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 76 (<a href="https://news.ycombinator.com/item?id=24035132">thread link</a>) | @cockpitherald
<br/>
August 3, 2020 | https://kokpitherald.com/klm-to-lay-off-5000-employees-due-to-covid-19-crisis/ | <a href="https://web.archive.org/web/*/https://kokpitherald.com/klm-to-lay-off-5000-employees-due-to-covid-19-crisis/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://kokpitherald.com/wp-content/uploads/2020/08/51225945-56BB-4A94-A791-264925C3315B.jpeg" data-caption="A total of 4,500 to 5,000 positions in the entire KLM Group will cease to exist.

Photo: KLM
"><img width="500" height="375" src="https://kokpitherald.com/wp-content/uploads/2020/08/51225945-56BB-4A94-A791-264925C3315B.jpeg" srcset="https://kokpitherald.com/wp-content/uploads/2020/08/51225945-56BB-4A94-A791-264925C3315B.jpeg 500w, https://kokpitherald.com/wp-content/uploads/2020/08/51225945-56BB-4A94-A791-264925C3315B-300x225.jpeg 300w, https://kokpitherald.com/wp-content/uploads/2020/08/51225945-56BB-4A94-A791-264925C3315B-80x60.jpeg 80w, https://kokpitherald.com/wp-content/uploads/2020/08/51225945-56BB-4A94-A791-264925C3315B-265x198.jpeg 265w" sizes="(max-width: 500px) 100vw, 500px" alt="" title="KLM pliots and cabin crew"></a><figcaption>A total of 4,500 to 5,000 positions in the entire KLM Group will cease to exist.

Photo: KLM
</figcaption></figure></div>
            <!-- content -->
 <!-- A generated by theme --> 



 <!-- end A --> 


<p><strong>KLM is in the throes of a crisis of unprecedented magnitude. Since the outbreak of the COVID-19 virus at the start of 2020, numerous measures have already been taken to deal with the current circumstances. Expectations are that the road to recovery will be long and fraught with uncertainty. This means that KLM’s structure and size must be rigorously adjusted even further in the years ahead. Consequently, a total of 4,500 to 5,000 positions in the entire KLM Group (expressed in FTEs) will cease to exist.</strong></p>



<p>In the wake of the coronavirus outbreak, KLM gradually began reducing the size of its network in February to operate less than 10% of its original number of <a href="https://kokpitherald.com/united-airlines-plans-to-resume-service-on-more-than-25-international-routes-in-september/">flights</a> by the start of April. In the second quarter, only 15% of the original number of flights were operated. In July, 30% of the original flights were operated and load factors are lagging behind. As a result, while the network is again being gradually and carefully expanded, revenues are lagging far behind.</p>
 <!-- A generated by theme --> 



 <!-- end A --> 





<p>Prospects for the airline industry – and KLM in particular – are uncertain. Different countries are now beginning to tighten their more relaxed travel restrictions. This is making customers more cautious when it comes to booking a ticket. In all scenarios, demand is only expected to recover by 2023 or 2024 at the earliest. The degree and speed of recovery will depend on a number of factors including the development of the virus, economic recovery and customer travel behaviour.</p>



<h2>KLM Adjusting to the New Reality</h2>



<p>Government support in the form of a direct state loan and guaranteed bank credit facilities amounting to a maximum of €3.4 billion will enable KLM to navigate the crisis in the forthcoming period. <a href="https://news.klm.com/klm-adapts-organisation-further-due-to-covid-19-crisis/" target="_blank" aria-label="KLM (opens in a new tab)" rel="noreferrer noopener">KLM</a> is extremely grateful for this support provided by means of the loan. In order to guarantee KLM’s existence in the longer term, the airline must adapt its size to the new reality. KLM therefore finds itself compelled to reduce its workforce down to the number needed for the planned operation in 2021/2022. Of the current total of 33,000 FTEs in the entire KLM Group, the workforce must be reduced by 4,500 to 5,000 FTEs to 28,000 FTEs in the course of 2021.</p>



<p>KLM’s size is already becoming smaller – and will continue to be reduced – based on the current measures, which include the non-renewal of temporary contracts (1,500 FTEs) and the Voluntarily Departure Scheme (2,000 FTEs). Additionally, natural attrition (500 FTEs) through retirement and the like in 2020 and 2021 will also contribute to the reduction needed.</p>



<p>Hence, despite the measures already taken, even fewer people will be needed at KLM in the years ahead. Additionally, for positions on the ground we also need to deal with some mismatch in functional skills and capabilities.</p>



<p>Unfortunately, for this reason and taking into account the mismatch, alternative solutions will have to be found for ca. 1,500 positions. This relates to up to 500 ground positions, 300 cabin crew positions and 300 cockpit positions and approximately 400 positions at KLM subsidiaries and Air France-KLM group functions.</p>



<p>Given the high level of uncertainty, KLM keeps open the possibility of further reductions in case the production levels will be revised further down for 2021/2022 than the -20% planned now.</p>



<figure><img src="https://kokpitherald.com/wp-content/uploads/2020/08/klm_crew.jpg" alt="KLM" srcset="https://kokpitherald.com/wp-content/uploads/2020/08/klm_crew.jpg 500w, https://kokpitherald.com/wp-content/uploads/2020/08/klm_crew-300x169.jpg 300w" sizes="(max-width: 500px) 100vw, 500px"><figcaption>KLM crew in Sydney Photo: KLM</figcaption></figure>



<h3>KLM to Cooperate with Trade unions and Works Council</h3>



<p>KLM’s reorganisation plans tie in with organisation-wide changes at Air France KLM. In the forthcoming period, KLM will be cooperating closely with the trade unions to draft a social plan for each collective labour agreement domain and subsidiary, as well as maintaining close consultation with the Works Council about further defining the reorganisation. This will include a more detailed specification of the conditions set by the Dutch government on issuing the financing package. Expectations are that this will be finished in its entirety in the course of October.“</p>



<blockquote><p><em>A great deal has already been done in recent months with respect to adjusting the size of our company in the face of a new reality. Unfortunately, more measures are needed in the short term to guarantee KLM’s continued existence in the future. For this reason, we are elaborating the reorganisation plan to emerge from this crisis in a stronger position, while retaining as many jobs as we can in a responsible manner and repaying the loans as quickly as possible.</em></p><p><em>KLM employees are loyal, professional and hard working. They are always ready to serve our customers, one another, the company and society at large. Recent developments have again served to prove that this is true. It is incredibly difficult and sad for KLM to now have to bid farewell to valuable, committed colleagues. Certainly in view of how much we have succeeded in achieving together in recent years. The forthcoming period will be devoted to saying goodbye to colleagues who have to leave with due care and to reconstructing KLM.</em></p></blockquote>



<p>KLM President-directeur &amp; CEO Pieter Elbers</p>

 <!-- A generated by theme --> 



 <!-- end A --> 

        </div></div>]]>
            </description>
            <link>https://kokpitherald.com/klm-to-lay-off-5000-employees-due-to-covid-19-crisis/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24035132</guid>
            <pubDate>Mon, 03 Aug 2020 07:51:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MNT Reform open source laptop with trackball]]>
            </title>
            <description>
<![CDATA[
Score 93 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24034502">thread link</a>) | @brian_herman
<br/>
August 2, 2020 | https://mntre.com/media/reform_md/2020-05-08-the-much-more-personal-computer.html | <a href="https://web.archive.org/web/*/https://mntre.com/media/reform_md/2020-05-08-the-much-more-personal-computer.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
        <date>Published: 2020-05-08 Updated: 2020-05-22 21:36:37 +0200</date>
        

<p><a href="https://mntre.com/media/reform_v2_images/product-mnt-reform.jpg"><img src="https://mntre.com/media/reform_v2_images/product-mnt-reform.jpg" alt="MNT Reform in 2020"></a></p>

<p>This page serves as a link index to all Reform content, both historical and recent.</p>

<p>On 2020-05-08, MNT Research launched the MNT Reform open source mobile computer:</p>

<p><strong><a href="https://www.crowdsupply.com/mnt/reform">→ Go to the Crowd Supply Campaign</a></strong></p>

<h2 id="mnt-reform-final-version">MNT Reform Final Version</h2>

<p>Internally called MNT Reform 2.0, this is a version with many improvements, full aluminum enclosure, HD display and NXP i.MX8MQ module.</p>

<ul>
<li>2020-05-22: <a href="https://mntre.com/reform2-handbook/system.html">MNT Reform Interactive System Diagram and Interactive PCBs</a>, first version, published.</li>
<li>2020-05-08: <a href="https://www.crowdsupply.com/mnt/reform">Crowd supply campaign is launched</a>, running until 2020-06-18.</li>
<li>2020-05-08: <a href="https://www.crowdsupply.com/mnt/reform/updates/the-campaign-is-live">Launch Announcement</a> of the campaign.</li>
<li>8 Beta versions (D-3 / D-4) were sold in late 2019/early 2020 and are shipped in May 2020.</li>
<li>2020-01-18: <a href="https://mntre.com/media/reform_md/2020-01-18-finishing-reform.html">Finishing Reform</a></li>
<li>2019-05-20: <a href="https://mntre.com/media/reform_md/2019-05-20-reintroducing-reform.html">Re-Introducing Reform</a></li>
<li>Handbook not yet available</li>
<li><a href="https://source.mntmn.com/MNT/reform">Sources of Reform 2.0 (KiCAD, various)</a></li>
</ul>

<h2 id="mnt-reform-prototype-version-s-internally-called-mnt-reform-0-1-1-0">MNT Reform Prototype Version(s) (internally called MNT Reform 0.1 - 1.0)</h2>

<p>This was the original version of Reform, based on NXP i.MX6QP.</p>

<ul>
<li>2019-01-14: <a href="https://mntre.com/media/reform_md/2019-01-14-status_update_on_reform.html">Status Update on MNT Reform</a></li>
<li><a href="https://mntre.com/media/reform_md/reform-1-handbook.pdf">Handbook for Reform 1.0 (PDF, historical)</a></li>
<li><a href="https://source.mntmn.com/MNT/reform/src/branch/master/historic-reform1">Sources of Reform 1.0 (KiCAD, various, historical)</a></li>
<li>10 prototype versions were shipped to early supporters in late 2018. 13 exist in total.</li>
<li><a href="https://mntre.com/media/reform_md/reform-historic/reform-beta-1.html">MNT Reform: The Original Story</a></li>
</ul>

<h2 id="talks-appearances">Talks / Appearances</h2>

<ul>
<li><a href="https://media.ccc.de/v/34c3-9257-lightning_talks_day_3#t=3512">Reform Lightning Talk from 34c3</a></li>
<li><a href="https://media.ccc.de/v/dg-90">Longer Reform talk in German from CCCB Datengarten 90</a></li>
</ul>

<h2 id="irc-channel">IRC Channel</h2>

<ul>
<li>Chat in #reform on irc.freenode.net</li>
</ul>



<ul>
<li><a href="https://mastodon.social/@mntmn">Mastodon</a></li>
<li><a href="https://twitter.com/mntmn">Twitter</a></li>
</ul>

<h2 id="crowdfunding-campaign">Crowdfunding Campaign</h2>

<p><strong><a href="https://www.crowdsupply.com/mnt/reform">→ Go to the Crowd Supply Campaign</a></strong></p>

      </section></div>]]>
            </description>
            <link>https://mntre.com/media/reform_md/2020-05-08-the-much-more-personal-computer.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24034502</guid>
            <pubDate>Mon, 03 Aug 2020 05:34:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Autodesk criticised by architects]]>
            </title>
            <description>
<![CDATA[
Score 289 | Comments 369 (<a href="https://news.ycombinator.com/item?id=24034211">thread link</a>) | @nsoonhui
<br/>
August 2, 2020 | http://extranetevolution.com/2020/07/autodesk-criticism-extends/ | <a href="https://web.archive.org/web/*/http://extranetevolution.com/2020/07/autodesk-criticism-extends/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
                
                    
                                        
                    <div id="content-main">
                    
	
		
		        
<div id="post-14835">
		
	<div>
    	
                    	<p>
            	<span>Jul</span>
                <span>31</span>
                	                <span>2020</span>
                            </p>
            
		        
		
			    <ul>
	    		        <li>
	        	By  in <span><a href="http://extranetevolution.com/category/aec/">AEC</a>, <a href="http://extranetevolution.com/category/bim/">BIM</a>, <a href="http://extranetevolution.com/category/businessfinancial/">Business/Financial</a>, <a href="http://extranetevolution.com/category/collaboration/">Collaboration</a>, <a href="http://extranetevolution.com/category/digital-transformation/">Digital transformation</a>, <a href="http://extranetevolution.com/category/functionality/">Functionality</a>, <a href="http://extranetevolution.com/category/future/">Future</a>, <a href="http://extranetevolution.com/category/vendors/">Vendors</a></span>	        </li>
	        	        <li>
	        	<p><em>31 July 2020</em></p>	        </li>
	        	        <li>
	        	        <span>
            <i></i>
        </span>
    	        </li>
	        	    </ul>
    		
				<div>
							
										
						<div itemscope="" itemtype="http://schema.org/BlogPosting"><p><em><strong>Architectural unrest about Autodesk and its support for the industry’s design businesses is growing. Discontent has been simmering for a decade or more, and has led to calls for EU action.</strong></em></p>
<p><a href="https://www.autodesk.co.uk/"><img src="http://extranetevolution.com/files/2013/11/autodesk.logo2013.jpg" alt="autodesk logo" width="181" height="31"></a>The recent <a href="https://letters-to-autodesk.com/" target="_blank" rel="noopener noreferrer">open letter to Autodesk</a> from 17 named members of a 25-strong group of leading architects (28 July 2020: <a href="http://extranetevolution.com/2020/07/design-firms-demand-change-at-autodesk/"><em>Design firms demand change at Autodesk</em></a>) has been supported by more firms. They extend the geographical reach of those prepared to publicly criticise the US AEC software vendor over its support for architectural design businesses – many of them heavily reliant upon Autodesk’s Revit design software. An additional 18 practices now stand alongside the original letter’s signatories, bringing the total to 35. A further 10 practices are supportive, but have not gone public. In total, more than 50 firms have therefore backed the group’s grievances.</p>
<p><img src="http://extranetevolution.com/files/2020/07/Revit_2014_branding-150x150.png" alt="Autodesk Revit_2014_branding" width="150" height="150" srcset="http://extranetevolution.com/files/2020/07/Revit_2014_branding-150x150.png 150w, http://extranetevolution.com/files/2020/07/Revit_2014_branding-300x300.png 300w, http://extranetevolution.com/files/2020/07/Revit_2014_branding-160x160.png 160w, http://extranetevolution.com/files/2020/07/Revit_2014_branding.png 316w" sizes="(max-width: 150px) 100vw, 150px">The new signatories are: BC Architects and SAOTA (both from South Africa); Cooper Carry, Portman Architects, Idesign-solutions, Studio 3 Architecture, Goody Clancy, SGA, Bohlin, Cywinski, Jackson, and Workshop Collaborative (all from the US), Atelier Tisso (France),&nbsp;CGL,&nbsp;Shepheard Epstein Hunter, and PDPLondon (all from the UK); Vibes (Netherlands); MIZA (Canada); Oslo works (Norway); and Mochly-Eldar Architects (Israel).</p>
<p>It is clear that Iain Godwin has tapped into a growing sense of unease, though, to be fair, rumblings of discontent have been heard many times over the past decade or so from a variety of software commentators and end-users. For example:</p>
<ul>
<li><a href="https://www.worldcadaccess.com/blog/2009/12/solidworks-accuses-autodesk-of-attempting-to-monopolize-use-of-dwg-as-file-extension.html" target="_blank" rel="noopener noreferrer"><em><strong>SolidWorks accuses Autodesk of attempting to monopolize use of .dwg as file extension</strong></em></a> (Ralph Grabowski – WorldCADAccess, December 2009)</li>
<li><a href="https://gfxspeak.com/2011/04/28/is-the-european-commission-investigating-autodesk/" target="_blank" rel="noopener noreferrer"><em><strong>Is the European Commission investigating Autodesk?</strong></em></a> (Randall Newton – GraphicSpeak, April 2011)</li>
<li><a href="https://www.cadnauseam.com/2017/07/26/autodesk-confirms-its-own-unconscionable-conduct/" target="_blank" rel="noopener noreferrer"><em><strong>Autodesk confirms its own unconscionable conduct</strong></em></a> (Steve Johnson – CAD Nauseum, July 2017)</li>
<li><a href="http://debunkthebim.blogspot.com/2018/04/here-is-why-autodesks-monopoly-over.html" target="_blank" rel="noopener noreferrer"><em><strong>Here is why Autodesk’s monopoly over the Global AEC is not good, not even for Autodesk</strong></em></a> (Zolna Murray – Debunk the BIM, February 2018)</li>
<li><a href="https://www.linkedin.com/pulse/avoiding-carillions-mistakes-joining-autodesks-monopoly-john-ford/" target="_blank" rel="noopener noreferrer"><em><strong>Avoiding Carillion`s Mistakes of Joining Autodesk’s Monopoly Unnecessarily</strong></em></a> (John Ford – LinkedIn Pulse, May 2018)</li>
<li><a href="http://mes100.com/blog/the-state-of-bim-software-and-autodesk/" target="_blank" rel="noopener noreferrer"><em><strong>The state of BIM software and Autodesk</strong></em></a> (MES – MES blog, July 2018)</li>
<li><a href="https://thinkmoult.com/why-revit-is-shit.html" target="_blank" rel="noopener noreferrer"><em><strong>Why Revit is shit</strong></em></a> (Dion Moult – ThinkMoult, December 2018)</li>
</ul>
<p>Concerns about some software vendors’ monopolistic positions have also been raised internationally by industry organisations, including the <a href="http://www.fiec.eu/" target="_blank" rel="noopener noreferrer">European Construction Industry Federation (FIEC</a>).</p>
<h3>FIEC position paper</h3>
<p><a href="http://www.fiec.eu/"><img src="http://extranetevolution.com/files/2020/07/FIEC-logo-300x117.jpg" alt="FIEC logo" width="300" height="117" srcset="http://extranetevolution.com/files/2020/07/FIEC-logo-300x117.jpg 300w, http://extranetevolution.com/files/2020/07/FIEC-logo-160x62.jpg 160w, http://extranetevolution.com/files/2020/07/FIEC-logo.jpg 443w" sizes="(max-width: 300px) 100vw, 300px"></a>On 24 February 2020, the FIEC published a position paper on the relationship between users and software companies/ editors/ service providers (<em><a href="http://extranetevolution.com/files/2020/07/2020-02-24-FIEC_position_on_software_companies-Final.pdf" target="_blank" rel="noopener noreferrer">download</a></em>). No vendors are named, but, from the similarity of the claims, Autodesk is clearly one of the software providers that FIEC is sufficiently concerned about to urge the European Commission to review competition and data management regulations. Its paper discusses challenges and makes recommendations under four headings:</p>
<ol>
<li><strong>The dominant position of a few software companies/editors/providers raises major concerns.</strong> – The FIEC urges the European Commission to target competition issues relating to software user contracts</li>
<li><strong>The non-EU origin of these suppliers and their infrastructure is exacerbating the lack of autonomy in software capability in the EU.</strong> – The FIEC says EU software users should be allowed to decide where their data is stored (“EU companies should be able to have their data hosted on EU territory, by EU servers/companies, under EU legislation”), calls for the creation of a secure European Cloud, and says software services “should be required to meet EU standards for interoperability and open access”.</li>
<li><strong>Contracting authorities must remain software-neutral and promote open standards</strong>. – The FIEC calls for enforcement of EU public procurement rules, and urges promotion of open standards for data, protocols and file formats in public procurement.</li>
<li><strong>Rules need to be established for multiple-user-access platforms such as BIM models.</strong> – Similarly, the FIEC urges EU measures aimed at protecting the data owner while ensuring appropriate data access rights for other users.</li>
</ol>
<h3>Autodesk not alone in hosting, US dominance, interoperability issues</h3>
<p>Some of these issues are already familiar. Where project data is hosted has been a concern ever since Software-as-a-Service applications began to be deployed in the 1990s, and as use of construction collaboration platforms expanded in the early 2000s, most leading vendors have responded by creating localised hosting centres to serve different operational regions. Hosting&nbsp; project data in the United States, for example, has been resisted by most clients based in Europe and other parts of the world (eg: the Middle East, southeast Asia, etc; read <em>EE</em> August 2014 post <a href="http://extranetevolution.com/2014/08/no-saas-safe-harbor/"><em>No SaaS ‘Safe Harbor’</em></a>).</p>
<p>Concentration of large portions of the construction software industry through merger and acquisition activity has resulted in an increasingly dominant position for US software giants. By gobbling up strong players in Europe, Australasia and elsewhere, Autodesk, Bentley, Oracle and Trimble have assembled strong AEC software portfolios, while some of the more generic US software providers such as Microsoft and IBM have also developed applications, services and relationships that give them an increasingly strong foothold in the AEC space.</p>
<p>And software interoperability has been a perennial issue that <em>EE</em> has covered since it started in 2005 (eg: <a href="http://extranetevolution.com/2005/09/new_roi_return_/"><em>New ROI: Return on Interoperability</em></a>, September 2005). BuildingSmart (formerly the International Alliance for Interoperability) started out as an Autodesk initiative in the mid-1990s, but, over 25 years later, global shifts towards open standards are still proceeding almost glacially slowly, with Autodesk’s leading BIM authoring product “widely ridiculed” for its IFC import/export capabilities.</p>
<h3>Autodesk responds on key themes</h3>
<p><strong><a href="https://adsknews.autodesk.com/views/reply-to-open-letter-on-revit"><img src="http://extranetevolution.com/files/2020/07/Autodesk-reply-300x205.jpg" alt="Autodesk reply" width="400" height="273" srcset="http://extranetevolution.com/files/2020/07/Autodesk-reply-300x205.jpg 300w, http://extranetevolution.com/files/2020/07/Autodesk-reply-590x403.jpg 590w, http://extranetevolution.com/files/2020/07/Autodesk-reply-768x524.jpg 768w, http://extranetevolution.com/files/2020/07/Autodesk-reply-160x109.jpg 160w, http://extranetevolution.com/files/2020/07/Autodesk-reply.jpg 1157w" sizes="(max-width: 400px) 100vw, 400px"></a>Autodesk’s initial response</strong> (<a href="http://extranetevolution.com/2020/07/design-firms-demand-change-at-autodesk/"><em>see update to earlier EE post</em></a>) to the Godwin group’s open letter <strong>did not mention interoperability</strong> at all. However, a <a href="https://adsknews.autodesk.com/views/reply-to-open-letter-on-revit" target="_blank" rel="noopener noreferrer">follow-up blog post by Amy Bunzsel</a> published today (31 July 2020) addresses the main themes, and on openness and interoperability says: “<strong>We continue to invest in supporting IFC</strong> and based on customer feedback we’ve <strong>recently increased development for new industry requirements, focusing on IFCv4 certification</strong>.”</p>
<p>Bunzsel continues:</p>
<p>“Looking to the future, we believe that ways of working will evolve, from the direct modeling of today to outcome-based design driven by analysis…, to the convergence of manufacturing and construction, and that <strong>data needs to be unlocked from native formats and flow more readily throughout Autodesk and non-Autodesk products</strong>.”</p>
<h3>No more software silos</h3>
<p><a href="https://3drepo.com/"><img src="http://extranetevolution.com/files/2017/03/3DRepo-300x102.png" alt="" width="150" height="51" srcset="http://extranetevolution.com/files/2017/03/3DRepo-300x102.png 300w, http://extranetevolution.com/files/2017/03/3DRepo.png 385w" sizes="(max-width: 150px) 100vw, 150px"></a>Jozef Dobos is CEO and founder of London, UK-based technology vendor, <a href="https://3drepo.com/" target="_blank" rel="noopener noreferrer"><strong>3DRepo</strong></a>&nbsp;(which has been <a href="https://3drepo.com/4670-2/" target="_blank" rel="noopener noreferrer">a supporter of Open BIM since 2017</a>), and recently argued&nbsp;<a href="https://www.bimplus.co.uk/analysis/making-case-fair-competition-software-use/" target="_blank" rel="noopener noreferrer"><em>The case for fair competition in software use</em></a>, in a <em>BIM+</em> article. He writes:</p>
<p><img src="http://extranetevolution.com/files/2020/07/Jozef-Dobos-271x300.png" alt="Jozef Dobos" width="271" height="300" srcset="http://extranetevolution.com/files/2020/07/Jozef-Dobos-271x300.png 271w, http://extranetevolution.com/files/2020/07/Jozef-Dobos-145x160.png 145w, http://extranetevolution.com/files/2020/07/Jozef-Dobos-363x400.png 363w, http://extranetevolution.com/files/2020/07/Jozef-Dobos.png 502w" sizes="(max-width: 271px) 100vw, 271px">“<strong>All the issues raised by the FIEC must be addressed to enable the genuine digitisation of the construction industry</strong>.</p>
<p>3D Repo was created to enable the construction industry to work better together and to create better buildings. This is why projects like the&nbsp;<strong><a href="https://www.bimplus.co.uk/news/project-explores-faster-bim-data-transfer-between-/" target="_blank" rel="noopener noreferrer">AEC Delta Mobility</a></strong> [open-source] initiative in collaboration with BuroHappold Engineering and Speckle Systems are so important, creating a new standard for designers, integrators and fabricators to improve the flow of data.</p>
<p>The current method of sharing information as files of entire 3D models can hinder collaboration. Tracking changes can also be problematic and inefficient for design communication. AEC Delta Mobility breaks down the file barriers to enable small design changes, known as ‘Deltas’, to be shared faster, more openly and more efficiently.</p>
<p><strong>This is how the software industry should be working with the construction industry. Real solutions that involve working with customers and providing them with tools they will not want to walk away from, based on commercial terms that actually promote the collaborative behaviours we want to see, not divide us into software silos</strong>.”</p>
</div>								</div>
		
			    
    	</div>

</div>


<p><span><strong>Permanent link to this article: </strong><span>http://extranetevolution.com/2020/07/autodesk-criticism-extends/</span></span></p>











	
    

            
  

                </div><!-- #content-main -->
        
            
<!-- #sidebar1 -->        
        
    </div></div>]]>
            </description>
            <link>http://extranetevolution.com/2020/07/autodesk-criticism-extends/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24034211</guid>
            <pubDate>Mon, 03 Aug 2020 04:20:27 GMT</pubDate>
        </item>
    </channel>
</rss>
