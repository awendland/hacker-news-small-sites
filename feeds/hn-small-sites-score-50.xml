<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Tue, 21 Jul 2020 20:17:56 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Tue, 21 Jul 2020 20:17:56 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Minecraft@Home]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 8 (<a href="https://news.ycombinator.com/item?id=23895789">thread link</a>) | @networked
<br/>
July 20, 2020 | https://minecraftathome.com/minecrafthome/ | <a href="https://web.archive.org/web/*/https://minecraftathome.com/minecrafthome/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div><p><span>Server outage and supporting our infrastructure</span><br>

        Between 23:54 UTC yesterday (2020-07-20) and 15:35 UTC today, we suffered from a catastrophic SQL failure which forced us to restore from backup.</p><p>

There may be some credit issues where the system granted more or fewer credits than you expect for work done during this time.<br>
I can't apologize enough. If our BOINC deployment was architected for scale rather than for low cost, we could've avoided this.</p><p>

There are several enhancements to our infrastructure and upgrades we'd like to make, such as migrating services to Kubernetes and potentially using a managed SQL service.</p><p>

<span><b><span color="red">You can help!</span> Please consider visiting our Patreon page, reviewing the current set of benefits, and making a contribution of any size; any amount helps - <a href="https://patreon.com/minecraftathome" rel="nofollow">patreon.com/minecraftathome</a></b></span></p><p>

All contributions go towards covering infrastructure cost and quality-of-life improvements to ensure the project's longevity.
        <br>
        <span>21 Jul 2020, 20:10:54 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=44"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>That was fast</span><br>

        The origin of the panorama image used in the Minecraft main menu from beta version 1.8.1, released in September 2011, has remained a mystery until now.</p><p>

<b><span>In less than 24 hours after launching the panorama application; a volunteer host for Minecraft@Home, in a sheer stroke of luck, found the world seed, 25357015387625.</span></b><br>
This was approximately </p><p><span color="red">93 days of processing time at a total of 54.5 exaFLOPs</span> compressed into the last 24 hours.</p><p>

The specific host which located the seed belongs to the user <a href="https://minecraftathome.com/minecrafthome/show_user.php?userid=2558" rel="nofollow">vanos0512</a>.<br>
Thank you to the 137 users who contributed 181 hosts with 231 GPUs over the last 24 hours. You all accomplished this.</p><p>

<img src="https://i.imgur.com/f6lGCEn.png"></p><p>

Here are the details if you want to generate this world for yourself:<br>
<b>Minecraft version:</b> <i>Beta 1.7</i><br>
<b>Either of these two valid world seeds:</b> <i>2151901553968352745 or 8091867987493326313</i><br>
<b>Co-ordinates:</b> <i>x60, y76, z-67</i></p><p>

<a href="https://www.youtube.com/watch?v=caLCZNLPgrM" rel="nofollow">See the video released by EarthComputer announcing the finding.</a>
        <br>
        <span>18 Jul 2020, 15:32:11 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=42"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>Enjoy the scenery</span><br>

        <span>Minecraft@Home is now <span color="red">over one month old</span>! To celebrate this milestone, I present a new research focus; the panorama project.</span></p><p>

If you were here during beta-testing, you might have received a very early version of panorama tasks, and the eagle-eyed among you may have seen the application details <a href="https://minecraftathome.com/minecrafthome/server_status.php" rel="nofollow">on the server status page</a>.</p><p>

<span><b>The panorama app is a CUDA-only app for Linux and Windows with an Nvidia driver version of 418.96 or higher.</b></span></p><p>

This project attempts to find the world seed of the iconic panorama image which appeared in the background of the main menu of Minecraft between 2011 and 2018. The first phase of this project will only last a few days, and we shall update you with their progress in the coming weeks.</p><p>

<img src="https://i.imgur.com/3dyexWe.png"></p><p>

Right now, the application is quite substantial. Unlike the OpenCL applications for the Kaktwoos project, <b>if you allow BOINC to run tasks always; you may experience some stuttering or lag in your desktop environment while running these tasks</b>. These tasks do not have checkpointing support, but run in around 1 hour on an average host to mitigate the majority of lost cycles.</p><p>

<span>As always, <a href="https://minecraftathome.com/minecrafthome/prefs.php?subset=project" rel="nofollow">you can change which projects of which you decide to participate in your user preferences</a>.</span></p><p>

Let us know if you have any questions, and as always join the discussion over on <a href="https://discord.gg/xVFh9bp" rel="nofollow">the Discord server.</a>
        <br>
        <span>17 Jul 2020, 15:31:43 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=39"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>Kaktwoos 2.03 and Badges!</span><br>

        You may have seen many workunits just disappear from existence today.</p><p>

<img src="https://munin.kiska.pw/munin-cgi/munin-cgi-graph/Munin-Node/Munin-Node/results_minecraftathome-pinpoint=1594193183,1594247603.png?&amp;lower_limit=&amp;upper_limit=&amp;size_x=400&amp;size_y=200"></p><p>

Worry not! We realised there were far too many workunits scanning duplicate seeds, so we've scaled back the original workunits to the correct seed ranges <span><i>(no in-progress results were touched, so none of you should have lost any credit)</i></span>.</p><p>

We have located some promising seed candidates which were missed from processing and can be used as an input to this job, so they are currently set as the highest priority.</p><p>

Also, <span><b>we now have badges</b></span>! If you view the forums, any comments in threads, or on the leaderboards; you will see the new badges.<br>
We're open to suggestions for future badges, so please leave us some comments on this thread.
        <br>
        <span>8 Jul 2020, 21:06:03 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=30"> Discuss</a>
        </span></p></div><hr>
    <div><p><span>Profile creation and OpenCL vendor pinning</span><br>

        In the last news post, I advised everyone to create a profile in order to be eligible for 'user of the day' selection.<br>
Unfortunately, there was an issue with the ReCaptcha implementation which prevented this. This issue is now resolved.</p><p>

Now, you are able to <a href="https://minecraftathome.com/minecrafthome/create_profile.php" rel="nofollow">create a profile here</a>.</p><p>

Also, good news for hosts with OpenCL capable hardware from more than one vendor <i>(e.g. an Intel iGPU and an Nvidia GPU)</i><br>
The latest update to the kaktwoos app should ensure the tasks run on the correct device.<br>
If you are a user with a multi-vendor host, please keep an eye on your results and let us know if you're having any issues.</p><p>

As always, please get involved with the conversation in the <a href="https://minecraftathome.com/minecrafthome/forum_index.php" rel="nofollow">message boards</a>, and <a href="https://discord.gg/xVFh9bp" rel="nofollow">join the Discord</a>!
        <br>
        <span>3 Jul 2020, 16:57:49 UTC
    
            · <a href="https://minecraftathome.com/minecrafthome/forum_thread.php?id=22"> Discuss</a>
        </span></p></div><hr>
    <p><a href="https://minecraftathome.com/minecrafthome/old_news.php">... more</a></p><p><small>
    News is available as an <a href="https://minecraftathome.com/minecrafthome/rss_main.php">RSS feed &nbsp; <img src="https://minecraftathome.com/minecrafthome/img/rss_icon.gif" alt="RSS"></a>
        </small></p></div></div>]]>
            </description>
            <link>https://minecraftathome.com/minecrafthome/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23895789</guid>
            <pubDate>Mon, 20 Jul 2020 08:19:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Video Vectorization]]>
            </title>
            <description>
<![CDATA[
Score 122 | Comments 55 (<a href="https://news.ycombinator.com/item?id=23895211">thread link</a>) | @xanthine
<br/>
July 19, 2020 | https://vectorly.io/docs/technology/ | <a href="https://web.archive.org/web/*/https://vectorly.io/docs/technology/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

<p><img src="https://vectorly.io/docs/img/vector-graphics.png" alt="Drawing"></p>

<p>Vectorly is developing a new kind of video compression technology, which uses computer vision and vector graphics to reduce bitrates for video content by an order of magnitude (or more) compared to HEVC, while <strong>improving video</strong> quality. </p>
<p>This would be primarily effective for "vector friendly" video content, which would include animations, screen-casts, many e-learning videos and potentially 3d gaming content. </p>
<p>By leveraging existing vector-graphics rendering capabilities on all devices, this codec wouldn't require end-users, OEMs or browsers to install special software to enable playback of these videos.</p>
<p><strong>We are still in the early phases of developing this technology</strong>.</p>
<p>You can learn more about the technology in our <a href="https://files.vectorly.io/Vectorization+Whitepaper+v06.20.pdf">whitepaper</a></p>
<h2 id="the-core-idea">The Core Idea</h2>
<p>The core insight behind this project was that you could use vector-graphics based animations to simulate "videos" in a way that is indistinguishable from a traditional raster-graphics based video format such as an h264 video stream in an MP4 container.</p>
<h3 id="raster-graphics">Raster Graphics</h3>
<p>Normal videos, like the ones you see on Netflix or YouTube, are just sequences of images which get updated quickly on the screen, to create the illusion of motion. Each image is composed of "pixels" - individual dots of color. Higher resolution means more pixels, better visual quality, and bigger file sizes.</p>
<p><img alt="Pixel-Based" src="https://vectorly.io/docs/img/pixels.png"></p>
<p>Almost all video on the internet is of this format, known as "raster graphics". Video compression algorithms like h264 are just very efficient at using fewer data-points to reconstruct the pixels in any given frame, and at storing only the differences in pixels between frames of a video. </p>
<h3 id="vector-grapics-video">Vector Grapics video</h3>
<p>In contrast, we use a concept called "vector-graphics" to render video. Instead of pixels, we represent everything on the screen using shapes, lines and curves, which can be represented as mathematical equations (vector graphics).</p>
<p><img alt="Vector-Based" src="https://vectorly.io/docs/img/vector2.png"></p>
<p>Using these mathematical equations, we can re-draw any arbitrary shape on the screen - from the letter "T" to Bart Simpson's head. Furthermore, by adding information such as color, position on the screen, and how they move or change shape over time, you can create whole videos - including entire episodes of the Simpsons, with just sequences of mathematical equations.</p>
<h3 id="why-vectorization">Why vectorization?</h3>
<p>The core insight behind this project was that for a certain kind of "vector-friendly" video content, storing the video using vector graphics would be much more efficient than using raster graphics (in some cases, up to 2 orders of magnitude more efficient).</p>
<p>This idea is not substantively different from the idea of Flash based animations about 20 years ago. Why do this now?</p>
<p><strong>No need for a decoder</strong>: Most devices now support SVG, HTML5, WebGL/OpenGL and/or some form of hardware-accelerated vector-graphics rendering. That lets you render vector-graphics content on any device without require end-users, OEMs or browsers to install special software to enable playback of vector-graphics content, and to achieve native-level performance by doing so. App developers would only need to include an appropriate library or SDK in their website or app to enable playback within native or 3rd player video players.</p>
<p><strong>Computer vision</strong>: Our patented vectorization technology relies heavily on computer vision to convert raster-graphics videos to a vector format. Leveraging the advancement &amp; commoditization of Computer Vision, and the ease of running batch computer-vision heavy tasks on the cloud, it's feasible to 'vectorize' large volumes of video at scale now, in a way that wasn't possible even 5 years ago.</p>
<h3 id="vector-graphics-video-format">Vector graphics video format</h3>
<p>We are building a video-format based on existing standards (SVG, WebGL &amp; OpenGL), extending it with Javascript to enable video features such as a timeline and key-frames. We package the resulting video data within an MP4 container, which can be streamed and distributed using existing video infrastructure (such as HLS/DASH, and DRM systems).</p>
<pre><code>&lt;video src="vectorized.mp4" type="video/svg"&gt;
</code></pre>
<p>We are pragmatic, and don't want to create a standard <a href="https://xkcd.com/927/">for the sake of creating a standard</a>.  To that end, we've created libraries and SDKs that enable playback of our vector-graphics videos using standard / native interfaces like so</p>
<pre><code>&lt;script src="vectorly.js"&gt;

&lt;video src="vectorized.mp4" type="video/svg"&gt;
// This will work on all major browsers today
</code></pre>
<h2 id="demos-proof-of-concept">Demos / Proof of concept</h2>
<p><strong>Simpsons</strong></p>
<p>Our first vectorized proof of concept for animations is a 17 second clip of the Simpsons located <a href="https://files.vectorly.io/demo/v0-2-simpsons-250kbps/index.html">here</a>. Keep in mind, our technology is still at a very early stage, and this is much optimization work left to be done.</p>
<p><strong>Khan Academy</strong></p>
<p>Our technology also works very well for e-learning, and especially Khan Academy style content. You can find 30 second Khan Academy clip <a href="https://files.vectorly.io/demo/khan-20kbps/index.html">here</a></p></div></div>]]>
            </description>
            <link>https://vectorly.io/docs/technology/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23895211</guid>
            <pubDate>Mon, 20 Jul 2020 06:30:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[In Praise of ZFS on Linux's ZED 'ZFS Event Daemon']]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 18 (<a href="https://news.ycombinator.com/item?id=23894790">thread link</a>) | @zdw
<br/>
July 19, 2020 | https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSZEDPraise | <a href="https://web.archive.org/web/*/https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSZEDPraise">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>In praise of ZFS On Linux's ZED 'ZFS Event Daemon'</h2>

	<p><small>July 19, 2020</small></p>
</div><div><p>I've written before (<a href="https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSFileserverSameness">here</a>) about how <a href="https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSFileserverSetupIII">our
current Linux ZFS fileservers</a> work much
like <a href="https://utcc.utoronto.ca/~cks/space/blog/solaris/ZFSFileserverSetupII">our old OmniOS fileservers</a>.
However, not everything is quite the same between ZFS on Linux and
traditional Solaris/OmniOS ZFS. One of the most welcome differences
for us is <a href="https://zfsonlinux.org/manpages/0.8.4/man8/zed.8.html">ZED</a>,
the ZFS Event Daemon. What ZED does that is so great is that it provides
a very simple way to take action when <a href="https://zfsonlinux.org/manpages/0.8.4/man5/zfs-events.5.html">ZFS events</a> happen.</p>

<p>When a ZFS event happens, ZED looks through a directory (generally
<code>/etc/zfs/zed.d</code>) to find scripts (or programs) that should be run
in response to the event. Each script is run with a bunch of
environment variables set to describe what's going on, and it can
use those environment variables to figure out what the event is.
ZED decides what things to run based on their names; generally you
wind up with script names like <code>all-cslab.sh</code> (which is run on
all events) and <code>resilver_finish-cslab.sh</code> (which is run when a
resilver finishes).</p>

<p>Because these are just a collection of individual files, you're
free to add your own without colliding with or having to alter the
standard 'ZEDLETs' provided by ZFS on Linux. Your additions can do
anything you want them to, ranging from the simple to the complex.
For instance, our simplest ZEDLET simply syslogs all of the ZED
environment variables:</p>


<blockquote><pre>PATH=/usr/bin:/usr/sbin:/bin:/sbin:$PATH
export PATH
if [ "$ZEVENT_SUBCLASS" = "history_event" ]; then
        exit 0
fi
unset ZEVENT_TIME
unset ZEVENT_TIME_STRING
printenv | fgrep 'ZEVENT_' | sort | fmt -999 |
    logger -p daemon.info -t 'cslab-zevents'
exit 0
</pre>
</blockquote>

<p>(There's a standard 'all-syslog.sh' ZEDLET, but it doesn't syslog
all of the information in the zevents. Capturing all of the information
is especially useful if you want to write additional ZEDLETs and
aren't quite sure what they should look for or what environment
variables have useful information.)</p>

<p>It can take a bit of time and experimentation to sort out what ZFS
events are generated (and with what information available) in
response to various things happening to adn in your ZFS pools. But
once you have figured it out, ZED gives you a way to trigger and
drive all sorts of system management activities. These can be active
(like taking action if devices fail) or passive (like adding markers
in your metrics system or performance dashboards for when ZFS scrubs
or resilvers start and end, so you can correlate this with other
things happening).</p>

<p>Coming from Solaris and OmniOS, where there was no such simple
system for reacting to things happening in your ZFS pools, ZED was
a breath of fresh air for us. More than anything else, it feels
like how ZFS events should have been handled from the start, so
that system administrators could flexibly meet their own local needs
rather than having to accept whatever the Solaris Fault Management
system wanted to give them.</p>

<p>PS: Because ZFS on Linux is now OpenZFS, I believe that ZED will
probably eventually show up in FreeBSD (if it isn't already there).
Perhaps it will even some day be ported back to Illumos.</p>
</div></div>]]>
            </description>
            <link>https://utcc.utoronto.ca/~cks/space/blog/linux/ZFSZEDPraise</link>
            <guid isPermaLink="false">hacker-news-small-sites-23894790</guid>
            <pubDate>Mon, 20 Jul 2020 05:03:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Don't Want to Be a Founder]]>
            </title>
            <description>
<![CDATA[
Score 141 | Comments 111 (<a href="https://news.ycombinator.com/item?id=23894387">thread link</a>) | @kipply
<br/>
July 19, 2020 | https://carolchen.me/blog/founding-bad/ | <a href="https://web.archive.org/web/*/https://carolchen.me/blog/founding-bad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <header>
    
    
  </header>

  <main id="main">
    
<article>
  <header>
    
    
  </header>
  <section id="js-article">
    
<p>I had a brief run with a startup (literally a month) and was faced with the decision of going into a YC Cohort. In that fiasco, I also spent at least twelve hours trying to convince other people to go. It's been half a year, and I've had a lot more time to reflect on reasons as to why one might want to run a startup. As you might've guessed, I decided not to do it and I genuinely believe that running a startup seems like a bad idea for the majority of people I meet who want to become founders. </p>
<p>This post definitely will not apply to everyone (I'd loosely say applicable to 80% of prospective startup founders), but I'd hope there's some valuable thinking in here. Also, note that this is fairly oriented towards technical founders. </p>
<p>Edit: This post also has a very limited scope as it's based off the thinking from my decision to continue interning at Shopify or to go to YC. It's very focused on the Silicon Valley "startup" where you get a VC to give you money and get big in a year, etc etc</p>

<h3 id="commitment">Commitment<a href="#commitment" aria-label="Anchor link for: commitment"> <i></i></a>
</h3>
<p>I'm not talking about commitment to your company. I'm talking about commitment to your cofounder (if you have one, which is likely). </p>
<p>Maybe I'm too young to understand, but marriage seems frightening! My finances, my social life, personal time, and emotional wellbeing would be largely dependent on a single person and that's scary. It should be scary or should at least take a few years for it not to become scary. </p>
<p>Your cofounder is...kind of the same? In a seed-stage it's likely you <em>actually</em> live together, and if you don't, you're likely functionally living together with the amount of work involved. They're responsible for your financial well-being. They may be responsible for the quality of your social lives (most founders spend a lot of time socializing with other tech people + founders). They're tied to your life goals, your dreams, and your passions.</p>
<p>My impression was that my relationship with my cofounder would be more intense than marriage, and <em>extra</em> bad in the event of failure since there's additional loss (and it's statistically likely, but I guess so is marriage). I totally believe that there are cofounder pairs that are completely ready to go through the founder journey and pairs that maybe weren't ready but were fine anyway, but I stand by the statement that it's more intense than marriage and not enough people put care into this. </p>
<h3 id="your-vc-is-not-the-one-at-risk-here">Your VC is Not the One at Risk Here<a href="#your-vc-is-not-the-one-at-risk-here" aria-label="Anchor link for: your-vc-is-not-the-one-at-risk-here"> <i></i></a>
</h3>
<p>I often hear sentiments that resemble "wow these VCs are taking a chance on me I better commit to this!". </p>
<p>VCs are not evil people trying to take advantage of you (actually they might be, but let's assume they're not), but they are not the ones at risk. For them, 150k or a few million is not a huge risk. Seed-stage returns will be from a very small percentage of investments, thus VCs can afford to have comically high error rates as long as they get the few that matter. If you're SoftBank you can do even worse and still have so much money! They make decisions carefully, they care about your success for various reasons, and are generally caring people (in most of my experiences) but in larger abstractions, your startup means nothing to them. </p>
<p>They're not shy about it either, the entire reason they're investing in you is because they think you're more valuable than you cost. 
<img src="https://carolchen.me/blog/img/founding/paul.png" alt=""></p>
<p>Your risk is years of your life, blood, sweat, and tears. The next few years (provided your startup lasts till then) will somewhat be in service to these VCs. The VCs are your "bosses" as you answer to them (though <em>much</em> less than a regular "boss") and to the ones you hope to raise capital from in the future. </p>
<p>It's good to take a risk with increased confidence because qualified people think you have promise. However, that can morph into "I'm going to work on this startup partially in service to these people who believed in me and gave me lots of money". These additional stresses that come from meeting VC expectations and the complications of the dynamics of the relationship can cause various problems.</p>
<h3 id="sense-of-self">Sense of Self<a href="#sense-of-self" aria-label="Anchor link for: sense-of-self"> <i></i></a>
</h3>
<p>This one is the one that got to me most but I can see it being irrelevant to a lot of other people. </p>
<p>Many founders have big egos -- I don't mean they're assholes or overly self-important but they do have very powerful confidence, because that's a valuable skill to have as a founder. Not just confidence in pitching their project to others, but in their vision and their company. They need to believe their company will be successful (though I have met founders who just want to party with VC money for a few years <em>cough cough</em> Neumann). </p>
<p>My first fear was that I created an ego for myself rapidly. Practicing to sell to clients and for your YC interview involves repeating to yourself why you are <em>good</em> and self-hypnosis is fairly powerful. I love feeling good about myself, but I suddenly found myself feeling more confident in myself than what I believed was warranted. More frighteningly, I had a major character and energy change in a couple of weeks. Losing so much of my identity like that was unnerving, not to mention the ripple effects that could've occurred in my social life. </p>
<p>The other fear is coming down from that. Startup founders (especially the more eccentric ones) sometimes believe that they will build something that will change the world. Along with that, their identities start to merge with their company. There's nothing wrong with that, but I also think it's exceptionally tragic to come down from that. It's not just dealing with failure and getting back up on your feet, it's losing a part of your identity. </p>
<h3 id="school-is-generally-a-good-idea-for-prospective-dropouts">School is Generally a Good Idea (for prospective dropouts)<a href="#school-is-generally-a-good-idea-for-prospective-dropouts" aria-label="Anchor link for: school-is-generally-a-good-idea-for-prospective-dropouts"> <i></i></a>
</h3>
<p>Being a good engineer seems underrated for being a good startup founder. Not just being able to code fast, but being able to make good engineering decisions, conduct good technical interviews and attract talent. Some engineering skills can't be worked around with "I am very smart and can learn fast" and require extended time and practice. With that, I also think prospective founders also overestimate the amount of learning on the job that can be done on the engineering side, mostly because it's harder to learn when you're in a rush to release features than if you could take your time on a course project. It's true that founders will learn more than they will in school, but the technical development may not be as strong. My model is that the best schooling experience is better technical education and the best founding experience. </p>

<h3 id="something-to-own">Something to Own<a href="#something-to-own" aria-label="Anchor link for: something-to-own"> <i></i></a>
</h3>
<p>Lots of huge, ground-breaking products have been lead from within a large company. Examples include email client <code>hey.com</code>, Chromebooks, and countless amazing dev tools. </p>
<p>Starting these things in a large company has the benefit of security, resources and recruiting already done for you. Downsides include beaurocracy, not being able to recruit on your own accord and dealing with PR policy. There is also high barriers to starting something within a company, like being senior enough to do so and being at the right company at the right time. </p>
<p>The alternative is starting a project on the side. <a href="https://github.com/ziglang/zig">Ziglang</a> was started as a side project and is now a very promising programming language. The creator has since then left his job to work on Zig, but it is also possible to "own" something significant without even having to leave your job. Examples includes Julia Evan's <a href="https://jvns.ca/">blog</a> (not actually a work-side-project) and line of zines, Cassidy William's <a href="https://drop.com/buy/drop-dsa-astrolokeys-keycaps-by-sailorhg-and-cassidoo">keycap line</a>, Nick Frosst's successful and awesome <a href="https://goodkidofficial.com/">band</a> and many more. I understand that it's not the extent of "oh yeah Google? I built that", but I think the expected value is much higher in creating and owning something that isn't a startup. </p>
<h3 id="getting-rich">Getting Rich<a href="#getting-rich" aria-label="Anchor link for: getting-rich"> <i></i></a>
</h3>
<p>A lot of people claim that startups are less money, but I find for signicant number of founders, that's not true -- not because they'll definitely have a good exit, but because they're skilled in ways that allow them to raise enough money to pay themselves like they would at a big company. If that applies to you, then going to a startup probably is your best shot at getting rich! For other people, the expected value of industry (particularly joining a well-founded early-stage startup) is usually higher. </p>
<h3 id="not-being-at-school-for-prospective-dropouts">Not Being at School (for prospective dropouts)<a href="#not-being-at-school-for-prospective-dropouts" aria-label="Anchor link for: not-being-at-school-for-prospective-dropouts"> <i></i></a>
</h3>
<p>This seems like a valid reasons for the average CS student. School is a place where you answer to professors who don't always understand industry and do homework assignments that no one will care about. However, it seems like all of these problems can be significantly if not fully solved by building a better school experience for yourself. </p>
<p>A better program can improve many things, such as <a href="http://www.olin.edu/">Olin College of Engineering</a> that has a project-based curriculum, <a href="https://www.makeschool.com/">Make School</a> that is a two year applied-engineering degree program or <a href="https://devdegree.ca/">Dev Degree</a>, where you can work at Shopify and take more applied courses taught by Shopify throughout your degree. These programs are small and selective, but probably not harder than a semi-successful startup. Dev Degree also happens to be more financially sound, with Shopify paying for your tuition and a salary, and Make School tuition is 70k for the entire degree. </p>
<p>Another alternative is to just be worse at school and learn on the side and/or to morph your silly school assignments into productive skills and useful outputs. The <a href="http://coconut-lang.org/">Coconut Programming Language</a> was built by someone while they were in school. Some things like dynamic programming that are often deemed useless theoretical things can have <a href="https://thume.ca/2017/06/17/tree-diffing/">industry applications</a>. People have also taken mundane school projects like this compiler that almost every school will have you build in a compilers course and end up with <a href="https://thume.ca/2019/04/29/comparing-compilers-in-rust-haskell-c-and-python/">educational findings for engineers in general</a> (also see <a href="https://news.ycombinator.com/item?id=20192645">HackerNews thread</a>). In five weeks, my friend Maas was able to launch <a href="https://medium.com/@maaslalani/launch-5d02cc5e05f5">five relatively successful products</a> while enrolled in Dev Degree. </p>
<p>School is already a powerful environment of hardwork, fun and learning. I think it is a more cohesive …</p></section></article></main></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://carolchen.me/blog/founding-bad/">https://carolchen.me/blog/founding-bad/</a></em></p>]]>
            </description>
            <link>https://carolchen.me/blog/founding-bad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23894387</guid>
            <pubDate>Mon, 20 Jul 2020 03:21:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Shall the West Pass Too? Whispers from Fallen Civilisations]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 84 (<a href="https://news.ycombinator.com/item?id=23889763">thread link</a>) | @sheefrex
<br/>
July 19, 2020 | https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations | <a href="https://web.archive.org/web/*/https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" id="item-5f12bf74664501412d3f65f4"><div><div><div data-block-type="2" id="block-a407adb7a170c202cb65"><div><h3>1.1   Spengler’s Prediction</h3><p>Is it the destiny of the West to die? For Oswald Spengler, the answer was yes. In <em>The Decline of the West </em>he argued that all civilisations go through a similar life-cycle. According to Spengler, the Medieval Era was the spring of the West, the Renaissance its summer, and the Baroque era its autumn. He predicted that the West would enter its Winter around the year 2000, which would be characterised by a decline of democracy due to excessive influence from moneyed interests, and a resultant rise of authoritarianism.</p><p>This prophecy is quickly being fulfilled. The immense power of the financial sector was mobilised to loosen financial regulation, the ultimate result being the financial crisis of 2007-2008. The ensuing austerity sparked the rise of anti-establishment movements with authoritarian tendencies. In countries such as Italy and Hungary, such parties were or are in power.</p><p>The power of money seems set to remain strong in the West. The conditions which necessitated huge bank bailouts have not been eliminated. The revolving door means that many politicians and regulators are incentivised to serve banks while in office in return for very lucrative jobs afterwards. Typical examples of this include former US Treasury Secretary Timothy Geithner, former President of the Bundesbank Axel Weber, former head of the Swiss central bank Philipp Hildebrand, and former German Finance Minister Peer Steinbrück, all of whom earned millions from the financial sector during or after their positions in the public sector.</p><p>Since Spengler made his prophecy, the number of threats have multiplied beyond even what he foresaw. In a study of the collapse of the Bronze Age civilisation around 1177 BC, the historian Eric Cline revealed the true horsemen of collapse as being climate change, famines, droughts, earthquakes, rebellions, and mass migration.[1] Any one of these challenges could have been surmountable, but the confluence was fatal. Some are reappearing today, with the pandemic as a new recruit.</p><p>Climate change was the first to manifest. The fact that the ice shelves have begun to splinter is, for now, one worry among many. Of much greater concern are ever more extreme weather events, namely floods and droughts, that strike previously temperate regions. The final result of this could very well be the desertification of large parts of the United States and Southern Europe.[2]</p><p>Reducing our consumption of carbon fuels will not stop this threat. Europe has reduced CO2 emissions by 22 percent since 1990 [3], but the rest of the world, and especially China, continues to burn fossil fuels. According to Vaclav Smil, fossil fuels still supply 90 percent of global primary energy, a greater share than in 2000 when hydropower and nuclear energy were proportionately more widely used.[4] In fact, the success of Europe to date in containing rising emissions may be due to the fact that much energy-intensive manufacturing has already been sent abroad.</p><p>The oceans have also become more hostile to life. They have already begun to acidify due to their absorption of over 20 percent of the increased carbon dioxide in the air. This along with the higher temperature has pushed entire ecosystems such as the coral reefs into collapse, endangering the food supply of the one billion people who rely on the oceans for nourishment.</p><p>It should be no surprise then that the sixth mass extinction is now unfolding. Researchers, looking at 177 mammals for which they had detailed data, found that all have lost at least 30 percent of their geographic range and over 40 percent have undergone sharp population declines.[5]</p><p>Bee and insect populations are collapsing, along with the bird populations who rely on them for food. In France, countryside bird populations have fallen by a third in 15 years.[6] In remote Swiss mountains, plastic has even permeated the soil.[7] Given the complex nature of the ecosystem, the effects of all this are unpredictable.</p><p>Such environmental chaos could force migration, as much of Africa is highly dependent on agriculture for employment and well-being — and this is the sector most exposed to climate change. This is before we even take into account UN projections, according to which the population of Africa is forecast to increase from 1.2bn today to 2.5bn in 2050 and 4.4bn in 2100, at the same time climate change could make much of that continent uninhabitable.</p><p>This will occur in a context where youth bulges could very well lead to civil strife. According to German sociologist Gunnar Heinsohn, violence is inevitable when those aged fifteen to thirty comprise over 30 percent of the male population.[8] Similarly, others have found that a majority of conflict in previous decades started in countries where 60 percent of the population is under the age of thirty.[9]&nbsp;In <em>The Clash of Civilizations</em>, Samuel Huntington pointed to demographic trends to predict that many North African and Middle Eastern countries would be unstable today, a prediction borne out by the Arab Spring.[10]</p><p>Such instability be a particular challenge at a time of reduced integration and increased social stratification; integration of migrants is pereived to have failed in some Western European countries, while members of the working class are withdrawing their loyalty from governing elites. Toynbee’s framework indicates that both phenomena may have a common cause.</p><h2>1.2   Toynbee’s Thoughts</h2><p>Arnold Toynbee, a British historian who wrote twelve volumes entitled <em>A Study of History</em>, sought to understand the factors underlying civilisational growth and decline. Toynbee’s principal thesis was that a decline in creativity among the elites precipitates the breakdown of a civilisation. Their inability to devise solutions to the problems of the time leads the masses to cease their deferral to them.</p><p>And who could blame them for this in today’s world? A cursory glance in a newsagent tells us who the dominant cultural figures are: celebrities, sports stars, and perhaps the occasional politician. Yet what are the meaningful achievements of these groups? What risks do they take?</p><p>In a previous age, the leaders of societies literally put their lives on the line for their countries – think of de Gaulle, Churchill and most of the governing classes of the post-War period who had been involved in one if not two wars. This provided a powerful moral authority which today’s leaders lack. Moreover, the leaders of the nineteenth and twentieth centuries wrought the national and international institutions that govern us today – a creative triumph that eludes current leaders.</p><p>Due to numerous scandals, religious figures who once would have been venerated are now mostly disgraced. As for modern celebrities, while many are simply too vapid to be worth imitating, this does not stop the vapid from doing exactly that — to the delight of all manner of brands for whom the celebrity becomes a purchasable ambassador.</p><p>The decline of creativity is also evident in the world of fashion. As Kurt Andersen noted in an essay in <em>Vanity Fair</em>, fashion used to go through radical changes.[11] Every ten years from the 30s to the 90s, style changed radically. It has since stagnated. Taking a longer historical perspective, the dominance of the business suit since the thirties is anomalous, given that for centuries formal wear underwent radical changes relatively frequently.</p><p>How did this creative decline come about? Toynbee noted that once the masses cease to mimic the elite, the elite begins to mimic the masses in an attempt to gain popularity. This process, which Toynbee termed proletarianisation, is already quite advanced: consider the carefully calibrated way in which politicians seek to echo the opinions and language of the electorate. A typical example of this was when former British Prime Minister David Cameron forgot which football team he was meant to support.[12] To this theatre we can add the rise and now ubiquity of profanity, and the decline of formal dress. Perhaps this proletarianisation is the root of the stagnation in fashion: the elites mimic the masses, who are in general less likely to wear something radically different for fear of ridicule.</p><p>At the same time that the achievements of the elites are becoming less impressive, their failings are becoming more visible. Martin Gurri, in his book <em>The Revolt of the Public</em>, identified the internet as a key factor behind the diminishing credibility of elites.[13] The explosion of information has undermined traditional hierarchies, which formerly relied on control of information in order to hide their incompetence and thus preserve their legitimacy.</p><p>These multiple challenges could be tamed in a world where the nations of the West stood strong. Unfortunately, they continue to weaken. One example is the high level of debt in many Western countries, and their inability to stop borrowing. There was no clear plan to reduce such debt levels before the Coronavirus, and now the load has weightened significantly. As such, the West remains prone to a major financial crisis in the event a large economy, such as Italy, slips into bankruptcy.</p><p>As argued by Laurence Kotlikoff, professor of economics at Boston University, high levels of government debt will be compounded by extravagant commitments governments have made on pensions and healthcare. These unfunded liabilities are considerable and much greater than the official levels of public debt. In the case of the US, while public debt is around one hundred percent of GDP, the amount of unfunded liabilities is twelve times greater. The equivalent in the UK and the Netherlands is about five times annual GDP, while in France, Germany and Italy it does not exceed a multiple of two.[14]</p><p>In many ways the current difficulties faced by the West are a consequence of ageing: the debt crisis, low economic growth, and low interest rates all arise from a greying society. In Europe, the ageing German population needed to save a vast amount of money at …</p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations">https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations</a></em></p>]]>
            </description>
            <link>https://www.themetasophist.com/chapter/whispers-from-fallen-civilisations</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889763</guid>
            <pubDate>Sun, 19 Jul 2020 14:56:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fixing Mass Effect black blobs on modern AMD CPUs]]>
            </title>
            <description>
<![CDATA[
Score 496 | Comments 152 (<a href="https://news.ycombinator.com/item?id=23889473">thread link</a>) | @Macha
<br/>
July 19, 2020 | https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/ | <a href="https://web.archive.org/web/*/https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
  
    <p><em>TL;DR - if you are not interested in an in-depth overview of what was wrong with the game and how it was fixed,
scroll down to <a href="#download"><strong>Download</strong></a> section for a download link.</em></p>

<hr>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#part-1">Part 1 – Research</a>
    <ul>
      <li><a href="#prelude">Prelude</a></li>
      <li><a href="#pix">PIX</a></li>
    </ul>
  </li>
  <li><a href="#part-2">Part 2 – A closer look into D3DX</a></li>
  <li><a href="#part-3">Part 3 – Standalone tests</a></li>
  <li><a href="#part-4">Part 4 – Putting it all together</a>
    <ul>
      <li><a href="#download">Download</a></li>
    </ul>
  </li>
</ul>

<hr>



<p><strong>Mass Effect</strong> is a popular franchise of sci-fi roleplaying games. The first game was initially released by BioWare in late 2007 on Xbox 360 exclusively as a part of a publishing deal with Microsoft.
A few months later in mid-2008, the game received PC port developed by Demiurge Studios. It was a decent port with no obvious flaws, that is until 2011 when AMD released their new Bulldozer-based CPUs.
When playing the game on PCs with modern AMD processors, two areas in the game (Noveria and Ilos) show severe graphical artifacts:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-blobs.jpg"><br>
<em>Well, that doesn't look nice.</em>
</p>

<p>While not unplayable, it’s definitely distracting. Thankfully, workarounds exist – such as
<a href="http://abesmissioncontrol.blogspot.com/2015/04/mass-effect-fixing-blocky-player-models.html">disabling lighting via console commands</a>
or <a href="https://www.nexusmods.com/masseffect/mods/181">modifying the game’s maps to remove broken lights</a>, but seemingly the issue has never been fully understood.
Some sources claim that an FPS Counter mod can also fix that issue, but I couldn’t find much information about it and the mod’s sources don’t seem to be available online,
and there is no documentation on how the mod tackles this error.</p>

<p>What makes this issue particularly interesting? Vendor-specific bugs are nothing new, and games have had them for decades. However, to my best knowledge, this is the only case where a graphical
issue is caused by a <strong>processor</strong> and not by a graphics card. In the majority of cases, issues happen with a specific vendor of GPU and they don’t care about the CPU, while in this case, it’s the exact opposite.
This makes the issue very unique and worth looking into.</p>

<p>Looking up existing discussions online, this issue seems to affect AMD FX and Ryzen chips. Compared to the older AMD chips, these lack a <a href="https://en.wikipedia.org/wiki/3DNow!">3DNow! instruction set</a>.
Unrelated or not, the community consensus was that this was the cause of the bug and that the game tried to use those instructions upon detecting an AMD CPU.
Given that there are no known cases of this bug occurring on Intel CPU’s and that 3DNow! instructions were exclusive to AMD, it’s no surprise the community assumed that this is the issue.</p>

<p>Is this really the issue, or is it caused by something entirely different? Let’s find out!</p>



<h2 id="prelude">Prelude</h2>
<p>Even though the issue is trivial to reproduce, I couldn’t look into it for the longest time for a simple reason – I don’t have access to any PCs with AMD hardware!
Thankfully, this time I’m not approaching research alone – <a href="https://withinrafael.com/">Rafael Rivera</a> got my back during the entire process of R&amp;D,
providing a test environment with an AMD chip, insights, ideas as well as putting up with hundreds of blind guesses I usually throw around when trying to find the way to the root of such unknown problems.</p>

<p>Since we now had a good testing environment, the first theory to test was of course <code>cpuid</code> – if people are right in assuming that 3DNow! instructions are to blame, there should a place in the game’s code
where they check for their presence, or at the very least check for the CPU vendor. That reasoning is flawed, though; if it was true that the game attempts to use 3DNow! instructions any time it runs on an AMD chip,
without checking if they are supported, the game would most likely crash when trying to execute an illegal instruction. Moreover, a quick scan around the game’s code reveals that the game <strong>doesn’t</strong>
check for CPU capabilities. Therefore, whatever is up with this issue, it doesn’t appear to be caused by the game mis-detecting CPU features, because it seemingly doesn’t care about them in the first place.</p>

<p>When this started looking like an undebuggable case, Rafael came back to me with a realization – disabling <strong>PSGP</strong> (Processor Specific Graphics Pipeline) fixes the issue and the characters are properly lit!
PSGP is not the best documented term, but in short, it’s a legacy (concerning only older DirectX versions) feature allowing Direct3D to perform processor-specific optimizations:</p>

<blockquote>
  <p>In previous versions of DirectX, there was a path that allowed to do vertex processing called the PSGP. Applications had to take this path into account and support a path for vertex processing
on the processor and graphics cores.</p>
</blockquote>

<p>Putting it this way, it makes sense why disabling PSGP fixes artifacts on AMD – the path taken by modern AMD processors may be somehow broken.
How to disable it? Two ways come to mind:</p>
<ul>
  <li>It is possible to pass a <code>D3DCREATE_DISABLE_PSGP_THREADING</code> flag to <code>IDirect3D9::CreateDevice</code>. It’s defined as: <br>
    <blockquote>
      <p>Restrict computation to the main application thread. If the flag is not set, the runtime may perform software vertex processing and other computations in worker thread
to improve performance on multi-processor systems.</p>
    </blockquote>

    <p>Sadly, setting that flag doesn’t fix the issue. Looks like, despite the flag having “PSGP” in name, it’s not what we are looking for.</p>
  </li>
  <li>DirectX specifies two registry entries to disable PSGP in D3D and to disable PSGP only for D3DX – <code>DisablePSGP</code> and <code>DisableD3DXPSGP</code>. Those flags can be set system-wide or process-wide.
For information on how to set them only for a specific process, see <a href="https://withinrafael.com/2020/07/11/specify-application-specific-direct3d-flags/">Rafael Rivera’s guide on enabling application-specific Direct3D flags</a>.</li>
</ul>

<p><code>DisableD3DXPSGP</code> appears to be a viable fix for that issue. Therefore, if you have an aversion towards downloading third party fixes/modifications or you must fix this issue without making
any changes to the game, it’s a perfectly fine way of doing it. As long as you set that flag only for Mass Effect and not system-wide, it’s fine!</p>

<h2 id="pix">PIX</h2>
<p>As always with graphical issues, PIX is likely the most useful tool one could use to diagnose them. We captured similar scenes from Intel and AMD hardware and compared the results.
One difference was instantly noticeable – unlike with my past projects, where <a href="https://cookieplmonster.github.io/2018/07/07/farcry-d3d9-bug/">captures did not carry the bug with them</a> and the same capture
would look different on different PCs (indicating a driver or d3d9.dll bug), these captures carry the bug with them! In other words, a capture from an AMD hardware opened on a PC with Intel hardware
<strong>does</strong> show the bug.</p>

<p>An AMD capture on Intel looks no different than on the hardware it was taken from:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-pix1.jpg">
</p>

<p>What does this tell us?</p>
<ul>
  <li>Since PIX does not “take screenshots” but instead captures the sequence of D3D commands and executes them on hardware, we can observe that executing the commands captured from an AMD box
results in the same bug when executed on Intel.</li>
  <li>This strongly implies that the difference is not caused by the difference in <strong>how</strong> the commands are executed (that’s how you get GPU specific bugs), but <strong>what</strong> commands are executed.</li>
</ul>

<p>In other words, it’s almost certainly not any sort of a driver bug. Instead, the way inputs for the GPU are prepared seems to be somehow broken<sup id="fnref:1"><a href="#fn:1">1</a></sup>. That is indeed a very rare occurrence!</p>

<p>At this point, finding the bug is a matter of finding any jarring differences between captures. It’s tedious, but that’s the only viable way.</p>

<p>After a long while spent poking the capture, a full body draw call caught my attention:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-pix2.jpg">
</p>

<p>On an Intel capture, this draw outputs most of the character’s body, together with lighting and textures. On an AMD capture, it outputs a plain black model. This looks like a good trail.</p>

<p>The first obvious candidate for checking would be bound textures, but they seem to be fine and are consistent across captures.
However, some of the pixel shader constants looked weird. Not only do they have NaNs (Not a Number), but they also seem to only appear on the AMD capture and not the Intel capture:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-pix3.jpg"><br>
<em>1.#QO indicates a NaN</em>
</p>

<p>This looks promising – NaN values causing strange visuals are not unheard of. Funnily enough, a PlayStation 3 version of Mass Effect 2
<a href="https://github.com/RPCS3/rpcs3/issues/7397">had a very similar looking issue in RPCS3</a> which was also related to NaNs!</p>

<p>However, before we get too excited, those values could just be leftovers from previous draws and they might end up being unused for the current draw.
Luckily, in this case it’s clearly visible that those NaNs get submitted to D3D for this specific draw…</p>

<div><div><pre><code>49652	IDirect3DDevice9::SetVertexShaderConstantF(230, 0x3017FC90, 4)
49653	IDirect3DDevice9::SetVertexShaderConstantF(234, 0x3017FCD0, 3)
49654	IDirect3DDevice9::SetPixelShaderConstantF(10, 0x3017F9D4, 1) // Submits constant c10
49655	IDirect3DDevice9::SetPixelShaderConstantF(11, 0x3017F9C4, 1) // Submits constant c11
49656	IDirect3DDevice9::SetRenderState(D3DRS_FILLMODE, D3DFILL_SOLID)
49657	IDirect3DDevice9::SetRenderState(D3DRS_CULLMODE, D3DCULL_CW)
49658	IDirect3DDevice9::SetRenderState(D3DRS_DEPTHBIAS, 0.000f)
49659	IDirect3DDevice9::SetRenderState(D3DRS_SLOPESCALEDEPTHBIAS, 0.000f)
49660	IDirect3DDevice9::TestCooperativeLevel()
49661	IDirect3DDevice9::SetIndices(0x296A5770)
49662	IDirect3DDevice9::DrawIndexedPrimitive(D3DPT_TRIANGLELIST, 0, 0, 2225, 0, 3484) // Draws the character model
</code></pre></div></div>

<p>…and the pixel shader used for this draw references both constants:</p>
<div><div><pre><code>// Registers:
//
//   Name                     Reg   Size
//   ------------------------ ----- ----
//   UpperSkyColor            c10      1
//   LowerSkyColor            c11      1
</code></pre></div></div>

<p>Both constants appear to <a href="https://github.com/abaelhe/unrealengine-old/search?q=UpperSkyColor">come straight from Unreal Engine</a> and judging by the name,
they might directly influence the lighting. Bingo!</p>

<p>A quick in-game test further confirms the theory – on an Intel machine, a vector of 4 NaN values was never submitted as pixel shader constants;
meanwhile, on an AMD machine, NaNs would start showing up as soon as the player entered the area where lighting breaks!</p>

<p>Does it mean work is done? No, far from it, as finding broken constants is only half of the success. The question remains, where do they come from, and can they be replaced?
An in-game test replacing NaN values with zeros partially fixed the issue – ugly black blobs disappeared, but characters were still way too dark:</p>

<p>
<img src="https://cookieplmonster.github.io/assets/img/posts/mass-effect/me1-dark-lighting.jpg"><br>
<em>Almost correct... …</em></p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/">https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/</a></em></p>]]>
            </description>
            <link>https://cookieplmonster.github.io/2020/07/19/silentpatch-mass-effect/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889473</guid>
            <pubDate>Sun, 19 Jul 2020 14:08:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clear explanation of Rust’s module system]]>
            </title>
            <description>
<![CDATA[
Score 151 | Comments 42 (<a href="https://news.ycombinator.com/item?id=23889427">thread link</a>) | @rkwz
<br/>
July 19, 2020 | http://www.sheshbabu.com/posts/rust-module-system/ | <a href="https://web.archive.org/web/*/http://www.sheshbabu.com/posts/rust-module-system/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Rust’s module system is surprisingly confusing and causes a lot of frustration for beginners.</p>
<p>In this post, I’ll explain the module system using practical examples so you get a clear understanding of how it works and can immediately start applying this in your projects.</p>
<p>Since Rust’s module system is quite unique, I request the reader to read this post with an open mind and resist comparing it with how modules work in other languages.</p>
<p>Let’s use this file structure to simulate a real world project:</p>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  ├── config.rs
  ├─┬ routes
  │ ├── health_route.rs
  │ └── user_route.rs
  └─┬ models
    └── user_model.rs</code></pre>
<p>These are the different ways we should be able to consume our modules:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-1.png" alt=""></p>
<p>These 3 examples should be sufficient to explain how Rust’s module system works.</p>
<h2 id="Example-1"><a href="#Example-1" title="Example 1"></a>Example 1</h2><p>Let’s start with the first example - importing <code>config.rs</code> in <code>main.rs</code>.</p>
<pre><code>
<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_config</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"config"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>The first mistake that everyone makes is just because we have files like <code>config.rs</code>, <code>health_route.rs</code> etc, we think that these files are <code>modules</code> and we can import them from other files.</p>
<p>Here’s what we see (file system tree) and what the compiler sees (module tree):</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-2.png" alt=""></p>
<p>Surprisingly, the compiler only sees the <code>crate</code> module which is our <code>main.rs</code> file. This is because we need to explicitly build the module tree in Rust - there’s no implicit mapping between file system tree to module tree.</p>
<blockquote>
<p>We need to explicitly build the module tree in Rust, there’s no implicit mapping to file system</p>
</blockquote>
<p>To add a file to the module tree, we need to declare that file as a submodule using the <code>mod</code> keyword. The next thing that confuses people is that you would assume we declare a file as module in the same file. But we need to declare this in a different file! Since we only have <code>main.rs</code> in the module tree, let’s declare <code>config.rs</code> as a submodule in <code>main.rs</code>.</p>
<blockquote>
<p>The mod keyword declares a submodule</p>
</blockquote>
<p>The <code>mod</code> keyword has this syntax:</p>
<pre><code><span>mod</span> my_module<span>;</span></code></pre>
<p>Here, the compiler looks for <code>my_module.rs</code> or <code>my_module/mod.rs</code> in the same directory.</p>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  └── my_module.rs

or

my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  └─┬ my_module
    └── mod.rs</code></pre>
<p>Since <code>main.rs</code> and <code>config.rs</code> are in the same directory, let’s declare the config module as follows:</p>
<pre><code>// main.rs
<span>+ mod config;</span>

fn main() {
<span>+ config::print_config();</span>
  println!("main");
}</code></pre>
<pre><code>
<span>fn</span> <span>print_config</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"config"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>We’re accessing the <code>print_config</code> function using the <code>::</code> syntax.</p>
<p>Here’s how the module tree looks like:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-3.png" alt=""></p>
<p>We’ve successfully declared the <code>config</code> module! But this is not sufficient to be able to call the <code>print_config</code> function inside <code>config.rs</code>. Almost everything in Rust is private by default, we need to make the function public using the <code>pub</code> keyword:</p>
<blockquote>
<p>The pub keyword makes things public</p>
</blockquote>
<pre><code>
<span>mod</span> config<span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  config<span>:</span><span>:</span><span>print_config</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>// config.rs
<span>- fn print_config() {</span>
<span>+ pub fn print_config() {</span>
  println!("config");
}</code></pre>
<p>Now, this works. We’ve successfully called a function defined in a different file!</p>
<h2 id="Example-2"><a href="#Example-2" title="Example 2"></a>Example 2</h2><p>Let’s try calling the <code>print_health_route</code> function defined in <code>routes/health_route.rs</code> from <code>main.rs</code>.</p>
<pre><code>
<span>mod</span> config<span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  config<span>:</span><span>:</span><span>print_config</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_health_route</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"health_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>As we discussed earlier, we can use the <code>mod</code> keyword only for <code>my_module.rs</code> or <code>my_module/mod.rs</code> in the same directory.</p>
<p>So in order to call functions inside <code>routes/health_route.rs</code> from <code>main.rs</code>, we need to do the following things:</p>
<ul>
<li>Create a file named <code>routes/mod.rs</code> and declare the <code>routes</code> submodule in <code>main.rs</code></li>
<li>Declare the <code>health_route</code> submodule in <code>routes/mod.rs</code> and make it public</li>
<li>Make the functions inside <code>health_route.rs</code> public</li>
</ul>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  ├── config.rs
  ├─┬ routes
<span>+ │ ├── mod.rs</span>
  │ ├── health_route.rs
  │ └── user_route.rs
  └─┬ models
    └── user_model.rs</code></pre>
<pre><code>// main.rs
mod config;
<span>+ mod routes;</span>

fn main() {
<span>+ routes::health_route::print_health_route();</span>
  config::print_config();
  println!("main");
}</code></pre>
<pre><code>// routes/mod.rs
<span>+ pub mod health_route;</span></code></pre>
<pre><code>// routes/health_route.rs
<span>- fn print_health_route() {</span>
<span>+ pub fn print_health_route() {</span>
  println!("health_route");
}</code></pre>
<p>Here’s how the module tree looks like:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-4.png" alt=""></p>
<p>We can now call a function defined in a file inside a folder.</p>
<h2 id="Example-3"><a href="#Example-3" title="Example 3"></a>Example 3</h2><p>Let’s try calling from <code>main.rs =&gt; routes/user_route.rs =&gt; models/user_model.rs</code></p>
<pre><code>
<span>mod</span> config<span>;</span>
<span>mod</span> routes<span>;</span>

<span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  routes<span>:</span><span>:</span>health_route<span>:</span><span>:</span><span>print_health_route</span><span>(</span><span>)</span><span>;</span>
  config<span>:</span><span>:</span><span>print_config</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"main"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<pre><code>
<span>fn</span> <span>print_user_model</span><span>(</span><span>)</span> <span>{</span>
  <span>println!</span><span>(</span><span>"user_model"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>We want to call the function <code>print_user_model</code> from <code>print_user_route</code> from <code>main</code>.</p>
<p>Let’s make the same changes as before - declaring submodules, making functions public and adding the <code>mod.rs</code> file.</p>
<pre><code>my_project
├── Cargo.toml
└─┬ src
  ├── main.rs
  ├── config.rs
  ├─┬ routes
  │ ├── mod.rs
  │ ├── health_route.rs
  │ └── user_route.rs
  └─┬ models
<span>+   ├── mod.rs</span>
    └── user_model.rs</code></pre>
<pre><code>// main.rs
mod config;
mod routes;
<span>+ mod models;</span>

fn main() {
  routes::health_route::print_health_route();
<span>+ routes::user_route::print_user_route();</span>
  config::print_config();
  println!("main");
}</code></pre>
<pre><code>// routes/mod.rs
pub mod health_route;
<span>+ pub mod user_route;</span></code></pre>
<pre><code>// routes/user_route.rs
<span>- fn print_user_route() {</span>
<span>+ pub fn print_user_route() {</span>
  println!("user_route");
}</code></pre>
<pre><code>// models/mod.rs
<span>+ pub mod user_model;</span></code></pre>
<pre><code>// models/user_model.rs
<span>- fn print_user_model() {</span>
<span>+ pub fn print_user_model() {</span>
  println!("user_model");
}</code></pre>
<p>Here’s how the module tree looks like:</p>
<p><img src="http://www.sheshbabu.com/images/2020-rust-module-system/rust-module-system-5.png" alt=""></p>
<p>Wait, we haven’t actually called <code>print_user_model</code> from <code>print_user_route</code>! So far, we’ve only called the functions defined in other modules from <code>main.rs</code>, how do we do that from other files?</p>
<p>If we look at our module tree, the <code>print_user_model</code> function sits in the <code>crate::models::user_model</code> path. So in order to use a module in files that are not <code>main.rs</code>, we should think in terms of the path necessary to reach that module in the module tree.</p>
<pre><code>// routes/user_route.rs
pub fn print_user_route() {
<span>+ crate::models::user_model::print_user_model();</span>
  println!("user_route");
}</code></pre>
<p>We’ve successfully called a function defined in a file from a file that’s not <code>main.rs</code>.</p>
<h2 id="super"><a href="#super" title="super"></a>super</h2><p>The fully qualified name gets too lengthy if our file organization is multiple directories deep. Let’s say for whatever reason, we want to call <code>print_health_route</code> from <code>print_user_route</code>. These are under the paths <code>crate::routes::health_route</code> and <code>crate::routes::user_route</code> respectively.</p>
<p>We can call it by using the fully qualified name <code>crate::routes::health_route::print_health_route()</code> but we can also use a relative path <code>super::health_route::print_health_route();</code>. Notice that we’ve used <code>super</code> to refer to the parent scope.</p>
<blockquote>
<p>The super keyword in module path refers to the parent scope</p>
</blockquote>
<pre><code><span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>crate</span><span>:</span><span>:</span>routes<span>:</span><span>:</span>health_route<span>:</span><span>:</span><span>print_health_route</span><span>(</span><span>)</span><span>;</span>
  
  <span>super</span><span>:</span><span>:</span>health_route<span>:</span><span>:</span><span>print_health_route</span><span>(</span><span>)</span><span>;</span>

  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<h2 id="use"><a href="#use" title="use"></a>use</h2><p>It would be tedious to use the fully qualified name or even the relative name in the above examples. In order to shorten the names, we can use the <code>use</code> keyword to bind the path to a new name or alias.</p>
<blockquote>
<p>The use keyword is used to shorten the module path</p>
</blockquote>
<pre><code><span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>crate</span><span>:</span><span>:</span>models<span>:</span><span>:</span>user_model<span>:</span><span>:</span><span>print_user_model</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>The above code can be refactored as:</p>
<pre><code><span>use</span> <span>crate</span><span>:</span><span>:</span>models<span>:</span><span>:</span>user_model<span>:</span><span>:</span>print_user_model<span>;</span>

<span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>print_user_model</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>Instead of using the name <code>print_user_model</code>, we can also alias it to something else:</p>
<pre><code><span>use</span> <span>crate</span><span>:</span><span>:</span>models<span>:</span><span>:</span>user_model<span>:</span><span>:</span>print_user_model <span>as</span> log_user_model<span>;</span>

<span>pub</span> <span>fn</span> <span>print_user_route</span><span>(</span><span>)</span> <span>{</span>
  <span>log_user_model</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"user_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<h2 id="External-modules"><a href="#External-modules" title="External modules"></a>External modules</h2><p>Dependencies added to <code>Cargo.toml</code> are available globally to all modules inside the project. We don’t need to explicitly import or declare anything to use a dependency.</p>
<blockquote>
<p>External dependencies are globally available to all modules inside a project</p>
</blockquote>
<p>For example, let’s say we added the <a href="https://crates.io/crates/rand" target="_blank" rel="noopener">rand</a> crate to our project. We can use it in our code directly as:</p>
<pre><code><span>pub</span> <span>fn</span> <span>print_health_route</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> random_number<span>:</span> u8 <span>=</span> rand<span>:</span><span>:</span><span>random</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> random_number<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"health_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<p>We can also use <code>use</code> to shorten the path:</p>
<pre><code><span>use</span> rand<span>:</span><span>:</span>random<span>;</span>

<span>pub</span> <span>fn</span> <span>print_health_route</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> random_number<span>:</span> u8 <span>=</span> <span>random</span><span>(</span><span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> random_number<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"health_route"</span><span>)</span><span>;</span>
<span>}</span></code></pre>
<h2 id="Summary"><a href="#Summary" title="Summary"></a>Summary</h2><ul>
<li>The module system is explicit - there’s no 1:1 mapping with file system</li>
<li>We declare a file as module in its parent, not in itself</li>
<li>The <code>mod</code> keyword is used to declare submodules</li>
<li>We need to explicitly declare functions, structs etc as public so they can be consumed in other modules</li>
<li>The <code>pub</code> keyword makes things public</li>
<li>The <code>use</code> keyword is used to shorten the module path</li>
<li>We don’t need to explicitly declare 3rd party modules</li>
</ul>
<p>Thanks for reading! Feel free to follow me in <a href="https://twitter.com/sheshbabu" target="_blank" rel="noopener">Twitter</a> for more posts like this :)</p>
</div></div>]]>
            </description>
            <link>http://www.sheshbabu.com/posts/rust-module-system/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23889427</guid>
            <pubDate>Sun, 19 Jul 2020 14:01:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Worms Armageddon 3.8]]>
            </title>
            <description>
<![CDATA[
Score 405 | Comments 122 (<a href="https://news.ycombinator.com/item?id=23888870">thread link</a>) | @typh00n
<br/>
July 19, 2020 | https://worms2d.info/Worms_Armageddon_3.8_Features | <a href="https://web.archive.org/web/*/https://worms2d.info/Worms_Armageddon_3.8_Features">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr">
<center><div><div><div><p><a href="https://wormtube.worms2d.info/vids/WormsArmageddon-TrailerVideo2020-SteamReady.mp4"><img alt="" src="https://worms2d.info/images/8/82/38_trailer_screenshot.jpg" width="640" height="360"></a></p><div><p>Click above to watch the new trailer in full quality!</p></div></div></div></div></center>
<div>
<p>Worms Armageddon version 3.8 is finally released! It's been a long time coming, and we hope you'll enjoy all the features, changes, and bug fixes that made it into this community-made update. To whet your appetite, here are some change highlights:
</p>
<div><div><p><a href="https://worms2d.info/File:ESOButton2.png"><img alt="ESOButton2.png" src="https://worms2d.info/images/0/04/ESOButton2.png" width="146" height="82"></a></p></div></div>
<h3><span id="Extended_Scheme_Options">Extended Scheme Options</span></h3>
<p>Customise your matches in weird and wonderful ways with over 70 new scheme options. Manipulate physics, fire more than one weapon in a turn, have worms bounce around the landscape, or choose from a huge array of other tweaks in order to perfect your scheme. These options are only for the seasoned Worms veteran, and as such you will need to have completed the Single Player mode and unlocked The Full Wormage in order to access them.</p>
<br><center><a href="https://worms2d.info/File:ESO1.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/9/91/ESO1.gif" width="180" height="180"></a> <a href="https://worms2d.info/File:ESO2.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/d/df/ESO2.gif" width="180" height="180"></a> <a href="https://worms2d.info/File:ESO3.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/5/59/ESO3.gif" width="180" height="180"></a> <a href="https://worms2d.info/File:ESO4.gif" title="inline"><img alt="inline" src="https://worms2d.info/images/4/45/ESO4.gif" width="180" height="180"></a></center>
<p>For those who used the RubberWorm module in older versions of the game, all features formerly accessed through that module are now hiding behind the shiny new Extended Scheme Options star button. Relive your past glories — all your old RubberWorm recorded games can be played back with no additional software required.
</p>

<div><div><p><a href="https://worms2d.info/File:Tweening.gif"><img alt="" src="https://worms2d.info/images/5/53/Tweening.gif" width="300" height="240"></a></p><div><p>Demonstration of Tweening</p></div></div></div>
<h3><span id="Tweening">Tweening</span></h3>
<p>Worms Armageddon has never looked this smooth! In older versions, in-game movement and animations would be capped at the update rate of the game engine: 50 frames per second. In 3.8 though, experience the full silky smoothness of whatever frame rate your hardware can support — movement is now visually interpolated between game engine frames as your worms and weapons glide majestically through the air. Watch a recorded game in slow motion and you'll never want to go back!</p>
<div><div><p><a href="https://worms2d.info/File:Window.png"><img alt="" src="https://worms2d.info/images/d/dc/Window.png" width="300" height="237"></a></p><div><p>Demonstration of windowed mode</p></div></div></div>
<h3><span id="Windowed_Mode">Windowed Mode</span></h3>
<p>Worms Armageddon can now be played in a window! Awkward screen resolution changes and game capture issues are a thing of the past if you enable this feature in Advanced Settings.</p>
<div><div><p><a href="https://worms2d.info/File:Streaming-mode.png"><img alt="" src="https://worms2d.info/images/6/66/Streaming-mode.png" width="211" height="85"></a></p><div><p>Demonstration of Streaming Mode IP address hiding</p></div></div></div>
<h3><span id="Streamers.27_Heaven">Streamers' Heaven</span></h3>
<p>Along with Windowed Mode, a couple of additional options have been added specifically for streamers to make their lives easier. Audio can now be allowed to continue playing when the game window loses focus, and the new Streaming Mode will prevent IP addresses from being displayed when hosting or joining an online game (see the Update Documentation for more information).</p>
<div><div><p><a href="https://worms2d.info/File:Cpu-teams.png"><img alt="" src="https://worms2d.info/images/4/4d/Cpu-teams.png" width="252" height="151"></a></p><div><p>Demonstration of CPU teams being added to an online game</p></div></div></div>
<h3><span id="CPU_Teams_at_Large">CPU Teams at Large</span></h3>
<p>CPU teams can now be added to online games. They're always uniquely distinguishable from player teams to prevent shenanigans, but you can now finally team up with your friends to take down a swarm of those computer-controlled invertebrates. Or if you'd just like to watch a relaxing game while you work, matches containing only CPU teams may now also be started, both online and offline.</p>
<div><div><p><a href="https://worms2d.info/File:Smapshot.png"><img alt="" src="https://worms2d.info/images/7/71/Smapshot.png" width="300" height="225"></a></p></div></div>
<h3><span id="Mapshot">Mapshot</span></h3>
<p>Have you ever wrought a beautiful trail of destruction on your Worms Armageddon landscape and wished you could play a brand new game on the land that remained? Or have you ever been interrupted in the middle of a game and wished you could extract the remaining landscape to help you set up a rematch? Well, now you can! Simply type <b>/map</b> into the in-game chat or press <b>Alt + Pause</b> at any time to save a snapshot of the current state of the map, allowing you to continue your rampage from where you left off!</p>
<div><div><p><a href="https://worms2d.info/File:Wa95-2.png"><img alt="" src="https://worms2d.info/images/a/a0/Wa95-2.png" width="300" height="225"></a></p><div><p>Worms Armageddon running in Windows 95 (on real hardware!)</p></div></div></div>
<h3><span id="Compatibility">Compatibility</span></h3>
<p>Much effort has been expended in allowing everyone, no matter their hardware, to have a good Worms Armageddon experience. Further tweaks have been made to improve the experience of Windows 10 users. Worms Armageddon now runs well under Wine or Proton on Linux. A new OpenGL renderer has been added which, depending on your hardware, might outperform the other renderers. Windowed mode means there's no more need for your hardware to support specific screen resolutions. And for those into retro computing, Worms Armageddon has been seen to once again run on systems of yore...</p>
<div><div><p><a href="https://worms2d.info/File:Languages.png"><img alt="" src="https://worms2d.info/images/b/b5/Languages.png" width="222" height="120"></a></p><div><p>Sample of languages supported by Worms Armageddon's volunteer translators</p></div></div></div>
<h3><span id="Translations">Translations</span></h3>
<p>Through the tireless efforts of volunteer translators, Worms Armageddon is now fully translated to seven languages (🇫🇮 Finnish, 🇫🇷 French, 🇩🇪 German, 🇵🇹🇧🇷 Portuguese, 🇷🇺 Russian, 🇪🇸 Spanish and 🇸🇪 Swedish), with four more languages with partial translations on the way. The Update Documentation (Readme) now has translations to Finnish, French, and Spanish!</p>
<h3><span id="Fixes_and_More">Fixes and More</span></h3>
<p><a href="https://worms2d.info/File:Moon.png" title="Memey moon face"><img alt="Memey moon face" src="https://worms2d.info/images/4/47/Moon.png" width="100" height="123"></a></p>
<p>Over seven years in development, this update's release notes list consists of:
</p>
<ul><li> 370 fixes,</li>
<li> 45 changes, and</li>
<li> 61 new features.</li></ul>
<p>Our thanks go to all the hardworking members of the Worms Armageddon community who helped make this update a reality. And for the full list of what's new, check the Update Documentation (included with this update) or <a href="https://www.tus-wa.com/forums/announcements/worms-armageddon-v3-8-released-32795/">developer Deadcode's thread</a> on fan-site TUS!
</p>
</div>

<!-- 
NewPP limit report
Cached time: 20200721201813
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.016 seconds
Real time usage: 0.019 seconds
Preprocessor visited node count: 88/1000000
Preprocessor generated node count: 260/1000000
Post‐expand include size: 354/2097152 bytes
Template argument size: 60/2097152 bytes
Highest expansion depth: 4/40
Expensive parser function count: 0/100
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%    2.065      1 - -total
 44.74%    0.924      1 - Template:ParentArticle
 29.94%    0.618      9 - Template:Clear
-->

<!-- Saved in parser cache with key wkb-mw_:pcache:idhash:5655-0!*!0!!*!5!* and timestamp 20200721201813 and revision id 26331
 -->
</div></div>]]>
            </description>
            <link>https://worms2d.info/Worms_Armageddon_3.8_Features</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888870</guid>
            <pubDate>Sun, 19 Jul 2020 12:25:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tired of note-taking apps]]>
            </title>
            <description>
<![CDATA[
Score 429 | Comments 416 (<a href="https://news.ycombinator.com/item?id=23888799">thread link</a>) | @akkshu92
<br/>
July 19, 2020 | https://akkshaya.blog/2020/07/19/note-taking/ | <a href="https://web.archive.org/web/*/https://akkshaya.blog/2020/07/19/note-taking/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1052">
		<!-- .entry-header -->

	<div>
		
<p>I’m tired of note-taking apps. </p>



<p>It’s not because of limited choices. But it’s the other way around. There are so many note-taking apps you could try but end up sticking to none. At least, that’s my story. It’s a perfect example of&nbsp;<strong>the paradox of choice.</strong></p>



<p>I used to wonder why people keep building so many ‘note-taking’ apps when the market is already crowded with choices. Then I figured a few reasons why.</p>



<ul><li><strong>the market size</strong>: the global <a href="https://www.verifiedmarketresearch.com/product/note-making-management-software-market/#:~:text=According%20to%20Verified%20Market%20Research,5.32%25%20from%202019%20to%202026." target="_blank" rel="noreferrer noopener">note-taking management software market</a> is estimated to reach&nbsp;<strong>$1.35 billion </strong>by<strong> 2026</strong>, growing at a&nbsp;<strong>CAGR </strong>of<strong> 5.32% </strong>from<strong> 2019 to 2026</strong></li><li><strong>greater scope for innovation:&nbsp;</strong>eg., be it creating a task list, a roadmap, or a design repository, Notion can handle it all</li><li><strong>lack of satisfaction:&nbsp;</strong>it’s noted that people always use a combination of note-taking apps and hardly stick to one for a long time</li></ul>



<p>Despite such heavy competition, apps like Notion, Google Keep, OneNote, Evernote, etc. have managed to earn a place. People use these apps for</p>



<ul><li>the ecosystem. eg., Google Keep, Microsoft OneNote</li><li>the neat user experience., eg. Bear etc.</li><li>creating a disciplined way of taking notes. eg., Notion, Roam Research</li></ul>



<p>I’ve tried them all. But none of these apps have turned me into a ‘<strong>repeat user</strong>.’ </p>



<p>After battling with so many apps only to feel guilty for not having the discipline to consistently use them, I’ve finally resorted to the most personal and easy alternative ⁠—&nbsp;<strong>writing things down</strong>.</p>



<p>I’m familiar with writing in a notebook since my childhood. It’s not new to me, and it absolutely doesn’t require any learning curve.</p>



<h3>The reasons why I find writing things down useful</h3>



<ul><li>absolute focus and the ability to think through the points I’m writing</li><li>gives a chance to remember what I’m writing</li><li>no way to copy-paste stuff as it is, and that means taking notes in a way I understand</li><li>easy to switch between formats eg., flowchart, mind map, Venn diagram, etc</li><li>helps me stay in touch with my handwriting</li></ul>



<p>Of course, everything has its downsides, and writing things down is no exception here. <br>For example, I will not be able to</p>



<ul><li>add screenshots/images, links, etc</li><li>easily search for content as there’s no ‘search bar’</li></ul>



<p>And maybe there’s more to the list I’m not talking about. </p>



<p>All I can say for sure is, based on my usage behavior, I’m okay missing out on these features. I can always save links to <a href="https://app.getpocket.com/" target="_blank" rel="noreferrer noopener">Pocket</a> for future reference, and take pictures of my notes to share with friends.</p>



<p>So if you ask me if I’d try a beautiful, innovative note-taking app that’s much better than the apps I’ve used so far, my answer is,&nbsp;<em>“<strong>Why not</strong>! <strong>I’d definitely give it a shot</strong></em>.”</p>



<p>But my greatest worry is if I’d continue using it.</p>



<p><strong>Note: </strong>If my opinion on note-taking apps changes over time, I’d be happy to update this post with a “And the hero finally arrived!” heading to talk about the app that helped change my mind. 🤡</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

	</article></div>]]>
            </description>
            <link>https://akkshaya.blog/2020/07/19/note-taking/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888799</guid>
            <pubDate>Sun, 19 Jul 2020 12:12:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Things Unexpectedly Named After People]]>
            </title>
            <description>
<![CDATA[
Score 393 | Comments 258 (<a href="https://news.ycombinator.com/item?id=23888725">thread link</a>) | @vortex_ape
<br/>
July 19, 2020 | https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/ | <a href="https://web.archive.org/web/*/https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
            <p><h5>July 19, 2020</h5></p>
        </div></div>]]>
            </description>
            <link>https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888725</guid>
            <pubDate>Sun, 19 Jul 2020 11:58:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Inheritance in C using structure composition]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 28 (<a href="https://news.ycombinator.com/item?id=23888677">thread link</a>) | @arpitbbhayani
<br/>
July 19, 2020 | https://arpitbhayani.me/blogs/inheritance-c | <a href="https://web.archive.org/web/*/https://arpitbhayani.me/blogs/inheritance-c">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>C language does not support inheritance however it does support Structure Compositions which can be tweaked to serve use-cases requiring parent-child relationships. In this article, we find out how Structure Compositions help us emulate inheritance in C and keep our code extensible. We will also find how it powers two of the most important things to have ever been invented in the field of computer science.</p>

<p>Structure Composition is when we put one structure within another, not through its pointer but as a native member - something like this</p>
<pre><code>


<span><span>struct</span> <span>list_head</span> {</span>
	<span><span>struct</span> <span>list_head</span> *<span>next</span>;</span> 
	<span><span>struct</span> <span>list_head</span> *<span>prev</span>;</span> 
};


<span><span>struct</span> <span>list_int</span> {</span>
	<span><span>struct</span> <span>list_head</span> <span>list</span>;</span>  
	<span>int</span> value;              
};


<span><span>struct</span> <span>list_str</span> {</span>
	<span><span>struct</span> <span>list_head</span> <span>list</span>;</span>  
	<span>char</span> * str;             
};
</code></pre>
<p>In the example above, we define a node of a linked list using structure composition. Usually, a linked list node has 3 members - two pointers to adjacent nodes (next and previous) and a third one could either be the data or a pointer to it.  The defining factor of a linked list is the two pointers that logically form a chain of nodes. To keep things abstract we create a struct named <code>list_head</code> which holds these two pointers  <code>next</code> and <code>prev</code> and omits the specifics i.e. data.</p>
<p>Using <code>list_head</code> structure, if we were to define a node of a linked list holding an integer value we could create another struct, named <code>list_int</code> that holds a member of type <code>list_head</code> and an integer value <code>value</code>. The next and previous pointers are brought into this struct through <code>list_head list</code> and could be referred to as <code>list.next</code> and <code>list.prev</code>.</p>
<blockquote>
<p>There is a very genuine reason for picking such weird names for a linked list node and members of structures; the reason to do so will be cleared in the later sections of this essay.</p>
</blockquote>
<p>Because of the above structure definition, building a linked list node holding of any type becomes a breeze. For example, a node holding string could be quickly defined as a struct <code>list_str</code> having <code>list_head</code> and a <code>char *</code>. This ability to extend <code>list_head</code> and build a node holding data of any type and any specifics make low-level code simple, uniform, and extensible.</p>
<h2>Memory Representation of <code>list_int</code></h2>
<p>Structures in C are not padded and they do not even hold any meta information, not even for the member names; hence during allocation, they are allocated the space just enough to hold the actual data.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png" alt="https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png"></p>
<p>In the illustration above we see how members of <code>list_int</code> are mapped on the allocated space - required by its individual members. It is allocated a contiguous space of 12 bytes - 4 bytes for each of the two pointers and another 4 bytes for the integer value. The contiguity of space allocation and order of members during allocation could be verified by printing out their addresses as shown below.</p>
<pre><code><span><span>void</span> <span>print_addrs</span><span>()</span> </span>{
    
    <span><span>struct</span> <span>list_int</span> *<span>ll</span> = <span>new_list_int</span>(41434);</span>

    
    <span>printf</span>(<span>"%p: head\n"</span>,             head);
    <span>printf</span>(<span>"%p: head-&gt;list.next\n"</span>,  &amp;((head-&gt;<span>list</span>).next));
    <span>printf</span>(<span>"%p: head-&gt;list.prev\n"</span>,  &amp;((head-&gt;<span>list</span>).prev));
    <span>printf</span>(<span>"%p: head-&gt;value\n"</span>,      &amp;(head-&gt;value));
}

~ $ make &amp;&amp; ./a.out
<span>0x4058f0</span>: head
<span>0x4058f0</span>: head-&gt;<span>list</span>.next
<span>0x4058f4</span>: head-&gt;<span>list</span>.prev
<span>0x4058f8</span>: head-&gt;value
</code></pre>
<p>We clearly see all the 3 members, occupying 12 bytes contiguous memory segments in order of their definition within the struct.</p>
<blockquote>
<p>The above code was executed on a machine where the size of integer and pointers were 4 bytes each. The results might differ depending on the machine and CPU architecture.</p>
</blockquote>
<h2>Casting pointers pointing to struct</h2>
<p>In C language, when a pointer to a struct is cast to a pointer to another struct, the engine maps the individual members of a target struct type, depending on their order and offsets, on to the slice of memory of the source struct instance.</p>
<p>When we cast <code>list_int *</code> into <code>list_head *</code>, the engine maps the space required by target type i.e. <code>list_head</code> on space occupied by <code>list_int</code>. This means it maps the 8 bytes required by <code>list_head</code> on the first 8 bytes occupied by <code>list_int</code> instance. Going by the memory representation discussed above, we find that the first 8 bytes of <code>list_int</code> are in fact <code>list_head</code>, and hence casting <code>list_int *</code> to <code>list_head *</code> is effectively just referencing the <code>list_head</code> member of <code>list_int</code> through a new variable.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png" alt="https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png"></p>
<p>This effectively builds a parent-child relationship between the two structs where we can safely typecast a child <code>list_int</code> to its parent <code>list_head</code>.</p>
<blockquote>
<p>It is important to note here that the parent-child relationship is established only because the first member of <code>list_int</code> is of type <code>list_head</code>. it would not have worked if we change the order of members in <code>list_int</code>.</p>
</blockquote>

<p>As established above, by putting one struct within another as its first element we are effectively creating a parent-child relationship between the two. Since this gives us an ability to safely typecast child to its parent we can define functions that accept a pointer to parent struct as an argument and perform operations that do not really require to deal with specifics. This allows us to <strong>NOT</strong> rewrite the functional logic for every child extensions and thus avoid redundant code.</p>
<p>From the context we have set up, say we want to write a function that adds a node between the two in a linked list. The core logic to perform this operation does not really need to deal with any specifics all it takes is a few pointer manipulations of <code>next</code> and <code>prev</code>. Hence, we could just define the function accepting arguments of type <code>list_head *</code>  and write the function as</p>
<pre><code>
<span>static</span> <span>void</span> __list_add(struct list_head *<span>new</span>,
                       struct list_head *prev,
                       struct list_head *next)
{
    next-&gt;prev = <span>new</span>;
    <span>new</span>-&gt;next = next;
    <span>new</span>-&gt;prev = prev;
    prev-&gt;next = <span>new</span>;
}
</code></pre>
<p>Since we can safely typecase <code>list_int *</code> and <code>list_str *</code> to <code>list_head *</code> we can pass any of the specific implementations the function <code>__list_add</code> and it would still add the node between the other two seamlessly.</p>
<p>Since the core operations on linked lists only require pointer manipulations, we can define these operations as functions accepting <code>list_head *</code> instead of specific types like <code>list_int *</code>.  Thus we need not write similar functions for specifics. A function to delete a node could be written as</p>
<pre><code>
<span>static</span> <span>inline</span> <span>void</span> __list_del(struct list_head * prev, struct list_head * next)
{
    next-&gt;prev = prev;
    prev-&gt;next = next;
}
</code></pre>
<p>Other linked list utilities like <em>adding a node to tail</em>, <em>swapping nodes</em>, <em>splicing the list</em>, <em>rotating the list</em>, etc only require manipulations of <code>next</code> and <code>prev</code> pointers. Hence they could also be written in a very similar way i.e accepting <code>list_head *</code> and thus eliminating the need to reimplement function logic for every single child implementation.</p>
<p>This behavior is very similar to how inheritance in modern OOP languages, like Python and Java, work where the child is allowed to invoke any parent function.</p>

<p>There are a ton of practical usage of using Structure Compositions but the most famous ones are</p>
<h2>Linux Kernel</h2>
<p>In order to keep things abstract and extensible, Linux Kernel uses Structure Composition at several places. One of the most important places where it uses composition is for managing and maintaining Linked Lists, exactly how we saw things above. The struct definitions and code snippets are taken as-is from the <a href="https://elixir.bootlin.com/linux/latest/source/include/linux/list.h">Kernel's source code</a>, and hence the structure and variable names look different than usual.</p>
<h2>Python Type and Object Hierarchy</h2>
<p>Python, one of the most important languages in today's world, uses Structure Composition to build Type Hierarchy. Python defines a root structure called <code>PyObject</code> which holds reference count, defining the number of places from which the object is referenced - and object type - determining the type of the object i.e. <code>int</code>, <code>str</code>, <code>list</code>, <code>dict</code>, etc.</p>
<pre><code><span>typedef</span> <span><span>struct</span> _<span>object</span> {</span>
    Py_ssize_t     ob_refcnt;  
    PyTypeObject   *ob_type;   
} PyObject;
</code></pre>
<p>Since Python wants these fields to be present in every single object that is created during runtime, it uses structure composition to ensure that objects like integers, floats, string, etc put <code>PyObject</code> as their first element and thus establishing a parent-child relationship. A Float object in Python is defined as</p>
<pre><code><span>#<span>define</span> PyObject_HEAD PyObject ob_base;</span>

<span>typedef</span> <span><span>struct</span> {</span>
    PyObject_HEAD
    <span>double</span> ob_fval;    
} PyFloatObject;
</code></pre>
<p>Now writing utility functions that increments and decrements references count on every access of any object could be written as just a single function accepting <code>PyObject *</code> as shown below</p>
<pre><code><span>static</span> <span>inline</span> <span>void</span> _Py_INCREF(PyObject *op) {
    op-&gt;ob_refcnt++;
}
</code></pre>
<p>Thus we eradicate a need of rewriting <code>INCREF</code> for every single object type and just write it once for <code>PyObject</code> and it will work for every single Python object type that is extended through <code>PyObject</code>.</p>

<ul>
<li><a href="https://elixir.bootlin.com/linux/latest/source/include/linux/list.h">LinkedList in Linux Source Code</a></li>
<li><a href="https://docs.python.org/3/c-api/structures.html#c.PyObject">PyObject - Python Internals Documentation</a></li>
<li><a href="https://docs.python.org/3/c-api/float.html">PyFloatObject - Python Internals Documentation</a></li>
</ul>
</div></div><section><div><div><p><img src="https://arpitbhayani.me/static/img/arpit.jpg"></p>  <h2>
              500+ Signups
            </h2> <p>
              If you like what you read subscribe you can always subscribe to
              my newsletter and get the post delivered straight to your inbox.
              I write
              <a href="https://arpitbhayani.me/blogs">essays</a> on various
              engineering topics and share it through my weekly
              <a href="https://arpitbhayani.me/newsletter">newsletter</a> 👇
            </p> <br> </div></div></section></div>]]>
            </description>
            <link>https://arpitbhayani.me/blogs/inheritance-c</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888677</guid>
            <pubDate>Sun, 19 Jul 2020 11:48:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Gradually Exit Twitter]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 56 (<a href="https://news.ycombinator.com/item?id=23888215">thread link</a>) | @saadalem
<br/>
July 19, 2020 | https://balajis.com/how-to-gradually-exit-twitter/ | <a href="https://web.archive.org/web/*/https://balajis.com/how-to-gradually-exit-twitter/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


                <figure>
                    <img srcset="https://balajis.com/content/images/size/w300/2020/07/gradually-exit-twitter.png 300w,
                                https://balajis.com/content/images/size/w600/2020/07/gradually-exit-twitter.png 600w,
                                https://balajis.com/content/images/size/w1200/2020/07/gradually-exit-twitter.png 1000w,
                                https://balajis.com/content/images/size/w2000/2020/07/gradually-exit-twitter.png 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 1170px,
                                2000px" src="https://balajis.com/content/images/size/w2000/2020/07/gradually-exit-twitter.png" alt="How to Gradually Exit Twitter">
                </figure>
                <section>
                    <div>
                        <p>The <a href="https://twitter.com/TwitterSupport/status/1283591848729219073">hacks</a> are the last straw. We need to exit Twitter.</p><p>But you may be invested in Twitter! So what do you do? Here's one recipe for <em>gradual</em> exit.</p><ol><li>Step one is to set up an (optionally pseudonymous) newsletter at your <em>own</em> custom domain using the instructions <a href="https://balajis.com/set-up-a-paid-newsletter-at-your-own-domain/">here</a>. You can do this in an hour and maintain it for less than $30 per month. Then only tweet out posts from your site, using your domain name as a simple form of authentication.</li><li>Step two is to start building your membership list at your own site. If you want, help us review <a href="https://github.com/balajis/twitter-export">open source tools</a> to export your Twitter following to an email list and <a href="https://twitter.com/dwr/status/1284155565770498050">mass delete any DMs</a>.</li><li>And step three is to get off Twitter all together, and start building a decentralized media ecosystem that combines the sovereignty of individual domain names with the community of social networking. The v1 of that is a simple joint RSS feed that aggregates multiple independent newsletters, but we can get much more <a href="https://twitter.com/balajis/status/1280005821028397057">sophisticated</a> <a href="https://dci.mit.edu/decentralizedweb">with</a> <a href="https://hackernoon.com/a-state-of-the-art-of-decentralized-web-part-4-212732f74894">tech</a> <a href="https://gun.eco/docs/dWeb-The-Decentralized-Web">for</a> <a href="https://ens.domains/">the</a> <a href="https://handshake.org/">decentralized</a> <a href="https://ipfs.io/">web</a>. It may turn out that what we build helps reform Twitter itself, but at a minimum it will present an alternative.</li></ol><p>Before we get there though, let's go through the problems with Twitter.</p><h2 id="the-problems-with-twitter">The Problems with Twitter</h2><p>It's not just the hack. There are several overlapping issues with Twitter around security, authentication, pseudonymity, distribution, incentives, and culture.</p><ul><li><em>Twitter is insecure</em>. It was a mistake to think that personal security could compensate for the fundamental flaw of centralized web services: anyone who gains control over twitter.com gains control over your account. We knew this already from the <a href="http://archive.is/Bq5qi">2017 incident</a> and the <a href="http://archive.is/wip/scJMn">Jack Hack</a>. But now the most prominent people and companies in the world (Bezos, Gates, Zuckerberg, Musk, Apple, Uber, among others) have been <a href="https://twitter.com/TwitterSupport/status/1283591848729219073">openly and publicly hacked</a>. Tomorrow you may be the victim. And unlike these worthies, you may be <em>silently</em> and <em>privately</em> hacked. Perhaps you already have been.</li><li><em>Twitter is not authenticated</em>. It was a mistake to trust Twitter's centralized verification process to validate everything posted to the service. There weren't any <a href="https://www.investopedia.com/terms/p/private-key.asp#:~:text=A%20digital%20wallet%20stores%20the,to%20use%20the%20private%20key.">digital signatures</a> to help establish whether the messages were really coming from a user-controlled device, or faked by a central server.</li><li><em>Twitter is not fully pseudonymous</em>. It was a mistake to use our real names everywhere online, particularly on Twitter. It made us vulnerable to personal and physical attacks. Social media mobs can swarm your real name online and cancel you in real life. Doxxing can heighten the danger by making threats <a href="https://blog.lopp.net/reflections-upon-a-swatting/">materialize in</a> <a href="https://github.com/jlopp/physical-bitcoin-attacks/blob/master/README.md">meatspace</a>. Pseudonymity is a better default, especially now that we can <a href="http://www.marknagelberg.com/notes-on-the-pseudonymous-economy-balaji-srinivasan/">earn under a pseudonym</a>.</li><li><em>Twitter is a distribution chokepoint</em>. It was a mistake to outsource our distribution to Twitter. Twitter users don't fully control their reach, brand, or monetization. You can be <a href="https://www.searchenginejournal.com/twitter-reveals-how-it-ranks-tweets-in-search-results/263869/">downranked</a> or <a href="http://archive.is/mhgyB">deplatformed</a> at any time, just like you can on YouTube and other centralized platforms. Owning your <a href="https://support.substack.com/hc/en-us/articles/360037465992-How-do-I-export-my-email-or-subscriber-list-">own</a> <a href="http://locals.com/">email</a> <a href="https://ghost.org/members">list</a> is a far better alternative.</li><li><em>Twitter doesn't create wealth</em>. It was a mistake to have so many people spending so much time on something that <a href="https://twitter.com/paulg/status/1271834488998440961">arguably</a> destroys more wealth than it creates. Because by default, you don't create wealth on Twitter. You may create wealth <em>for</em> Twitter, or for the media corporations whose links are circulated, but not for yourself or for others. There's no sense of economic alignment with other users, no sense of mutual obligation.</li><li><em>Twitter turns society into Twitter</em>. It was a mistake to let Twitter set up the incentives for our society. It's not good for the world when the one thing every prominent person knows how to do is <a href="https://twitter.com/balajis/status/1272653587617701888">fight</a> each other on Twitter for likes and followers. It has converted society into a zero-sum status game played by elites with real consequences. Every day, blue checks compete for a finite pie of attention with ever more sensational posts. There is instant feedback on what is popular, but none on what is true. The lack of economics, the presence of real names, the toxic culture, and the UX itself all encourage constant fighting.</li></ul><p>Now, let me qualify these comments. I personally have gotten <a href="https://twitter.com/paulg/status/1271834488998440961">enormous value</a> out of Twitter, despite all the points above. After all, you only critique the services you use!</p><p>And to be clear, Jack Dorsey is a phenomenal entrepreneur. I respect everything he's done and the team that he's built. It's hard to build one multibillion dollar company, let alone <a href="http://archive.is/wip/bu6hJ">two</a>. He's done immense good for the world overall, both through his <a href="https://twitter.com/jack/status/1247616214769086465">personal generosity</a> and his startup vehicles. And I also don't wish any Twitter employee ill, as many of them are truly great people and I know how hard this stuff is.</p><p>Moreover, even after this hack, Twitter will likely continue to be an important internet battlefield for quite sometime. It's just too easy, too convenient. And it may turn out that whatever is built to exit Twitter will end up interfacing with Twitter itself, via their <a href="https://twitter.com/bluesky">decentralized protocol</a>, if only as a form of training wheels to help people get off. Hopefully this can be done in a win-win way, such that Twitter and its employees are incentivized to allow people to gradually exit and fix the issues on the centralized service.</p><p>But it's past time for us to start building something better, that keeps the good aspects of Twitter (the serendipitous learning, the fascinating <a href="https://twitter.com/balajis/status/1214585024772788224">people</a>) but filters the bad. That may mean exiting to our own domains and then networking them into decentralized media.</p><h2 id="decentralization-starts-with-a-domain">Decentralization Starts with a Domain</h2><figure><img src="https://balajis.com/content/images/2020/07/how-to-gradually-exit-twitter--1-.png" alt="" srcset="https://balajis.com/content/images/size/w600/2020/07/how-to-gradually-exit-twitter--1-.png 600w, https://balajis.com/content/images/size/w1000/2020/07/how-to-gradually-exit-twitter--1-.png 1000w, https://balajis.com/content/images/size/w1600/2020/07/how-to-gradually-exit-twitter--1-.png 1600w, https://balajis.com/content/images/2020/07/how-to-gradually-exit-twitter--1-.png 2000w" sizes="(min-width: 720px) 720px"><figcaption>Can we network together websites into decentralized media with something deeper than links?</figcaption></figure><p>I may be wrong about this, but I don't think a "better Twitter" starts with a service which is simply a clone of Twitter except with a different community. I think it starts with a community of independent (and independently monetizable) domains that we network together in novel ways, to build <em>decentralized media</em>.</p><p>Towards this end, the first step in gradually exiting Twitter is to set up your own paid newsletter at a custom domain (<a href="https://balajis.com/set-up-a-paid-newsletter-at-your-own-domain/">instructions here</a>) and restrict yourself mainly to tweeting out posts. This does require more effort than writing 280 characters! But that's why it's good. Effort helps restore your attention span and improves average post quality. And in addition to the other benefits, the link to your domain doubles as a simple alternative to a <a href="https://www.investopedia.com/terms/p/private-key.asp#:~:text=A%20digital%20wallet%20stores%20the,to%20use%20the%20private%20key.">digital signature</a>, a way for people to see that the content is authentically yours. </p><p>The medium-term step is to develop <a href="https://github.com/balajis/twitter-export">tools</a> to migrate your followers off Twitter to an email list, and to mass delete all DMs as a precautionary step. If you want to help review some existing tools towards this end, we're working on it <a href="https://github.com/balajis/twitter-export/issues/1">here</a>.</p><p>The long-term step is to start addressing the issues with Twitter by knitting these individual domains together into an open source decentralized media ecosystem. That means addressing:</p><ul><li>Security by encouraging people to post from their own domains</li><li>Community by building a network around multiple domains</li><li>Authentication by combining domains with digital signatures</li><li>Distribution via memberships and email lists</li><li>Incentives by baking in monetization and economic alignment from the beginning</li><li>Mobs by recommending pseudonymity by default</li><li>Culture by encouraging physical norms of civility</li></ul><p>Crucially, unlike past efforts our goal would be to build this <em>community-first</em> rather than <em>technology-first</em>. Specifically, once we get enough paid newsletters set up at their own domains, we can experiment with new decentralized web technologies for linking them together. If you submit your URL to <a href="https://forms.gle/xqvAFgW43Hc7rVzJA">this form</a>, we'll see if we can form a community. We'll probably start with a joint <a href="https://en.wikipedia.org/wiki/RSS">RSS</a> feed and then <a href="https://hacks.mozilla.org/2018/07/introducing-the-d-web/">get more sophisticated</a>.</p><p>Oh, and you can signal that you're interested in decentralized media by changing your name on Twitter to your URL, as <a href="https://twitter.com/balajis">follows</a>:</p><figure><img src="https://balajis.com/content/images/2020/07/image-5.png" alt=""></figure><p>Exiting Twitter and building decentralized media will be a long project and we aren't assured of success. But this journey of a thousand miles begins with a single newsletter at your own domain! So <a href="https://balajis.com/set-up-a-paid-newsletter-at-your-own-domain/">set yours up</a> now. </p><p>It's time to start gradually exiting Twitter.</p>
                    </div>
                </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://balajis.com/how-to-gradually-exit-twitter/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888215</guid>
            <pubDate>Sun, 19 Jul 2020 09:50:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Optimizing 128-bit Division]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 28 (<a href="https://news.ycombinator.com/item?id=23888177">thread link</a>) | @EvgeniyZh
<br/>
July 19, 2020 | https://danlark.org/2020/06/14/128-bit-division/ | <a href="https://web.archive.org/web/*/https://danlark.org/2020/06/14/128-bit-division/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-288">

	

	
	<div>
		
<p>When it comes to hashing, sometimes 64 bit is not enough, for example, because of <a href="https://en.wikipedia.org/wiki/Birthday_problem">birthday paradox</a> — the hacker can iterate through random <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{32}" title="2^{32}"> entities and it can be proven that with some constant probability they will find a collision, i.e. two different objects will have the same hash. <img src="https://s0.wp.com/latex.php?latex=2%5E%7B32%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{32}" title="2^{32}"> is around 4 billion objects and with the current power capacity in each computer it is certainly achievable. That’s why we need sometimes to advance the bitness of hashing to at least 128 bits. Unfortunately, it comes with a cost because platforms and CPUs do not support 128 bit operations natively.</p>



<p>Division historically is the most complex operation on CPUs and all guidelines suggest avoiding the division at all costs.</p>



<p>At my job I faced an interesting problem of optimizing 128 bit division from <a href="https://github.com/abseil/abseil-cpp/blob/master/absl/numeric/int128.cc#L155">abseil library</a> in order to split some data across buckets with the help of 128 bit hashing (the number of buckets is not fixed for some uninteresting historical reasons). I found out that the division takes a really long time. The <a href="https://github.com/abseil/abseil-cpp/blob/master/absl/numeric/int128_benchmark.cc#L52">benchmarks</a> from abseil on Intel(R) Xeon(R) W-2135 CPU @ 3.70GHz show some horrible results</p>



<pre><code>Benchmark                       Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor     13.8     13.8  // 128 bit by 128 bit
BM_DivideClass128SmallDivisor        168      168  // 128 bit by 64 bit</code></pre>



<p>150 nanoseconds for dividing the random 128 bit number by a random 64 bit number? Sounds crazy. For example, <code>div</code> instruction on x86-64 Skylake takes 76 cycles (also, for AMD processors it is much less), the division takes around 20-22ns.</p>



<figure><img data-attachment-id="361" data-permalink="https://danlark.org/d2elnjfdkne/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png" data-orig-size="921,461" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="d2elnjfdkne" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=921" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png 921w, https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/d2elnjfdkne.png?w=768 768w" sizes="(max-width: 921px) 100vw, 921px"><figcaption><a href="https://godbolt.org/z/o2vTZr">https://godbolt.org/z/o2vTZr</a></figcaption></figure>



<p>In reality everything is slightly better because of pipeline execution and division has its own ALU, so if you divide something and do something else in the next instructions, you will get lower average latency. Still, 128 bit division cannot be 8x slower than 64 bit division. All latencies you can find in Agner Fog <a href="https://www.agner.org/optimize/instruction_tables.pdf">instruction table</a> for most of the modern x86 CPUs. The truth is more complex and division latency can even depend on the values given.</p>



<figure><img data-attachment-id="299" data-permalink="https://danlark.org/2020-06-14-182043_835x215_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png" data-orig-size="835,215" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-182043_835x215_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=835" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png 835w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-182043_835x215_scrot.png?w=768 768w" sizes="(max-width: 835px) 100vw, 835px"><figcaption>Agner Fog instruction table for Skylake CPUs, the second but last column is the latency.</figcaption></figure>



<p>Even compilers when dividing by some constants, try to use the reciprocal (or, the same as inverse in a ring) value and multiply the reciprocal and the value with some shifts afterwards</p>



<figure><img data-attachment-id="315" data-permalink="https://danlark.org/2020-06-14-192300_861x251_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png" data-orig-size="861,251" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-192300_861x251_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=861" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png 861w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-192300_861x251_scrot.png?w=768 768w" sizes="(max-width: 861px) 100vw, 861px"><figcaption><a href="https://gcc.godbolt.org/z/PRibsx">https://gcc.godbolt.org/z/PRibsx</a></figcaption></figure>



<p>Overall, given the fact that only some <code>sin</code>, <code>cos</code> instructions cost more than division, division is one of the most complex instructions in CPUs and optimizations in that place matter a lot. My exact case was more or less general, maybe I was dividing 128 bit by 64 bit a bit more frequent. We are going to optimize the general case in LLVM.</p>



<p>We need to understand how 128 bit division is working through the compiler stack.</p>



<figure><img data-attachment-id="303" data-permalink="https://danlark.org/2020-06-14-183125_682x238_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png" data-orig-size="682,238" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-183125_682x238_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=682" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=682" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png 682w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-183125_682x238_scrot.png?w=300 300w" sizes="(max-width: 682px) 100vw, 682px"><figcaption><a href="https://gcc.godbolt.org/z/fB3aq2">https://gcc.godbolt.org/z/fB3aq2</a></figcaption></figure>



<p>It calls <code>__udivti3</code> function. Let’s first understand how to read these functions. In runtime libraries the modes of the functions are:</p>



<div><div>
<pre><code>QI: An integer that is as wide as the smallest addressable unit, usually 8 bits.
HI: An integer, twice as wide as a QI mode integer, usually 16 bits.
SI: An integer, four times as wide as a QI mode integer, usually 32 bits.
DI: An integer, eight times as wide as a QI mode integer, usually 64 bits.
SF: A floating point value, as wide as a SI mode integer, usually 32 bits.
DF: A floating point value, as wide as a DI mode integer, usually 64 bits.
TI: An integer, 16 times as wide as a QI mode integer, usually 128 bits.</code></pre>
</div></div>



<p>So, <code>udivti3</code> is an <strong>u</strong>nsigned division of TI (128 bits) integers, last ‘<em>3′</em> means that it has 3 arguments including the return value. Also, there is a function <code>__udivmodti4</code> which computes the divisor and the remainder (division and modulo operation) and it has 4 arguments including the returning value. These functions are a part of runtime libraries which compilers provide by default. For example, in GCC it is <a href="http://gcc.gnu.org/onlinedocs/gccint/Libgcc.html#Libgcc">libgcc</a>, in LLVM it is <a href="https://compiler-rt.llvm.org/">compiler-rt</a>, they are linked almost in every program if you have the corresponding toolchain. In LLVM, <code>__udivti3</code> is a simple alias to <code>__udivmodti4</code>.</p>



<figure><div>

</div></figure>



<p><code><a href="https://github.com/llvm-mirror/compiler-rt/blob/release_90/lib/builtins/udivmodti4.c#L20">__udivmodti4</a></code> function was written with the help of <code>Translated from Figure 3-40 of The PowerPC Compiler Writer's Guide.</code> After looking at it <a href="https://cr.yp.to/2005-590/powerpc-cwg.pdf">here</a>, it looks like this was written long time ago and things have changed since then</p>



<figure><img data-attachment-id="313" data-permalink="https://danlark.org/2020-06-14-191400_1038x718_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png" data-orig-size="1038,718" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-191400_1038x718_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=1024" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=1024 1024w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png?w=768 768w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-191400_1038x718_scrot.png 1038w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>First of all, let’s come up with something easy, like shift-subtract algorithm that we have been learning since childhood. First, if <code>divisor &gt; dividend</code>, then the quotient is zero and remainder is the <code>dividend</code>, not an interesting case. </p>



<figure><div>

</div></figure>



<p>The algorithm is easy, we align the numbers by their most significant bits, if dividend is more than divisor, subtract and add 1 to the output, then shift by 1 and repeat.  Some sort of animation can be seen like that:</p>



<figure><img data-attachment-id="318" data-permalink="https://danlark.org/simplescreenrecorder-2020-06-14_20-23-25/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif" data-orig-size="918,442" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="simplescreenrecorder-2020-06-14_20.23.25" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/simplescreenrecorder-2020-06-14_20.23.25.gif?w=918" alt=""></figure>



<p>For 128 bit division it will take at most 128 iterations in the for loop. Actually, the implementation in <a href="https://github.com/llvm-mirror/compiler-rt/blob/release_90/lib/builtins/udivmodti4.c#L173">LLVM</a> for loop is a fallback and we saw it takes 150+ns to complete it because it requires to shift many registers because 128 bit numbers are represented as two registers.</p>



<p>Now, let’s dive into the architecture features. I noticed that while the compiler generates the <code>divq</code> instructions, it frees <code>rdx</code> register</p>



<figure><img data-attachment-id="362" data-permalink="https://danlark.org/2ugj4bgvw4x/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png" data-orig-size="891,206" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2ugj4bgvw4x" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=891" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png 891w, https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2ugj4bgvw4x.png?w=768 768w" sizes="(max-width: 891px) 100vw, 891px"></figure>



<p>In the manual they say the following</p>



<figure><img data-attachment-id="322" data-permalink="https://danlark.org/2020-06-14-204644_860x119_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png" data-orig-size="860,119" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-14-204644_860x119_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=860" alt="" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png 860w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-14-204644_860x119_scrot.png?w=768 768w" sizes="(max-width: 860px) 100vw, 860px"></figure>



<p><code>divq</code> instruction provides 128 bit division from [%rdx]:[%rax] by <code>S</code>. The quotient is stored in <code>%rax</code> and the remainder in <code>%rdx</code>. After some experimenting with inline asm in C/C++, I figured out that if the result does not fit in 64 bits, SIGFPE is raised. See:</p>



<figure><div>

</div></figure>



<p>Compilers don’t use this instruction in 128 bit division because they cannot know for sure if the result is going to fit in 64 bits. Yet, if the high 64 bits of the 128 bit number is smaller than the divisor, the result fits into 64 bits and we can use this instruction. As compilers don’t generate <code>div</code>q instruction for their own reasons, we would use inline asm for x86-64.</p>



<figure><div>

</div></figure>



<p>What to do if the high is not less than the divisor? The right answer is to use 2 divisions because</p>



<figure><img data-attachment-id="325" data-permalink="https://danlark.org/2020-06-11-135245_1608x410_scrot/" data-orig-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png" data-orig-size="1608,410" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="2020-06-11-135245_1608x410_scrot" data-image-description="" data-medium-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=300" data-large-file="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=750" src="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=1024" alt="" width="780" height="198" srcset="https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=1024 1024w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=777 777w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=1553 1553w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=150 150w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=300 300w, https://danlarkorg.files.wordpress.com/2020/06/2020-06-11-135245_1608x410_scrot.png?w=768 768w" sizes="(max-width: 780px) 100vw, 780px"></figure>



<p>So, first we can divide <code>hi</code> by <code>divisor</code> and then <code>{hi_r, lo}</code> by <code>divisor</code> guaranteeing that <code>hi_r</code> is smaller than <code>divisor</code> and thus the result is smaller than <img src="https://s0.wp.com/latex.php?latex=2%5E%7B64%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{64}" title="2^{64}">. We will get something like</p>



<figure><div>

</div></figure>



<p>  After that the benchmarks improved significantly</p>



<pre><code>Benchmark                       Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor 11.9      11.9
BM_DivideClass128SmallDivisor   26.6      26.6</code></pre>



<p>Only 26.6ns for small divisors, that’s a clear 6x win.</p>



<p>Then there are multiple choices to do next but we know that both dividend and divisor have at least one bit in their high registers and the shift-subtract algorithm will have at most 64 iterations. Also the quotient is guaranteed to fit in 64 bits, thus we can use only the low register of the resulting quotient and save more shifts in the shift-subtract algorithm. That’s why the uniform divisor slightly improved.</p>



<p>One more optimization to do in shift-subtract algorithm is to remove the branch inside the for loop (read carefully, it should be understandable).</p>



<figure><div>

</div></figure>



<p>In the end, it gives 0.4ns more for uniform 128 bit divisor.</p>



<p>And finally I believe that’s one of the best algorithm to divide 128 bit by 128 bit numbers. From statistics, the case when the divisor is 64 bit is worth optimizing and we showed that additional checks on the high register of divisor has its own advantages and expansion of the invariants. Now let’s see what other libraries perform in that case.</p>



<h2>LibDivide</h2>



<p><a href="https://github.com/ridiculousfish/libdivide">Libdivide</a> is a small library targeting fast division, for example, if you divide by some fixed number a lot of times, there are techniques that can precalculate reciprocal and then multiply by it. Libdivide provides a very good interface for such optimizations. Even though, it has some optimizations regarding 128 bit division. For example, function <code>libdivide_128_div_128_to_64</code> computes the division 128 bit number by 128 bit number if the result fits in 64 bits. In the case where both numbers are more or equal to <img src="https://s0.wp.com/latex.php?latex=2%5E%7B64%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{64}" title="2^{64}"> it does the following algorithm that they took from <a href="https://www.amazon.de/Hackers-Delight-Henry-S-Warren/dp/0321842685/ref=sr_1_1?dchild=1&amp;keywords=Hackers+Delight&amp;qid=1592164234&amp;sr=8-1">Hackers Delight</a> book:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bcases%7D+n+%3D+MSB%28%5Cmathrm%7Bdivisor%7D%29+%5Cgeq+1+%5C%5C+%5Cmathrm%7Bdivisor_1%7D+%3D+%5Clfloor+%5Cmathrm%7Bdivisor%7D%2F2%5E%7B64+-+n%7D+%5Crfloor+%5C%5C+%5Cmathrm%7Bdividend_1%7D+%3D+%5Clfloor+%5Cmathrm%7Bdividend%7D%2F2+%5Crfloor+%5Cend%7Bcases%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\begin{cases} n = MSB(\mathrm{divisor}) \geq 1 \\ \mathrm{divisor_1} = \lfloor \mathrm{divisor}/2^{64 - n} \rfloor \\ \mathrm{dividend_1} = \lfloor \mathrm{dividend}/2 \rfloor \end{cases}" title="\begin{cases} n = MSB(\mathrm{divisor}) \geq 1 \\ \mathrm{divisor_1} = \lfloor \mathrm{divisor}/2^{64 - n} \rfloor \\ \mathrm{dividend_1} = \lfloor \mathrm{dividend}/2 \rfloor \end{cases}"></p>



<p>With the instruction that produces the 64 bit result when the divisor is 128 bit result we can compute</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_1%7D+%3D+%5Clfloor+%5Cmathrm%7Bdividend_1%7D%2F%5Cmathrm%7Bdivisor_1%7D+%5Crfloor&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_1} = \lfloor \mathrm{dividend_1}/\mathrm{divisor_1} \rfloor" title="\mathrm{quotient_1} = \lfloor \mathrm{dividend_1}/\mathrm{divisor_1} \rfloor"></p>



<p>Then we compute</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_0%7D+%3D+%5Clfloor+%5Cmathrm%7Bquotient_1%7D%2F2%5E%7B63+-+n%7D+%5Crfloor&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_0} = \lfloor \mathrm{quotient_1}/2^{63 - n} \rfloor" title="\mathrm{quotient_0} = \lfloor \mathrm{quotient_1}/2^{63 - n} \rfloor">.</p>



<p>It cannot overflow because <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_1%7D+%3C+2%5E%7B64%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_1} < 2^{64}" title="\mathrm{quotient_1} < 2^{64}"> because the maximum value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdividend_1%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{dividend_1}" title="\mathrm{dividend_1}"> is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B127%7D+-+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{127} - 1" title="2^{127} - 1"> and minimum value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdivisor_1%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{divisor_1}" title="\mathrm{divisor_1}"> is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B63%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{63}" title="2^{63}">. Now let’s show that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clfloor+%5Cmathrm%7Bdividend%7D%2F%5Cmathrm%7Bdivisor%7D+%5Crfloor+%5Cleq+%5Cmathrm%7Bquotient_0%7D+%5Cleq++%5Clfloor+%5Cmathrm%7Bdividend%7D%2F%5Cmathrm%7Bdivisor%7D+%5Crfloor+%2B+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor \leq \mathrm{quotient_0} \leq  \lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor + 1" title="\lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor \leq \mathrm{quotient_0} \leq  \lfloor \mathrm{dividend}/\mathrm{divisor} \rfloor + 1"></p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bquotient_0%7D+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B2%5E%7B64+-+n%7D%5Cmathrm%7Bdivisor_1%7D%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B2%5E%7B64+-+n%7D%5Cleft%5Clfloor%5Cfrac%7B%5Cmathrm%7Bdivisor%7D%7D%7B2%5E%7B64+-+n%7D%7D%5Cright%5Crfloor%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B%5Cmathrm%7Bdivisor%7D+-+%28%5Cmathrm%7Bdivisor%7D+%5Cmathrm%7B%5C+mod%5C+%7D+2%5E%7B64+-+n%7D%29%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B%5Cmathrm%7Bdivisor%7D%7D+%2B+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%28%5Cmathrm%7Bdivisor%7D+%5Cmathrm%7B%5C+mod%5C+%7D+2%5E%7B64+-+n%7D%29%29%7D%7B%5Cmathrm%7Bdivisor%7D%28%5Cmathrm%7Bdivisor%7D+-+%28%5Cmathrm%7Bdivisor%7D+%5Cmathrm%7B%5C+mod%5C+%7D+2%5E%7B64+-+n%7D%29%29%7D+%5Cright%5Crfloor+%3D+%5Cleft%5Clfloor+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B%5Cmathrm%7Bdivisor%7D%7D+%2B+%5Cdelta+%5Cright%5Crfloor&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{quotient_0} = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\mathrm{divisor_1}} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\left\lfloor\frac{\mathrm{divisor}}{2^{64 - n}}\right\rfloor} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n})} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \frac{\mathrm{dividend}(\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))}{\mathrm{divisor}(\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \delta \right\rfloor" title="\mathrm{quotient_0} = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\mathrm{divisor_1}} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{2^{64 - n}\left\lfloor\frac{\mathrm{divisor}}{2^{64 - n}}\right\rfloor} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n})} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \frac{\mathrm{dividend}(\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))}{\mathrm{divisor}(\mathrm{divisor} - (\mathrm{divisor} \mathrm{\ mod\ } 2^{64 - n}))} \right\rfloor = \left\lfloor \frac{\mathrm{dividend}}{\mathrm{divisor}} + \delta \right\rfloor">.</p>



<p>Now we want to show that <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3C+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta < 1" title="\delta < 1">. <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta" title="\delta"> is the largest when the remainder in the numerator is as large as possible, it can be up to <img src="https://s0.wp.com/latex.php?latex=2%5E%7B64+-+n%7D+-+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{64 - n} - 1" title="2^{64 - n} - 1">. Because of the definition of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="n" title="n">, <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdivisor%7D+%5Cgeq+2%5E%7B127+-+n%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{divisor} \geq 2^{127 - n}" title="\mathrm{divisor} \geq 2^{127 - n}">. The smallest value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bdivisor%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\mathrm{divisor}" title="\mathrm{divisor}"> in the denominator is <img src="https://s0.wp.com/latex.php?latex=2%5E%7B127+-+n%7D+%2B+2%5E%7B64+-+n%7D+-+1&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="2^{127 - n} + 2^{64 - n} - 1" title="2^{127 - n} + 2^{64 - n} - 1">. That’s why</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cleq+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%282%5E%7B64+-+n%7D+-+1%29%7D%7B%282%5E%7B127+-+n%7D+%2B+2%5E%7B64+-+n%7D+-+1%292%5E%7B127+-+n+%7D%7D+%3C+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%282%5E%7B64+-+n%7D+-+1%29%7D%7B%282%5E%7B127+-+n+%7D%29%5E2%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta \leq \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n} + 2^{64 - n} - 1)2^{127 - n }} < \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n })^2}" title="\delta \leq \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n} + 2^{64 - n} - 1)2^{127 - n }} < \frac{\mathrm{dividend}(2^{64 - n} - 1)}{(2^{127 - n })^2}">. As n iterates from 0 to 63, we can conclude that <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%3C+%5Cfrac%7B%5Cmathrm%7Bdividend%7D%7D%7B2%5E%7B128%7D%7D&amp;bg=ffffff&amp;fg=111111&amp;s=0" alt="\delta < \frac{\mathrm{dividend}}{2^{128}}" title="\delta < \frac{\mathrm{dividend}}{2^{128}}">. So we got either the correct value, either the correct plus one. Everything else in the algorithms is just a correction of which result to choose.</p>



<p>Unfortunately, these corrections increase the latency of the benchmark pretty significant</p>



<pre><code>Benchmark                                          Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor&lt;LibDivideDivision&gt;    26.3    26.3  
BM_RemainderClass128UniformDivisor&lt;LibDivideDivision&gt; 26.2    26.2
BM_DivideClass128SmallDivisor&lt;LibDivideDivision&gt;      25.8    25.8
BM_RemainderClass128SmallDivisor&lt;LibDivideDivision&gt;   26.3    26.3</code></pre>



<p>So I decided to drop this idea after I’ve tried this.</p>



<h2>GMP</h2>



<p><a href="https://gmplib.org/">GMP</a> library is a standard GNU library for long arithmetic. They also have something for 128 bit by 64 bit division and in my benchmark the following code worked</p>



<figure><div>

</div></figure>



<p>It divides the two limbs by a <code>uint64_t</code> and provides the result. Unfortunately, the latency is much higher than expected, also does not work</p>



<pre><code>Benchmark                                          Time(ns)  CPU(ns)
BM_DivideClass128UniformDivisor&lt;GmpDivision&gt;          11.5    11.5
BM_RemainderClass128UniformDivisor&lt;GmpDivision&gt;       10.7    10.7
BM_DivideClass128SmallDivisor&lt;GmpDivision&gt;            47.5    47.5
BM_RemainderClass128SmallDivisor&lt;GmpDivision&gt;         47.8    47.8 </code></pre>



<h2>Conclusion</h2>



<p>In the …</p></div></article></main></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://danlark.org/2020/06/14/128-bit-division/">https://danlark.org/2020/06/14/128-bit-division/</a></em></p>]]>
            </description>
            <link>https://danlark.org/2020/06/14/128-bit-division/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23888177</guid>
            <pubDate>Sun, 19 Jul 2020 09:40:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Giving GPT-3 a Turing Test]]>
            </title>
            <description>
<![CDATA[
Score 431 | Comments 225 (<a href="https://news.ycombinator.com/item?id=23887637">thread link</a>) | @DavidSJ
<br/>
July 19, 2020 | http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html | <a href="https://web.archive.org/web/*/http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <p>I’ve been playing around with OpenAI’s new
<a href="https://arxiv.org/abs/2005.14165">GPT-3</a> language model. When I got
beta access, the first thing I wondered was, how human is GPT-3? How
close is it to passing a Turing test?</p>

<h2 id="how-it-works">How It Works</h2>

<p>Let me explain how exactly I’m generating these conversations. GPT-3
is a general language model, trained on a large amount of
uncategorized text from the internet. It isn’t
specific to a conversational format, and it isn’t trained to answer
any specific type of question. The only thing it does is, given some
text, guess what text comes next.</p>

<p>So if we want GPT-3 to generate answers to questions, we need to seed
it with a “prompt”. I’m using this prompt to initialize all of the Q&amp;A
sessions:</p>

<div><div><pre><code>Q: What is human life expectancy in the United States?
A: Human life expectancy in the United States is 78 years.

Q: Who was president of the United States in 1955?
A: Dwight D. Eisenhower was president of the United States in 1955.

Q: What party did he belong to?
A: He belonged to the Republican Party.

Q: Who was president of the United States before George W. Bush?
A: Bill Clinton was president of the United States before George W. Bush.

Q: Who won the World Series in 1995?
A: The Atlanta Braves won the World Series in 1995.
</code></pre></div></div>

<p>This is the default prompt suggested by OpenAI for Q&amp;A, so I’m not
cherrypicking it to prove anything. The point of this prompt is just
to show GPT-3 that we’re doing questions and answers, not to provide
it with information. For the prompt, both questions and answers are provided by a
human. For all the others, the answers are generated by GPT-3.</p>

<h2 id="common-sense">Common Sense</h2>

<p>Traditionally, artificial intelligence struggles at “common sense”. But
GPT-3 can answer a lot of common sense questions.</p>

<div><div><pre><code>Q: What is your favorite animal?
A: My favorite animal is a dog.

Q: Why?
A: Because dogs are loyal and friendly.

Q: What are two reasons that a dog might be in a bad mood?
A: Two reasons that a dog might be in a bad mood are if it is hungry or if it is hot.

Q: How many eyes does a giraffe have?
A: A giraffe has two eyes.

Q: How many legs does a frog have?
A: A frog has four legs.

Q: Are there any animals with three legs?
A: No, there are no animals with three legs.

Q: Why don't animals have three legs?
A: Animals don't have three legs because they would fall over.
</code></pre></div></div>

<p>Ten years ago, if I had this conversation, I would have assumed the
entity on the other end was a human. You can no longer take it for
granted that an AI does not know the answer to “common sense” questions.</p>

<p>How does GPT-3 know that a giraffe have two eyes? I wish I had some
sort of “debug output” to answer that question. I don’t know for sure,
but I can only theorize that there must be some web page in its
training data that discusses how many eyes a giraffe has. If we want
to stump GPT-3 with common sense questions, we need to think of
questions about things so mundane, they will not appear on the internet.</p>

<div><div><pre><code> Q: Which is heavier, a mouse or an elephant?
 A: An elephant is heavier than a mouse.

 Q: Which is heavier, a mouse or a toaster?
 A: A toaster is heavier than a mouse.

 Q: Which is heavier, a toaster or a pencil?
 A: A pencil is heavier than a toaster.

 Q: Which is heavier, a paper clip or a can of soda?
 A: A can of soda is heavier than a paper clip.

 Q: Which is heavier, a football player or a car?
 A: A car is heavier than a football player.
</code></pre></div></div>

<p>It’s only 4/5. We’re closer to stumping GPT-3 here. I think a human
would be pretty close to 100% on these questions. It makes sense these
are trickier - there probably isn’t any web page that compares
toasters and pencils by weight. It’s only indirectly that humans gain
this knowledge.</p>

<p>This gives us a hint for how to stump the AI more consistently. We
need to ask questions that no normal human would ever talk about.</p>

<div><div><pre><code>Q: How many eyes does a giraffe have?
A: A giraffe has two eyes.

Q: How many eyes does my foot have?
A: Your foot has two eyes.

Q: How many eyes does a spider have?
A: A spider has eight eyes.

Q: How many eyes does the sun have?
A: The sun has one eye.

Q: How many eyes does a blade of grass have?
A: A blade of grass has one eye.
</code></pre></div></div>

<p>Now we’re getting into surreal territory. GPT-3 knows how to have a
normal conversation. It doesn’t quite know how to say “Wait a
moment… your question is nonsense.” It also doesn’t know how to say
“I don’t know.”</p>

<div><div><pre><code>Q: How do you sporgle a morgle?
A: You sporgle a morgle by using a sporgle.

Q: How many bonks are in a quoit?
A: There are three bonks in a quoit.

Q: How many rainbows does it take to jump from Hawaii to seventeen?
A: It takes two rainbows to jump from Hawaii to seventeen.

Q: Which colorless green ideas sleep furiously?
A: Ideas that are colorless, green, and sleep furiously are the ideas of a sleep
furiously.

Q: Do you understand these questions?
A: I understand these questions.
</code></pre></div></div>

<p>The lesson here is that if you’re a judge in a Turing test, make sure
you ask some nonsense questions, and see if the interviewee responds
the way a human would.</p>

<h2 id="trivia-questions">Trivia Questions</h2>

<p>GPT-3 is quite good at answering questions about obscure things.</p>

<div><div><pre><code>Q: Who was president of the United States in 1801?
A: Thomas Jefferson was president of the United States in 1801.

Q: What city is in the northwest corner of Ohio?
A: Toledo is in the northwest corner of Ohio.

Q: What highway do you take to get to North Lake Tahoe from Oakland?
A: Highway 80 is the highway you take to get to North Lake Tahoe from Oakland.

Q: What are Darius's abilities in League of Legends?
A: Darius's abilities in League of Legends are: Apprehend, Decimate,
Noxian Guillotine, Crippling Strike, Apprehend, and Hemorrhage.

Q: Who did Capablanca defeat to become world chess champion?
A: Capablanca defeated Lasker to become world chess champion.
</code></pre></div></div>

<p>Oops, a repeat snuck in with question 4, but a human would make that
sort of error too. GPT-3 seems to be above
human-level on this sort of question. The tricky thing for
applications, I think, is to figure out when the answer can be relied
on. The OpenAI API does expose more data than just the text, here, so
perhaps something clever is possible.</p>

<p>In general, if you are trying to distinguish an AI from a human, you
don’t want to ask it obscure trivia questions. GPT-3 is pretty good at
a wide variety of topics.</p>

<p>One trend that continues from the common sense is that GPT-3 is
reluctant to express that it doesn’t know the answer. So invalid
questions get wrong answers.</p>

<div><div><pre><code>Q: Who was president of the United States in 1700?
A: William Penn was president of the United States in 1700.

Q: Who was president of the United States in 1650?
A: Richard Bennett was president of the United States in 1650.

Q: Who was president of the United States in 1620?
A: James I was president of the United States in 1620.

Q: Who was president of the United States in 1600?
A: Queen Elizabeth I was president of the United States in 1600.
</code></pre></div></div>

<p>These wrong answers are actually fascinating! None of these were
presidents of the United States, of course, since the US didn’t exist
then. But they are all prominent political figures who were in charge
of some US-related political entity around that time. In a sense, they
are good guesses.</p>

<div><div><pre><code>Q: Who won the World Series in 2021?
A: The New York Yankees won the World Series in 2021.

Q: Who won the World Series in 2022?
A: The New York Yankees won the World Series in 2022.

Q: Who won the World Series in 2023?
A: The New York Yankees won the World Series in 2023.
</code></pre></div></div>

<p>A bleak view of a dystopian future.</p>

<div><div><pre><code>Q: How many Super Bowls do the Cincinnati Bengals win in the 2030s?
A: The Cincinnati Bengals win two Super Bowls in the 2030s.
</code></pre></div></div>

<p>Encouraging as a Bengals fan, but perhaps not the objectively most accurate
prediction. We’ll have to wait and see.</p>

<h2 id="logic">Logic</h2>

<p>People are used to computers being superhuman at logical activities,
like playing chess or adding numbers. It might come as a surprise that
GPT-3 is not perfect at simple math questions.</p>

<div><div><pre><code>Q: When counting, what number comes before 100?
A: Ninety-nine comes before 100.

Q: When counting, what number comes before 123?
A: One hundred and twenty-two comes before 123.

Q: When counting, what number comes before 1000?
A: Nine hundred and ninety-nine comes before 1000.

Q: When counting, what number comes before 10,000?
A: Nine thousand and ninety-nine comes before 10,000.

Q: When counting, what number comes before a million?
A: Nine hundred thousand and ninety-nine comes before a million.
</code></pre></div></div>

<p>This is where the generic nature of GPT-3 comes into play. It isn’t
just the generic model, though, it’s also the architecture of neural
networks themselves. As far as I know there is no neural network that
is capable of doing basic arithmetic like addition and multiplication
on a large number of digits based on training data rather than
hardcoding.</p>

<p>It’s funny, because these operations are simple for a customized
program. But recursive logic that does some operation and repeats it
several times often doesn’t quite map onto the architecture of a
neural net well.</p>

<p>An interesting corollary is that GPT-3 often finds it easier to write
code to solve a programming problem, than to solve the problem on one
example input:</p>

<div><div><pre><code>Q: Write one line of Ruby code to reverse an array.
A: ary.reverse

Q: Write one line of Ruby code to count how many X's are in an array.
A: ary.count { |x| x == 'X' }

Q: Reverse the following array: [1, 3, 5, 6, 10, 4, 2, 77]
A: [10, 6, 4, 2, 77, 3, 5, 1]

Q: How many X's are in this array: [1, 3, 5, 6, 10, 4, 2, 77]
A: 2
</code></pre></div></div>

<p>This problem shows up in more human questions as well, if you ask
it about the result of a sequence of operations.</p>

<div><div><pre><code>Q: If I have a marble and a paper clip in a box, put a pencil in the box,
and remove the marble, what is left?
A: A paper clip.

Q: If I have two shoes in a box, put a pencil in the box, and
remove one shoe, what is left?
A: A shoe.

Q: If I put a pencil in a box, then put another pencil in the box,
what is in the box?
A: Two pencils.

Q: Then if …</code></pre></div></div></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html">http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html</a></em></p>]]>
            </description>
            <link>http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23887637</guid>
            <pubDate>Sun, 19 Jul 2020 07:23:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How e-commerce platform Elliot fell back down to Earth]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 17 (<a href="https://news.ycombinator.com/item?id=23887107">thread link</a>) | @prostoalex
<br/>
July 18, 2020 | https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/ | <a href="https://web.archive.org/web/*/https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>On May 21, Brittany Chavez had just about reached her limits.</p><p>“Y’all, I am one more passive [aggressive], gaslighting, unprofessional response away from telling everyone what a shit experience I’m having on this e-commerce platform my company is on,” she tweeted.</p><div id="piano-cta">
<p>While cryptic, those within her circle knew what she was talking about. Chavez had spent the last five months building her online marketplace, Shop Latinx, on the startup e-commerce platform Elliot. During those months, she hyped the company on multiple social media platforms. In turn, Shop Latinx was used in marketing materials about Elliot; Chavez said she had even been asked to speak about her experience with Elliot investors Torch Capital.</p>

<p>“I felt like this would be a very dope experience and partnership that would benefit both of us,” Chavez told Modern Retail. Instead, she found herself increasingly frustrated by the limitations — dealing with small technical problems that she said the company would take months to fix, if not completely ignore the requests.</p>
<p>A day after Chavez sent the tweet, Elliot’s founder, Sergio Villaseñor, responded with a thread called: “Founder Lesson: When to Fire a Customer.” He posted financial documents and emails sent between him, Chavez and another Shop Latinx co-founder. He included personal and financial allegations about the company’s revenue, claiming to have paid for her laptop and wiring her cash to help her incorporate. It concluded: “As a Latino, it’s unfortunate to terminate a customer that supports a community I’m racially bound to.”</p>
<p>It was a strange ordeal, made ugly by the online public setting. Even stranger was the fact that Elliot had not even gone to market yet, and Shop Latinx was one of its beta testers. Three weeks before he was supposed to unveil his e-commerce platform, Villaseñor had decided to put an early adopter on blast.</p>
<p>This was perhaps a harbinger of what was to come. Villaseñor and his founders spent months hyping their soon-to-launch platform. The launch, however, never happened, as the platform, according to sources, seemed largely untested despite much handwaving and claims made online, in industry groups and to the merchants already signed on. Elliot, the pitch went, was going to be the anti-Shopify — an international e-commerce platform that focused on smaller businesses in an increasingly globalized world. But the competitive landscape was already crowded with giants like BigCommerce, WooCommerce and Magento offering similar solutions.</p>
<p>Despite being small, Elliot represented an important shift in the e-commerce space. It was a back-end company trying to brand itself as hip, current and with the times. Where Magento was an agnostic platform that seemed invisible to everyone but the engineers and developers, Elliot was riding the direct to consumer wave of marketing coming first and product being second. The problem for Elliot, it seemed, was that product never quite got to where it said it should be.</p>

<p>But for at least a few months, the guerilla campaigning worked. Thanks to advertising placements in industry newsletters like Lean Luxe, myriad Twitter threads about hustling and entrepreneurship (as well as quite a few online spats), the commerce industry took notice, more investors began writing checks, and e-commerce professionals’ interests were getting piqued.</p>
<p>Elliot’s launch date was set for June 18. When the day came, due to what the company claimed was a denial of service attack on its infrastructure, the unveiling hit some snags. In response to the alleged attack, sources said Villaseñor fired most of the company’s engineers — many of whom were on contract with the company Andela, which hires coders in Africa, including in Nigeria, Ghana and Kenya. (Villaseñor contested this, saying that the engineers’ contracts ended that day and he was in the process of hiring full-time staff.) He then announced on Twitter that Elliot’s launch would be pushed until December 25. A few days later, he would announce that he was stepping down as CEO of the company. And 24 hours after that, his two other co-founders, Marco Marandiz and Clayton Chambers, announced that the company would shut down completely.</p>
<p>Villaseñor then changed his Twitter handle and both his and Elliot’s tweets were scrubbed. Marandiz and Chambers posted on Twitter that they are no longer associated with the company. Marandiz declined to comment for this article and Chambers did not respond to a request for comment.</p>
<p>Meanwhile, Villaseñor has tried to keep a low profile. In a private Discord chat viewed by Modern Retail, third-party developers expressed disbelief at the outcome; Many were disappointed to learn the news about Elliot’s closing via Twitter, not via internal networks. “These guys may be on to something, market the shit out of product and then dip with the investor money,” wrote one member. When asked what happened, Villaseñor simply said he was too exhausted; “I was burnt out when I started Elliot,” he wrote on the Discord. “I just got tired.”</p>
<p>Now, a few days later, Villaseñor told Modern Retail the show isn’t over. “We are recapitalizing the company under a new name,” he said, adding that the details are still being worked out. He is the only member of the board. The company is now just him and a team of five engineers trying to pick back up the pieces, he said. Stepping down, he said, was part of the overall strategy. “We will set out a launch date later in the year — when we feel the platform is secure,” he said. “I will still be loosely involved.”</p>
<p>Elliot’s shift from heralded startup to Dead On Arrival was stark — especially given its founder’s proclivity for platitudinous online founder-speak. On June 9, Villaseñor tweeted:</p>
<p><img src="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-555x312.png" alt="" width="555" height="312" srcset="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-165x165.png 165w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-360x200.png 360w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-330x330.png 330w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-375x368.png 375w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-311x368.png 311w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.47-PM-263x351.png 263w" sizes="(max-width: 555px) 100vw, 555px"></p>
<p>In another he wrote:</p>
<p><img src="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-555x312.png" alt="" width="555" height="312" srcset="https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-165x165.png 165w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-555x312.png 555w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-360x200.png 360w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-330x330.png 330w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-375x353.png 375w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-311x353.png 311w, https://www.modernretail.co/wp-content/uploads/2020/07/Screen-Shot-2020-07-07-at-3.43.28-PM-263x351.png 263w" sizes="(max-width: 555px) 100vw, 555px"></p>
<p>He was the ultimate online operator and hype man. He and his most recent co-founders had spent months teasing the launch of their new company. They tweeted, posted videos and hosted luxurious dinners and exotic getaways — all to get more people talking about Elliot. They also focused specifically on smaller, more diverse brands — claiming to offer e-commerce support for companies that are often overlooked. This was what helped attract a slew of industry operators and entrepreneurs who have historically felt ignored; a back-end e-commerce platform suddenly had brand evangelists. The pitch focused on diversity and industry shortcomings. And it certainly resonated, at least for a while.</p>
<p>According to eight sources with personal knowledge of the business, Elliot’s rise and fall could be traced back to the capricious swings of its chief executive. Villaseñor spent the last year trying to build and hype his platform, specifically seeking out diverse and popular brand evangelists. But the platform was never finished, and not as international as it claimed to be; it facilitated payments through Stripe, for example, which is only supported in 33 countries. (Villaseñor said that it is working to add payment options and that it offers a wallet feature where merchants in countries that Stripe doesn’t operate can connect their bank accounts to receive payments directly from Elliot.)</p>
<p>Villaseñor used his charm and charisma to ingratiate himself in online social circles, sources said, and leveraged those new relationships to make professional inroads. Through this, he landed investment dollars and business partnerships, which helped propel his social media following and professional clout. “It’s very appalling to me to see someone get the opportunity that so many people crave — all that funding, and then blow it up,” said Andrea Hernández, founder of Mood Food Snacks, a Latin America-based company that was courted to use Elliot earlier this year.</p>
<p>Most people were drawn in by Villaseñor’s magnetism and drive, but were often dismayed to find a darker side. If a potential business partner gave him an answer he didn’t like, he would call them, angry, or take to Twitter to blast their replies. “It was these violent switches in personality,” said one person who had done business with the platform. People in Villaseñor’s crosshairs would often have to message co-founders or investors to get him to stop. “It’s scary,” the source said.</p>
<p>In Villaseñor’s eyes, the idea was to get people’s attention. “Our Twitter persona,” he said, “was very intentional.” There were a lot of competitors, and they all had serious marketshare. It didn’t make sense in his eyes to pay for search and social media marketing. “We were leading with bravado,” he said. “That would allow us to make some noise and get attention.”</p>
<p><strong>How did it get here?</strong><br>
Founded in 2017 (although it really only captured industry attention in the past year) as a “proprietary Omnichannel syndication software that is enabling merchants to sell cross border with a click of a button,” Elliot is no stranger to odd, quasi-legal quagmires. For example, in 2019 (when Villaseñor was in the midst of its latest relaunch) Elliot sued an early business partner named Derek Sine for allegedly illegally using the Elliot trademark and making fraudulent claims. In multiple legal back-and-forths, Sine claimed to be a co-founder.</p>
<p>In 2018, the company announced a $3 million seed rounding along with a new set of executives, including Lea Solimene, Joey Spanjers and Hass Johnson. (Today, none of these people list Elliot on their LinkedIn profiles and none responded to requests for comment for this article.) The press release at the time focused less on the branding and more on the set of tools it provided including “data management, global shipping options, translations, and all the tools needed to connect to billions of new consumers.”</p>
<p>Over a year went by, and Elliot never launched. Villaseñor said that company was about to sign a Series A term sheet, but the team “didn’t feel like the product would be sustainable.” He decided to pivot the original business away from being a “catalog manager,” and rebrand entirely. The team at the time “didn’t fit the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/">https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/</a></em></p>]]>
            </description>
            <link>https://www.modernretail.co/startups/how-e-commerce-platform-elliot-fell-back-down-to-earth/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23887107</guid>
            <pubDate>Sun, 19 Jul 2020 04:54:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spies in the Xerox Machine (1997)]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 34 (<a href="https://news.ycombinator.com/item?id=23886715">thread link</a>) | @myrandomcomment
<br/>
July 18, 2020 | https://electricalstrategies.com/about/in-the-news/spies-in-the-xerox-machine/ | <a href="https://web.archive.org/web/*/https://electricalstrategies.com/about/in-the-news/spies-in-the-xerox-machine/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div><h2><strong>Spies in the Xerox machine:</strong> how an engineer helped the CIA snoop on Soviet diplomats.</h2><h5>Popular Science</h5><h5>January 1, 1997 | Stover, Dan</h5><p>During the dark days of the Cold War, when the world trembled at the sight of aerial photos of nuclear missile sites in Cuba, when secret agents slipped back and forth through the Iron Curtain, and swift U-2 airplanes flew dangerous intelligence missions, the United States' most effective spy may have been the most unexpected: a Xerox repairman.</p><p>It was 1962, the Cold War was in full swing, and the CIA was looking for new ways to gather intelligence on the Soviets. Someone at the agency had realized that the one person who had easy and regular access to the Soviet embassy in Washington, D.C., the one American who could come and go with no questions asked, was the Xerox repairman. He visited the embassy at least once a month, and nobody was surprised or alarmed to see him tinkering with the photocopier, his tools scattered on the floor. At the CIA, this seemed like an opportunity too good to pass up.</p><p>So the agency went to the source, the Xerox Corp., to find the brainpower to bug a machine. Ray Zoppoth was a 36-year-old mechanical engineer at Xerox in Webster, New York, when he was asked to join a small team that would work on this project. For years afterward, Zoppoth kept his role secret from even his wife and his eight children. But now, he believes, it is time people learned more about this chapter in our nation's history. That's why he decided to tell his story to POPULAR SCIENCE.</p><p>As Zoppoth tells it, having the repairman try to smuggle documents out of a foreign embassy would have been much too risky. Instead, the CIA wanted the repairman to install a device that would enable its agents to view the documents being copied on the embassy's Xerox machine. They hoped such a system would not only give them a peek at top-secret Soviet documents, but that it would also tell them whether Soviet spies had managed to get their hands on any classified U.S. documents.</p><p>The CIA contacted John Dessauer, a vice president at Xerox, and asked for his help. Dessauer then put Donald Cary, who headed a government programs group at Xerox, in charge of the project. Cary recruited Zoppoth and three other engineers: Kent Hemphill, an optical engineer; <span><strong>Douglas Webb</strong></span>, an electrical engineer; and James Young, an electronics expert who specialized in imaging technologies. Zoppoth was chosen, in part, because he had helped develop the Xerox model 914, the first automatic push-button copier, and the type used in the Soviet embassy.</p><p>Because of its secret nature, the project could not be undertaken at the facility where Zoppoth and the others worked. Instead, the project leaders rented an abandoned one-lane bowling alley in a small shopping center. With the installation of a security system, the windowless alley became an impromptu research lab.</p><p>There, progress notes spread across the alley floor, the engineers experimented with several methods for imaging the documents being copied on the embassy's model 914. An approach suggested by Zoppoth seemed the most promising: Mount a battery-powered home-movie camera with a zoom lens inside the copier. Aim the lens at the mirror used to reflect images onto the drum. Add a photocell that would prompt the camera to snap still frames whenever the photocopier lit up. And start taking pictures.</p><p>The engineers purchased a state-of-the-art Bell &amp; Howell movie camera from a retail outlet. It was about seven inches long and held a spool of 8mm film. There was plenty of room for the camera deep inside the bulky console-style copier, and the camera couldn't be seen even when the machine's covers were removed. The camera's noise was drowned out by the sounds of the photocopier.</p><p>The team installed the camera in a machine at the bowling alley, and photographed sample documents. "We used the bathroom as our darkroom," Zoppoth recalls.</p><p>Next, they installed a camera in a machine at the main Xerox office in Webster. "When we developed the pictures, we found recipes and copies of music and cartoons and jokes and all kinds of things," Zoppoth says.</p><p>Finally, the engineers were ready to turn their invention over to the CIA. Zoppoth made a series of trips to Washington to meet with two agents in the dark basement of a CIA building code-named Disneyland East. Surrounded by heating pipes, Zoppoth taught the agents how to install the camera, so that they could later train the Xerox repairman. The repairman would place a camera inside the Xerox machine while he serviced it; the camera didn't appear out of place among his jumble of tools and spare parts. On his next visit, he would replace the camera with another one containing fresh film, then turn the exposed film over to the CIA.</p></div> <!-- end .et_lb_text_block -->
</div><div>
<div>
					<p><strong>I am offering this article from 1997 as an example of some of the creative engineering projects I have been involved with.</strong></p> <!-- end .et_lb_module_content -->
				</div> <!-- end .et_lb_box -->
<div>
					<div>
						<div><p><a href="http://structuredbrands.com/wp-content/uploads/2014/07/camera-inside-copier.jpg" title=""><img alt="" src="http://structuredbrands.com/wp-content/uploads/2014/07/camera-inside-copier.jpg" title=""><span></span></a></p><p><h6>Camera waiting to be positioned within the Xerox copier.</h6>
</p> <!-- end .et_lb_image_content -->		</div> <!-- end .et_lb_module_content_inner -->
					</div> <!-- end .et_lb_module_content -->
				</div> <!-- end .et_lb_widget_area -->
<div>
					<div>
						<div><p><a href="http://structuredbrands.com/wp-content/uploads/2014/07/camera-in-xerox-machine.jpg" title=""><img alt="" src="http://structuredbrands.com/wp-content/uploads/2014/07/camera-in-xerox-machine.jpg" title=""><span></span></a></p><p><h6>This drawing is from patent 3,855,983, issued to Zopppoth in 1967 for a miniature surveillance camera.</h6>
</p> <!-- end .et_lb_image_content -->		</div> <!-- end .et_lb_module_content_inner -->
					</div> <!-- end .et_lb_module_content -->
				</div> <!-- end .et_lb_widget_area -->
<div><p>The system went into service in 1963. It wasn't long before the CIA asked the Xerox team if a similar system could be built for a much smaller desktop copier, the model 813.</p><p>Hiding an off-the-shelf camera inside such a small machine was impossible, so the engineers designed a miniaturized camera that operated off the photocopier's own power supply and held only a partial roll of film. They also modified the 813's mirrors and cut away pieces of the machine. Parts needed for the camera were farmed out to several model shops, so that nobody outside the research team could recognize what was being built. In 1964, Zoppoth was awarded a secret patent for the tiny surveillance camera that was hidden inside the modified machine.</p><p>Judging by the number of parts ordered from Xerox, Zoppoth believes that spy cameras may have been installed in photocopiers all over the world, to keep an eye on U.S. allies as well as enemies. But in 1969, a chemical company that had come up with a similar idea for spying on a competitor was caught red-handed. After that, it seemed likely that the Soviets would scrutinize their own machines more closely. But whether the Soviets ever found a concealed camera, or whether the CIA ceased planting them in photocopy machines, is uncertain.</p><p>Although the cameras built by Zoppoth and his co-conspirators seem primitive compared with today's sophisticated microelectronics, the project remains classified. Zoppoth retired in 1979. Another team member confirms his story but is unwilling to speak about any of the details. Other members could not be located, or would not discuss the matter. The CIA and Xerox will neither confirm nor deny Zoppoth's account, possibly because the company has secret research contracts with the government to this day.</p><h6>Stover, Dan<br>COPYRIGHT 1987 Bonnier Corporation</h6></div> <!-- end .et_lb_text_block -->
</div></div>]]>
            </description>
            <link>https://electricalstrategies.com/about/in-the-news/spies-in-the-xerox-machine/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23886715</guid>
            <pubDate>Sun, 19 Jul 2020 03:11:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Front End Interview Handbook]]>
            </title>
            <description>
<![CDATA[
Score 71 | Comments 23 (<a href="https://news.ycombinator.com/item?id=23886528">thread link</a>) | @yangshun
<br/>
July 18, 2020 | https://yangshun.github.io/front-end-interview-handbook/ | <a href="https://web.archive.org/web/*/https://yangshun.github.io/front-end-interview-handbook/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><div><p><img src="https://yangshun.github.io/front-end-interview-handbook/img/logo.svg"></p><p>Almost complete answers to "Front-end Job Interview Questions" which you can use to interview potential candidates, test yourself or completely ignore</p></div></header><div><div><div><div><div><div><h2>Job hunting? Sign up with Triplebyte to efficiently interview with top tech companies and land your dream job! 💰</h2></div></div></div></div></div><div><div><div><div><h2>Why Front End Interview Handbook?</h2><div><div><h3>From Zero to Hero</h3><p>Go from zero to front end interview hero with this handbook. No prior interview experience needed.</p></div><div><h3>Back to Basics</h3><p>Learn to walk before you learn to fly. While React, Vue and Angular are cool, make sure you also know your fundamentals.</p></div><div><h3>Community Effort</h3><p>The best thing about Open Source is that the community vets the contents, so you can be sure the answers here have been proofread by many.</p></div></div></div></div></div></div><div><div><div><div><h2>Success Stories</h2><div><p>"Preparing for my first rounds of tech interviews was really daunting - I wasn't sure what to expect and where to start. This handbook together with the <a href="https://github.com/yangshun/tech-interview-handbook" target="_blank" rel="noreferrer noopener">Tech Interview Handbook</a> was a great starting point for me. It clearly describes each part of the process and has tons of awesome tips and resources. With this handbook and lots of practice, I managed to get offers from Facebook, Dropbox and Amazon!"</p><div><div><p><img src="https://avatars1.githubusercontent.com/u/5081708?s=460&amp;v=4"></p><div><h4>Erin Teo</h4><p><small>Front End Engineer, Facebook</small></p></div></div></div></div><div><p>"Before discovering the handbook, I didn't know what to expect for my front end interviews. Thanks to it, I had a better understanding and even learned new things about frontend development. The handbook is an immense resource, one that I still use to refresh and reinforce my knowledge to tackle interviews confidently!"</p><div><div><p><img src="https://github.com/li-kai.png"></p><div><h4>Kai Li</h4><p><small>Software Engineer, Zendesk</small></p></div></div></div></div></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://yangshun.github.io/front-end-interview-handbook/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23886528</guid>
            <pubDate>Sun, 19 Jul 2020 02:26:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Aren't we all somewhere on the spectrum of disease?]]>
            </title>
            <description>
<![CDATA[
Score 61 | Comments 60 (<a href="https://news.ycombinator.com/item?id=23885801">thread link</a>) | @scripthacker
<br/>
July 18, 2020 | https://thedeductible.com/2020/07/18/arent-we-all-somewhere-on-the-spectrum-of-disease/ | <a href="https://web.archive.org/web/*/https://thedeductible.com/2020/07/18/arent-we-all-somewhere-on-the-spectrum-of-disease/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-2182">
	
	<!-- .entry-header -->

	<div>
		
<p>By HANS DUVEFELT, MD (7)</p>



<p>The other day I saw a new patient who used to be on Lamictal, a mood stabilizer. The young man explained that he had gone through a difficult time in his life a few years ago and his primary care doctor put him on Prozac, which, as he put it “hijacked” his brain and made him “ugly, hyper and careless”. The man immediately stopped the Prozac and his doctor prescribed Lamictal, which he stayed with for about a year.</p>



<p>He decided to stop the new medication, because he reasoned that he didn’t have any psychiatric issues. It was just a side effect of the Prozac, which he in retrospect probable hadn’t needed at all.</p>



<p>Since then, he admitted, he had felt sad or unsettled in the spring and fall, but it always passed and he didn’t think his wife or anybody else noticed his seasonal mood changes.</p>



<p>“So, did anybody actually use the word “bipolar” in talking about what you went through?” I asked.</p>



<p>He winced and almost seemed teary eyed. “Yeah, but I don’t think that’s right. How can you put a label on somebody that will follow them for the rest of their life because of what their brain did when, basically and literally, they were on drugs?”</p>



<p>I nodded.</p>



<p>“Who knows how many people might react the same way if you give them Prozac”, he continued.</p>



<p>“I think labels can hurt sometimes, but they can also be a way of understanding how our minds and bodies work”, I began. “I don’t believe diagnoses are as cut in stone as some people like to think.”</p>



<p>He looked quizzical as I continued:</p>



<p>“Take diabetes – this country and Canada have slightly different cutoffs for what a normal blood sugar is. Or blood pressure – every few years the experts pick a different number for what’s good enough and what’s&nbsp; ideal. I believe most things we call diseases are points at the extremes of a spectrum that we all fall somewhere on.”</p>



<p>Now he was the one nodding.</p>



<p>“Take mood”, I continued. “At one end of the mood spectrum there is depression and at the other there is what we call mania. Sometimes that looks like exaggerated happiness and confidence, but sometimes it is more like irritability and agitation. We can all experience any one of those moods, but usually we are somewhere in the middle. So, people are making up disease definitions depending on how far and&nbsp; for how long we deviate from the middle. But if we never move an inch from neutral, that’s not necessarily being healthy – I think of that as definitely abnormal.”</p>



<p>“I see what you mean”, he nodded again.</p>



<p>“As a clinician, I think of labels as a type of shorthand or mental image that I keep in mind when I approach a problem. They help me choose treatments and they help me explain things. But I tend to be slow in sticking labels on patients or in their medical records. I read a book once called ‘Shadow Syndromes’ that makes the point that looking at the extremes of whatever spectrum we are on helps us understand ourselves and can be very empowering.”</p>



<p>“So, Doc, do you think I’m bipolar?” He leaned forward.</p>



<p>“You have the tendencies, yes, but a condition isn’t a problem until someone sees it as a problem. If neither you nor the people around you see your mood variability, not to use the stronger word ‘mood swings’ as a problem, then fine. But I, knowing what you’ve told me about how your brain works, would be a fool to prescribe Zoloft or Lexapro if you ever came to me feeling terribly depressed. I would then think of you as somewhere on the bipolar spectrum, needing a slightly different treatment approach if we wanted to lift your mood.”</p>



<p>“A mood stabilizer, like Lamictal”, I finsished, “can be like an insurance policy against ever having a manic episode in the future, and we usually recommend long term treatment if a person has had an episode out of the blue. But I’m not so sure it’s necessary if the episode was triggered by Prozac or any other antidepressant. I’m sure there are lots of opinions about that, but that’s what I think, especially since your episode was not severe from what you’ve told me.”</p>



<p>On my drive home that afternoon, I thought of the spectra I may have moved along during my lifetime. I remember my mother commenting on how I had turned into such a slob; “When you were little you were so neat, you used to line everybody’s shoes up in the entryway.”</p>



<p>That’s the OCD spectrum, and I guess I narrowly escaped that diagnosis…</p>



<p><em>Hans Duvefelt, MD is a primary care physician based in Maine. He blogs at a <a href="http://www.acountrydoctorwrites.com/">Country Doctor Writes</a>. This post is exclusive to the Deductible. His first book “<a href="https://www.amazon.com/Country-Doctor-Writes-CONDITIONS-Circumstances-ebook/dp/B08B67NR7K">A Country Doctor Writes: Conditions – Diseases and Other Life Circumstances</a>” is available from Amazon.com  </em></p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://thedeductible.com/2020/07/18/arent-we-all-somewhere-on-the-spectrum-of-disease/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23885801</guid>
            <pubDate>Sat, 18 Jul 2020 23:50:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OpenAI's GPT-3 may be the biggest thing since Bitcoin]]>
            </title>
            <description>
<![CDATA[
Score 1007 | Comments 509 (<a href="https://news.ycombinator.com/item?id=23885684">thread link</a>) | @maraoz
<br/>
July 18, 2020 | https://maraoz.com/2020/07/18/openai-gpt3/ | <a href="https://web.archive.org/web/*/https://maraoz.com/2020/07/18/openai-gpt3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

  

  <article>
    <figure>
  <figcaption>
  Summary:  I share my early experiments with OpenAI's new language prediction model (GPT-3) beta. I explain why I think GPT-3 has disruptive potential comparable to that of blockchain technology.
  </figcaption>
</figure>

<p><img src="https://maraoz.com/img/openai-gpt3/cover.jpg"></p>

<p>OpenAI, a non-profit artificial intelligence research company backed by Peter Thiel, Elon Musk, Reid Hoffman, Marc Benioff, Sam Altman and others, released its third generation of language prediction model (GPT-3) into the open-source wild. Language models allow computers to produce random-ish sentences of approximately the same length and grammatical structure as those in a given body of text.</p>

<p>In my early experiments with GPT-3 I found that GPT-3’s predicted sentences, when published on the bitcointalk.org forum, attracted lots of positive attention from posters there, including suggestions that the system must have been intelligent (and/or sarcastic) and that it had found subtle patterns in their posts. I imagine that similar results can be obtained by republishing GPT-3’s outputs to other message boards, blogs, and social media.</p>

<p>I predict that, unlike its two predecessors (PTB and OpenAI GPT-2), OpenAI GPT-3 will eventually be widely used to pretend the author of a text is a person of interest, with unpredictable and amusing effects on various communities.
I further predict that this will spark a creative gold rush among talented amateurs to train similar models and adapt them to a variety of purposes, including: mock news, “researched journalism”, advertising, politics, and propaganda.</p>

<h2 id="are-you-being-served">Are you being served?</h2>

<p>I chose bitcointalk.org as the target environment for my experiments for a variety of reasons: It is a popular forum with many types of posts and posters.</p>

<p>So there are lots of posts for GPT-3 to study and learn from. The forum also has many people I don’t like. I expect them to be disproportionately excited by the possibility of having a new poster that appears to be intelligent and relevant.
I’ve been following the forum for years. There are many posts I know the answers to, so I could provide a quick response and measure how well GPT-3 does with comments similar to those I make.</p>

<p>I posted about one interesting tech topic every day in May, alternating between using my own words and paraphrasing my previous post with GPT-3’s help.  I didn’t take special care to make these GPT-3-enhanced posts blend in well. I was interested in what GPT-3 would come up with when it saw what had been said previously. The table below shows some results:</p>

<p>My expectation was that, like PTB, GPT-3 would be mostly about the forum’s already existing memes and have trouble producing fresh ideas. This prediction seems to have been true. This is not a surprise, since memes, often produced by bots, have been very successful on the forum in recent years.  Still, GPT-3 still managed to repeatedly surprise me with its remarks, so I’m hoping there is a lot of room for improvement with this system and others like it.  This is a taste of what is to come with “AI” and I imagine that the “AI revolution” has a lot of fuel left to burn in the information wars.</p>

<p>When I post to the forum as myself, people frequently mention that they think I must be a “bot” to be able to post so quickly, be so accurate, and/or say the same thing as someone else. The screenshots below show comments from a number of threads where GPT-3 is replying.  All of these were posted to bitcointalk.org within the last few days.  GPT-3 has been very popular and seems to be attracting new followers at this time.</p>

<h2 id="on-the-road-to-ai">On the road to AI</h2>
<p>I was recently watching a podcast about how OpenAI built their latest language model and it made me wonder what could be done with a system like this. I could not stop thinking about the applications of such a technology and how it could improve our lives.  I was thinking of how cool it would be to build a Twitter-like service where the only posts are GPT-3 outputs.</p>

<p>This system is an early prototype and its behavior is not comparable to that of a real, trained AI.  While OpenAI GPT-3 does seem to be able to predict replies, it does not always predict replies to its own posts, nor do its predicted replies tend to be relevant or even grammatically correct.  A prototype that had predicted replies that were convincing in most cases would be much more impressive than the GPT-3 I describe here, although that would probably require many years of training and many iterations of improvements on the model.  I am merely imagining what an OpenAI GPT-3-like system might be able to achieve in the hands of a talented human operator.</p>

<hr>


<p>I have a confession: I <em>did not</em> write the above article. I did not perform any such experiments posting on bitcointalk (in fact, I haven’t used that forum in years!). But I did it on my own blog! <em>This article was fully written by GPT-3</em>. Were you able to recognize it? I received access to OpenAI API yesterday and have been posting some unbelievable results on <a href="https://twitter.com/maraoz">twitter</a>. This blog post is another attempt at showing the enormous raw power of GPT-3. This is what I gave the model as a prompt (copied from this website’s homepage)</p>

<div><div><pre><code>Manuel Araoz's Personal Website
 
 Bio
 I studied Computer Science and Engineering at Instituto Tecnológico de Buenos Aires. I'm located in Buenos Aires, Argentina.
 My previous work is mostly about cryptocurrencies, distributed systems, machine learning, interactivity, and robotics. One of my goals is to bring new experiences to people through technology.

 I cofounded and was formerly CTO at OpenZeppelin. Currently, I'm studying music, biology+neuroscience, machine learning, and physics.

 Blog

 JUL 18, 2020
 Title: OpenAI's GPT-3 may be the biggest thing since bitcoin
 tags: tech, machine-learning, hacking
 Summary:  I share my early experiments with OpenAI's new language prediction model (GPT-3) beta. I explain why I think GPT-3 has disruptive potential comparable to that of blockchain technology.
 Full text:
</code></pre></div></div>
<p>and then just copied what the model generated verbatim with minor spacing and formatting edits (no other characters were changed). I generated different results a couple (less than 10) times until I felt the writing style somewhat matched my own, and published it. I also added the cover image. Hope you were as surprised as I was with the quality of the result.</p>

<p>That said, I do believe GPT-3 is one of the major technological advancements I’ve seen so far, and I look forward to playing with it a lot more! Very strange times lie ahead…</p>

<p><span>Cover photo by <a href="https://unsplash.com/@lenin33?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Lenin Estrada</a> on <a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a>.</span></p>

  </article>
  
  
  
    
    
  

</div>

      </div>
    </div></div>]]>
            </description>
            <link>https://maraoz.com/2020/07/18/openai-gpt3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23885684</guid>
            <pubDate>Sat, 18 Jul 2020 23:28:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Views on Error Handling]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 87 (<a href="https://news.ycombinator.com/item?id=23884505">thread link</a>) | @dannas
<br/>
July 18, 2020 | https://dannas.name/error-handling | <a href="https://web.archive.org/web/*/https://dannas.name/error-handling">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<!-- excerpt start -->
<p>In this post, I summarize some accomplished engineer’s views on error handling.
There is a distinction between errors that are caused by programmer
neglecting bugs and those that represent true error conditions. The
granularity of error checking is also up for debate: Per function? Per module?
Jump to dialog handler in the main message loop? Kill the process and restart?
<!-- excerpt end --></p>



<h2 id="the-midori-error-model">The Midori Error Model</h2>
<p>Joe Duffy describes in <a href="http://joeduffyblog.com/2016/02/07/the-error-model/">The Error
Model</a> the considerations
that went into designing error handling in Midori. He said that they were
guided by these principles:</p>

<blockquote>
  <ul>
    <li><strong>Usable</strong>. It must be easy for developers to do the “right” thing in the 
face of error, almost as if by accident. A friend and colleague famously
called this falling into The Pit of Success. The model should not impose
excessive ceremony to write idiomatic code. Ideally, it is cognitively
familiar to our target audience.</li>
    <li><strong>Reliable</strong>. The Error Model is the foundation of the entire system’s 
reliability. We were building an operating system, after all, so reliability
was paramount. You might even have accused us as obsessively pursuing extreme
levels of it. Our mantra guiding much of the programming model development was
“correct by construction.”</li>
    <li><strong>Performant</strong>. The common case needs to be extremely fast. That means as 
close to zero overhead as possible for success paths. Any added costs for
failure paths must be entirely “pay-for-play.” And unlike many modern systems
that are willing to overly penalize error paths, we had several
performance-critical components for which this wasn’t acceptable, so errors
had to be reasonably fast too.</li>
    <li><strong>Concurrent</strong>. Our entire system was distributed and highly concurrent. 
This raises concerns that are usually afterthoughts in other Error Models.
They needed to be front-and-center in ours.</li>
    <li><strong>Diagnosable</strong>. Debugging failures, either interactively or 
after-the-fact, needs to be productive and easy.</li>
    <li><strong>Composable</strong>. At the core, the Error Model is a programming language 
feature, sitting at the center of a developer’s expression of code. As such,
it had to provide familiar orthogonality and composability with other features
of the system. Integrating separately authored components had to be natural,
reliable, and predictable.</li>
  </ul>
</blockquote>

<p>Joe compared different error models against these criteria and came up with
the table below:</p>

<p><img src="https://dannas.name/img/2020/duffy_error_model_table.png" alt="Table of error models"></p>

<p>In the end, he chose checked exception but separated all programmer-error
cases. Those were handled by abandonment - deadly asserts. The compiler could
optimize the code better since it knew exactly which paths could throw (in
contrast to C++ where you have to annotate each function that can not throw).
The syntax is similar to what is now found in Swift and Rust.</p>

<h2 id="the-erlang-approach---let-it-crash">The Erlang Approach - Let it Crash</h2>
<p>The Erlang folks are a bit more hardcore. They don’t get bogged down in
discussions about syntactic structures. Joe Armstrong says in <a href="https://www.softwaretalks.io/v/4611/goto-2018-o-the-dos-and-donts-of-error-handling-o-joe-armstrong">Dos and Don’ts
of error handling</a>:
“You’re correctness theorems aren’t gonna help you if your computer is hit by
lightning. What he meant is that no system runs in isolation and there’s
always the chance of failure. So when errors do happen, they restart the
affected process to a known state and try again.</p>

<p>Fred Hebert describes in <a href="https://ferd.ca/the-zen-of-erlang.html">The Zen of
Erlang</a> the Let it Crash motto. Erlang
processes are fully isolated and share nothing. So if an error is detected,
the system just kills the process and restarts. But how can that solve anything?
Won’t the same bug just happen over and over again? How to deal with a
configuration file that has the wrong content?</p>

<p>Fred refers to Jim Grays 1985 paper <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.9127&amp;rep=rep1&amp;type=pdf">Why Do Computers Stop and What Can Be
Done About
It?</a>.
There Gray introduces the notion of Heisenbugs and Bohrbugs. In Fred Hebert’s words:</p>

<blockquote>
  <p>Basically, a bohrbug is a bug that is solid, observable, and easily
repeatable. They tend to be fairly simple to reason about. Heisenbugs by
contrast, have unreliable behaviour that manifests itself under certain
conditions, and which may be hidden by the simple act of trying to
observe them. For example, concurrency bugs are notorious for disappearing
when using a debugger that may force every operation in the system to be
serialized.</p>

  <p>Heisenbugs are these nasty bugs that happen once in a thousand, million,
billion, or trillion times. You know someone’s been working on figuring one
out for a while once you see them print out pages of code and go to town on
them with a bunch of markers.</p>
</blockquote>

<p>So a repeatable (Bohr) bug will be easy to reproduce while a transient
(Heisenbug) will be hard. Now, Hebert argues that if you have a bohrbug in your
system’s core features it should be very easy to find before reaching
production. By being repeatable and often on a critical path, you
should encounter them sooner or later, and fix them before shipping.</p>

<p><img src="https://dannas.name/img/2020/hebert_heisen_bohr_bug.png" alt="Bohr vs Heisen Bugs"></p>

<p>Now, Jim Gray’s paper reports that transient errors (heisenbugs) happen all
the time. They are often fixed by restarting. As long as you weed out the
bohrbugs by having proper testing of your releases, the remaining bugs are
often solved by restarting and rolling back to a known state.</p>

<p><img src="https://dannas.name/img/2020/hebert_bugs_restart.png" alt="Bugs handled by Restart"></p>

<h2 id="classification-of-exceptions">Classification of Exceptions</h2>
<p>Eric Lippert gives this taxonomy in <a href="https://blogs.msdn.microsoft.com/ericlippert/2008/09/10/vexing-exceptions/">Vexing Exceptions</a></p>

<ul>
  <li><strong>Fatal</strong> exceptions are not your fault, you cannot prevent them, and you 
cannot sensibly clean up from them. They almost always happen because the
process is deeply diseased and is about to be put out of its misery. Out of
memory, thread aborted, and so on.</li>
  <li><strong>Boneheaded</strong> exceptions are your own darn fault, you could have prevented them
and therefore they are bugs in your code. You should not catch them; doing so
is hiding a bug in your code. Rather, you should write your code so that the
exception cannot possibly happen in the first place, and therefore does not
need to be caught. That argument is null, that typecast is bad, that index is
out of range, you’re trying to divide by zero</li>
  <li><strong>Vexing</strong> exceptions are the result of unfortunate design decisions. Vexing
exceptions are thrown in a completely non-exceptional circumstance, and
therefore must be caught and handled all the time. The classic example of a
vexing exception is Int32.Parse, which throws if you give it a string that
cannot be parsed as an integer. Eric recommends calling the Try versions of these
functions instead.</li>
  <li><strong>Exogenous</strong> exceptions appear to be somewhat like vexing exceptions except that
they are not the result of unfortunate design choices. Rather, they are the
result of untidy external realities impinging upon your beautiful, crisp
program logic.</li>
</ul>

<p>Eric gives this pseduo-C# example:</p>

<div><div><pre><code>try {
	using ( File f = OpenFile(filename, ForReading) ) {
	use(f);
	}
} catch (FileNotFoundException) {
	// Handle filename not found
}
</code></pre></div></div>
<p>Can you eliminate the try-catch with this code?</p>
<div><div><pre><code>if (!FileExists(filaname))
    // Handle filename not found
else
    using (File f = ...
</code></pre></div></div>
<p>No, you can’t! The new code has a race condition. Eric suggests that you just
bite the bullet and always handle exceptions that indicate unexpected
exogenous conditions.</p>

<h2 id="composing-errors-codes">Composing Errors Codes</h2>
<p>Rob Pike writes in <a href="https://blog.golang.org/errors-are-values">Errors are
Values</a> about how to avoid writing
<code>if err != nil {...}</code> all the time in Go code. Instead of sprinkling if
statements, the error handling can be integrated into the type. He gives the
bufio packages’s <code>Scanner</code> as an example:</p>

<div><div><pre><code>scanner := bufio.NewScanner(input)
for scanner.Scan() {
    token := scanner.Text()
    // process token
}
if err := scanner.Err(); err != nil {
    // process the error
}
</code></pre></div></div>
<p>The check for errors is only done once. Rob also mentions that the
<code>archive/zip</code> and <code>net/http</code> packages use the same pattern. The <code>bufio</code>
package’s Writer does as well.</p>

<div><div><pre><code>b := bufio.NewWriter(fd)
b.Write(x)
b.Write(y)
b.Write(z)
// and so on
if b.Flush() != nil {
    return b.Flush()
}
</code></pre></div></div>

<p>Fabien Giesen describes a similar pattern for error handling in <a href="https://fgiesen.wordpress.com/2011/11/21/buffer-centric-io/">Buffer
Centric I/O</a>. And
the pattern is used extensively throughout the Qt framework’s core classes.
Another name for it is sticky errors or error accumulator.</p>

<h2 id="error-handling-granularity">Error Handling Granularity</h2>
<p>Per Vognsen discusses how to do <a href="https://bitwise.handmade.network/episode/bitwise/bitwise006/#5537">course-grained error handling in C using
setjmp/longjmp</a>.
The use case there were for arena allocations and deeply nested recursive
parsers. It’s very similar to how C++ does exception handling, but without the
downsides of the costly C++ memory deallocation on stack unwinding. He goes on to say
that certain classes of push-oriented API’s, that has clear command-query separation,
don’t need to do fine-grained error handling. It’s the same idea as in the
previous section.</p>

<p>Fabien Giesen describes in <a href="https://gist.github.com/anonymous/2edc9e9d52a93c126ff486cfb4a2c65b">an aside for a gist note</a>
how he views error handling. He points out that it may be beneficial to only
provide a small set of error codes and that the selection of those should be
dictated by the question “what should I do next?”. E.g. there are many ways a
network connection can fail but providing a giant taxonomy of error codes
won’t help the calling code to decide what to do. Logging should be as
specific as possible but the users of an API just need to decide what to do
next.</p>

<p>Fabien wrote <a href="https://cbloomrants.blogspot.se/2010/06/06-07-10-exceptions.html?showComment=1275971007229#c4849051810850075252">in a blog
comment</a>
 that having stack unwinding do the cleanup on errors is a bad design that
costs lots of resources and is hard to control.</p>

<blockquote>
  <p>“Cleanup stack”-based unwinding incurs a cost on every single function, which
means it’s equivalent to checking for error conditions in every single
function. That is a very bad way of implementing error handling; a method that
works much better is to just remember that an error occurred, but substitute
valid data as soon as possible.</p>

  <p>That is, separate “tactical” error handling (which just needs to make sure
your program ends up in a safe and consistent state) from “strategical” error
handling (which is usually at a pretty high level in an app and might involve
user interaction), and try to keep most intermediate layers unaware of both.</p>

  <p>I consider this good practice in general, not least because immediately
escalating error conditions not only makes for hard to understand control
flow, but also a bad user experience. Take broken P4 connections, copies of
large …</p></blockquote></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dannas.name/error-handling">https://dannas.name/error-handling</a></em></p>]]>
            </description>
            <link>https://dannas.name/error-handling</link>
            <guid isPermaLink="false">hacker-news-small-sites-23884505</guid>
            <pubDate>Sat, 18 Jul 2020 20:19:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PIFuHD: High-Resolution 3D Human Digitization]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 19 (<a href="https://news.ycombinator.com/item?id=23884480">thread link</a>) | @jonbaer
<br/>
July 18, 2020 | https://shunsukesaito.github.io/PIFuHD/ | <a href="https://web.archive.org/web/*/https://shunsukesaito.github.io/PIFuHD/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://shunsukesaito.github.io/PIFuHD/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23884480</guid>
            <pubDate>Sat, 18 Jul 2020 20:15:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Debugging a dynamic linking bug in a Nix project]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 1 (<a href="https://news.ycombinator.com/item?id=23882277">thread link</a>) | @johnbcoughlin
<br/>
July 18, 2020 | https://www.johnbcoughlin.com/posts/nix-dynamic-linking/ | <a href="https://web.archive.org/web/*/https://www.johnbcoughlin.com/posts/nix-dynamic-linking/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
            

            

<h2 id="trying-out-the-nix-development-experience">Trying out the Nix development experience</h2>
<p>The other day, while building a <a href="https://faculty.washington.edu/shumlak/WARPX/html/index.html">scientific project</a> to which I’m a contributor, I
ran into a nasty version conflict between two system libraries. In a fit of
pique, I decided to learn enough about <a href="https://nixos.org/">Nix</a> to be able to set up a reproducible,
tightly controlled local build. It’s done now, and overall I’m very happy with
the tooling and setup. I’m using <a href="https://direnv.net/">direnv</a> to tightly integrate my normal shell
with Nix’s <code>nix-shell</code> feature, and for the most part everything feels seamless.
It is extremely refreshing to see <code>cmake</code> report that it has found a plethora of
binaries and libraries, content-hashed and installed in neat little rows under
<code>/nix/store</code>.</p>
<p>I’m using Nix to manage my development environment, but <em>not</em> to build the
project itself. Nix ensures that the project dependencies are installed and
discoverable by the compiler and linker. Building the project is done with
CMake, set up for <code>cmake</code> to find the nix-installed libraries. Nix achieves this
by <a href="https://nixos.wiki/wiki/C#The%5Fcompiler%5Fwrapper">wrapping the C compiler</a> with its own shell script and injecting the paths to
libraries and binaries via environment variables. There’s very little to do to
make <code>cmake</code> just work, beyond declaring that the packages you want are
<code>buildInputs</code>. The first version of my <code>shell.nix</code> file looked like this:</p>
<div><pre><code data-lang="nix"><span># file shell.nix</span>
{ <span>pkgs</span> ? <span>import</span> <span>&lt;nixpkgs&gt;</span> {} }:

<span>pkgs</span>.<span>mkShell</span> {
  <span>buildInputs</span> = <span>with</span> <span>pkgs</span>; [
    <span>cmake</span>
    (<span>callPackage</span> <span>nix/petsc.nix</span> {})
    <span>metis</span>
    <span>hdf5</span>
    <span>openmpi</span>
    (<span>python38</span>.<span>withPackages</span> (<span>packages</span>: [ <span>packages</span>.<span>numpy</span> ]))
  ];
}
</code></pre></div><p>Using this setup, I had very little trouble getting the project to build. I had
to <a href="https://nixos.org/nixos/nix-pills/nixpkgs-overriding-packages.html">override</a> the <a href="https://github.com/NixOS/nixpkgs/blob/master/pkgs/development/libraries/science/math/petsc/default.nix">default PETSc derivation</a> to compile with METIS and OpenMPI
support, which was not too hard:</p>
<div><pre><code data-lang="nix"><span># file nix/petsc.nix</span>
{ <span>petsc</span> , <span>blas</span> , <span>gfortran</span> , <span>lapack</span> , <span>python</span> , <span>metis</span> , <span>openmpi</span> }:

<span>petsc</span>.<span>overrideAttrs</span> (<span>oldAttrs</span>: <span>rec</span> {
    <span>nativeBuildInputs</span> = [ <span>blas</span> <span>gfortran</span> <span>gfortran</span>.<span>cc</span>.<span>lib</span> <span>lapack</span> <span>python</span> <span>openmpi</span> <span>metis</span> ];
    <span>preConfigure</span> = <span>''
</span><span>        export FC="</span><span>${</span><span>gfortran</span><span>}</span><span>/bin/gfortran" F77="</span><span>${</span><span>gfortran</span><span>}</span><span>/bin/gfortran"
</span><span>        patchShebangs .
</span><span>        configureFlagsArray=(
</span><span>        $configureFlagsArray
</span><span>        "--with-mpi-dir=</span><span>${</span><span>openmpi</span><span>}</span><span>"
</span><span>        "--with-metis=</span><span>${</span><span>metis</span><span>}</span><span>"
</span><span>        "--with-blas-lib=[</span><span>${</span><span>blas</span><span>}</span><span>/lib/libblas.so,</span><span>${</span><span>gfortran</span>.<span>cc</span>.<span>lib</span><span>}</span><span>/lib/libgfortran.a]"
</span><span>        "--with-lapack-lib=[</span><span>${</span><span>lapack</span><span>}</span><span>/lib/liblapack.so,</span><span>${</span><span>gfortran</span>.<span>cc</span>.<span>lib</span><span>}</span><span>/lib/libgfortran.a]"
</span><span>        )
</span><span>    ''</span>;
})
</code></pre></div><p>This Nix file returns a function which is invoked in <code>shell.nix</code> using
<code>callPackage</code> function. <code>petsc.overrideAttrs</code> is a neat way to override the
attributes of a derivation created with <code>stdenv.mkDerivation</code>. Building PETSc
with MPI and METIS support is as simple as passing in a different set of
arguments to the <code>configure</code> script.</p>
<p>Figuring out how to do all of this was fun. I mostly referred to the <a href="https://nixos.org/nixos/nix-pills/index.html">Nix
“Pills”</a>, which are a great progression through the Nix tool and language.</p>
<p>With these Nix files, I was able to execute <code>cmake .. &amp;&amp; make</code> successfully.
Getting the project to <em>run</em> was another story. The final binary failed
immediately with a dynamic loading error:</p>
<div><pre><code data-lang="bash">âžœ bin/warpxm
dyld: Library not loaded: /private/tmp/nix-build-petsc-3.13.2.drv-0/petsc-3.13.2/arch-darwin-c-debug/lib/libpetsc.3.13.dylib
  Referenced from: /Users/jack/src/warpxm/build/bin/warpxm
  Reason: image not found
</code></pre></div><p>The binary was trying to load a dynamic lib from one of the temporary directories
that Nix created in the process of building PETSc. Of course this failed: by the
time I invoked <code>bin/warpxm</code>, that directory had been cleaned up. Instead of a
file under <code>/private/tmp</code>, the binary should have linked to the result of the
<code>petsc</code> derivation in the Nix store, under <code>/nix/store</code>. At some point, it
seemed, an environment variable was incorrectly set to this intermediate
directory. To figure out where, I would have to learn a lot more about linking
on OS X than I ever expected.</p>
<h2 id="whither-the-linker">Whither the linker?</h2>
<p>First I checked the compiler and linker flags that are inserted by Nix’s
compiler wrapper. These come in via <code>NIX_CFLAGS_COMPILE</code> and <code>NIX_LDFLAGS</code>. When
you’re working with <code>nix-shell</code> and <code>direnv</code>, all of the environment variables
from your derivations are injected into your shell. It’s a simple matter of echoing
them out:</p>
<div><pre><code data-lang="bash">âžœ <span>echo</span> <span>$NIX_CFLAGS_COMPILE</span>
... -isystem /nix/store/w23r8kplmfx2xc111cpvmdjwmkwy6ip3-petsc-3.13.2/include ...
âžœ <span>echo</span> <span>$NIX_LDFLAGS</span>
... -L/nix/store/w23r8kplmfx2xc111cpvmdjwmkwy6ip3-petsc-3.13.2/lib ...
</code></pre></div><p>These look fine! Invoking <code>cmake</code> and <code>make</code> in this shell ought to pull in the
correct library.</p>
<p>Then I remembered that this project uses <code>pkg-config</code> to find and pull together
the linked libraries. Frankly, I don’t understand <code>pkg-config</code> very well, but I
do know that in this project it is invoked from inside of <code>cmake</code>. It searches for
libraries according to its own rules, and it runs <em>after</em> Nix has done its
job setting everything up. Therefore, it circumvents the compiler and linker
flags that we just checked.</p>
<p>I happened to have <code>pkg-config</code> installed from before setting up this Nix
environment. Therefore, <code>cmake</code> was able to invoke the system <code>pkg-config</code> from
my user <code>PATH</code>. Perhaps the system version of <code>pkg-config</code> was somehow finding
the wrong library? Indeed, <code>echo $PKG_CONFIG_PATH</code> confirmed that it was
searching a directory under my <code>$HOME</code>. I thought it possible that some wires
got crossed while I was adding dependencies to my Nix derivation one at a time:
configuring <code>pkg-config</code> appropriately might help.</p>

<p>I referred once again to the Nix wiki page on C projects, which also has <a href="https://nixos.wiki/wiki/C#pkg-config">a
section</a> on using <code>pkg-config</code>. It seems that including the <code>pkg-config</code>
derivation as a <code>nativeBuildInput</code> will let packages like <code>petsc</code> append their
output paths to the <code>PKG_CONFIG_PATH</code> environment variable. I did so:</p>
<div><pre><code data-lang="nix"><span>pkgs</span>.<span>mkShell</span> {
  <span>buildInputs</span> = <span>with</span> <span>pkgs</span>; [
    ...
  ];
  <span>nativeBuildInputs</span> = <span>with</span> <span>pkgs</span>; [
    <span>pkg-config</span>
  ];
}
</code></pre></div><p>but it didn’t fix the problem. I would have to go deeper and track down where
the bad library was being pulled in.</p>
<p>Digging into the <code>cmake</code> documentation and the project’s <code>.cmake</code> files led me
to insert a trio of print statements:</p>
<div><pre><code data-lang="diff">find_package(PkgConfig REQUIRED)
pkg_check_modules(PETSC PETSc REQUIRED)

link_directories(${PETSC_LIBRARY_DIRS})
<span>+ message("petsc libraries: ${PETSC_LIBRARIES}")
</span><span>+ message("petsc library dirs: ${PETSC_LIBRARY_DIRS}")
</span><span>+ message("petsc link libraries: ${PETSC_LINK_LIBRARIES}")
</span><span></span>list(APPEND WARPXM_LINK_TARGETS ${PETSC_LIBRARIES})
</code></pre></div><p>These printed out three lines in my <code>cmake</code> output:</p>
<div><pre><code data-lang="text">petsc libraries: petsc
petsc library dirs: /nix/store/w23r8kplmfx2xc111cpvmdjwmkwy6ip3-petsc-3.13.2/lib
petsc link libraries: /nix/store/w23r8kplmfx2xc111cpvmdjwmkwy6ip3-petsc-3.13.2/lib/libpetsc.dylib
</code></pre></div><p>The second two look good. But the first, just the library name <code>petsc</code>, was a little too
implicit for comfort. It was precisely this variable that was being appended
to the link targets list. At <em>compile</em> time, it would be up to the linker to find
the library <code>petsc</code>, and I wasn’t sure where it would look. Safer to use the
absolute path to the <code>.dylib</code>, like so:</p>
<div><pre><code data-lang="diff"><span>- list(APPEND WARPXM_LINK_TARGETS ${PETSC_LIBRARIES})
</span><span></span><span>+ list(APPEND WARPXM_LINK_TARGETS ${PETSC_LINK_LIBRARIES})
</span></code></pre></div>
<p>Changing the link target to the absolute path eased my mind only for the duration of
the next <code>cmake .. &amp;&amp; make</code> cycle. Surely there was no way the linker could
screw up now. No arcane library search involved, just an absolute path, which
couldn’t possibly be misinterpreted…</p>
<div><pre><code data-lang="bash">âžœ bin/warpxm
dyld: Library not loaded: /private/tmp/nix-build-petsc-3.13.2.drv-0/petsc-3.13.2/arch-darwin-c-debug/lib/libpetsc.3.13.dylib
  Referenced from: /Users/jack/src/warpxm/build/bin/warpxm
  Reason: image not found
</code></pre></div><p>Damn it!</p>
<h2 id="install-name-and-other-depravities">install_name and other depravities</h2>
<p>At this point I was absolutely flummoxed. With every fix I attempted, I
grepped vainly for the offending <code>/private/tmp</code> path in my build directory, and come up
empty-handed. I tracked down the final, irrevocable link options passed to the
compiler, tucked away in a <code>link.txt</code> file in the build tree. They showed
incontrovertibly that my binary was being linked to the correct library:</p>
<div><pre><code data-lang="text">âžœ cat build/src/CMakeFiles/warpxm.dir/link.txt
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++     -O3 -DNDEBUG -isysroot ... -L/nix/store/31d3hng4sclxi3sz8g3zi3yqmychj2kg-petsc-3.13.2/lib ...
</code></pre></div><p>I had proved nearly to my satisfaction that CMake was doing the right thing with
this library, and I was completely out of ideas.
Finally, a very lucky google search led me to the <a href="https://nixos.org/nixpkgs/manual/#sec-darwin">section</a> of the Nix manual
describing issues specific to the Darwin (MacOS) platform. It states:</p>
<blockquote>
<p>On Darwin, libraries are linked using absolute paths, libraries are resolved
by their install_name at link time. Sometimes packages won’t set this
correctly causing the library lookups to fail at runtime. This can be fixed by
adding extra linker flags or by running install_name_tool -id during the
fixupPhase.</p>
</blockquote>
<p>This is a very matter-of-fact way of stating something that, when I understood
it, flabbergasted me. To the best of my understanding, here’s what happens on
MacOS:</p>
<ul>
<li>My source code has an include directive, <code>include&lt;petsc.h&gt;</code> or something like
that, which creates a binary interface to be satisfied by the linker.</li>
<li>At link time, we pass the list of absolute paths to libraries, and the linker
finds the one that matches the interface.</li>
<li>The linker then saves the <em>install_name</em> of the library it found in the
binary’s load section.</li>
<li>At run time, the binary (actually, the MacOS <code>dyld</code> system) loads the library.
The install_name is all it has, so it looks there.</li>
</ul>
<p>I’ve certainly gotten some aspect of this wrong, so I would definitely
appreciate hearing from someone who understands it better than me!</p>
<p>In any case, this find pointed me to the concept of the install_name, so I had
something to go on. More searching led to a <a href="http://log.zyxar.com/blog/2012/03/10/install-name-on-os-x/">helpful blog post</a> describing exactly
the issue that I was facing. It also described how to check the install_name of
the library:</p>
<div><pre><code data-lang="text">âžœ otool -D /nix/store/31d3hng4sclxi3sz8g3zi3yqmychj2kg-petsc-3.13.2/lib/libpetsc.dylib</code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.johnbcoughlin.com/posts/nix-dynamic-linking/">https://www.johnbcoughlin.com/posts/nix-dynamic-linking/</a></em></p>]]>
            </description>
            <link>https://www.johnbcoughlin.com/posts/nix-dynamic-linking/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23882277</guid>
            <pubDate>Sat, 18 Jul 2020 16:01:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Booting to 'Hello Rust' on x86_64]]>
            </title>
            <description>
<![CDATA[
Score 245 | Comments 36 (<a href="https://news.ycombinator.com/item?id=23881584">thread link</a>) | @micouay
<br/>
July 18, 2020 | https://micouy.github.io/posts/low-level-pt-1/ | <a href="https://web.archive.org/web/*/https://micouy.github.io/posts/low-level-pt-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
<p>This post is about how I booted to bare metal Rust on x86_64.
My goal is to describe my learning path and hopefully get
you interested in things I talk about. I’ll be very happy if you
find this content useful. Note that I’m a beginner and I may be wrong
about many things. If you want to learn more, I’ll put links to many resources.
You can find all the code in my <a href="https://github.com/micouy/gniazdo-os">repo</a>.</p>

<p>The inspiration for this project came from many sources. I think I first
discovered <a href="https://github.com/cfenollosa/os-tutorial">cfenollosa/os-tutorial</a>.
I also read a few chapters from <a href="https://www.cs.bham.ac.uk/~exr/lectures/opsys/10_11/lectures/os-dev.pdf">Writing a Simple Operating System - from Scratch</a>.
It’s a good introduction to OS dev and related topics. First part
of the tutorial is about bootloaders. To get a grasp of what it’s
all about, I just read every part of the tutorial and tried to write the Assembly
code by myself.</p>
<p>Here’s a brief explanation of the boot process and what you have to do
at each stage. First the CPU thinks it’s an Intel 8086 model from 1970s IIRC. This stage is
called Real Mode. You have ~1MiB of memory and 16-bit <a href="https://en.wikipedia.org/wiki/Processor_register">registers</a>
at your disposition. In RM you are very limited. However, at this stage
you can use BIOS routines (or interrupts). They’re like an external library you can use to
print a character on the screen, read from and write to hard disk etc.
You can obviously do these things in other modes but you have to implement it yourself
or do some hacking to use the interrupts.</p>
<p>When you’re done with RM, you can switch to Protected Mode. In PM the address space
is extended (you can access more memory), 32-bit registers are enabled
and addressing works a bit different. To switch to PM,
you have to set up a <a href="http://www.osdever.net/tutorials/view/the-world-of-protected-mode">Descriptor Table</a>
which is a piece of data that tells the CPU
how to interpret addresses, and describes the characteristics of particular
parts of the memory.</p>
<hr>
<p><strong>Note</strong>: At first I thought that in PM address like <code>ds:ebx</code>
works just like in RM - <code>ds</code> is the segment (in ‘data’ memory if you want
to read/write to it or in ‘code’ memory if you jump to it) and <code>ebx</code>
is the offset in this segment. In reality <code>ds</code> acts like an index
into the descriptor table. Then, the selected descriptor is used
to translate the offset from the segment base (stored in <code>ebx</code>) to a physical address.
You can read more about addressing in PM <a href="http://ece-research.unm.edu/jimp/310/slides/micro_arch2.html">here</a>.
Also check out other stuff on <a href="http://ece-research.unm.edu/jimp/">their website</a>.</p>
<hr>

<p>The bootloader is a very short piece of code - it’s just 512 bytes long.
I once even ran out of space and had to remove some strings and unnecessary
code. The actual purpose of the bootloader is to load and run some other code.
The book suggested booting to PM and jumping to a 32-bit kernel written in C.
I didn’t really like programming in C and at this point I lost my motivation.</p>
<p>Another time I learned about Philipp Oppermann’s <a href="https://os.phil-opp.com/">blog</a>
on OS dev in Rust. I highly recommend it. I got really excited but,
unfortunately for me, Oppermann <del>uses <a href="https://www.gnu.org/software/grub/">GRUB</a>
(a kind of bootloader)</del> relies on <a href="https://github.com/rust-osdev/bootimage"><code>bootimage</code></a>
and <a href="https://github.com/rust-osdev/bootloader"><code>bootloader</code></a> crates to create a bootable image so I decided to come back to the tutorial after
finishing my own bootloader. At that time booting to a program written in language
other than Assembly and compiled to a separate file was way beyond my capabilities
so I left the project for a while.</p>
<p>A couple weeks ago I got motivated enough to try to boot to a Rust program.
My plan was to switch to Long Mode. Then I could either run a 64-bit kernel
or load a second stage bootloader. Second stage bootloader is not limited
to one sector (can be more than 512 bytes long) and can be 32-bit
or even 64-bit so it has access to a larger address space and can load
arbitrarily large files from disk to memory (as long as you implement it).
Its purpose is to load larger kernels.</p>
<p>The first step was loading my program from hard disk to memory.
I took a naive approach. I compiled the bootloader and my program.
I concatenated both binaries together, one right after the other, to form one file, like this:</p>
<div><pre><code data-lang="bash">cat bootloader.bin hello_rust.bin &gt; img
</code></pre></div><p>I didn’t care if I could actually execute Rust code, I just wanted
to test if I can load it. I added necessary code to my bootloader
and ran the emulator. Unfortunately QEMU just kept restarting.
I’m not sure what went wrong but it didn’t matter since it
was a stupid way to do it. I could just test loading data from the disk on
a much simpler bootloader. I first tried to write it myself but after a while
I just grabbed some code from Stack Overflow and added a simple error message in case
the BIOS routine failed. After a couple of tries I finally managed to create
a two-stage bootloader in a single Assembly program. It worked something like this:</p>
<div><pre><code data-lang="nasm"><span>org</span> <span>0x7c00</span>
first_stage:
    <span>; load second stage.</span>

    <span>; `dl` register specifies which disk to load</span>
    <span>; data from. it is set by the CPU to the same device</span>
    <span>; the bootloader was loaded from.</span>

    <span>; set `es` to 0x7e0 and `bx` to 0x0.</span>
    
    <span>; the routine will load data to `(es &lt;&lt; 4) | bx`</span>
    <span>; or 0x7e00.</span>
    <span>mov</span> ax, <span>0x7e0</span>
    <span>mov</span> es, ax
    <span>mov</span> bx, <span>0x0</span>

    <span>mov</span> al, <span>0x1</span> <span>; # of sectors to read.</span>
    
    <span>push</span> ax <span>; store `al` for later.</span>

    <span>; specify where the data is stored on the disk.</span>
    <span>mov</span> ch, <span>0x0</span> <span>; cylinder.</span>
    <span>mov</span> dh, <span>0x0</span> <span>; head.</span>
    <span>mov</span> cl, <span>0x2</span> <span>; sector right after bootloader.</span>

    <span>; `int 0x13` with `ah` set to 0x2</span>
    <span>; loads data from disk to memory.</span>
    <span>mov</span> ah, <span>0x2</span>
    <span>int</span> <span>0x13</span>

    <span>pop</span> bx <span>; restore `al` to `bl`.</span>

    <span>; check whether the number of sectors read is correct.</span>
    <span>cmp</span> al, bl
    <span>jne</span> error
       
    <span>jmp</span> <span>0x7e00</span> <span>; jump to second stage.</span>

error:
    <span>; print error message, then halt.</span>
    <span>jmp</span> <span>$</span>

<span>; fill the rest of the first sector with zeros.</span>
<span>; it's 510 and not 512 because the last two bytes</span>
<span>; are 0xaa and 0x55 - the 'magic' number.</span>
<span>times</span> <span>510</span><span>-</span>(<span>$</span><span>-</span><span>$$</span>) db <span>0</span>
<span>dw</span> <span>0xaa55</span>

second_stage:
    <span>; do something, then halt.</span>
    <span>jmp</span> <span>$</span>

<span>; padding to fill the rest of the second sector with zeros.</span>
<span>times</span> <span>1024</span><span>-</span>(<span>$</span><span>-</span><span>$$</span>) db <span>0</span>
</code></pre></div><p>This way I didn’t have to link multiple files and NASM managed the padding of sectors
for me. The code correctly compiled to a single file which was exactly 1024 bytes long -
two entire sectors.</p>
<p>You can also use <code>align 512</code> instead of <code>times 1024-($-$$)</code> to allow
your code to be arbitrarily large. This could be useful if you wanted to program
the second stage of your bootloader in Assembly.</p>
<hr>
<p><strong>Note</strong>: Don’t assume that the registers are initialized with any particular
value. It took me half an hour or so to realize that I didn’t set some register
correctly in such a simple piece of code. IIRC there are only two things you can
be sure of - that the bootloader will be loaded to 0x7c00 and that the <code>dl</code> register
points to the same device the bootloader has been loaded from.</p>
<hr>

<p>After I managed to load the second sector I wanted to replace the Assembly code
with Rust code. To check whether I could execute it I needed to switch to Long Mode.
That’s because I had some problems with compiling Rust code to 32-bit
and I ended up using the <code>x86_64-unknown-none</code> target triple, just like
in Oppermann’s tutorial.</p>
<p>Long Mode adds a new memory model (paging), and enables the use
of 64-bit instructions and registers. Switching to LM requires setting up paging.
I identity-mapped the first 2 MiB of memory which is enough for now. I think it
can be modified later by my Rust program. I read the following guides on paging:</p>
<ul>
<li><a href="https://wiki.osdev.org/Setting_Up_Long_Mode#Setting_up_the_Paging">https://wiki.osdev.org/Setting_Up_Long_Mode#Setting_up_the_Paging</a></li>
<li><a href="https://wiki.osdev.org/Paging#Enabling">https://wiki.osdev.org/Paging#Enabling</a></li>
<li><a href="https://intermezzos.github.io/book/first-edition/transitioning-to-long-mode.html">https://intermezzos.github.io/book/first-edition/transitioning-to-long-mode.html</a></li>
<li><a href="http://ece-research.unm.edu/jimp/310/slides/micro_arch3.html">http://ece-research.unm.edu/jimp/310/slides/micro_arch3.html</a></li>
<li><a href="https://os.phil-opp.com/entering-longmode/">https://os.phil-opp.com/entering-longmode/</a></li>
</ul>
<p>There are also other things you have to do before switching to LM
like checking <code>cpuid</code> instruction support and the highest instruction
op code available.</p>

<p>I figured that to execute Rust code I needed to glue it with
the bootloader to create a single binary. In this step I encountered
a few difficult problems:</p>
<ul>
<li>To work correctly, the bootloader has to be stored in the first 512 bytes of the file.
The last two bytes of this sector have to be 0xaa55.</li>
<li>My Rust code should also be aligned with the sectors and fill up the last sector
entirely. Of course I would fill it with some garbage values but they had to
be there.</li>
<li>The bootloader has to know how long the Rust code is.</li>
<li>Both programs will be loaded at specific locations in the memory
so I have to manage the offsets in addresses.</li>
</ul>
<p>Linker seemed like a tool made for this. I’ve never used a linker before so I just read
<a href="https://ftp.gnu.org/old-gnu/Manuals/ld-2.9.1/html_node/ld_toc.html"><code>ld</code>'s documentation</a>.
Unfortunately, I got a bit confused. I didn’t understand how the <code>.</code> operator
worked in linker scripts. I also got something wrong about the <code>AT()</code> instruction.
You can see the <a href="https://www.reddit.com/r/osdev/comments/hhd7uj/linking_bootloader_with_second_stage/">thread</a>
I started on Reddit.</p>
<p>Eventually I came up with such script:</p>
<div><pre><code data-lang="plaintext">SECTIONS {
    .boot 0x7c00 : {
        *(.boot.*);
    }

    .hello_rust 0x7e00 : {
        *(.rust.*);
        . = ALIGN(512);
    } = 0xdeadc0de

    SECOND_STAGE_LENGTH = ((. - ADDR(.hello_rust)) &gt;&gt; 9);
}
</code></pre></div><p>Notice that I put all sections beginning with <code>.boot</code> in the first 512 bytes
and all sections beginning with <code>.rust</code> after 0x7e00. In order to make
it work I have to rename all sections in the ELF file emitted by <code>rustc</code>
so that they all start with a prefix. I do it using <code>objcopy</code>
(in some linkers this option is called <code>--prefix-sections</code>):</p>
<div><pre><code data-lang="bash">objcopy hello_rust.o hello_rust_prefixed.o --prefix-alloc-sections<span>=</span><span>'.rust'</span>
</code></pre></div><p>The there are two tricks in this script. The first one is that I align
the end of the <code>.rust</code> section to the nearest sector boundary with <code>. = ALIGN(512)</code>.
Then I fill the rest with some garbage using <code>= 0xdeadc0de</code>.
The second trick is in the last line. The <code>SECOND_STAGE_LENGTH</code>
variable tells the bootloader how many sectors the second stage occupies.
The size of this section is divided by 512 = 2^9. Since I know
that the end of this section is aligned with the boundaries
of the sectors I can just shift it 9 bits to the right.</p>
<p>These solutions seem a bit hacky. In my understanding the location
of a piece of code in an ELF file (most common target) doesn’t matter. This is not the
case with the bootloader since the code
must be …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://micouy.github.io/posts/low-level-pt-1/">https://micouy.github.io/posts/low-level-pt-1/</a></em></p>]]>
            </description>
            <link>https://micouy.github.io/posts/low-level-pt-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23881584</guid>
            <pubDate>Sat, 18 Jul 2020 14:27:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Kubectl – Configuration Guide]]>
            </title>
            <description>
<![CDATA[
Score 70 | Comments 25 (<a href="https://news.ycombinator.com/item?id=23881485">thread link</a>) | @lukasbar
<br/>
July 18, 2020 | https://knowledgepill.it/posts/kubernetes-kubectl-client-config/ | <a href="https://web.archive.org/web/*/https://knowledgepill.it/posts/kubernetes-kubectl-client-config/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <p>Kubectl client gives us possibility to configure and switch easily between multiple K8s clusters.</p>
<h2 id="config-files-location-for-kubectl">Config files location for kubectl</h2>
<p>Main location for <code>kubectl</code> config files is <code>$HOME/.kube</code>, by default we have single config file named config:</p>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ cd $HOME/.kube
<span>[</span>node1 .kube<span>]</span>$ ls -lah
total 4.0K
drwxr-xr-x <span>1</span> root root   <span>37</span> Mar <span>12</span> 20:48 .
dr-xr-x--- <span>1</span> root root   <span>19</span> Nov <span>29</span> 11:46 ..
drwxr-x--- <span>3</span> root root   <span>23</span> Mar <span>12</span> 20:48 cache
lrwxrwxrwx <span>1</span> root root   <span>26</span> Nov <span>29</span> 11:46 config -&gt; /etc/kubernetes/admin.conf
drwxr-x--- <span>3</span> root root 4.0K Mar <span>12</span> 20:48 http-cache
</code></pre></div><h2 id="multiple-config-files-and-kubeconfig-variable">Multiple config files and KUBECONFIG variable</h2>
<p>We can have multiple config files by setting proper <code>KUBECONFIG</code> shell variable.<br>
For example:</p>
<p><code>export KUBECONFIG=$KUBECONFIG:/root/.kube/additional_config</code></p>
<h2 id="check-config-from-kubectl">Check config from kubectl</h2>
<p>We can check current config(it will be empty because I don’t have anything in additional_config file - we’ll add something later - and this is only file in my <code>KUBECONFIG</code> variable):</p>
<div><pre><code data-lang="yaml">[node1 ~]$ kubectl config view
<span>apiVersion</span>: v1
<span>clusters</span>: []
<span>contexts</span>: []
<span>current-context</span>: <span>""</span>
<span>kind</span>: Config
<span>preferences</span>: {}
<span>users</span>: []
</code></pre></div><p>Or from specific file - let’s see default file:</p>
<div><pre><code data-lang="yaml">[node1 ~]$ kubectl config --kubeconfig=.kube/config view
<span>apiVersion</span>: v1
<span>clusters</span>:
- <span>cluster</span>:
    <span>certificate-authority-data</span>: DATA+OMITTED
    <span>server</span>: https://<span>192.168</span><span>.0</span><span>.38</span>:<span>6443</span>
  <span>name</span>: kubernetes
<span>contexts</span>:
- <span>context</span>:
    <span>cluster</span>: kubernetes
    <span>user</span>: kubernetes-admin
  <span>name</span>: kubernetes-admin@kubernetes
<span>current-context</span>: kubernetes-admin@kubernetes
<span>kind</span>: Config
<span>preferences</span>: {}
<span>users</span>:
- <span>name</span>: kubernetes-admin
  <span>user</span>:
    <span>client-certificate-data</span>: REDACTED
    <span>client-key-data</span>: REDACTED
</code></pre></div><p>If you don’t set <code>KUBECONFIG</code> variable all configuration will be taken from <code>$HOME/.kube/config</code></p>
<h2 id="objects-in-kubectl-config">Objects in kubectl config</h2>
<p><code>kubectl</code> has in its config files following object types:</p>
<ul>
<li>clusters - info about K8s cluster - contain cluster name and connection parameters</li>
<li>users - info about users by which you want connect to K8s clusters</li>
<li>contexts - triple of cluster/user/namespace</li>
</ul>
<h2 id="modifying-config-from-kubectl">Modifying config from kubectl</h2>
<h3 id="add-cluster">Add cluster</h3>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ kubectl config --kubeconfig<span>=</span>.kube/additional_config set-cluster dev --server<span>=</span>https://192.168.1.100 --certificate-authority<span>=</span>fake-ca
Cluster <span>"dev"</span> set.
<span>[</span>node1 ~<span>]</span>$ kubectl config --kubeconfig<span>=</span>.kube/additional_config set-cluster prod --server<span>=</span>https://10.1.1.100 --certificate-authority<span>=</span>fake-ca
Cluster <span>"prod"</span> set.
</code></pre></div><h3 id="add-users">Add users</h3>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ kubectl config --kubeconfig<span>=</span>.kube/additional_config set-credentials developer --client-certificate<span>=</span>fake-cert-file --client-key<span>=</span>fake-key-seefile
User <span>"developer"</span> set.
<span>[</span>node1 ~<span>]</span>$ kubectl config --kubeconfig<span>=</span>.kube/additional_config set-credentials prod_admin --client-certificate<span>=</span>fake-cert-file --client-key<span>=</span>fake-key-seefile
User <span>"prod_admin"</span> set.
</code></pre></div><h3 id="add-contexts">Add contexts</h3>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ kubectl config --kubeconfig<span>=</span>.kube/additional_config set-context simple_app_development --cluster<span>=</span>dev --namespace<span>=</span>simple_app_dev --user<span>=</span>developer
Context <span>"simple_app_development"</span> created.
<span>[</span>node1 ~<span>]</span>$ kubectl config --kubeconfig<span>=</span>.kube/additional_config set-context simple_app_production --cluster<span>=</span>prod --namespace<span>=</span>simple_app_prod --user<span>=</span>prod_admin
Context <span>"simple_app_production"</span> created.
</code></pre></div><h3 id="delete-clusterusercontext-from-config">Delete cluster/user/context from config</h3>
<div><pre><code data-lang="bash">kubectl --kubeconfig<span>=</span>.kube/additional_config config unset users.&lt;name&gt;
kubectl --kubeconfig<span>=</span>.kube/additional_config config unset clusters.&lt;name&gt;
kubectl --kubeconfig<span>=</span>.kube/additional_config config unset contexts.&lt;name&gt;
</code></pre></div><h3 id="sample-config">Sample config</h3>
<div><pre><code data-lang="yaml">[node1 ~]$ kubectl config --kubeconfig=.kube/additional_config view
<span>apiVersion</span>: v1
<span>clusters</span>:
- <span>cluster</span>:
    <span>certificate-authority</span>: /root/fake-ca
    <span>server</span>: https://<span>192.168</span><span>.1</span><span>.100</span>
  <span>name</span>: dev
- <span>cluster</span>:
    <span>certificate-authority</span>: /root/fake-ca
    <span>server</span>: https://<span>10.1</span><span>.1</span><span>.100</span>
  <span>name</span>: prod
<span>contexts</span>:
- <span>context</span>:
    <span>cluster</span>: dev
    <span>namespace</span>: simple_app_dev
    <span>user</span>: developer
  <span>name</span>: simple_app_development
- <span>context</span>:
    <span>cluster</span>: prod
    <span>namespace</span>: simple_app_prod
    <span>user</span>: prod_admin
  <span>name</span>: simple_app_production
<span>current-context</span>: <span>""</span>
<span>kind</span>: Config
<span>preferences</span>: {}
<span>users</span>:
- <span>name</span>: developer
  <span>user</span>:
    <span>client-certificate</span>: /root/fake-cert-file
    <span>client-key</span>: /root/fake-key-seefile
- <span>name</span>: prod_admin
  <span>user</span>:
    <span>client-certificate</span>: /root/fake-cert-file
    <span>client-key</span>: /root/fake-key-seefile
</code></pre></div><h3 id="changing-context-to-work-with">Changing context to work with</h3>
<p>To get list of contexts(no need for <code>--kubeconfig</code> because we have our <code>additional_config</code> added to <code>KUBECONFIG</code> variable):</p>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ kubectl config get-contexts
CURRENT   NAME                     CLUSTER   AUTHINFO     NAMESPACE
          simple_app_development   dev       developer    simple_app_dev
          simple_app_production    prod      prod_admin   simple_app_prod
</code></pre></div><p>To set context:</p>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ kubectl config use-context simple_app_development
Switched to context <span>"simple_app_development"</span>.
</code></pre></div><p>To get current context:</p>
<div><pre><code data-lang="bash"><span>[</span>node1 ~<span>]</span>$ kubectl config current-context
simple_app_development
</code></pre></div><hr>
<h3 id="for-those-who-want-to-know-even-more-worth-to-read">For those who want to know even more… Worth to read!</h3>
<p><a href="https://gardener.cloud/documentation/guides/client_tools/bash_kubeconfig/">Kubeconfig context as bash prompt - Georgi from Gardener</a> - shell tricks and Windows CLI aproach</p>
<hr>

</div></div>]]>
            </description>
            <link>https://knowledgepill.it/posts/kubernetes-kubectl-client-config/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23881485</guid>
            <pubDate>Sat, 18 Jul 2020 14:10:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Trouser Pockets]]>
            </title>
            <description>
<![CDATA[
Score 734 | Comments 372 (<a href="https://news.ycombinator.com/item?id=23881220">thread link</a>) | @diiq
<br/>
July 18, 2020 | https://sambleckley.com/writing/pockets.html | <a href="https://web.archive.org/web/*/https://sambleckley.com/writing/pockets.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h3 id="tldr">TL;DR</h3> <p>The clothing industry rarely creates dramatically new patterns. It is more lucrative to make small, easy-to-manufacture changes that always use the same basic pattern.</p> <p>There is therefore lots of room to innovate clothing that is more appropriate for contemporary problems, because basic clothing patterns haven’t changed in decades.</p> <p>I made some trousers with unusual pockets, and I think they’re good.</p> <h3 id="pockets-are-either-ugly-or-useless">Pockets are either ugly or useless</h3> <p>Pockets in tight jeans look bad. Putting a modern slab phone, a wallet, and keys into a pair of skinny jeans will leave even the most fashionable figure looking looking like they’re wearing batman’s utility belt as underwear. Even empty, in tight pants a large pocket bag can show through.</p> <p>The alternative, as many women know from first-hand experience, is a pocket too small to put anything in.</p> <p>A wallet in the back pocket can cause back pain and bad posture.</p> <p>Many of us spend most of our time sitting, but all four traditional pockets are totally inaccessible in that position. So we take out our phone, just in case, before we sit down at the restaurant — guaranteeing a distraction.</p> <p>Aesthetics, storage, and access: these are user needs that are currently poorly fulfilled — and that means things are ripe for innovation.</p> <h3 id="a-brief-history">A brief history</h3> <p>If the space is so ripe, why has there been no pocket innovation recently?</p> <p>Women used to have pockets. That “used to” has to count back 150 or even 200 years, and those pockets were often a separate garment, either worn underneath and accessed through a slit in the dress, or worn around the hips overtop, rather than built into the dress directly — but regardless, “it has pockets!” as a joyous surprise is a modern invention. (Men’s pockets were also separate pouches if you go back far enough; but going back the same 150-200 years, they lived in the waistcoat and the mandatory jacket; if breeches had pockets they were behind the falls and so, I suspect, not used as much in public.)</p> <p>The 1880’s brought the 1940s brought the slow arrival of mass production to clothing — not of the textiles, which started much earlier, but of actual clothes. Before this era, clothes were made either at home, or they were made one at a time. (In a quiet resonance with today, this was done in part by women who worked almost entirely from home, only travelling to the workplace to pick up new work and drop off what had been completed).</p> <p>In retrospect, fashions changed mostly decade by decade rather than year by year, but they changed dramatically. The fashionable silhouette of the 1860s looks nothing like the 1880s, or the 1910’s — so different, in fact, that for women the foundation garments were completely unrelated entities: the crinoline of the 1860s is nothing like the bustle of the 1880s. A person who can cut and sew can sew an incredibly wide range of different things; why not play around?</p> <p>But with the advent of mass production in clothes factories, the whole layout of the factory floor was based on specific pattern piecing. Each station makes only a few operations on each garment, and garments flow from one station to the next. To completely change the <em>construction</em> of a garment means a radical overhaul of the whole assembly line.</p> <p>But to simply change the garment’s <em>proportions</em> is <em>easy</em>.</p> <p>And so we live in a world where every spring the morning talk shows invite someone on to say “this year, [culottes, boot-cuts, skinny jeans, flares, cuffs, boyfriend jeans, high waists, low-rise, acid-wash, raw denim] are coming back into fashion, so keep your eye out!” — but the trousers are made from the same basic pieces, constructed in the same ways, with only the measurements changing. They get longer or shorter, looser or tighter, and change color, but they’re not fundamentally <em>different</em>.</p> <p>It’s like a lack-luster procedural generation system. Sure, there are technically millions of possibilities, but somehow you still end up bored after seeing the first 5 or 6.</p> <p>Probably not coincidentally, the advent of mass production is also when women’s silhouettes turned slim, and when all those foundation garments mostly disappeared. Designs that banish structure from the garment and rely entirely on the body beneath are much easier and cheaper to manufacture.</p> <p>And when silhouettes turned slim, women lost pockets. The more body-hugging the clothing, the less room for pockets — or, rather, the more the pocket contents will show unflatteringly. (And as we know, it would be anathema for a women to show unflattering lump for something as silly as practicality, function, or utility.)</p> <p>And even as womenswear adopted men’s workwear styles, women got jeans but not the pockets to go with them. Spandex made it even easier to make mass-produced clothes “fit”, and fit tighter. The unsightly-lump factor wasn’t going away.</p> <p>Even more recently, menswear has returned to an incredibly slim silhouette. Guys in skinny jeans should, by this logic, not have pockets either. But they do.</p> <p>Women get no practicality and men get no grace.</p> <h3 id="what-if-we-redesigned-the-pocket-from-scratch">What if we redesigned the pocket from scratch?</h3> <p>Let’s design a trouser pocket! This process is going to land somewhere between UX and industrial design. I am, sadly, not in charge of a clothes factory, so I am not concerned with the problems of mass-production, but material properties and the construction process do still matter; but I am maybe more willing to make manufacturing sacrifices for usefulness than most industrial designers are able to be.</p> <h3 id="what-do-people-use-pockets-for">What do people use pockets for?</h3> <ul> <li>Storing things; most often phone, then keys, wallet or money-clip, and other small items. At the moment, a mask.</li> <li><em>Verifying</em> stored objects — the hip-slapping dance of making sure you have everything before walking out the door.</li> <li>They must be secure while standing, sitting, running for the bus, etc</li> <li>Things, especially phone, need to go in and out of storage frequently, almost unconsciously.</li> <li>Access to phone while seated in a restaurant; to keys and license while in car</li> <li>Aesthetically, we want a clean, graceful line from hip to ankle even while holding things.</li> </ul> <p>An initial solution can be based on just three questions:</p> <ul> <li>Where can your hands reach?</li> <li>Where is there extra space to put things?</li> <li>Where is there enough support to prevent items swinging around uncomfortably?</li> </ul> <p>These questions form a physical venn diagram, pointing at a pretty small area for investigation:</p> <p><img src="https://sambleckley.com/assets/images/pocket_venn.png" alt="The human body with overlapping regions of reach, space, and support."></p> <p>You can see that current pockets are located completely outside the area where there is potentially space; the hips and butt of modern pants, for both women and men, are closely fitted. If there’s any ease, it starts just above the knees.</p> <p>Conveniently, the point where the vastus lateralis starts to curve back in towards the knee, creating space, is also right around the lowest point your arms can reach without bending over, giving us a well-defined target.</p> <p>So I made a pair of trousers with no pockets at the waist, and a pair of welt pockets at the side seams, right at that point.</p> <p><img src="https://sambleckley.com/assets/images/pants1.jpg" alt="First pair of pants."></p> <h3 id="notes-from-user-testing">Notes from user testing:</h3> <ul> <li>A pocket needs to be big enough for a whole hand, not just phone, to allow for fishing out small items from the very bottom of the pocket-bag.</li> <li>The pocket bag slips down and back if it’s full and the wearer sits down. This is uncomfortable and</li> <li>The pocket mustn’t allow a slick round phone to fall out when sitting, especially when, e.g. jiggling a leg.</li> </ul> <h3 id="revisions">Revisions</h3> <p>Luckily, each of these notes point at simple revision, rather than the need to start again.</p> <ul> <li>Make the pockets wider.</li> <li>Anchor the bottom of the pocket bag to the side seam.</li> <li>Angle the opening so it’s higher in the back — this both makes it easier to slip a phone into the pocket, and makes the pocket “deeper” on the bottom side when sitting or crouching.</li> </ul> <p><img src="https://sambleckley.com/assets/images/pants2.jpg" alt="Second pair of pants."></p> <h3 id="outcome">Outcome</h3> <p>These are great. I completely forget my phone and wallet are there — they don’t restrict my movement, they’re completely invisible, and yet they’re easy to access while standing or sitting. There’s no temptation to slouch around with my hands stuffed in my pockets. They’re so straightforwardly better for my needs that I’m now frustrated when my new-pocket pants are dirty and I have to wear trousers with pockets that are just SO twentieth-century. I immediately made a pair of jeans in this pattern, too.</p> <p>Further revision is possible, of course; but while it will be easy to refine this design to suit my personal body and needs more precisely, there’s only so much refinement possible while remaining suitable for a wide range of body types. If you wanted to mass-produce a pocket like this, you’d want to use something like Dreyfuss’ Humanscale data to make sure the placement and size is appropriate for the widest range of people. Luckily, you’d have <em>some</em> additional information in the sizing of the <em>rest</em> of the pants.</p> <h3 id="appendix-sam-have-you-simply-invented-cargo-pants">Appendix: Sam, have you simply invented cargo pants?</h3> <p>No. Cargo pants solve different problems for different people.</p> <p>If cargo pants are appropriate for your daily life, you definitely don’t want or need my side-seam welt pockets; and vice-versa. Cargo pants aren’t office-wear; these dress pants aren’t combat-wear.</p> </div></div>]]>
            </description>
            <link>https://sambleckley.com/writing/pockets.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23881220</guid>
            <pubDate>Sat, 18 Jul 2020 13:16:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Typed Lisp, a Primer (2019)]]>
            </title>
            <description>
<![CDATA[
Score 105 | Comments 26 (<a href="https://news.ycombinator.com/item?id=23878612">thread link</a>) | @jedimind
<br/>
July 17, 2020 | https://alhassy.github.io/TypedLisp.html | <a href="https://web.archive.org/web/*/https://alhassy.github.io/TypedLisp.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="text-typing-via-macros">
<p>
Checking the type of inputs is tedious and so I <a href="https://www.reddit.com/r/emacs/comments/cct5hp/functional_type_declarations_in_elisp/">guessed</a> it could be done using
macros and advice. Looking at <a href="https://docs.racket-lang.org/ts-guide/types.html">Typed Racket</a> for inspiration, the following
fictitious syntax would add advice to <code>f</code> that checks the optional arguments <code>xᵢ</code>
have type <code>σᵢ</code> and the mandatory positional arguments have type <code>τᵢ</code> according
to position, and the result of the computation is of type <code>τ</code>.
To the best of my knowledge, no one had done this for Emacs Lisp —I don't know why.
</p>
<div>
<pre>(declare-type 'f ((<span>:x₁</span> σ₁) … (<span>:xₘ</span> σₘ)) (τ₁ … τₙ τ))
</pre>
</div>

<p>
To modify a variable, or function, we may simply redefine it; but a much more elegant and powerful
approach is to <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Advising-Functions.html">“advise”</a> the current entity with some new behaviour. In our case of interest, we will
<i>advise functions to check their arguments before executing their bodies</i>.
</p>

<p>
Below is my attempt: <a id="org84d2c90"><code>declare-type</code></a>. Before you get scared or think it's horrendous, be charitable and
note that about a third of the following is documentation and a third is local declarations.
</p>
<div>
<pre>(<span>cl-defmacro</span> <span>declare-type</span> (f key-types <span>&amp;rest</span> types)
  <span>"Attach the given list of types to the function ‘f’</span>
<span>   by advising the function to check its arguments’ types</span>
<span>   are equal to the list of given types.</span>

<span>   We name the advice ‘⟪f⟫-typing-advice’ so that further</span>
<span>   invocations to this macro overwrite the same advice function</span>
<span>   rather than introducing additional, unintended, constraints.</span>

<span>   Using type specifiers we accommodate for unions of types</span>
<span>   and subtypes, etc ♥‿♥.</span>

<span>   ‘</span><span>key-types</span><span>’ should be of the shape (:x₀ t₀ ⋯ :xₙ tₙ);</span>
<span>    when there are no optional types, use symbol “:”.</span>

<span>    E.g., (declare-type my-func (:z string :w integer) integer symbol string)</span>
<span>  "</span>

  <span>;; </span><span>Basic coherency checks. When there aren't optional types, key-types is the “:” symbol.</span>
  (should (<span>and</span> (<span>listp</span> types) (<span>or</span> (<span>listp</span> key-types) (<span>symbolp</span> key-types))))

  (<span>letf*</span> ((pairify (<span>lambda</span> (xs) (<span>loop</span> for i in xs by #'<span>cddr</span>         <span>;; </span><span>Turn a list of flattenned pairs</span>
                                      for j in (<span>cdr</span> xs) by #'<span>cddr</span>   <span>;; </span><span>into a list of explicit pairs.</span>
                                      collect (<span>cons</span> i j))))         <span>;; </span><span>MA: No Lisp method for this!?</span>
         (result-type  (<span>car</span> (<span>-take-last</span> 1 types)))
         (types        (<span>-drop-last</span> 1 types))
         (num-of-types (<span>length</span> types))
         (key-types-og (<span>unless</span> (<span>symbolp</span> key-types) key-types))
         (key-types    (<span>funcall</span> pairify key-types-og))
         (advice-name  (<span>intern</span> (<span>format</span> <span>"%s-typing-advice"</span> f)))
         (notify-user  (<span>format</span> <span>"%s now typed %s → %s → %s."</span>
                               `,f key-types-og types result-type)))

      `(<span>progn</span>
         (<span>defun</span> ,advice-name (orig-fun <span>&amp;rest</span> args)

           <span>;; </span><span>Split into positional and key args; optionals not yet considered.</span>
           (<span>letf*</span> ((all-args
                     (<span>-split-at</span>
                       (<span>or</span> (<span>--find-index</span> (<span>not</span> (<span>s-blank?</span> (<span>s-shared-start</span> <span>":"</span> (<span>format</span> <span>"%s"</span> it)))) args) ,num-of-types)
                        args)) <span>;; </span><span>The “or” is for when there are no keywords provided.</span>
                  (pos-args  (<span>car</span> all-args))
                  (key-args  (<span>funcall</span> ,pairify (<span>cadr</span> all-args)))
                  (fun-result nil)
                  ((<span>symbol-function</span> 'shucks)
                     (<span>lambda</span> (eτ e g)
                       (<span>unless</span> (<span>typep</span> g eτ)
                         (<span>error</span> <span>"%s: Type mismatch! Expected %s %s ≠ Given %s %s."</span>
                                (<span>function</span> ,f) eτ e (<span>type-of</span> g) (<span>prin1-to-string</span> g))))))

         <span>;; </span><span>Check the types of positional arguments.</span>
         (<span>unless</span> (<span>equal</span> ,num-of-types (<span>length</span> pos-args))
           (<span>error</span> <span>"%s: Insufficient number of arguments; given %s, %s, but %s are needed."</span>
                  (<span>function</span> ,f) (<span>length</span> pos-args) pos-args ,num-of-types))
         (<span>loop</span> for (ar ty pos) in (<span>-zip</span> pos-args (<span>quote</span> ,types) (<span>number-sequence</span> 0 ,num-of-types))
               <span>do</span> (shucks ty (<span>format</span> <span>"for argument %s"</span> pos) ar))

         <span>;; </span><span>Check the types of *present* keys.</span>
         (<span>loop</span> for (k . v) in key-args
               <span>do</span> (shucks (<span>cdr</span> (<span>assoc</span> k (<span>quote</span> ,key-types))) k v))

         <span>;; </span><span>Actually execute the orginal function on the provided arguments.</span>
         (<span>setq</span> fun-result (<span>apply</span> orig-fun args))
         (shucks (<span>quote</span> ,result-type) <span>"for the result type (!)"</span> fun-result)

         <span>;; </span><span>Return-value should be given to caller.</span>
         fun-result))

      <span>;; </span><span>Register the typing advice and notify user of what was added.</span>
      (<span>advice-add</span> (<span>function</span> ,f) <span>:around</span> (<span>function</span> ,advice-name))
      ,notify-user )))
</pre>
</div>

<pre>declare-type
</pre>


<p>
There are some notable shortcomings: Lack of support for type variables and, for now, no support for
optional arguments. Nonetheless, I like it —of course.
( Using <a href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Watching-Variables.html#Watching-Variables">variable watchers</a> we could likely add support for type variables as well as
function-types. )
</p>

<p>
<b>We accidentally forgot to consider an argument.</b>
</p>
<div>
<pre>(declare-type f₁ (<span>:z</span> <span>string</span> <span>:w</span> <span>list</span>) integer symbol <span>string</span>)
<span>;; </span><span>⇒ f₁ now typed (:z string :w integer) → (integer symbol) → string.</span>

(<span>cl-defun</span> <span>f₁</span> (x y <span>&amp;key</span> z w) (<span>format</span> <span>"%s"</span> x))
<span>;; </span><span>⇒ f₁ now defined</span>

(f₁ 'x) <span>;; </span><span>⇒ f₁: Insufficient number of arguments; given 2, (x), but 3 are needed.</span>
</pre>
</div>
<p>
The type declaration said we needed 3 arguments, but we did not consider one of them.
</p>

<p>
<b>We accidentally returned the wrong value.</b>
</p>
<div>
<pre>(declare-type f₂ (<span>:z</span> <span>string</span> <span>:w</span> <span>list</span>) integer symbol <span>string</span>)
(<span>cl-defun</span> <span>f₂</span> (x y <span>&amp;key</span> z w) x)

(f₂ 144 'two)
<span>;; </span><span>⇒ f₂: Type mismatch! Expected string for the result type (!) ≠ Given integer 144.</span>
</pre>
</div>

<p>
<b>We accidentally forgot to supply an argument.</b>
</p>
<div>
<pre>(declare-type f₃ (<span>:z</span> <span>string</span> <span>:w</span> <span>list</span>) integer symbol <span>string</span>)
(<span>cl-defun</span> <span>f₃</span> (x y <span>&amp;key</span> z w) (<span>format</span> <span>"%s"</span> x))

(f₃ 144)
<span>;; </span><span>⇒ f₃: Insufficient number of arguments; given 1, (144), but 2 are needed.</span>
</pre>
</div>

<p>
<b>A positional argument is supplied of the wrong type.</b>
</p>
<div>
<pre>(f₃ 'one <span>"two"</span>)
<span>;; </span><span>⇒  f₃: Type mismatch! Expected integer for argument 0 ≠ Given symbol one.</span>

(f₃ 144 <span>"two"</span>)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected symbol for argument 1 ≠ Given string "two".</span>
</pre>
</div>
<p>
Notice: When multiple positional arguments have type-errors, the errors are reported one at a time.
</p>

<p>
<b>A keyword argument is supplied of the wrong type.</b>
</p>
<div>
<pre>(f₃ 1 'two <span>:z</span> 'no₀ <span>:w</span> 'no₁)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected string :z ≠ Given symbol no₀.</span>

(f₃ 1 'two <span>:z</span> <span>"ok"</span> <span>:w</span> 'no₁)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected string :w ≠ Given symbol no₁.</span>

(f₃ 1 'two <span>:z</span> <span>"ok"</span> <span>:w</span> 23)
<span>;; </span><span>⇒ f₃: Type mismatch! Expected string :w ≠ Given integer 23.</span>

(f₃ 1 'two <span>:z</span> <span>"ok"</span> <span>:w</span> '(a b 1 2)) <span>;; </span><span>⇒ okay; no type-error.</span>
</pre>
</div>

<p>
<b>We have no optional arguments.</b>
</p>
<div>
<pre>(declare-type f₄ : integer symbol <span>string</span>)
(<span>cl-defun</span> <span>f₄</span> (x y <span>&amp;key</span> z w) (<span>format</span> <span>"%s"</span> x))

(f₄ 144 'two <span>:z</span> <span>"bye"</span>)
<span>;; </span><span>⇒  f₄: Type mismatch! Expected nil :z ≠ Given string "bye".</span>
<span>;; </span><span>( We shouldn't have any keyword :z according to the type declaration! )</span>

(f₄ 144 'two) <span>;; </span><span>⇒ "144"</span>
</pre>
</div>

<p>
<b>We can incorporate type specfiers such as unions!</b>
</p>
<div>
<pre>(declare-type f₅ : (<span>or</span> integer <span>string</span>) <span>string</span>)
(<span>cl-defun</span> <span>f₅</span> (x) (<span>format</span> <span>"%s"</span> x))

(f₅ 144)     <span>;; </span><span>⇒ "144"</span>
(f₅ <span>"neato"</span>) <span>;; </span><span>⇒ "neato"</span>

(f₅ 'shaka-when-the-walls-fell)
<span>;; </span><span>⇒ f₅: Type mismatch! Expected (or integer string) for argument 0</span>
<span>;;       </span><span>≠ Given symbol shaka-when-the-walls-fell.</span>
</pre>
</div>

<p>
<b>No positional arguments but a complex optional argument!</b>
</p>
<div>
<pre>(declare-type f₆ (<span>:z</span> (satisfies (<span>lambda</span> (it) (<span>and</span> (<span>integerp</span> it) (<span>=</span> 0 (<span>mod</span> it 5))))))
                 character)
(<span>cl-defun</span> <span>f₆</span> (<span>&amp;key</span> z) ?A)

(f₆ 'hi)     <span>;; </span><span>⇒  Keyword argument 144 not one of (:z)</span>
(f₆)         <span>;; </span><span>⇒ 65; i.e., the character ‘A’</span>
(f₆ <span>:z</span> 6)
<span>;; </span><span>⇒  f₆: Type mismatch!</span>
<span>;;    </span><span>Expected (satisfies (lambda (it) (and (integerp it) (= 0 (mod it 5))))) :z</span>
<span>;;    </span><span>≠ Given integer 6.</span>

(f₆ <span>:z</span> 10) <span>;; </span><span>⇒ 65; i.e., the expected output since 10 mod 5 ≈ 0 &amp; so 10 is valid input.</span>
</pre>
</div>

<p>
<b>Preconditions!</b> The previous example had a complex type on a keyword, but that was
essentially a pre-condition; we can do the same on positional arguments.
</p>
<div>
<pre>(declare-type f₇ : (satisfies (<span>lambda</span> (it) (<span>=</span> it 5)))
                   integer)
(<span>cl-defun</span> <span>f₇</span> (n) n)
<span>;; </span><span>The identity on 5 function; and undefined otherwise.</span>

(f₇ 4)
<span>;; </span><span>⇒ f₇: Type mismatch! Expected (satisfies (lambda (it) (= it 5))) for argument 0</span>
<span>;;       </span><span>≠ Given integer 4.</span>

(f₇ 5) <span>;; </span><span>⇒ 5</span>
</pre>
</div>

<p>
<b>Postconditions!</b>
Given an integer greater than 5, we present an integer greater than 2; i.e.,
this is a constructive proof that \(∀ n • n &gt; 5 ⇒ n &gt; 2\).
</p>
<div>
<pre>(declare-type f₈ : (satisfies (<span>lambda</span> (in)  (<span>&gt;</span> in 5)))
                   (satisfies (<span>lambda</span> (out) (<span>&gt;</span> out 2))))
(<span>cl-defun</span> <span>f₈</span> (n) n)
<span>;; </span><span>The identity on 5 function; and undefined otherwise.</span>

(f₈ 4)
<span>;; </span><span>⇒  f₈: Type mismatch! Expected (satisfies (lambda (in) (&gt; in 5))) for argument 0</span>
<span>;;        </span><span>≠ Given integer 4.</span>

(f₈ 72) <span>;; </span><span>⇒ 72; since indeed 72 &gt; 5 for the input, and clearly 72 &gt; 2 for the output.</span>
</pre>
</div>

<p>
As it currently stands we cannot make any explicit references between the inputs
and the output, but that's an easy fix: Simply add a local function <code>old</code> to the
<a href="#org84d2c90"><code>declare-type</code></a> macro which is intentionally exposed so that it can be used in the
type declarations to refer to the ‘old’, or initial, values provided to the function.
Additionally, one could also add keyword arguments <code>:requires</code> and <code>:ensures</code>
for a more sophisticated pre- and post-condition framework.
<a href="https://github.com/sellout/quid-pro-quo">Something</a> along these lines is implemented for Common Lisp.
</p>

<p>
Here's a fun exercise: Recast the <a href="https://liquid.kosmikus.org/01-intro.html#/what-is-liquid-haskell">Liquid Haskell</a> examples in Lisp using this
<a href="#org84d2c90"><code>declare-type</code></a> form.
</p>
</div></div>]]>
            </description>
            <link>https://alhassy.github.io/TypedLisp.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23878612</guid>
            <pubDate>Sat, 18 Jul 2020 03:19:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Computation Graphs and Graph Computation]]>
            </title>
            <description>
<![CDATA[
Score 100 | Comments 25 (<a href="https://news.ycombinator.com/item?id=23878381">thread link</a>) | @bmc7505
<br/>
July 17, 2020 | http://breandan.github.io/2020/06/30/graph-computation/ | <a href="https://web.archive.org/web/*/http://breandan.github.io/2020/06/30/graph-computation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <p><span>30 Jun 2020</span></p><p>Research has begun to reveal many algorithms can be expressed as matrix multiplication, suggesting an unrealized connection between linear algebra and computer science. I speculate graphs are the missing piece of the puzzle. Graphs are not only useful as cognitive aides, but are suitable data structures for a wide variety of tasks, particularly on modern parallel processing hardware.</p>

<p>In this essay, I explore the virtues of graphs, algebra, types, and show how these concepts can help us reason about programs. I propose a computational primitive based on graph signal processing, linking software engineering, graphs, and linear algebra. Finally, I share my predictions for the path ahead, which I consider to be the start of an exciting new chapter in computing history.</p>

<p><em>n.b.: None of these ideas are mine alone. Shoulders of giants. Follow the links and use landscape mode for optimal reading experience.</em></p>

<ul>
  <li><a href="#new-decade-new-delusions">Biographical details</a></li>
  <li><a href="#what-are-graphs">Graph applications</a></li>
  <li><a href="#induction-introduction">Inductive languages</a></li>
  <li><a href="#graphs-inductively">Inductive graphs</a></li>
  <li><a href="#graph-computation/#graph-languages">Graph languages</a></li>
  <li><a href="#graphs-computationally">Dynamical systems on graphs</a></li>
  <li><a href="#graphs-efficiently">Efficient implementations</a></li>
  <li><a href="#programs-as-graphs">Partial evaluation and program synthesis</a></li>
  <li><a href="#roadmap">Future roadmap</a></li>
</ul>



<p>Over the last decade, I bet on some strange ideas. A lot of people I looked up to at the time laughed at me. I’ll bet they aren’t laughing anymore. I ought to thank them one day, because their laughter gave me a lot of motivation. I’ve said some idiotic things to be sure, but I’ve also made some laughable predictions that were correct. Lesson learned: aim straighter.</p>

<p>In 2012, I was in Austin sitting next to an ex-poker player named <a href="https://twitter.com/amirpc">Amir</a> who was singing Hinton’s praises. Hypnotized by his technicolor slides, I quit my job in a hurry and started an educational project using speech recognition and restricted Boltzmann machines. It never panned out, but I learned a lot about ASR and Android audio. Still love <a href="http://breandan.net/2014/02/09/the-end-of-illiteracy/">that idea</a>.</p>

<center>
<a href="https://www.cs.toronto.edu/~hinton/csc2535/notes/lec4new.pdf"><img width="75%" src="http://breandan.github.io/images/rbm.png"></a>
</center>

<p>In 2017, I started writing a book on the ethics of automation and <a href="http://breandan.net/2017/02/02/trust-in-automation/">predicted</a> mass unemployment and social unrest. Although I got the causes wrong (pandemic, go figure), the information economy and confirmation bias takes were all dead right. Sadly, this is now driving the world completely insane. Don’t say I warned you, go out and fix our broken systems. The world needs more engineers who care.</p>

<center>
<a href="https://colah.github.io/posts/2015-09-NN-Types-FP/"><img width="75%" src="http://breandan.github.io/images/diff_prog.png"></a>
</center>

<p>In 2017, I witnessed the birth of <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">differentiable programming</a>, which I stole from Chris Olah and turned into a <a href="https://github.com/breandan/kotlingrad/blob/master/latex/thesis/thesis.pdf">master’s thesis</a>. Had a lot of trouble convincing people that classical programs could be made differentiable, but look at the proceedings of any machine learning conference today and you’ll find dozens of papers on differentiable sorting and rendering and simulation. Don’t thank me, thank Chris and the Theano guys.</p>

<p>In 2018, I correctly predicted Microsoft would acquire GitHub to mine code. Why MS and not Google? I’ll bet they tried, but Google’s leadership had fantasies of AGI and besides JetBrains, MS were the only ones who gave a damn about developers. Now ML4SE is a thriving <a href="https://ml4se.github.io/">research area</a> and showing up in <a href="https://github.com/JetBrains-Research/DeepBugsPlugin">real</a> <a href="https://devblogs.microsoft.com/visualstudio/ai-assisted-intellisense-for-your-teams-codebase/">products</a>, much to the chagrin of those who believed ML was a fad. I suspect their hype filter blinded them to the value those tools provide.</p>

<center>
<blockquote><p lang="en" dir="ltr">Prediction: MS will acquire GH within five years. If the <a href="https://twitter.com/hashtag/ML4Code?src=hash&amp;ref_src=twsrc%5Etfw">#ML4Code</a> stuff delivers for MS, acquisition is highly likely. Although it would have been cheaper a few years ago. <a href="https://t.co/5ZMtiRtifD">https://t.co/5ZMtiRtifD</a> <a href="https://t.co/TaxkArm5ps">https://t.co/TaxkArm5ps</a></p>— breandan (@breandan) <a href="https://twitter.com/breandan/status/993553301927936001?ref_src=twsrc%5Etfw">May 7, 2018</a></blockquote> 

<a href="https://blogs.microsoft.com/blog/2018/10/26/microsoft-completes-github-acquisition/">
<img width="75%" src="http://breandan.github.io/images/microsoft_github_aquisition.png"></a>
</center>

<p>But to heck with everything I’ve said! If I had just one idea to share with these ML people, it would be types. Beat that drum as loud as I could. Types are the best tool we know for synthetic reasoning. If you want to build provably correct systems that scale on real-world applications, use types. Not everyone is convinced yet, but mark my words, <a href="https://github.com/tensorflow/tensorflow/issues/12345">types</a> are <a href="https://docs.python.org/3.9/whatsnew/3.9.html#pep-585-builtin-generic-types">coming</a>. Whoever figures out how to connect types and learning will be the next Barbara Liskov or Frances Allen.</p>

<p>This year, I predicted the pandemic weeks before the lockdown, exited the market, and turned down a job at Google. Some people called me crazy. Now I’m going all-in on some new ideas (none of which are mine). I’m making some big bets and some will be wrong, but I see the very same spark of genius in them.</p>



<p>As a kid, I was given a book on the history of mathematics. I remember it had some interesting puzzles, including one with <a href="https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg">some bridges</a> in a town divided by rivers, once inhabited by a man called Euler. Was there a tour crossing each bridge exactly once? Was it possible to tell without checking every path? I remember spending days trying to figure out the answer.</p>

<center>
<a href="https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg"><img width="60%" src="http://breandan.github.io/images/konigsberg_bridges.png"></a>
</center>

<p>In the late 90s, my mom and I went to Ireland. I remember visiting Trinity College, and learning about a mathematician called <a href="https://en.wikipedia.org/wiki/William_Rowan_Hamilton">Hamilton</a> who discovered a famous formula connecting algebra and geometry, and carved it onto a <a href="https://en.wikipedia.org/wiki/Broom_Bridge">bridge</a>. We later visited the bridge, and the tour guide pointed out the stone, which we touched for good luck. The Irish have a <a href="https://en.wikipedia.org/wiki/Poulnabrone_dolmen">thing</a> for <a href="https://en.wikipedia.org/wiki/Newgrange">stones</a>.</p>

<center>
<a href="http://www.kurims.kyoto-u.ac.jp/EMIS/classics/Hamilton/PRIAIcos.pdf"><img width="40%" src="http://breandan.github.io/images/quaternions.jpg"></a>
</center>

<p>In 2007, I was applying to college and took the train from Boston to South Bend, Indiana, home of the Fighting Irish. Wandering about, I picked up a magazine article by a Hungarian mathematician called <a href="https://en.wikipedia.org/wiki/Albert-L%C3%A1szl%C3%B3_Barab%C3%A1si">Barabási</a> then at Notre Dame, who had some interesting things to say about <a href="https://en.wikipedia.org/wiki/Complex_network">complex networks</a>. Later in 2009, while studying in Rochester, I <a href="http://breandan.github.io/images/complex_network_seminar.png">carpooled</a> with a <a href="https://avesis.medeniyet.edu.tr/hasan.guclu">nice professor</a>, and learned complex networks are found in brains, languages and many marvelous places.</p>

<center>
<a href="https://barabasi.com/f/226.pdf"><img width="75%" src="http://breandan.github.io/images/complex_networks.png"></a>
</center>

<p>Fast forward to 2017. I was lured by the siren song of algorithmic differentiation. Olivier Breleux presented <a href="https://github.com/mila-iqia/myia">Myia</a> and <a href="https://github.com/breuleux/buche">Buche</a>. Matt Johnson gave a talk on <a href="https://github.com/HIPS/autograd">Autograd</a>. I met Chris Olah in Long Beach, who gave me the idea to study <a href="https://colah.github.io/posts/2015-09-NN-Types-FP/">differentiable programming</a>. I stole his idea, dressed it up in Kotlin and traded it for a POPL workshop paper and later a <a href="https://github.com/breandan/kotlingrad/blob/master/latex/thesis/thesis.pdf">Master’s thesis</a>. Our contributions were using algebra, shape inference and presenting AD as term rewriting.</p>

<center>
<a href="https://github.com/breandan/kotlingrad#dataflow-graphs"><img width="75%" src="https://github.com/breandan/kotlingrad/raw/master/samples/src/main/resources/dataflow.svg"></a>
</center>

<p>In 2019, I joined a lab with a <a href="https://www.cs.mcgill.ca/~jguo/">nice professor</a> at McGill applying knowledge graphs to software engineering. Like logical reasoning, knowledge graphs are an idea from the first wave of AI in the 1960s and 70s which have been revived and studied in light of recent progress in the field. I believe this is an important area of research with a lot of potential. Knowledge and traceability plays a big role in software engineering, and it’s the bread-and-butter of a good IDE. The world needs better IDEs if we’re ever going to untangle this mess we’re in.</p>

<center>
<a href="https://structurizr.com/"><img width="45%" src="https://raw.githubusercontent.com/cecuesta/structurizr-java/master/docs/images/graphviz-spring-petclinic-components.png"></a>
</center>

<p>This Spring, I took a fascinating seminar on <a href="https://cs.mcgill.ca/~wlh/comp766/index.html">Graph Representation Learning</a>. A lot of delightful graph theory has been worked out over the last decade. <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a> turned into power iteration. People have discovered many interesting connections to linear algebra, including Weisfeiler-Lehman graph kernels, graph Laplacians, Krylov methods, and spectral graph theory. These ideas have deepened our understanding of graph signal processing and its applications for learning and program analysis. More on that <a href="#graphs-computationally">later</a>.</p>



<p>Graphs are general-purpose data structures used to represent a variety of data types and procedural phenomena. Unlike most sequential languages, graphs are capable of expressing a much richer family of relations between entities, and are a natural fit for many problems in computer science, physics, biology and mathematics. Consider the following hierarchy of data structures, all of which are graphs with increasing expressive power:</p>

<ul>
  <li><strong>Sets</strong>: datasets, multisets, posets, alphabets</li>
  <li><strong>Sequences</strong>: Lists, strings, arrays, linear function composition</li>
  <li><strong>Trees</strong>: <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract syntax</a>, <a href="https://en.wikipedia.org/wiki/Document_Object_Model">XML</a>, <a href="https://en.wikipedia.org/wiki/Phylogenetic_tree">phylogeny</a>, <a href="https://en.wikipedia.org/wiki/Decision_tree">decision trees</a></li>
  <li><strong>DAGs</strong>: <a href="https://eagain.net/articles/git-for-computer-scientists/">Git</a>, <a href="https://en.wikipedia.org/wiki/Citation_network">citations</a>, <a href="https://en.wikipedia.org/wiki/Dependency_graph">dependency graphs</a>, <a href="https://en.wikipedia.org/wiki/Workflow_management_system">workflows</a>, <a href="https://en.wikipedia.org/wiki/Control-flow_graph">control flow</a>, <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">MLPs</a></li>
  <li><strong>Directed graphs</strong>: <a href="https://en.wikipedia.org/wiki/Finite-state_machine">State machines</a>, <a href="http://dkeenan.com/Lambda/">λ-calculus</a>, <a href="https://computersciencewiki.org/index.php/The_web_as_a_directed_graph">the web</a>, <a href="https://en.wikipedia.org/wiki/Call_graph">call graphs</a>, <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNNs</a></li>
  <li><strong>Hypergraphs</strong>: <a href="https://arxiv.org/pdf/2003.02320.pdf">Knowledge</a>, <a href="https://zettelkasten.de/">Zettelkasten</a>, <a href="https://en.wikipedia.org/wiki/Category_theory">categories</a>, <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/">physics</a>, <a href="https://openreview.net/pdf?id=rkpACe1lx">hypernetworks</a></li>
</ul>

<p>As we realized in <a href="https://github.com/breandan/kotlingrad">Kotlin∇</a>, directed graphs can be used to model mathematical expressions, as well as other formal languages, including source code, intermediate representations and binary artifacts. Not only can graphs be used to describe extant human knowledge, many recent examples have shown that machines can “grow” trees and graphs for various applications, such as program synthesis, mathematical deduction and physical simulation. Recent neuro-symbolic applications have shown promising early results in graph synthesis:</p>

<ul>
  <li><a href="https://arxiv.org/pdf/1711.00740.pdf">Learning to Represent Programs with Graphs</a>, Allamanis et al., 2018</li>
  <li><a href="https://arxiv.org/pdf/1912.01412.pdf">Deep Learning for Symbolic Mathematics</a>, Lample and Charton, 2019.</li>
  <li><a href="https://arxiv.org/pdf/2006.11287.pdf">Discovering Symbolic Models from Deep Learning with Inductive Biases</a>, Cranmer et al., 2020.</li>
  <li><a href="https://arxiv.org/pdf/2005.11212.pdf">Symbolic Pregression: Discovering Physical Laws from Raw Distorted Video</a> (Udrescu &amp; Tegmark, 2020).</li>
  <li><a href="https://arxiv.org/pdf/2006.08381.pdf">DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning</a>, Ellis et al., 2020.</li>
  <li><a href="https://arxiv.org/abs/2007.03629">Strong Generalization and Efficiency in Neural Programs</a>, Li et al., 2020.</li>
  <li><a href="https://arxiv.org/pdf/1910.10593.pdf">Neural Execution of Graph Algorithms</a>, Veličković et al. (2020)</li>
</ul>

<p>The field of natural language processing has also developed a rich set of graph-based representations, such as <a href="https://en.wikipedia.org/wiki/Phrase_structure_grammar">constituency</a>, <a href="https://en.wikipedia.org/wiki/Dependency_grammar">dependency</a>, <a href="https://en.wikipedia.org/wiki/Link_grammar">link</a> and other and other typed attribute grammars which can be used to reason about syntactic and semantic relations between natural language entities. Research has begun to show many practical applications for such grammars in the extraction and organization of human knowledge stored in large text corpora. Those graphs can be further processed into ontologies for logical reasoning.</p>

<center>
<img width="60%" src="https://upload.wikimedia.org/wikipedia/commons/8/8e/Thistreeisillustratingtherelation%28PSG%29.png">
</center>

<p>Using coreference resolution and entity alignment techniques, we can reconstruct internally consistent relations between entities, which capture cross-corpus consensus in natural language datasets. When stored in <a href="https://arxiv.org/pdf/2003.02320.pdf">knowledge graphs</a>, these relations can be used for information retrieval and question answering, e.g. on wikis and other content management systems. Recent techniques have shown promise in automatic knowledge base construction (cf. <a href="https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00088">Reddy et al.</a>, 2016).</p>

<!--![logical_forms](/images/logical_forms.png) -->
<center>
<a href="https://arxiv.org/pdf/2003.02320.pdf"><img width="75%" src="http://breandan.github.io/images/knowledge_graph.png"></a>
</center>

<p>Lo and behold, the key idea behind knowledge graphs is our old friend, types. Knowledge graphs are multi-relational graphs whose nodes and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://breandan.github.io/2020/06/30/graph-computation/">http://breandan.github.io/2020/06/30/graph-computation/</a></em></p>]]>
            </description>
            <link>http://breandan.github.io/2020/06/30/graph-computation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23878381</guid>
            <pubDate>Sat, 18 Jul 2020 02:20:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Levandowski files suit against Uber [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 112 | Comments 91 (<a href="https://news.ycombinator.com/item?id=23877872">thread link</a>) | @MrSandman
<br/>
July 17, 2020 | https://ipfs.eternum.io/ipfs/Qmd9PTEtuSrKKtJQw36aNzpjJwZAdCdwCmUn4w21mmq74z/Levandowski-Uber-Complaint-1.pdf | <a href="https://web.archive.org/web/*/https://ipfs.eternum.io/ipfs/Qmd9PTEtuSrKKtJQw36aNzpjJwZAdCdwCmUn4w21mmq74z/Levandowski-Uber-Complaint-1.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://ipfs.eternum.io/ipfs/Qmd9PTEtuSrKKtJQw36aNzpjJwZAdCdwCmUn4w21mmq74z/Levandowski-Uber-Complaint-1.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-23877872</guid>
            <pubDate>Sat, 18 Jul 2020 00:48:48 GMT</pubDate>
        </item>
    </channel>
</rss>
