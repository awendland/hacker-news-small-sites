<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sat, 26 Sep 2020 12:30:43 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sat, 26 Sep 2020 12:30:43 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Show HN: Open-Source Memex – Alternative Approach to Roam/Obsidian]]>
            </title>
            <description>
<![CDATA[
Score 144 | Comments 41 (<a href="https://news.ycombinator.com/item?id=24572449">thread link</a>) | @steve1820
<br/>
September 23, 2020 | https://www.steveliu.co/memex | <a href="https://web.archive.org/web/*/https://www.steveliu.co/memex">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-block-type="2" id="block-33de0799eab39cfadbe1"><div><p>I’ve never been a huge online note taker. From high school to university, I’ve always relied on pen and paper as my weapon of choice. At the time and even to some extent now, I’ve felt like this was a good enough solution to my problems.</p><p>I’ve always felt the simpler the solution the better. Why complicate things?</p><p>This changed however after working as a software engineer in industry. As I worked on a software product that derived its core functionality from machine learning, it seemed that I was constantly drowning in a sea of information.&nbsp;</p><p>It was a constant repetition of learning something, forgetting about it 5 months later and then having to recycle through my notes and reread the article/paper/blog.</p><p>My brain was a leaky bucket. Every time I poured something in, something else would leak out.</p><p>It was during those dark times of desperation that I stumbled upon the “niche” industry of Knowledge Management Systems (KMS) and as an extension, the Memex.</p><p> I was fascinated with all the innovation coming from up and coming open source projects and companies in this space. Software like Athens (https://github.com/athensresearch/athens), Roam (https://roamresearch.com/), Obsidian (https://obsidian.md/) all seemed so promising. </p><p>I was particularly inspired by reading karlicoss’s blog (https://beepb00p.xyz/promnesia.html). He outlines so many good and intuitive reasons why the current solutions are broken (although in this particular post he focuses on browser history).</p></div></div></div></div>]]>
            </description>
            <link>https://www.steveliu.co/memex</link>
            <guid isPermaLink="false">hacker-news-small-sites-24572449</guid>
            <pubDate>Wed, 23 Sep 2020 21:44:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bleeding-edge tech will kill your startup]]>
            </title>
            <description>
<![CDATA[
Score 166 | Comments 118 (<a href="https://news.ycombinator.com/item?id=24571216">thread link</a>) | @bmaho
<br/>
September 23, 2020 | https://www.contrast.app/posts/bleeding-edge-tech-means-youll-bleed-to-death | <a href="https://web.archive.org/web/*/https://www.contrast.app/posts/bleeding-edge-tech-means-youll-bleed-to-death">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Remember those old don't-do-drug ads? A shaky camera pans to a narc wearing a wrinkled button-down:</p><p><strong>Egg is lifted:</strong></p><p>"This is your brain"</p><p><strong>Points to hot skillet:</strong></p><p>"This is drugs"</p><p><strong>Cracks egg into skillet. Egg starts frying:</strong></p><p>"This is your brain on drugs"</p><figure id="w-node-8595f4c9f275-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8cd757b4ac440ca61db3_Frame%202.png" loading="lazy" alt=""></p></figure><p>Firstly, makes no sense seeing everyone loves fried eggs. Secondly, this is how my brain looked when trying to start a "bleeding edge" tech company.</p><p>2018. Winter. NYC.</p><p>Lying on the floor, looking like the beached whale I am, I stared at the ceiling. For months, we had been trying to start a food delivery business. Nothing bleeding-edge, just pain.</p><p>It was in that moment, with my shirt gathering fuzz from the un-vacuumed carpet below, that we changed everything. New market, new users, new product, new tech. We just wanted to do something new—something no one else was doing.</p><p>As designers and engineers, we knew of a rather niche problem: if your company has a design system, it's practically impossible to know the adoption rate of those components in your product. Huzzah! we thought. We should build a tool that tells you the adoption rate of every component in your design system—across your product. Chefs kiss.</p><figure id="w-node-f806252cfa53-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8ce4411317e302386081_Frame%208.png" loading="lazy" alt=""></p></figure><p>Now, no worries if you don't know what this means. For the sake of the article, this was "bleeding-edge tech," and this simple fact got us psyched. We would be the first, the trailblazers. We felt like how I imagine all the kids who grow up watching "Baby Einstein" feel—pompous, cute, and little geniuses.</p><p>Little did I know, this mentality would fuck us...</p><p>Before continuing with my failures, a history lesson. After a quick Wikipedia search, the first time we see the term "bleeding edge technology" being used was during the early 80s—right when the drug-ad above aired (clearly some weird shit happening then). Bleeding edge was an iteration on the classic "cutting-edge" or "leading-edge" phrases—altered to show an even higher level of risk for both the company and the customer.</p><p>So how did this mentality fuck us? Well, I've come up with a rather clever framework I'm calling:</p><h5>"The Bloody Edged Circle of Death"</h5><p>‍</p><p>This loop consists of <strong>three parts:</strong></p><ol role="list"><li>Sales</li><li>Product</li><li>Mental game</li></ol><figure id="w-node-c5258bced305-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8bf55a4723fd0e275db4_Frame%2012-min.png" loading="lazy" alt=""></p></figure><h5>Sales</h5><p>Sales is hard and if you want to make it 10X harder, try selling something nobody knows anything about. Like literally nothing. When we started, prospects didn't know our name, the problem we were solving, our solution, how valuable it would be, what it should be priced at, and the list went on and on. And here's the kicker, because they knew nothing, they didn't trust us. We weren't proven and thus we were a huge risk to adopt.</p><p>With this bleeding edge product in hand, we were banging our heads against the wall, trying to sell to early users who'd be willing to try something incredibly new and incredibly risky. Not fun. Very difficult.</p><p>‍</p><figure id="w-node-b112f59a5983-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8bfe1409e87b269d51cd_Frame%2013-min.png" loading="lazy" alt=""></p></figure><h5>Product</h5><p>The brutal sales cycles then bled into our product. After a few months, we had convinced a handful of companies to test our tech. The day would arrive and we'd finally onboard them. But guess what, they had no idea how to use our tool or integrate it within their workflow. And to be honest, nor did we. Adoption was bleak and feedback was dry—silence in startups is not golden.</p><p>What I realized is when you're the first, there is very little to look at for inspiration, ideas, or as a benchmark. It's up to you as the trailblazer to build the path and hope it will lead somewhere. This is something I think many of wish we could do—but I think very few can.</p><p>‍</p><figure id="w-node-97e798a4e2ae-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8c07e074c53b56bc6989_Frame%2014-min.png" loading="lazy" alt=""></p></figure><h5>Mental game</h5><p>I think the worst stage is the mental-game. When you're trying to drag your zombie-death startup from the crypts of hell and into the light, there are so many things to stress about. Your messaging, positioning, pricing, product, customers, sales, employees—literally everything. But when you're building something truly new, there is an additional poisonous leach that creeps into your brain and plants its ugly seed: doubt.</p><p>Bleeding edge tech means you'll constantly be doubting whether what you've created will ever be something or not—there is no market data to tell you otherwise. You don't know if customers are already using a solution like yours. You don't know if anyone is willing to pay for your offering. You don't know how big the market is or will ever become. You don't know anything.</p><p>This was the hardest thing for me to deal with personally. The daily doubt. Constantly wondering if we needed to just tweak a few of the 4 P's or much worse—that this wasn't a real problem needed to be solved.</p><p>"The Bloody Edged Circle of Death" continued its cycle until one day, we decided no more. We didn't have the conviction in ourselves and the product to keep pushing through the long sales cycles, silence from inactive users, and the crippling doubt. We were done <strong>bleeding out from bleeding edge tech.</strong></p><p>Since then, I've been thinking a lot about why we were so keen on building tech no one had ever seen before. When I started to look for answers, I noticed something peculiar. All of the "hottest" tech companies: Airtable, Notion, Slack, Zoom—none of them are "bleeding edge" per se. Sure, they have incrementally innovated on existing products in existing categories—but they're not doing anything truly <em>new.</em></p><p>They identified problems with the status quo, in massive markets, with huge amounts of users and money. They then developed a better product by improving the UX, performance, and adding new features. Don't get me wrong, this is an insanely difficult feat to ever pull off. Yet, before I gave this any thought, I assumed they were successful because they were doing something new—because they were bleeding edge.</p><p>So why was I fed this lie and believed it to be true?</p><p>I think the answer is pretty simple—marketing and ego. If you're a founder, you want your company to be seen as revolutionary. Even if your startup just slapped lipstick on a pig, you want the world to believe you invented a super hot and sexy pig, one that sprouts wings and poops pearls.</p><p>And same thing goes for VCs. Investors want to be seen as picking the best and brightest—finding the diamonds in the rough. Their aim is to say: "I backed a ground breaking technology that changed the world," and not: "I invested in 10 slightly-better-than-Google-Docs competitors." No one on Twitter is getting jacked on that.</p><p>If you're anything like me, you've been reading, listening, and following the big players in Silicon-Valley for years. We've been told a narrative that sounds great and inspires, but it's not based in facts. Bleeding edge is a myth that fucks up your chances of success.</p><p>Since this realization, we've purposefully taken our company in a different direction. A new strategy, one where we purposefully have entered an existing market—a market where we have expertise. Yes, there is competition, but we've been using these tools for years, we know the problems they have, and we want to make something better—both for our team and customers.</p><p>Although this path doesn't have the same sparkle as a bleeding-edge tech company might, I can sleep easier knowing we have a higher chance of getting somewhere with it. I can finally focus on the problems every startup deals with without that doubt—that nose-crinkling, water-trash stench of doubt. </p><p>There is a market. There are users who pay. There is value to what we're doing. Now it's just up to us to make our product, positioning, and team the best it can be.</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.contrast.app/posts/bleeding-edge-tech-means-youll-bleed-to-death</link>
            <guid isPermaLink="false">hacker-news-small-sites-24571216</guid>
            <pubDate>Wed, 23 Sep 2020 19:56:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ray Marching Soft Shadows in 2D]]>
            </title>
            <description>
<![CDATA[
Score 160 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24569542">thread link</a>) | @rjkaplan
<br/>
September 23, 2020 | https://www.rykap.com/2020/09/23/distance-fields/ | <a href="https://web.archive.org/web/*/https://www.rykap.com/2020/09/23/distance-fields/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    


    <div role="main">
      <div>
        




<article>
  

<p><em>Disclaimer: the demos on this page use WebGL features that aren’t available on some mobile devices.</em></p>

<p>A couple of weeks ago I tweeted a video of a toy graphics project (below). It’s not done, but a lot of people liked it which was surprising and fun! A few people asked how it works, so that’s what this post is about.</p>



<p>Under the hood it uses something called a distance field. A distance field is an image like the one below that tells you how far each pixel is from your shape. Light grey pixels are close to the shape and dark grey pixels are far from it.</p>

<img src="https://www.rykap.com/images/ray-marching/distance-field.png">

<p>When the demo starts up, it draws some text on a 2D canvas and generates a distance field of it. It uses <a href="https://github.com/ryankaplan/gpu-distance-field">a library I wrote</a> that generates distance fields really quickly. If you’re curious how the library works, I wrote about that <a href="http://rykap.com/graphics/skew/2016/02/25/voronoi-diagrams/">here</a>.</p>

<p>Our lighting scheme works like this: when processing a particular pixel we consider a ray from it to the light, like so…</p>

<img src="https://www.rykap.com/images/ray-marching/ray-march-1.png">

<p>If the ray intersects a glyph, the pixel we’re shading must be in shadow because there’s something between it and the light.</p>

<p>The simplest way to check this would be to move along the ray in 1px increments, starting from the pixel we’re shading and ending at the light, repeatedly asking the distance field if we’re distance 0 from a shape. This would work, but it’d be really slow.</p>

<p>We could pick some specific length like 30px and move in increments of that size, but then we risk jumping over glyphs that are smaller than 30px. We might think we’re not in shadow when we should be.</p>

<p><strong>Ray marching’s core idea is this: the distance field tells you how far you are from the closest glyph. You can safely advance along your ray by that distance without skipping over any glyphs.</strong></p>

<p>Let’s walk through an example. We start as pictured above and ask the distance field how far we are from any glyph. Turns out in this case that the answer is 95px (pictured left). This means that we can move 95px along our ray without skipping over anything!</p>

<img src="https://www.rykap.com/images/ray-marching/ray-march-2.png">

<p>Now we’re a little closer to the light. We repeat the process until we hit the ascender of the b! If the b glyph weren’t there, we’d have kept going until we hit the light.</p>

<p>Below is a demo that shows the ray marching steps for a given pixel. The red box is the pixel we’re shading, and each circle along the ray represents a ray marching step and the distance from the scene at that step.</p>

<p>Try dragging the light and the pixel around to build an intuition for it.</p>



<p>Below is GLSL to implement this technique. It assumes you’ve defined a function <code>getDistance</code> that samples the distance field.</p>

<div><div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>

<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>;</span>
<span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>distance</span><span>(</span><span>rayOrigin</span><span>,</span> <span>lightPosition</span><span>))</span> <span>{</span>
    <span>// We hit the light! This pixel is not in shadow.</span>
    <span>return</span> <span>1</span><span>.;</span>
  <span>}</span>

  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span>
    <span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>It turns out that some pixels are really expensive to process. So in practice we use a for-loop instead of a while loop – that way we bail out if we’ve done too many steps. A common “slow case” in ray marching is when a ray is parallel to the edge of a shape in the scene…</p>



<p>The approach I’ve described so far will get you a scene that looks like the one below.</p>



<p>It’s cool, but the shadows are sharp which doesn’t look very good. The shadows in the demo look more like this…</p>

<img src="https://www.rykap.com/images/ray-marching/desired-shadows.png">

<p>One big disclaimer is that they’re not physically realistic! Real shadows look like hard shadows where the edges have been fuzzed. This approach does something slightly different: all pixels that were previously in shadow are still fully in shadow. We’ve just added a penumbra of partially shaded pixels around them.</p>

<p>The upside is that they’re pretty and fast to compute, and that’s what I care about! There are three “rules” involved in computing them.</p>

<p><strong>Rule 1:</strong> The closer a ray gets to intersecting a shape, the more its pixel should be shadowed. In the image below there are two similar rays (their distances to the shape pictured in yellow and green). We want the one that gets closer to touching the corner to be more shadowed.</p>

<img src="https://www.rykap.com/images/ray-marching/rule-1.png">

<p>This is cheap to compute because the variable <code>sceneDist</code> tells us how far we are from the closest shape at each ray marching step. So the smallest value of <code>sceneDist</code> across all steps is a good approximation for the yellow and green lines in the image above.</p>

<p><strong>Rule 2:</strong> if the pixel we’re shading is far from the point where it almost intersects a shape, we want the shadow to spread out more.</p>

<img src="https://www.rykap.com/images/ray-marching/rule-2.png">

<p>Consider two pixels along the ray above. One is closer to the almost-intersection and is lighter (its distance is the green line). The other is farther and darker (its distance is the yellow line). In general: the further a pixel is from its almost intersection, the more “in shadow” we should make it.</p>

<p>This is cheap to compute because the variable <code>rayProgress</code> is the length of the green and yellow lines in the image above.</p>

<p>So: we previously returned <code>1.0</code> for pixels that weren’t in shadow. To implement rules 1 and 2, we compute <code>sceneDist / rayProgress</code> on each ray marching step, keep track of its minimum value, and return that instead.</p>

<div><div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>
<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>.;</span>
<span>float</span> <span>stopAt</span> <span>=</span> <span>distance</span><span>(</span><span>samplePt</span><span>,</span> <span>lightPosition</span><span>);</span>
<span>float</span> <span>lightContribution</span> <span>=</span> <span>1</span><span>.;</span>
<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>64</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>stopAt</span><span>)</span> <span>{</span>
    <span>return</span> <span>lightContribution</span><span>;</span>
  <span>}</span>

  <span>// `getDistance` samples our distance field texture.</span>
  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span>
    <span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>lightContribution</span> <span>=</span> <span>min</span><span>(</span>
    <span>lightContribution</span><span>,</span>
    <span>sceneDist</span> <span>/</span> <span>rayProgress</span>
  <span>);</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>

<span>// Ray-marching took more than 64 steps!</span>
<span>return</span> <span>0</span><span>.;</span>
</code></pre></div></div>

<p>This ratio feels kind of magical to me because it doesn’t correspond to any physical value. So let’s build some intuition for it by thinking through why it might take on particular values…</p>

<ul>
  <li>
    <p>If <code>sceneDist / rayProgress &gt;= 1</code>, then either <code>sceneDist</code> is big or <code>rayProgress</code> is small (relative to each other). In the former case we’re far from any shapes and we shouldn’t be in shadow, so a light value of <code>1</code> makes sense. In the latter case, the pixel we’re shadowing is really close to an object casting a shadow and the shadow isn’t fuzzy yet, so a light value of <code>1</code> makes sense.</p>
  </li>
  <li>
    <p>The ratio is <code>0</code> only when <code>sceneDist</code> is <code>0</code>. This corresponds to rays that intersect an object and whose pixels are in shadow.</p>
  </li>
</ul>

<p>And here’s a demo of what we have so far…</p>



<p><strong>Rule #3</strong> is the most straightforward one: light gets weaker the further you get from it.</p>

<p>Instead of returning the minimum value of <code>sceneDist / rayProgress</code> verbatim, we multiply it by a <code>distanceFactor</code> which is <code>1</code> right next to the light, <code>0</code> far away from it, and gets quadratically smaller as you move away from it.</p>

<p>All together, the code for the approach so far looks like this…</p>

<div><div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>
<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>.;</span>
<span>float</span> <span>stopAt</span> <span>=</span> <span>distance</span><span>(</span><span>samplePt</span><span>,</span> <span>lightPosition</span><span>);</span>
<span>float</span> <span>lightContribution</span> <span>=</span> <span>1</span><span>.;</span>
<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>64</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>stopAt</span><span>)</span> <span>{</span>
    <span>// We hit the light!</span>
    <span>float</span> <span>LIGHT_RADIUS_PX</span> <span>=</span> <span>800</span><span>.;</span>

    <span>// fadeRatio is 1.0 next to the light and 0. at</span>
    <span>// LIGHT_RADIUS_PX away.</span>
    <span>float</span> <span>fadeRatio</span> <span>=</span>
      <span>1</span><span>.</span><span>0</span> <span>-</span> <span>clamp</span><span>(</span><span>stopAt</span> <span>/</span> <span>LIGHT_RADIUS_PX</span><span>,</span> <span>0</span><span>.,</span> <span>1</span><span>.);</span>

    <span>// We'd like the light to fade off quadratically instead of</span>
    <span>// linearly.</span>
    <span>float</span> <span>distanceFactor</span> <span>=</span> <span>pow</span><span>(</span><span>fadeRatio</span><span>,</span> <span>2</span><span>.);</span>
    <span>return</span> <span>lightContribution</span> <span>*</span> <span>distanceFactor</span><span>;</span>
  <span>}</span>

  <span>// `getDistance` samples our distance field texture.</span>
  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span><span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>lightContribution</span> <span>=</span> <span>min</span><span>(</span>
    <span>lightContribution</span><span>,</span>
    <span>sceneDist</span> <span>/</span> <span>rayProgress</span>
  <span>);</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>

<span>// Ray-marching took more than 64 steps!</span>
<span>return</span> <span>0</span><span>.;</span>
</code></pre></div></div>

<p>I forget where I found this soft-shadow technique, but I definitely didn’t invent it. Inigo Quilez <a href="https://www.iquilezles.org/www/articles/rmshadows/rmshadows.htm">has a great post on it</a> where he talks about using it in 3D.</p>

<p>Inigo’s post also talks about a gotcha with this approach that you might have noticed in the demos above: it causes banding artifacts. This is because Rule 1 assumes that the smallest value of <code>sceneDist</code> across all steps is a good approximation for the distance from a ray to the scene. This is not always true because we sometimes take very few ray marching steps.</p>

<p>So in my demo I use an improved approximation that Inigo writes about in his post. I also use another trick that is more effective but less performant: instead of advancing by <code>sceneDist</code> on each ray marching step, I advance by something like <code>sceneDist * randomJitter</code> where <code>randomJitter</code> is between <code>0</code> and <code>1</code>.</p>

<p>This improves the approximation because we’re adding more steps to our ray march. But we could do that by advancing by <code>sceneDist * .3</code>. The random jitter ensures that pixels next to each other don’t end up in the same band. This makes the result a little grainy which isn’t great. But I think looks better than banding… This is an aspect of the demo that I’m still not satisfied with, so if you have ideas for how to improve it please tell me!</p>

<p>Overall my demo has a few extra tweaks that I might write about in future but this is the core of it. Thanks for reading! If you have questions or comments, let me know <a href="https://twitter.com/ryanjkaplan">on Twitter</a>.</p>

<p><em>Thank you to Jessica Liu, Susan Wang, Matt Nichols and Kenrick Rilee for giving feedback on early drafts of this post! Also, if you enjoyed this post you might enjoy working with me at <a href="https://www.figma.com/careers/">Figma</a>!</em></p>

</article>






  
  
  






      </div>
    </div>
  </div></div>]]>
            </description>
            <link>https://www.rykap.com/2020/09/23/distance-fields/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24569542</guid>
            <pubDate>Wed, 23 Sep 2020 17:25:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Almost Realtime Live Data Visualization in QGIS – Air Traffic Use Case]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24565250">thread link</a>) | @geomatics99
<br/>
September 23, 2020 | https://www.geodose.com/2020/09/realtime%20live%20data%20visualization%20qgis.html | <a href="https://web.archive.org/web/*/https://www.geodose.com/2020/09/realtime%20live%20data%20visualization%20qgis.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-3280341045593062951" itemprop="articleBody">
<p>
  When working in a GIS software like QGIS, mostly we are working with static
  data like street, building, land cover, etc. Or might be a data which has time
  information so we can visualize the temporal change. What about visualize live
  data in almost real time? Do we need a GIS server, cloud or map service? I
  think this is an interesting topic, and I will discuss about it in this post
  with live air traffic data use case.
</p>
<p>
  In the previous post, I made a tutorial
  <a href="https://www.geodose.com/2020/08/create-flight-tracking-apps-using-python-open-data.html">how to build a flight tracking application with open air traffic data in
    Python</a>. The application is running in a web browser and the flight data will be
  updated in a specified time interval. In this tutorial we will do the same
  thing in QGIS. We will visualize the air traffic live data on QGIS map canvas
  and get the update data in every five or ten seconds. At the end of this
  tutorial we will get an almost realtime airplanes' location within an area as
  in figure 1 below.
</p>

<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-Uk3IIrnlhrg/X1ulZ9x2OlI/AAAAAAAACRA/E1JjjYxKqBYgOCpc2ConnZDGREkWEuNAwCNcBGAsYHQ/s997/live-data-sfo-airport3.gif"><img alt="Air Traffic Live Data in QGIS" data-original-height="602" data-original-width="997" height="386" src="https://1.bp.blogspot.com/-Uk3IIrnlhrg/X1ulZ9x2OlI/AAAAAAAACRA/E1JjjYxKqBYgOCpc2ConnZDGREkWEuNAwCNcBGAsYHQ/w640-h386/live-data-sfo-airport3.gif" title="Air Traffic Live Data in QGIS" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 1. Air Traffic Live Data in QGIS. Airplanes are queueing for
        landing at San Fransisco International Airport <br>
      </td>
    </tr>
  </tbody>
</table>

<p>
  Figure 2 is the schema that shows how the system works. Can be seen from the
  schema that Python plays role for requesting the data, get the response,
  process it and store the data into a CSV plain text. On the other side, QGIS
  will render the airplanes position based on the data and refresh it with in an
  interval to get the latest information. To make it happen, we don't need any
  GIS server, cloud or map service. Anyone could do this as long as there is
  internet connection available with Python and QGIS installed in a machine.<br>
</p>
<br>
<div>
  <table>
    <tbody>
      <tr>
        <td>
          <img alt="Schema how the system works (QGIS Live Data)" data-original-height="173" data-original-width="1053" height="106" src="https://1.bp.blogspot.com/-JaG5RhN6xJA/X19BMD_ygnI/AAAAAAAACRg/2u5J81mQquAFKSYqE6eBa_PMtrKRVuX_gCNcBGAsYHQ/w640-h106/schema%2Blive%2Bdata%2Bqgis.png" title="Schema how the system works (QGIS Live Data)" width="640">
        </td>
      </tr>
      <tr>
        <td>
          &nbsp;Figure2 . Schema how the system works<br>
        </td>
      </tr>
    </tbody>
  </table>
  
  
</div>






<p>
  Based on the schema, this tutorial consist of several sub-topics, such as:
  getting the data (send the request and process the response), Plotting the
  data on QGIS map canvas and render it within a time interval. Let's get
  started!<br>
</p>

<p><h3>Getting Air Traffic Live Data</h3></p>

<p>
  The data for this tutorial is coming from
  <a href="https://opensky-network.org/" rel="nofollow" target="_blank">OpenSky Network</a>
  which is an association that provides air traffic data around the globe. There
  are some APIs that can be used to retrieve data from OpenskyNetwork such as:
  Python API, Java API and REST API. In this tutorial we will use REST API to
  retrieve data within a specified boundary area.
</p>
<p>
  To retrieve air traffic data within an area we need to define minimum and
  maximum coordinate in geographic coordinate system. For example I want to
  fetch all planes over United States with minimum and maximum coordinate
  respectively -125.974,30.038 and -68.748,52.214. The REST API query to request
  the data anonymously will be as follow:
</p>
<p>
  <i>https://opensky-network.org/api/states/all?lamin=30.038&amp;lomin=-125.974&amp;
    <br>lamax=52.214&amp;lomax=-68.748</i><br>
</p>
<p>
  The anonymous request has resolution 10 seconds, it means we can send the
  request in every 10 seconds. On the other hand if you are a registered user,
  the resolution will be faster, about 5 seconds. To make a request as
  registered user, the username and password must be include in the query. Then
  the query will be:
</p>
<p>
  https://<span>username:password</span>@opensky-network.org/api/states/all?lamin=30.038lomin=-125.974&amp;
  lamax=52.214&amp;lomax=-68.748
</p>
<p>
  To try the query, simply copy it and paste into a browser. If you get a
  response like figure 3, means it works and we are ready to continue to the
  next step. Furthermore if you want to know in more detail about air traffic
  data response from OpenSky Network please visit
  <a href="https://opensky-network.org/apidoc/rest.html">REST API Documentation</a>.<br>
</p>

<table>
  <tbody>
    <tr>
      <td>
        <img alt="Live Air Traffic Data Response" data-original-height="452" data-original-width="793" height="364" src="https://1.bp.blogspot.com/-Y8dig7kzJZY/X2Dzni4ErgI/AAAAAAAACRs/AngxaoNxE94l0AU3j2rVDHTzH3I8FhFYgCNcBGAsYHQ/w640-h364/air-traffic-data-response.png" title="Live Air Traffic Data Response" width="640">
      </td>
    </tr>
    <tr>
      <td>
        Figure 3. Live Air Traffic Data Response<br>
      </td>
    </tr>
  </tbody>
</table>



<h3>
  Sending Request and Process The Live Data Response
</h3>
<p>
  In this step we will write Python code to request the live air traffic data
  and process the response. The complete code can be found at the end of this
  section.
</p>
<p>
  We are starting with importing some libraries namely: requests, json, csv and
  time. Then define the coordinate extent with minimum and maximum coordinate.
  Next at line 16 an output path where the response data will be stored is
  specified, so make sure to change with yours. If you are a registered OpenSky
  Network user, giver your username and also the password in
  <i>user_name</i> and <i>password</i> variable at line 19-20. The last part of
  the code is used to send the query using requests, get response in JSON format
  and save it into a CSV file. This process will be done in a loop within
  interval 10 seconds for anonymous request or 5 seconds for a registered user.
  &nbsp; <br>
</p>

<div>
  <table>
    <tbody>
      <tr>
        <td>
          <pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47</pre>
        </td>
        <td>
          <pre><span>'''</span>
<span>LIVE AIR DATA TRAFFIC REQUEST</span>
<span>by ideagora geomatics | www.geodose.com | @ideageo</span>
<span>'''</span>
<span>#IMPORTING LIBRARIES</span>
<span>import</span> <span>requests</span>
<span>import</span> <span>json</span>
<span>import</span> <span>csv</span>
<span>import</span> <span>time</span>

<span>#AREA EXTENT COORDINATE GCS WGS84</span>
<span>lon_min,lat_min</span><span>=-</span><span>125.974</span><span>,</span><span>30.038</span>
<span>lon_max,lat_max</span><span>=-</span><span>68.748</span><span>,</span><span>52.214</span>

<span>#CSV OUPUT PATH</span>
<span>csv_data</span><span>=</span><span>'/home/data.csv'</span>

<span>#REST API QUERY</span>
<span>user_name</span><span>=</span><span>''</span>
<span>password</span><span>=</span><span>''</span>
<span>url_data</span><span>=</span><span>'https://'</span><span>+</span><span>user_name</span><span>+</span><span>':'</span><span>+</span><span>password</span><span>+</span><span>'@opensky-network.org/api/states/all?'</span><span>+</span><span>'lamin='</span><span>+</span><span>str(lat_min)</span><span>+</span><span>'&amp;lomin='</span><span>+</span><span>str(lon_min)</span><span>+</span><span>'&amp;lamax='</span><span>+</span><span>str(lat_max)</span><span>+</span><span>'&amp;lomax='</span><span>+</span><span>str(lon_max)</span>
<span>col_name</span><span>=</span><span>[</span><span>'icao24'</span><span>,</span><span>'callsign'</span><span>,</span><span>'origin_country'</span><span>,</span><span>'time_position'</span><span>,</span><span>'last_contact'</span><span>,</span><span>'long'</span><span>,</span><span>'lat'</span><span>,</span><span>'baro_altitude'</span><span>,</span><span>'on_ground'</span><span>,</span><span>'velocity'</span><span>,</span>       
<span>'true_track'</span><span>,</span><span>'vertical_rate'</span><span>,</span><span>'sensors'</span><span>,</span><span>'geo_altitude'</span><span>,</span><span>'squawk'</span><span>,</span><span>'spi'</span><span>,</span><span>'position_source'</span><span>]</span>

<span>#REQUEST INTERVAL</span>
<span>if</span> <span>user_name</span> <span>!=</span><span>''</span> <span>and</span> <span>password</span> <span>!=</span><span>''</span><span>:</span>
    <span>sleep_time</span><span>=</span><span>5</span>
<span>else</span><span>:</span>
    <span>sleep_time</span><span>=</span><span>10</span>

<span>#GET DATA AND STORE INTO CSV</span>
<span>while</span> <span>col_name</span> <span>!=</span><span>''</span><span>:</span>
    <span>with</span> <span>open(csv_data,</span><span>'w'</span><span>)</span> <span>as</span> <span>csv_file:</span>
        <span>csv_writer</span><span>=</span><span>csv</span><span>.</span><span>writer(csv_file,delimiter</span><span>=</span><span>','</span><span>,quotechar</span><span>=</span><span>'"'</span><span>,quoting</span><span>=</span><span>csv</span><span>.</span><span>QUOTE_ALL)</span>
        <span>csv_writer</span><span>.</span><span>writerow(col_name)</span>
        <span>response</span><span>=</span><span>requests</span><span>.</span><span>get(url_data)</span><span>.</span><span>json()</span>
        
        <span>try</span><span>:</span>
            <span>n_response</span><span>=</span><span>len(response[</span><span>'states'</span><span>])</span>
        <span>except</span> <span>Exception</span><span>:</span>
            <span>pass</span>
        <span>else</span><span>:</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range(n_response):</span>
                <span>info</span><span>=</span><span>response[</span><span>'states'</span><span>][i]</span>
                <span>csv_writer</span><span>.</span><span>writerow(info)</span>
    <span>time</span><span>.</span><span>sleep(sleep_time)</span>
    <span>print(</span><span>'Get'</span><span>,len(response[</span><span>'states'</span><span>]),</span><span>'data'</span><span>)</span>
</pre>
        </td>
      </tr>
    </tbody>
  </table>
</div>

<p>
  Save the code with Python extension (.py) and run it from a command prompt or
  terminal. Type <i>python</i> or <i>python3</i> if you use python 3 followed by
  the file name. The code will be running as in figure 4 below.&nbsp;
</p>
<p>
  Don't close the terminal, because it will work continuously to get the latest
  air traffic data from OpenSky Network, and we will use it in QGIS. <br>
</p>
<table>
  <tbody>
    <tr>
      <td>
        <img alt="Flight Data Request" data-original-height="309" data-original-width="480" height="258" src="https://1.bp.blogspot.com/-RAkHV_x_kTE/X2Fl4gX-HsI/AAAAAAAACSA/IAMcBX7xGTMkROtvJ6GQp_FoQn2yRHMqQCNcBGAsYHQ/w400-h258/flight-request-code-running.png" title="Flight Data Request" width="400">
      </td>
    </tr>
    <tr>
      <td>
        Figure 4. Flight Data Request<br>
      </td>
    </tr>
  </tbody>
</table>

<h3>Visualize Live Air Traffic Data in QGIS</h3>
<p>
  We already get the live data streaming, now let's visualize it in QGIS with
  the following steps.
</p>
<p>
  Add the CSV data into QGIS. From <i>Data Source Manager</i>, select
  <i>Delimited Text</i> in the left menu. Then in the right side, select&nbsp;
  the <i>File Name. </i>In <i>Geometry Definition</i> section select
  <i>long</i> column for <i>X field</i> and <i>lat </i>column for
  <i>Y field</i>. Make sure to get the <i>Sample Data</i> correctly. If not try
  to change the delimiter properties in <i>File Format</i> section with
  <i>Custom delimiters</i> option.&nbsp; &nbsp; &nbsp;
</p>

<table>
  <tbody>
    <tr>
      <td>
        <img alt="Add flight data QGIS" data-original-height="601" data-original-width="856" height="450" src="https://1.bp.blogspot.com/-nTLyM8xeWeE/X2JDVOIugiI/AAAAAAAACSY/tUnwZ1Z0Gxg2-FMWbU_Db3QcWIAMH0bowCNcBGAsYHQ/w640-h450/qgis-add-flight-data.png" title="Add flight data QGIS" width="640">
      </td>
    </tr>
    <tr>
      <td>
        Figure 5. Add Air Traffic Data to QGIS<br>
      </td>
    </tr>
  </tbody>
</table>
<p>
  After pushing the <i>Add</i> button in the <i>Data Source Manager</i>, the air
  plane's position within the requested area will be plotted on QGIS map canvas.
  To make it more meaningful in a geospatial extent, add a basemap. To add a
  basemap I used
  <a href="https://www.geodose.com/2018/11/qgis3-basemap-plugin-tile-plus.html">Tile+</a>
  plugin which provides some popular basemaps. For this case I used STAMEN
  TERRAIN basemap.&nbsp; Figure 6 shows all aircraft's position over the US with
  STAMEN TERRAIN basemap.<br>
</p>
<table>
  <tbody>
    <tr>
      <td>
        <img alt="Aircraft Position in QGIS Map Canvas" data-original-height="599" data-original-width="1091" height="352" src="https://1.bp.blogspot.com/-6FDJtf32igE/X2JFxFj06TI/AAAAAAAACSs/2VvbdqRN8WA5YRMYS7hv0RueuEWFBGviACNcBGAsYHQ/w640-h352/flight-data-plot-qgis-basemap.png" title="Aircraft Position in QGIS Map Canvas" width="640">
      </td>
    </tr>
    <tr>
      <td>
        Figure 6. Aircraft Position in QGIS Map Canvas<br>
      </td>
    </tr>
  </tbody>
</table>
<p>
  So far we already get airplane position in a static way. The position of
  airplanes will not change because it doesn't fetch any updated data. Therefore
  in this last step, we will make it dynamic. The position of aircraft will be
  updated every 5 or 10 seconds. Then we will change the dot marker with
  airplane icon and also rotate it respectively with the track direction.
</p>
<p>
  Firstly let's change the dot marker into airplane icon. Right click on data
  layer and then select <i>Properties</i>. On the left menu select
  <i>Symbology</i> and chose <i>topo airport</i> icon as in figure 7.<br>
</p>

<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-LKKiEhhdoSQ/X2JKDHl_NkI/AAAAAAAACS4/pjayRfO1zGo4kYT062mVtN7zqkaFxiA0gCNcBGAsYHQ/s823/change-symbology-qgis.png"><img alt="Change Symbology" data-original-height="566" data-original-width="823" height="440" src="https://1.bp.blogspot.com/-LKKiEhhdoSQ/X2JKDHl_NkI/AAAAAAAACS4/pjayRfO1zGo4kYT062mVtN7zqkaFxiA0gCNcBGAsYHQ/w640-h440/change-symbology-qgis.png" title="Change Symbology" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 7. Change Symbology<br>
      </td>
    </tr>
  </tbody>
</table>
<p>To set rotation angle of the marker, select the menu at the right of
<i>Rotation</i> parameter then&nbsp; select <i>Field type:....</i> and then
select <i>true_track</i> column (see figure 8).&nbsp; <br>

<ins data-ad-client="ca-pub-5632482621101280" data-ad-format="fluid" data-ad-layout="in-article" data-ad-slot="7655392618"></ins></p>
<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-w2UafXoJYDg/X2JLHf8QBuI/AAAAAAAACTA/z3V9zCZTI6s9hb4iwPLifcEcTp-UXvBuwCNcBGAsYHQ/s513/select-rotation-angle.png"><img alt="Set Rotation Angle" data-original-height="294" data-original-width="513" height="229" src="https://1.bp.blogspot.com/-w2UafXoJYDg/X2JLHf8QBuI/AAAAAAAACTA/z3V9zCZTI6s9hb4iwPLifcEcTp-UXvBuwCNcBGAsYHQ/w400-h229/select-rotation-angle.png" title="Set Rotation Angle" width="400"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 8. Set Rotation Angle<br>
      </td>
    </tr>
  </tbody>
</table>
&nbsp;

<p>
  Before proceeding to the next step, click <i>Apply </i>or <i>OK </i>button.
  You should see the airplane marker and&nbsp; it rotates in flight direction
  angle as shown in figure 9.
</p>
<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-T9sCVld99NY/X2JMuwFYwWI/AAAAAAAACTM/RAB0B78Cihw7oel_TPmZmigrsxAOoAAuACNcBGAsYHQ/s1089/airplanes-marker-rotate.png"><img alt="Airplane marker with it's rotation angle" data-original-height="600" data-original-width="1089" height="352" src="https://1.bp.blogspot.com/-T9sCVld99NY/X2JMuwFYwWI/AAAAAAAACTM/RAB0B78Cihw7oel_TPmZmigrsxAOoAAuACNcBGAsYHQ/w640-h352/airplanes-marker-rotate.png" title="Airplane marker with it's rotation angle" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 9. Airplane marker with it's rotation angle<br>
      </td>
    </tr>
  </tbody>
</table>

<p>
  Finally let's update the data within a time interval. Select data layer and
  choose Properties again. On the left menu select <i>Rendering</i>. In the
  right window check <i>Refresh layer at interval (seconds)</i> option, and set
  it to 5 or 10 as in figure 10. It means the layer will refreshed every 5 or 10
  seconds. If there is any data change after refreshing is taking place, the
  position of airplanes will be updated and we get an almost realtime air
  traffic live data that visualized in QGIS as seen before in figure 1.<br>
</p>
<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-xXMt1izln4g/X2JN5Hnp72I/AAAAAAAACTY/UKczXaiKqGovSKw8gQrWjM9uuEEUW-ekQCNcBGAsYHQ/s824/refreshed-layer.png"><img alt="Set refresh layer interval time" data-original-height="616" data-original-width="824" height="478" src="https://1.bp.blogspot.com/-xXMt1izln4g/X2JN5Hnp72I/AAAAAAAACTY/UKczXaiKqGovSKw8gQrWjM9uuEEUW-ekQCNcBGAsYHQ/w640-h478/refreshed-layer.png" title="Set refresh layer interval time" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 10. Set refresh layer interval time<br>
      </td>
    </tr>
  </tbody>
</table>

<p>
  That's all this tutorial how to visualize an almost realtime live data in
  QGIS. I think&nbsp; this approach can be applied for other cases like
  visualize data from a sensor that taking measurement in a field like water level, temperature, humidity, and many more. Hope this post could
  inspire you and thanks for reading!<br>
</p>

<!--large_rectangle_336x280-->

<p><i></i>
<a href="https://www.geodose.com/search/label/Live%20Data?&amp;max-results=8" rel="tag" title="Live Data">Live Data</a>
<a href="https://www.geodose.com/search/label/QGIS?&amp;max-results=8" rel="tag" title="QGIS">QGIS</a>
<a href="https://www.geodose.com/search/label/Tutorial?&amp;max-results=8" rel="tag" title="Tutorial">Tutorial</a></p>

</div></div>]]>
            </description>
            <link>https://www.geodose.com/2020/09/realtime%20live%20data%20visualization%20qgis.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24565250</guid>
            <pubDate>Wed, 23 Sep 2020 10:23:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Is a Minimally Good Life?]]>
            </title>
            <description>
<![CDATA[
Score 92 | Comments 140 (<a href="https://news.ycombinator.com/item?id=24565154">thread link</a>) | @bertdc
<br/>
September 23, 2020 | https://psyche.co/ideas/what-is-a-minimally-good-life-and-are-you-prepared-to-live-it | <a href="https://web.archive.org/web/*/https://psyche.co/ideas/what-is-a-minimally-good-life-and-are-you-prepared-to-live-it">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><strong>As a basic minimum</strong>, what do members of a society owe to one another? How we answer this question determines what safety nets societies provide for their members, and so helps to shape the structure of the society at large. It is crucial, then, that we formulate a method in which to figure out what, at a minimum, we owe to others. The way to do that is simple: we should consider whether <em>we</em> would be content to live the lives that the least fortunate in our society actually live. We should put ourselves into each otherâ€™s shoes â€“ and then consider what each person needs to live well<em>.</em></p>
<p>People need many of the same things, by virtue of being human. Everyone must be able to meet their basic needs for things such as food, water and shelter. But thatâ€™s not all. To live at least minimally well, the good things in each personâ€™s life (such as relationships, pleasures, knowledge, appreciation, worthwhile activities) must compensate for their difficulties, pains, losses and frustrations. Everyone also needs decent opportunities and the capabilities to realise them. Or, at least, each person should get as close as possible to meeting this standard.</p>
<p>However, the differences between people also matter immensely, and our differences as people explain why it is not enough if everyone has exactly the same things. Pregnant women, for instance, need more food than those who arenâ€™t pregnant. Those who canâ€™t walk might need help getting around. And, in some cases, we have to consider cultural differences just to ensure that everyone can eat.</p>
<p>We should ask whether <em>we</em> would really be content to live each other personâ€™s life in our society. To be clear, this is different from asking each person directly what they need. The danger here is that people can be mistaken about their needs. Some get so used to poor conditions that they no longer strive to improve them. Others are so poorly off that they simply donâ€™t understand that their conditions are poor in the first place. The thought is that having some distance from each personâ€™s experience will help us see whether that person really needs all the things they think they need. We might likewise consider whether the person needs resources, opportunities, capabilities and so forth that they think they donâ€™t need, but in fact do.</p>
<p>Of course, not everyone will agree on what we all need in order to live a minimally good life. But it is my contention that free, reasonable and caring people <em>should</em>. To see why, it is important to understand what it actually is that makes people reasonable, caring and free. People are reasonable when they are appropriately impartial; they donâ€™t privilege the greater needs of some over others. People are caring when they empathise with others: understanding their circumstances, their history, their perspectives. Caring people want to promote othersâ€™ interests in proportion to their weight. And people are free when they can reason about, make and carry out plans for themselves. Free people also have decent options and bargaining power.</p>
<p>I believe that no one really deserves to be born with what they have â€“ their natural resources, institutions or tools</p>
<p><strong>Now, consider why</strong> reasonable, caring, free people â€“ who have all the relevant information â€“ will agree that everyone should have adequate resources, opportunities, capabilities and so forth to live a minimally good life. If weâ€™re appropriately impartial, weâ€™ll set for others only that standard under which weâ€™re content to live as others do. If weâ€™re caring, weâ€™ll set a standard that we believe is sufficient for others with their particular interests. If weâ€™re free and caring, and have all the relevant information, we wonâ€™t make a mistake about whether the standard is sufficient for others with those interests.</p>
<p>There is a sense in which even some of the most impoverished, oppressed and disadvantaged people can live excellent, never mind minimally good, lives. As the philosopher Dan Haybron suggests, it is often reasonable to affirm lives, even when they lack many of the things that people can justifiably aspire to as a matter of basic rights<em>.</em> Still, I am interested here in the latter sense of what makes lives minimally good â€“ I am concerned with what people can justifiably aspire to as a matter of basic right.</p>
<p>My proposal is this: in order to figure out what this kind of minimally good life requires, we should attempt to avail ourselves of anotherâ€™s perspective on their own life, and consider what weâ€™d need to live such a life. And when we are reasonable, caring and free, weâ€™ll set a standard that is sufficient for others given their particular interests. Moreover, if we put ourselves in othersâ€™ shoes in trying to figure out what a minimally good life requires, we wonâ€™t set the threshold too high. The question is not whether a fortunate individual would be willing to trade places with someone who is able to live only a minimally good life. Rather, the question is only whether the free, reasonable and caring person would be content if they had to live as that person does.</p>
<p>Since people have different backgrounds, goals, tools and resources, one might argue that different standards are appropriate for those who grow up in different circumstances (whether it be the cornfields of Nebraska or the slums of New York City). Furthermore, itâ€™s commonly held that people deserve the advantages they have: since everyone has grown up in the â€˜real worldâ€™, they should know what to expect for their efforts. I believe that no one really deserves to be born with what they have â€“ their natural resources, institutions or tools. Everyone will try hard enough to live minimally well if they can. So, while some might need more than others, we should help everyone live at least minimally well. This doesnâ€™t mean we have to give everyone exactly the same things â€“ still, if we are, or consider ourselves to be, reasonable, caring and free, then we must help everyone secure the things they need to live minimally well.</p></div></div></div>]]>
            </description>
            <link>https://psyche.co/ideas/what-is-a-minimally-good-life-and-are-you-prepared-to-live-it</link>
            <guid isPermaLink="false">hacker-news-small-sites-24565154</guid>
            <pubDate>Wed, 23 Sep 2020 10:08:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Haskell's Children]]>
            </title>
            <description>
<![CDATA[
Score 170 | Comments 180 (<a href="https://news.ycombinator.com/item?id=24565019">thread link</a>) | @xiaodai
<br/>
September 23, 2020 | https://owenlynch.org/posts/2020-09-16-haskells-children/ | <a href="https://web.archive.org/web/*/https://owenlynch.org/posts/2020-09-16-haskells-children/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
          
          <p>
    Posted on September 16, 2020
    
</p>

<p>If I were to travel back in time 4 years ago, and tell my old self that Haskell was starting to lose its shine, I wouldn’t believe it. I grew up on Haskell, my appetite for category theory was whetted by Haskell, my biggest programming projects have been in Haskell, and my dream job was to work at a company that used Haskell.</p>
<p>But now, I find myself simply not as excited about Haskell as I used to be. What changed?</p>
<p>I think there are a couple things. I think one primary factor is that the kind of programming that Haskell really excells in; i.e.&nbsp;creating abstract, correct interfaces for things, is just not a type of programming that’s interesting to me anymore. When I wanted to work on software as a career, a language that allowed incredible facilities in not repeating yourself was very useful. Types that ensure correctness of data interchange, or lenses that allow access to complicated data structures are all very well for implementing, say, a compiler, or complicated business logic in a web backend. However, my interests in software are now primarily as a scientific/mathematical tool. Numerical algorithms can be done in Haskell, but they don’t really gain much benefit from the type system, and they also don’t have great library support.</p>
<p>No doubt Haskell could be made into the kind of language to use for the problems that I am interested in, but given the choice between working on the problems that interest me, and working on infrastructure for the problems that interest me, I would rather work on the problems that interest me. The general feeling that I have is that Haskell is a great tool for a software engineer, but I don’t want to be a software engineer, I want to be a mathematician that sometimes uses computers.</p>
<p>But there is another reason too. While I think that Haskell is still a great language, it is close to 30 years old at this point. It manages to stay fresh and relevant with an ever-growing list of extensions, and constantly changing best practices and libraries (which is itself a problem…), but it would be very sad if we as a society of programmers had failed to surpass it in any respect with any of the programming languages that have had the advantage of starting from a clean slate. In this post, I want to talk about these successor languages, and what I think about them.</p>
<h2 id="rust">Rust</h2>
<p>Surprisingly, one of Haskell’s great strengths is as a systems language. It manages to be much faster than most dynamic languages, while allowing a much higher-level interface than traditional systems languages like C (obviously). One great example of a “systems” program written in Haskell is git-annex. It is a git addition that adds tracking of large files, and was my primary backup system for a long time (I eventually decided that I didn’t need the additional power from it, and was better served by a more seamless solution).</p>
<p>However, in 2020, the premier system’s language is surely Rust. It would be unfair to compare the performance of Rust and Haskell, because Haskell is optimized for things other than performance. That being said, Rust is <em>faster</em> and <em>lower-latency</em> than Haskell, both of which are important. However, it also has a great type system, unlike C or C++ (we don’t talk about Go…). The type system in Rust is obviously very influenced by the type system of Haskell, but they also implemented “ownership” which allows for the killer feature garbage-collection free automatic memory management.</p>
<p>When I first started using Rust, I really missed monads. But here’s the thing. Having used lots of monads in Haskell, and read lots of blog posts about monads, I’ve learned that in systems contexts, it’s often best to just have a simple monad stack that just consists of Reader + IO (and Maybe’s and Option’s sprinkled about occasionally). Huge monad transformer stacks often raise more problems than they solve. But Reader + IO is <em>essentially</em> the “default monad stack” of Rust.</p>
<p>Rust also has some other killer features, like the ability to compile to webassembly (yes there is ghcjs, but, really, do you want to use ghcjs?) It also from the beginning was targetted towards industry, and consequentially has a much more vibrant ecosystem.</p>
<p>This all being said, I think it is worth looking at the features that are prominent in Haskell that ended up going to Rust</p>
<ul>
<li>Typeclasses (in Rust they are Traits)</li>
<li>Sum types (you may take this for granted, but a lot of languages don’t have them….)</li>
<li>Pervasive pattern matching</li>
<li>Hindley-Mindler type inference (automatic type inference for variables)</li>
<li>Pervasiveness of things being <em>expressions</em> rather than statements</li>
<li>Parametric Polymorphism</li>
<li>The feeling that once your program compiles, it will run</li>
</ul>
<p>I think that we should recognize Rust for what it is, a child of Haskell and the Haskell community, and like all good parents, we should want it to do better than the previous generation. In as much as Haskell is the ideas that form Haskell, the success of Rust is the success of Haskell.</p>
<h2 id="idris">Idris</h2>
<p>OK, mainstream programming languages are great, but sometimes you just want to make the perfect type-based interface to your stuff and show the imperative scrubs what a wiz-kid you are. Or alternatively, sometimes you really care that your software is correct. Or you want to concretize a new category-theory inspired design for a part of a compiler. Nowadays, the language for that is not Haskell, it is Idris.</p>
<p>There are about six different ways to sort of have dependent types in Haskell (types that depend on values, like a length-<span>n</span>) array. I don’t really fully understand any of them, and it is totally unclear to me how they work together. Presumably, there are blogs which outline the One True Way, but… it’s tough. In Idris, it just seems perfectly natural to use dependent types, like, why wouldn’t you able to have a type parameter which was a value? In many ways other than dependent types, Idris is a much cleaner language than Haskell too. And with Idris2, it has support for <em>linear types</em>, which allow mutability in a functional context via guarantees that nobody is going to try and use the old value. If I want to play around with a cool type system in a language that can also actually do things with the real world (i.e., unlike Agda or Coq), I would go to Idris rather than Haskell.</p>
<p>But Idris is undeniably Haskell’s child. The first version was written in Haskell (it is now self-hosting). They are similar in more ways than it is worth counting. Enough said.</p>
<h2 id="julia">Julia</h2>
<p>Unlike the first two, Julia doesn’t really muscle into Haskell’s territory. Scientific computing was never really Haskell’s forte, despite there being some very cool libraries written in it, like <code>ad</code> for autodifferentiation, or various array-handling packages that automatically fused consecutive array operations.</p>
<p>Also, Julia is a dynamically typed language. How could a filthy dynamically typed language ever claim to be Haskell’s child??</p>
<p>Well, for one it steals some of those cool libraries, and makes them much better! Flux is a neural networks library which essentially is just autodifferentiation + some nice utilities, and it is already competitive in my mind with TensorFlow. Julia also has StaticArrays, which integrates the size of the array into the type, and Julia has some neat fusion abilities too for making array operations really fast.</p>
<p>But wait, you ask, how can it do this if it’s not a statically typed language? Well, Julia is not your average dynamically typed language. It actually has a very interesting type system, a full discussion of which is beyond the scope of this post, and the focus on types as the unit of programming is (somewhat?) similar to Haskell (now I’m stretching it a little though).</p>
<p>The real reason I include Julia, however, is because for me personally, it has replaced Haskell as the place to do category theory. This is because of a shift of viewpoint: rather than providing a type system into which category theory can be embedded in to guide typical software engineering tasks, Julia provides a system in which <em>computations</em> in category theory can be carried out in an efficient way. Specifically, I’m talking about <a href="https://github.com/olynch/Catlab.jl">Catlab.jl</a>. A discussion of Catlab.jl is also beyond the scope of this post, but I encourage you to check it out.</p>
<p>Therefore, I count Julia as a child of Haskell (or maybe, I count Catlab.jl as a child of Haskell) because the idea of organizing computation with category theory would not exist in the same way if it weren’t for Haskell.</p>
<h2 id="conclusion">Conclusion</h2>
<p>If I could talk to the Haskell-obsessed teenager that was me four years ago, I would tell him to keep his mind open. Haskell is still great for a lot of things (compilers come to mind), but if Haskell couldn’t inspire superior successors, there wouldn’t be worthwhile ideas in Haskell. There are those on the internet who are talking about how Haskell is dying, and they may or may not be wrong. Stephen Diehl, one of my main Haskell idols, is distancing himself from the Haskell community because of Haskell’s use as intellectual eye-candy on scam cryptocurrencies, and I think that there may be a tipping point where Haskell loses the zeitgeist of being exciting, and because it never had much of a foothold to begin with in industry, slips into irrelevance. But Haskell will always live on; it had a huge impact on many programmers and many languages disproportionate to its actual use, and it will always have a special place in my heart.</p>





        </div>
      </div></div>]]>
            </description>
            <link>https://owenlynch.org/posts/2020-09-16-haskells-children/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24565019</guid>
            <pubDate>Wed, 23 Sep 2020 09:47:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Studio Ghibli releases 400 free-to-use images]]>
            </title>
            <description>
<![CDATA[
Score 627 | Comments 119 (<a href="https://news.ycombinator.com/item?id=24564775">thread link</a>) | @DyslexicAtheist
<br/>
September 23, 2020 | http://www.ghibli.jp/info/013344/ | <a href="https://web.archive.org/web/*/http://www.ghibli.jp/info/013344/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>今月からスタジオジブリ全作品の場面写真を順次提供することになりました。今月は、新しい作品を中心に 8作品、合計400枚提供します。</p>
<p>常識の範囲でご自由にお使いください。</p>
</div></div>]]>
            </description>
            <link>http://www.ghibli.jp/info/013344/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24564775</guid>
            <pubDate>Wed, 23 Sep 2020 09:12:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Frugality Is Non-Linear (2019)]]>
            </title>
            <description>
<![CDATA[
Score 105 | Comments 156 (<a href="https://news.ycombinator.com/item?id=24564669">thread link</a>) | @luu
<br/>
September 23, 2020 | https://scattered-thoughts.net/writing/frugality-is-non-linear/ | <a href="https://web.archive.org/web/*/https://scattered-thoughts.net/writing/frugality-is-non-linear/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>Most people have a mental model of budgeting which is roughly linear. If you spend half as much money, your money will last twice as long. As you approach zero spending, your runway goes up to infinity.</p>
<p>In this model, the space of options looks like this:</p>
<div>
<p>[I am an interactive graph made of javascript!]</p>
</div>
<p><label for="growth">Return on investment =&nbsp;</label>
  
</p>
<p>This model is wrong.</p>
<p>It's wrong because your savings grow over time. If you change the return rate above to 5%, you can see that someone who has 500k in savings and spends 75k per year has a runway of 7 years. At 50k per year that extends to 13 years. But if they can cut their spending to 25k per year they have a runway of 62 years!</p>
<p>Effectively, including growth in the model moves the asymptote to the right - your runway goes up to infinity as your spending approaches some percentage of your total savings, rather than as it approaches zero.</p>
<p>So halving your expenses can much more than double your runway. Or to put it another way - halving your expenses can much more than halve the number of years of your life you need to spend working.</p>
<hr>
<p>I picked the examples above with a particular motive in mind. According to <a href="https://danluu.com/startup-tradeoffs/">Dan Luu's conservative estimates</a> a fresh grad at a big tech company can safely earn ~$500k post-tax in 5 years. The US median income post-tax is ~$25k, and investing in an index fund has historically earned ~5% average returns in the long run. So as a tech worker, if you can manage to live as 'frugally' as the average American, you can <a href="https://networthify.com/calculator/earlyretirement?income=120000&amp;initialBalance=0&amp;expenses=25000&amp;annualPct=5&amp;withdrawalRate=4">comfortably retire</a> before 30.</p>
<p>In the tech industry we have some very loud voices arguing that if you desire autonomy or leverage, the best path forwards is to start a VC-backed startup. But reducing spending and saving towards early retirement has some compelling advantages:</p>
<ul>
<li>It's much more reliable - most startups fail, but most people who work at a large tech company make sufficient money to be able to retire early.</li>
<li>Financial independence is a huge safety net - reducing stress and lowering the risk of later projects. If you still want to run a startup, doing it from a position of infinite personal runway will be a lot less stressful.</li>
<li>By separating the means of earning money from the freedom you are pursuing, it enables pursuing goals in that under-served intersection of valuable but not profitable. Whether that's supporting free software, producing art or home-schooling your children, trying to fit such activities into a profitable enterprise inevitably produces uncomfortable compromises which can be avoided by removing the need to earn money.</li>
</ul>
<p>The last point is particularly compelling if you have strong ethical/political/economic beliefs that would benefit from the leverage of financial independence.</p>
<hr>
<h3 id="faq">FAQ</h3>
<p><strong>What about inflation?</strong> Inflation is essentially negative growth, so you can subtract it from the return rate and then keep the rest of the calculations in today-dollars. 5% seems to be a reasonable estimate of average inflation-adjusted returns on stocks based on recent decades, but see below for better models.</p>
<p><strong>What about volatility?</strong> I used a fixed average return rate above, which doesn't tell you odds of running out of money early due to a string of bad years. <a href="https://retirementplans.vanguard.com/VGApp/pe/pubeducation/calculators/RetirementNestEggCalc.jsf">But simulations based on historical data</a> produce similar results to those above, and <a href="https://www.kitces.com/wp-content/uploads/2014/11/Kitces-Report-March-2012-20-Years-Of-Safe-Withdrawal-Rate-Research.pdf">retirement planning literature</a> tends to put the asymptote at around 4-5% which is consistent with the numbers above. You should definitely use a more detailed model than this if you are seriously considering this path, but I think the simple model accurately conveys the underlying intuition - that the returns to reducing spending are non-linear.</p>
<p><strong>What about crashes?</strong> The simulation linked above uses data that covers existing crashes, including the Great Depression. But in the event that they are overly optimistic, I think there is a strong argument that having large savings and cheap habits are as useful for weathering a crash as having a filled-in employment history. Especially if you used the additional free time to build useful non-tech skills or strong communities.</p>
<p><strong>What about other countries?</strong> Dan Luu's article suggests that similar salaries are available in many major hubs. I've built a reasonably detailed model for my own situation in the UK and arrived at similar numbers. (Salaries are lower, unless you can land a remote job, but free healthcare and lower cost of living make up a lot of the difference.) It's worth at least running the numbers for your own country, just so you know what your options are.</p>
<p><strong>Hasn't the <a href="https://en.wikipedia.org/wiki/FIRE_movement">FIRE community</a> already said all of this?</strong> Yes, but I very rarely see it discussed in tech circles, so it seems worth repeating. Also I haven't seen the calculation in terms of runway before, and the graph above improved my intuition on the subject.</p>



</article></div>]]>
            </description>
            <link>https://scattered-thoughts.net/writing/frugality-is-non-linear/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24564669</guid>
            <pubDate>Wed, 23 Sep 2020 08:57:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Drive on Sand Without Getting Stuck (2017)]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 19 (<a href="https://news.ycombinator.com/item?id=24564542">thread link</a>) | @luu
<br/>
September 23, 2020 | https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach | <a href="https://web.archive.org/web/*/https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>Do you have the pleasure to live or travel in a sandy location? Tropical beach, rolling sand hills or maybe it’s the desert for work. No matter where your sandy destination is be alert of the challenges of driving in sand. Too many people join the ‘digging club’ and it’s one club you’d rather avoid.</p><p>Please note this is general Sand not Desert Driving, although many of the basic principles are the same, Desert Driving is a “long” discussion in itself.</p><p><img src="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/58e720ad6dba09dcc4206c19856bec9f/SandOffRoading_640.JPG" width="640" height="532" srcset="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/58e720ad6dba09dcc4206c19856bec9f/SandOffRoading_360.JPG 360w, https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/58e720ad6dba09dcc4206c19856bec9f/SandOffRoading_640.JPG 640w" sizes="(min-width: 768px) 720px, 100vw" alt="How to Drive Through Sand without Getting Stuck"></p><p><em>This content was developed by and owned by Paul Sinkinson, Xplorability owner. Paul is a 4wd Defensive Driver Training Consultant/Trainer and Programme Developer.</em></p><h2>Basic Driving in Sand Tips</h2><ul><li>A variety of conditions may co-exist - Learn to recognise surface conditions</li><li>Beware of “wet sand”. These areas can seem bottomless and usually require assistance from other vehicles with a winch to extricate</li><li>If unsure carry out reconnaissance on foot</li><li>Engage high ratio 4wd for long beach runs on hard sand. Engage low ratio 4wd for soft sand and dunes to avoid overheating transmission</li><li>Avoid sharp turns and wheel spin</li><li>Utilise speed for controlled momentum</li><li>Keep gear changes to a minimum – Normally choose a gear and stay in it to avoid baulking</li><li>Low tyre pressures are highly recommended</li><li>If it becomes necessary to stop on soft sand, try to choose an area that allows a down hill restart</li><li>To get re-started on flat, soft, dry sand. Reverse 1-2 metres and form hard sand ramps so as to get a good starting speed before hitting soft sand again</li><li>To ascend a sand hill, utilise controlled momentum. If you fail to ascend, back down the same wheel tracks far enough to allow a faster approach up the same wheel tracks</li><li>To descend keep vertical to the sand hill/descent and avoid brakes, accelerate gently if necessary to aid descent.</li></ul><h2>There is Nothing Quite Like Driving on Sand</h2><p>Most off-roaders may never come across sand unless they happen to make it to coastal regions and hit the beach. However, if they do start to undertake longer distance adventure treks, they may at some time
make it to the more arid regions and the odd few may even make it to the “Real” Desert. Perhaps like myself, when I first flew in on a light aircraft to a desert strip in the deep Sahara to run driver training, their answer to the question from colleagues as to what they thought of the place will be the same as mine. <strong>“I think I’m going to need a bigger bucket and spade!”</strong></p><h3>Beach Sand</h3><p>Beach Sand is totally different to the sand you find in the desert. Much of it tends to be much coarser in grain texture and therefore can at time be more forgiving that the desert sand that runs like water. Of course, being close to the sea it can also be damp, wet, extremely wet or waterlogged. It can also be dry, gravelly and have
the odd rock thrown in for good measure. Away from the water’s edge you will often find dunes, some with vegetation, some without and in certain areas around the World you will find larger dunes, similar to the ones you find in the desert. While those dunes look similar and do have some of the same features they are very different. We’ll come to that later but only touch on them briefly as “real” desert driving needs lots of space for discussion on techniques.</p><p>So, we’ve hit the beach, some great scenery, lots of fresh air, all your mates are with you looking at those long, sometimes wide strips of sand with the waves lapping at the edge. This is usually where common sense and safety goes out of the window! Competitive, idiotic behavior, spurred on usually by male testosterone reacting with the ozone fresh sea breeze leads to frantic and erratic driving at high speed along the sand and often in and out of the water. <strong>Beaches are great BUT they are not playgrounds with your truck. In a 4wd Vehicle
they can be one of the most dangerous places you will ever drive and can be like playing Russian Roulette, with a Round in every chamber.</strong> The normal <a href="https://www.offroaddiscovery.com/off-road-driver-training/hazard-identification-safety-environment">Hazard Identifications</a> when off-roading apply, so read the earlier article on this on the site.</p><h3>Three Common Sense Driving Tips for Beaches</h3><ol><li><p><strong>Keep out of the Sea</strong>: Salt Water is not good for 4wd vehicle chassis and body components. Driving in the Sea as many will do despite this article, will throw salt water everywhere and it will get into the chassis, it will in time cause rust erosion and pressure washing after may nod get rid of it. Vehicle electrics
and water do not go well together at any time. Hot radiator fins are thin, if salt water passes through the fins the heat dries it and the salt deposit remains to eat its way into the core. I’ve seen many a vehicle being taken home on a recovery truck from the beach due to water getting into badly position ECU and ignition units in the engine bay. It may look fantastic, cause lots of spray and a splash – BUT KEEP OUT OF THE SALT WATER!</p></li><li><p>Ok, hopefully you’ve got the message on point one and you’re just going to drive on the beach sand. It looks nice and flat; it may even look fairly dry. However, it’s a beach, normally the tide comes in and out twice a day that means, although it may look dry on the surface because of all that “Sunshine” in some Countries, it could well be soggy under the surface. You can <strong>drive it in either high or low range but KEEP THE SPEED DOWN</strong>.</p></li><li><p>As mentioned, it can be soggy below the surface or it can also be dry with patches of deeper sand. <strong>Keep driving in straight lines on it and NEVER turn sharply</strong> as if a front wheel happens to dig in the softer areas it will immediately act as a brake, weight will transfer on to it and you’ll become a statistic in the rollover records. These beach rollovers happen every day around the globe. IF you are going to turn, firstly slow down
and secondly turn in a wide arc. It doesn’t take much for a loaded 4wd, especially if you have equipment high up on a roof rack, for the dynamics to transfer weight. Even experienced and regular beach drivers forget this simple message. I’ve been on beaches around the world and viewed many who thought they were “experts,” being scraped up by the medics or recovery vehicles. Don’t join their gang!</p></li></ol><p><strong>NOTE: In an effort to try and keep you SAFE watch some videos</strong>
There are plenty of good and instructional videos covering
Sand on the Internet - (YouTube etc.) One of the best is <strong>“Guide to Off-Roading – Driving on Sand –
by Land Rover Experience</strong>. There are also lots of videos that highlight the art of BAD
Driving on both Beach and Desert Sand. They cover the accidents! Watch those ones several times and then look around you at the kids and the rest of your family and consider the implications should you drive in Sand (or anywhere else for that matter) like some of the idiots involved in these incidents.
Keep everyone’s Seat Belts on – Don’t let the kids hang out of the Sunroof or Side Windows. If you see some of the more serious rollover accidents you will understand why.</p><p><iframe width="640" height="360" src="https://www.youtube.com/embed/0-1__frSTno" frameborder="0" allowfullscreen=""></iframe></p><h3>Driving on Sand</h3><p>It is reasonable to assume that you can drive on firm sand if you take notice of the previous three tips but there will come a time when the surface will not be firm and supple sand is a different thing altogether.
Once you start driving in the softer sand, traction can be at a premium, any increase in speed may be and usually is, difficult if not impossible, so to make headway you need to maintain momentum.
Should you lose that, it is unlikely you will regain it and you will likely become bogged down.</p><p><em>Once you lose traction in sand and have wheel-spin, abort immediately or you
will just dig yourself in further. Engage reverse gear and “Gently” try to drive out
along your tracks.</em></p><p><img src="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/f56ccdf56a9223c94760e725fd691e55/4x4inSand_640.png" width="640" height="536" srcset="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/f56ccdf56a9223c94760e725fd691e55/4x4inSand_360.png 360w, https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/f56ccdf56a9223c94760e725fd691e55/4x4inSand_640.png 640w" sizes="(min-width: 768px) 720px, 100vw" alt="Tips for Driving on Sand and not getting Stuck"></p><h3>Tyre Pressures</h3><p>Assuming you are going to be on the beach or coastal sand for any length of time rather than just sticking your nose in for a few minutes to take in the sea air, it is advisable to reduce your tyre pressures.
The reason for this is that it provides what is termed as a better footprint reducing the ground pressure and allowing better “flotation” which in turn improves traction. (Think of the “Snow Shoe” effect on
Snow)</p><p>Most people think that reducing the tyre pressure makes the contact with the ground wider because they see the tyre “bulge” out and become fatter. In reality, this is not the case, the tyre footprint actually
becomes longer although there may be a slight increase in width. If you have ever seen a vehicle with a punctured tyre you will remember it was flat at the bottom along the ground for maybe 18 to
20 inches whereas when it was inflated, only 8 inches were in contact with the ground. When it comes to low ground pressure, “Size Matters.” With lower ground pressure there is less strain on the
vehicles steering components as well as the power unit and transmission as they don’t have to work as hard.
Before you consider lowering the pressures you must of course ensure that you have a suitable pump to re-inflate them when you come back to the normal roads and tracks. Assuming you have the
pump you can now reduce the pressure but to what level? Often you have the pump but no pressure gauge. Oops! I forgot to pack it in the truck. Now what do I do?</p><p><strong>For general sand driving a tyre pressure of circa 15 to 16 psi is the norm</strong>. If you don’t have a gauge, you can use the old explorer’s trick of using a stick or a small rock placed an inch away from the edge of
the tyre sidewall. On the average 4wd tyre of say 235x85x16, if you now let the air out of the tyre valve until the sidewall touches the stick on each tyre, you should have all the tyres down to the same level
and roughly with a suitable amount of deflation. You can use the same method the other way around when you re-inflate so you have them back to the same level for normal use until you can check and adjust with a gauge.</p><p>You must remember, now that you have the lower pressure and perhaps with the bulging side-walls, that the tyres are more susceptible to wear or damage. You need to balance the risk when lowering the pressure between the increased traction it delivers and the possible tyre damage and wear. The correct balance will certainly make the …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach">https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach</a></em></p>]]>
            </description>
            <link>https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach</link>
            <guid isPermaLink="false">hacker-news-small-sites-24564542</guid>
            <pubDate>Wed, 23 Sep 2020 08:38:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Firefox usage is down despite Mozilla's top exec pay going up]]>
            </title>
            <description>
<![CDATA[
Score 1568 | Comments 1211 (<a href="https://news.ycombinator.com/item?id=24563698">thread link</a>) | @todsacerdoti
<br/>
September 22, 2020 | http://calpaterson.com/mozilla.html | <a href="https://web.archive.org/web/*/http://calpaterson.com/mozilla.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <article>
            
            <p>September 2020</p>
            <p id="article-description">Mozilla is in an absolute state: high
            overheads, falling usage of Firefox, questionable sources of revenue and
            now making big cuts to engineering as their income falls.</p>
            <hr>
            <figure>
                <img src="http://calpaterson.com/assets/mozilla-boss-pay.svg" alt="a graph showing that executive pay has grown fast while Firefox's market share has fallen">
                <figcaption>
                    Mozilla's top exec pay has gone up hugely even as usage has
                    crashed.
                </figcaption>
            </figure>
            <p>Mozilla recently announced that they would be dismissing 250 people.
            That's a quarter of their workforce so there are some deep cuts to their
            work too. The victims include: the MDN docs (those are the web standards
            docs everyone likes better than w3schools), the Rust compiler and even some
            cuts to Firefox development. Like most people I want to see Mozilla do well
            but those three projects comprise pretty much what I think of as the whole
            point of Mozilla, so this news is a big let down.</p>
            <p>The stated reason for the cuts is falling income. Mozilla largely relies
            on "royalties" for funding. In return for payment, Mozilla allows big
            technology companies to choose the default search engine in Firefox - the
            technology companies are ultimately paying to increase the number of
            searches Firefox users make with them. Mozilla haven't been particularly
            transparent about why these royalties are being reduced, except to blame
            the coronavirus.</p>
            <p>I'm sure the coronavirus is not a great help but I suspect the bigger
            problem is that Firefox's market share is now a tiny fraction of its
            previous size and so the royalties will be smaller too - fewer users, so
            fewer searches and therefore less money for Mozilla.</p>
            <p>The real problem is not the royalty cuts, though. Mozilla has already
            received more than enough money to set themselves up for financial
            independence. Mozilla received up to half a billion dollars a year (each
            year!) for many years. The <em>real problem</em> is that Mozilla didn't use
            that money to achieve financial independence and instead just spent it each
            year, doing the organisational equivalent of living hand-to-mouth.</p>
            <p>Despite their slightly contrived legal structure as a non-profit that
            owns a for-profit, Mozilla are an NGO just like any other. In this article
            I want to apply the traditional measures that are applied to other NGOs to
            Mozilla in order to show what's wrong.</p>
            <p>These three measures are: overheads, ethics and results.</p>
            <h2>Overheads</h2>
            <p>One of the most popular and most intuitive ways to evaluate an NGO is to
            judge how much of their spending is on their programme of works (or
            "mission") and how much is on other things, like administration and
            fundraising. If you give money to a charity for feeding people in the third
            world you hope that most of the money you give them goes on food - and not,
            for example, on company cars for head office staff.</p>
            <p>Mozilla looks bad when considered in this light. Fully 30% of all
            expenditure goes on administration. Charity Navigator, an organisation that
            measures NGO effectiveness, would give them <a href="https://www.charitynavigator.org/index.cfm?bay=content.view&amp;cpid=48#PerformanceMetricTwo">
            zero out of ten</a> on the relevant metric. For context, to achieve 5/10 on
            that measure Mozilla admin would need to be under 25% of spending and, for
            10/10, under 15%.</p>
            <p>Senior executives have also done very well for themselves. Mitchell
            Baker, Mozilla's top executive, was paid $2.4m in 2018, a sum I personally
            think of as instant inter-generational wealth. Payments to Baker have more
            than doubled in the last five years.</p>
            <p>As far as I can find, there is no UK-based NGO whose top executive makes
            more than £1m ($1.3m) a year. The UK certainly has its fair share of big
            international NGOs - many much bigger and more significant than
            Mozilla.</p>
            <p>I'm aware that <a href="https://concepts.effectivealtruism.org/concepts/relationship-between-overheads-and-effectiveness/">
            some people dislike overheads as a measure</a> and argue that it's possible
            for administration spending to increase effectiveness. I think it's hard to
            argue that Mozilla's overheads are correlated with any improvement in
            effectiveness.</p>
            <h2>Ethics</h2>
            <p>Mozilla now thinks of itself less as a custodian of the old Netscape
            suite and more as a 'privacy NGO'. One slogan inside Mozilla is: "Beyond
            the Browser".</p>
            <p>Regardless of how they view themselves, most of their income comes from
            helping to direct traffic to Google by making that search engine the
            default in Firefox. Google make money off that traffic via a big targeted
            advertising system that tracks people across the web and largely without
            their consent. Indeed, one of the reasons this income is falling is because
            as Firefox's usage falls less traffic is being directed Google's way and so
            Google will pay less.</p>
            <p>There is, as yet, no outbreak of agreement among the moral philosophers
            as to a universal code of ethics. However I think most people would
            recognise hypocrisy in Mozilla's relationship with Google. Beyond the
            ethical problems, the relationship certainly seems to create conflicts of
            interest. Anyone would think that a privacy NGO would build anti-tracking
            countermeasures into their browser right from the start. In fact, this was
            only added relatively recently (<a href="https://blog.mozilla.org/blog/2019/06/04/firefox-now-available-with-enhanced-tracking-protection-by-default/">in
            2019</a>), after both Apple (<a href="https://webkit.org/blog/7675/intelligent-tracking-prevention/">in
            2017</a>) and Brave (since release) paved the way. It certainly seems like
            Mozilla's status as a Google vassal has played a role in the absence of
            anti-tracking features in Firefox for so long.</p>
            <p>Another ethical issue is Mozilla's big new initiative to <a href="https://vpn.mozilla.org/">move into VPNs</a>. This doesn't make a lot of
            sense from a privacy point of view. Broadly speaking: VPNs are not a useful
            privacy tool for people browsing the web. A VPN lets you access the
            internet through a proxy - so your requests superficially appear to come
            from somewhere other than they really do. This does nothing to address the
            main privacy problem for web users: that they are being passively tracked
            and de-anonymised on a massive scale by the baddies at Google and
            elsewhere. This tracking happens regardless of IP address.</p>
            <p>When I tested Firefox through <a href="https://vpn.mozilla.org/">Mozilla
            VPN</a> (a rebrand of <a href="https://mullvad.net/">Mullvad VPN</a>) I
            found that I could be de-anonymised by browser fingerprinting - already a
            fairly widespread technique by which various elements of your browser are
            examined to create a "fingerprint" which can then be used to re-identify
            you later. Firefox does not include as many countermeasures against this as
            some other browsers (<strong>this is a correction</strong> - I previously
            said Firefox contained none but it's been pointed out to me that <a href="https://blog.mozilla.org/security/2020/01/07/firefox-72-fingerprinting/">since
            earlier this year</a> it does block some kinds of fingerprinting).</p>
            <figure>
                <img src="http://calpaterson.com/assets/panopticlick-firefox.png" alt="firefox's results on panopticlick - my browser has a unique fingerprint">
                <figcaption>
                    Even when using Mozilla's "secure and private" VPN, Firefox is
                    trackable by browser fingerprinting, as demonstrated by the
                    <a href="https://panopticlick.eff.org/">EFF's Panopticlick
                    tool</a>. Other browsers use randomised fingerprints as a
                    countermeasure against this tracking.
                </figcaption>
            </figure>
            <p>Another worry is that many of these privacy focused VPN services have a
            nasty habit of turning out to keep copious logs on user behaviour. A few
            months ago several "no log" VPN services inadvertently released terabytes
            of private user data that they had promised not to collect <a href="https://www.vpnmentor.com/blog/report-free-vpns-leak/">in a massive
            breach</a>. VPN services are in a great position to eavesdrop - and even if
            they promise not to, your only option is to take them at their word.</p>
            <h2>Results</h2>
            <p>I've discussed the Mozilla chair's impressive pay: $2.4m/year. Surely
            such impressive pay is justified by the equally impressive results Mozilla
            has achieved? Sadly on almost every measure of results both quantitative
            and qualitative, Mozilla is a dog.</p>
            <p>Firefox is now so niche it is in danger of garnering a cult following:
            it has just 4% market share, down from 30% a decade ago. Mobile browsing
            numbers are bleak: Firefox barely exists on phones, with a market share of
            less than half a percent. This is baffling given that mobile Firefox has a
            rare feature for a mobile browser: it's able to install extensions and so
            can block ads.</p>
            <p>Yet despite the problems within their core business, Mozilla, instead of
            retrenching, has diversified rapidly. In recent years Mozilla has
            created:</p>
            <ul>
                <li>a mobile app for making websites</li>
                <li>a federated identity system</li>
                <li>a large file transfer service</li>
                <li>a password manager</li>
                <li>an internet-of-things framework/standard</li>
                <li>an email relay service</li>
                <li>a completely new phone operating system</li>
                <li>an AI division (but of course)</li>
                <li>and spent $25 million buying the reading list management startup,
                Pocket</li>
            </ul>
            <p>Many of the above are now abandoned.</p>
            <p>Sadly <a href="https://www.mozilla.org/en-US/foundation/annualreport/2018/">Mozilla's
            annual report</a> doesn't break down expenses on a per-project basis so
            it's impossible to know how much of the spending that <em>is</em> on
            Mozilla's programme is being spent on Firefox and how …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://calpaterson.com/mozilla.html">http://calpaterson.com/mozilla.html</a></em></p>]]>
            </description>
            <link>http://calpaterson.com/mozilla.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24563698</guid>
            <pubDate>Wed, 23 Sep 2020 06:38:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A look at /e/OS on the FairPhone 3 – a FOSS OS for phones [video]]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24563576">thread link</a>) | @indidea
<br/>
September 22, 2020 | https://share.tube/videos/watch/ef3e5eec-27aa-45c8-b87b-64a57fc6e176 | <a href="https://web.archive.org/web/*/https://share.tube/videos/watch/ef3e5eec-27aa-45c8-b87b-64a57fc6e176">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://share.tube/videos/watch/ef3e5eec-27aa-45c8-b87b-64a57fc6e176</link>
            <guid isPermaLink="false">hacker-news-small-sites-24563576</guid>
            <pubDate>Wed, 23 Sep 2020 06:17:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Visualizing Gzip Compression with Python]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24563372">thread link</a>) | @brenns10
<br/>
September 22, 2020 | https://brennan.io/2020/09/22/compression-curves/ | <a href="https://web.archive.org/web/*/https://brennan.io/2020/09/22/compression-curves/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan • 22 September 2020</em></p><p>Not that long ago, I found myself wanting to understand gzip. I didn’t necessarily want to learn to implement the algorithm, but rather I just wanted to understand how it was performing on a particular file. Even more specifically, I wanted to understand which parts of a file compressed well, and which ones did not.</p>

<p>There may be readily available tools for visualizing this, but I didn’t find anything. Since I know gzip is implemented in the Python standard libraries, and I’m familiar with Python plotting libraries, I thought I would try to make my own visualization. This blog post (which is in fact just a Jupyter notebook) is the result.</p>

<h2 id="what-to-measure">What to measure</h2>

<p>Sometimes the hardest part of a data analysis problem is just figuring out what you want to measure. The data is all there, and you have a computer at your disposal, so the possibilities are endless. Knowing <em>what</em> to compute is tricky. In my case, I want to understand which parts of a file compress well. So it makes sense that whatever I visualize should include the position in the file along the X axis, and the compressed size along the Y axis. An uncompressed file would simply be a diagonal line. The better the compression, the more this line would stay <em>under</em> the diagonal line of an uncompressed file.</p>

<h2 id="how-to-measure-it">How to measure it?</h2>

<p>Since Python supports gzip in the standard library, let’s see how we can measure these X and Y coordinates. First, let’s create a file with some compressed data. My favorite to use in this instance is <a href="http://www.gutenberg.org/ebooks/11">Alice’s Adventures in Wonderland</a>, downloaded from Project Gutenberg. We’ll compress it on the command line for simplicity.</p>

<p>(Note that code prefixed by ‘!’ is executed via bash - everything else is executed in Python).</p>

<div><div><pre><code>!gzip -k alice.txt
!ls -lh alice*
</code></pre></div></div>

<div><div><pre><code>-rw-r--r-- 1 stephen stephen 171K Sep 22 20:25 alice.txt
-rw-r--r-- 1 stephen stephen  60K Sep 22 20:25 alice.txt.gz
</code></pre></div></div>

<p>gzip does a pretty decent job at compressing this, far better than I could do myself. Now, let’s use Python to decompress just a little bit of it, and how much of the original file is consumed as we go.</p>

<div><div><pre><code><span>import</span> <span>gzip</span>
<span>compressed</span> <span>=</span> <span>open</span><span>(</span><span>'alice.txt.gz'</span><span>,</span> <span>'rb'</span><span>)</span>
<span>gzip_file</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>compressed</span><span>)</span>
</code></pre></div></div>

<p>In the above code, we save the open file object as <code>compressed</code> before giving it over to the <code>GzipFile</code>. That way, as we read the decompressed data out of <code>gzip_file</code>, we’ll be able to use the <code>tell()</code> method to see how far we are through the compressed file.</p>

<div><div><pre><code><span>first_100_bytes</span> <span>=</span> <span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
<span>first_100_bytes</span>
</code></pre></div></div>

<div><div><pre><code>b'\xef\xbb\xbfThe Project Gutenberg EBook of Alice\xe2\x80\x99s Adventures in Wonderland, by Lewis Carroll\r\n\r\nThis eBook'
</code></pre></div></div>





<p>This feels disappointing. We only read 100 bytes and yet it took 8212 bytes of gzip to give us that data? Well, we have to consider that compression algorithms need to store some tables of data which help decompress the rest of the file, so we should cut gzip some slack. Let’s do this a few more times.</p>



<div><div><pre><code>b' is for the use of anyone anywhere at no cost and with\r\nalmost no restrictions whatsoever.  You may '
</code></pre></div></div>





<p>This feels wrong. After reading 8212 bytes of compressed data for the first 100 bytes, it takes zero bytes to get the next 100?</p>



<div><div><pre><code>b'copy it, give it away or\r\nre-use it under the terms of the Project Gutenberg License included\r\nwith '
</code></pre></div></div>





<p>Clearly there is some buffering going on here. 8212 is suspiciously close to 8192 (20 bytes away) which is a power of two, and thus likely to be a common buffer size. Python’s file I/O machinery is responsible for the buffering, but we can actually get rid of it by disabling buffering.</p>

<div><div><pre><code><span>gzip_file</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span> <span>=</span> <span>open</span><span>(</span><span>'alice.txt.gz'</span><span>,</span> <span>'rb'</span><span>,</span> <span>buffering</span><span>=</span><span>0</span><span>)</span>
<span>gzip_file</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>compressed</span><span>)</span>
<span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
<span>compressed</span><span>.</span><span>tell</span><span>()</span>
</code></pre></div></div>



<p>Hm. We made <code>compressed</code> an unbuffered file, but maybe <code>GzipFile</code> has its own internal buffering. To avoid this, let’s do a bad thing. We can actually set the buffer size for all I/O operations by modifying <code>io.DEFAULT_BUFFER_SIZE</code>. If we set it to a small value, then we can reduce the impact of buffering on our measurements. Just for fun, let’s try setting it to 1.</p>

<div><div><pre><code><span>import</span> <span>io</span>
<span>old_buffer_size</span> <span>=</span> <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span>
<span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>1</span>
<span>gzip_file</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span><span>.</span><span>close</span><span>()</span>

<span>compressed</span> <span>=</span> <span>open</span><span>(</span><span>'alice.txt.gz'</span><span>,</span> <span>'rb'</span><span>,</span> <span>buffering</span><span>=</span><span>0</span><span>)</span>
<span>gzip_file</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>compressed</span><span>)</span>
<span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
<span>compressed</span><span>.</span><span>tell</span><span>()</span>
</code></pre></div></div>



<p>This seems <em>much</em> more believable. To read 100 bytes of decompressed data, gzip had to read 205 bytes of compressed data (again, this is probably due to tables and other header information). Let’s continue for a bit:</p>

<div><div><pre><code><span>bytes_unc</span> <span>=</span> <span>100</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>5</span><span>):</span>
    <span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
    <span>bytes_unc</span> <span>+=</span> <span>100</span>
    <span>bytes_cmp</span> <span>=</span> <span>compressed</span><span>.</span><span>tell</span><span>()</span>
    <span>print</span><span>(</span><span>f</span><span>'uncompressed: {bytes_unc} / compressed: {bytes_cmp}'</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code>uncompressed: 200 / compressed: 279
uncompressed: 300 / compressed: 335
uncompressed: 400 / compressed: 376
uncompressed: 500 / compressed: 449
uncompressed: 600 / compressed: 541
</code></pre></div></div>

<p>We can see that after reading 400 uncompressed bytes, the gzip compression has caught up! 376 compressed bytes needed to be read to give us those 400. The gap continues to widen as we go on.</p>

<p>Now that we’re confident that this approach is giving us interesting data, let’s make some functions to get all of this data for a particular file, so we can visualize it!</p>

<div><div><pre><code><span>gzip_file</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span><span>.</span><span>close</span><span>()</span>
<span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>old_buffer_size</span>
</code></pre></div></div>

<p>That was just some cleanup. Since modifying the buffer size would likely impact other code we run, it’s best to only modify the buffer size when we need it, and reset it back to its original value when we’re done. This can be done with a context manager.</p>

<div><div><pre><code><span>import</span> <span>contextlib</span>
<span>@contextlib.contextmanager</span>
<span>def</span> <span>buffer_size</span><span>(</span><span>newsize</span><span>=</span><span>1</span><span>):</span>
    <span>old_buffer_size</span> <span>=</span> <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span>
    <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>newsize</span>
    <span>try</span><span>:</span>
        <span>yield</span>
    <span>finally</span><span>:</span>
        <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>old_buffer_size</span>
        
        
<span>with</span> <span>buffer_size</span><span>():</span>
    <span>print</span><span>(</span><span>f</span><span>'size: {io.DEFAULT_BUFFER_SIZE}'</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>'size: {io.DEFAULT_BUFFER_SIZE}'</span><span>)</span>
</code></pre></div></div>



<p>Now for a function to retrieve compressed and uncompressed sizes. We can do this with a “chunk size” as a parameter. The larger our chunk size, the fewer data points we will have, but the code will run faster. We used 100 as a chunk size above, which seems good enough, but I do prefer a good <a href="https://xkcd.com/1000/">round number</a>, so I’ll change it to 64.</p>

<div><div><pre><code><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span>def</span> <span>create_compression_curve</span><span>(</span><span>filename</span><span>,</span> <span>chunksize</span><span>=</span><span>64</span><span>):</span>
    <span>with</span> <span>buffer_size</span><span>(</span><span>1</span><span>),</span> <span>open</span><span>(</span><span>filename</span><span>,</span> <span>'rb'</span><span>)</span> <span>as</span> <span>fileobj</span><span>:</span>
        <span>gf</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>fileobj</span><span>)</span>
        <span>records</span> <span>=</span> <span>[]</span>
        <span>read</span> <span>=</span> <span>0</span>
        <span>while</span> <span>True</span><span>:</span>
            <span>data</span> <span>=</span> <span>gf</span><span>.</span><span>read</span><span>(</span><span>chunksize</span><span>)</span>
            <span>if</span> <span>len</span><span>(</span><span>data</span><span>)</span> <span>==</span> <span>0</span><span>:</span>
                <span>break</span>  <span># end of file</span>
            <span>else</span><span>:</span>
                <span>read</span> <span>+=</span> <span>len</span><span>(</span><span>data</span><span>)</span>
                <span>records</span><span>.</span><span>append</span><span>((</span><span>read</span><span>,</span> <span>fileobj</span><span>.</span><span>tell</span><span>()))</span>
                
    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>records</span><span>,</span> <span>columns</span><span>=</span><span>[</span><span>'uncompressed'</span><span>,</span> <span>filename</span><span>])</span>
    <span>return</span> <span>df</span><span>.</span><span>set_index</span><span>(</span><span>'uncompressed'</span><span>)</span>


<span>ccurve</span> <span>=</span> <span>create_compression_curve</span><span>(</span><span>'alice.txt.gz'</span><span>)</span>
<span>ccurve</span>
</code></pre></div></div>

<div>

<table>
  <thead>
    <tr>
      <th></th>
      <th>alice.txt.gz</th>
    </tr>
    <tr>
      <th>uncompressed</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>64</th>
      <td>176</td>
    </tr>
    <tr>
      <th>128</th>
      <td>224</td>
    </tr>
    <tr>
      <th>192</th>
      <td>271</td>
    </tr>
    <tr>
      <th>256</th>
      <td>315</td>
    </tr>
    <tr>
      <th>320</th>
      <td>345</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>174272</th>
      <td>61351</td>
    </tr>
    <tr>
      <th>174336</th>
      <td>61362</td>
    </tr>
    <tr>
      <th>174400</th>
      <td>61379</td>
    </tr>
    <tr>
      <th>174464</th>
      <td>61404</td>
    </tr>
    <tr>
      <th>174484</th>
      <td>61417</td>
    </tr>
  </tbody>
</table>
<p>2727 rows × 1 columns</p>
</div>

<p>The above function simply reads the gzipped file in chunks, measuring the distance we’ve gone through the compressed file each time, and adding it to a list of “records”. This list is converted into a Pandas Dataframe, which is commonly used to hold tabular data like this. We set the “uncompressed” column to be the “index”, since that’s what we’d consider the X-axis.</p>

<p>The result looks exciting! We can even go right ahead and plot it from here.</p>

<div><div><pre><code><span># Some style changes to make the plots more pretty</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>matplotlib</span> <span>as</span> <span>mpl</span>
<span>plt</span><span>.</span><span>style</span><span>.</span><span>use</span><span>(</span><span>'ggplot'</span><span>)</span>
<span>mpl</span><span>.</span><span>rcParams</span><span>[</span><span>'figure.figsize'</span><span>]</span> <span>=</span> <span>[</span><span>16</span><span>,</span> <span>8</span><span>]</span>

<span>ccurve</span><span>.</span><span>plot</span><span>()</span>
</code></pre></div></div>

<p><img src="https://brennan.io/images/ccurves/output_28_1.png" alt="Plot 1"></p>

<p>Well, I gotta give it to gzip – it’s pretty consistent. I can’t really see anything interesting in the plot, except that the gzipped data is smaller than the uncompressed version (duh). We can add this in to make it more explicit:</p>

<div><div><pre><code><span>ccurve</span><span>[</span><span>'uncompressed'</span><span>]</span> <span>=</span> <span>ccurve</span><span>.</span><span>index</span>
<span>ccurve</span><span>.</span><span>plot</span><span>()</span>
</code></pre></div></div>

<p><img src="https://brennan.io/images/ccurves/output_30_1.png" alt="Plot 2"></p>

<h2 id="what-to-do-with-this-new-power">What to do with this new power?</h2>

<p>So, the result here seems to be blindingly mundane. gzip compresses reasonably well, it’s obviously better than uncompressed.</p>

<p>Well, let’s try to make things less mundane. First, a peek at the gzip(1) manual page indicates that it has different compression levels 1-9. I’ll let the manual do the explaining:</p>

<div><div><pre><code>   -# --fast --best
          Regulate the speed of compression using the specified digit #, where -1
          or  --fast  indicates the fastest compression method (less compression)
          and -9 or --best indicates the slowest compression  method  (best  com‐
          pression).   The  default  compression level is -6 (that is, biased to‐
          wards high compression at expense of speed).
</code></pre></div></div>

<p>What if we used this compression curve plot to compare the gzip compression levels?</p>

<div><div><pre><code><span>import</span> <span>os</span>
<span>files</span> <span>=</span> <span>[]</span>
<span>for</span> <span>level</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>10</span><span>):</span>
    <span>os</span><span>.</span><span>system</span><span>(</span><span>f</span><span>'gzip -k -S .gz.{level} -{level} alice.txt'</span><span>)</span>
    <span>files</span><span>.</span><span>append</span><span>(</span><span>f</span><span>'alice.txt.gz.{level}'</span><span>)</span>
    <span>print</span><span>(</span><span>f</span><span>'Created {files[-1]}'</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code>Created alice.txt.gz.1
Created alice.txt.gz.2
Created alice.txt.gz.3
Created alice.txt.gz.4
Created alice.txt.gz.5
Created alice.txt.gz.6
Created alice.txt.gz.7
Created alice.txt.gz.8
Created alice.txt.gz.9
</code></pre></div></div>

<p>Above I went ahead and created all the different compression levels. Now, we can get compression curves for all of them and plot them:</p>

<div><div><pre><code><span>ccurves</span> <span>=</span> <span>pd</span><span>.</span><span>concat</span><span>([</span>
    <span>create_compression_curve</span><span>(</span><span>fn</span><span>)</span> <span>for</span> <span>fn</span> <span>in</span> <span>files</span>
<span>],</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>ccurves</span><span>.</span><span>plot</span><span>(</span><span>title</span><span>=</span><span>"gzip Compression Level Comparison (Alice's Adventures in Wonderland)"</span><span>)</span>
</code></pre></div></div>

<p><img src="https://brennan.io/images/ccurves/output_35_1.png" alt="Plot 3"></p>

<p>That seems slightly more interesting. The default compression level of 6 seems to be chosen well. Beyond level 6, the reduction in file size seems pretty difficult to notice. However, the difference between the compression ratios is rather small compared to the uncompressed line:</p>

<div><div><pre><code><span>ccurves</span><span>[</span><span>'uncompressed'</span><span>]</span> <span>=</span> <span>ccurves</span><span>.</span><span>index</span>
<span>ccurves</span><span>.</span><span>plot</span><span>()</span>
</code></pre></div></div>
<hr>
<p><img src="https://brennan.io/images/ccurves/output_37_1.png" alt="Plot 4"></p>

<h2 id="making-an-interesting-graph">Making an …</h2></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://brennan.io/2020/09/22/compression-curves/">https://brennan.io/2020/09/22/compression-curves/</a></em></p>]]>
            </description>
            <link>https://brennan.io/2020/09/22/compression-curves/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24563372</guid>
            <pubDate>Wed, 23 Sep 2020 05:41:31 GMT</pubDate>
        </item>
    </channel>
</rss>
