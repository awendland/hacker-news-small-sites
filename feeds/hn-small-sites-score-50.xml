<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 25 Nov 2020 20:22:28 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 25 Nov 2020 20:22:28 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Guide to OOMKill Alerting in Kubernetes Clusters]]>
            </title>
            <description>
<![CDATA[
Score 60 | Comments 26 (<a href="https://news.ycombinator.com/item?id=25192733">thread link</a>) | @draganm
<br/>
November 23, 2020 | https://www.netice9.com/blog/guide-to-oomkill-alerting-in-kubernetes-clusters | <a href="https://web.archive.org/web/*/https://www.netice9.com/blog/guide-to-oomkill-alerting-in-kubernetes-clusters">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>   <span>Monday 23 Nov 2020, 18:30</span> <h2>Intro</h2> <p>RAM is most likely the scarcest resource that is first exhausted on your servers. If you’re serious about running software under Linux/Unix, you’re certainly aware of what an OOMKill is.</p> <p>Short refresher: when a program requests a new memory page from the kernel two things can happen.</p> <ul><li>There is a free memory page: The kernel page assigns the page to the process and everything is great.</li> <li>The system is Out Of Memory (OOM): The kernel chooses a process based on its ‘badness’ (mainly by how much ram it uses). It sends a SIGKILL to the process. This forces the receiving process to exit with exit code <code>137</code>. All the memory pages belonging to that process are free and now the kernel can fulfill the memory request.</li></ul> <p>Lately, I had a task to add alerting to a sizeable Kubernetes cluster. The cluster has ~100 active Deployments with autoscaling of nodes up to ~50 nodes at peak times. The cluster is well maintained and has a robust autoscaling strategy. All deployments have resource limits defined. Sometimes, some of the deployed pods would breach the memory limits. In those cases, it would be nice to find out when that happens and investigate the cause of it.</p> <p>Prometheus and Alertmanager were already deployed. So I’ve thought that alerting on OOMKills will be as easy. I just had to find the right metric(s) indicating that OOMKill has happened and write an alerting rule for it. Given the length of this post, you could imagine how wrong I was!</p> <h2>First Attempt</h2> <p>A brief Google search has led me to the <a href="https://github.com/kubernetes/kube-state-metrics/blob/master/docs/pod-metrics.md" rel="nofollow">kube pod state metric</a>. It turns out it has a metric called <code>kube_pod_container_status_last_terminated_reason</code>. The value of the metric is <code>1</code> when a container in a pod has terminated with an error. Based on the exit code, the <code>reason</code> label will be set to <code>OOMKilled</code> if the exit code was <code>137</code>. That sounded promising! So I’ve created an alert for that.</p> <p>As usual, things are rarely straightforward. As soon as the container restarts, the value of this metric will be <code>1</code>. For alerting purposes, one has to combine it with another metric that will change when a pod restarts. <code>kube_pod_container_status_restarts_total</code> does that. Combine the two - and Bingo! It Worked!</p> <h2>“Invisible” OOMKills</h2> <p>For a brief moment, I’ve thought that I was done. I was about to declare victory over OOMKills in production! But then a puzzle came my way: One of our software developers has come forward. He claimed that one of his pods was running out of memory and he couldn’t see any alerts for it.</p> <p>At first, I wasn’t inclined to believe that his diagnosis of running out of memory was correct. Mainly because his Pod didn’t even restart! But then I looked at the graph of the memory use of the Pod. It did show the usual pattern: Memory usage would grow, reach its peak at the memory limit, and then suddenly drop.</p> <p>I’ve asked the developer for the gory details of the implementation. It turned out that the init process in the container would start a child process and wait for the result of it. If the child process would exit with an error, it would return an error to the requester and not terminate (because - why should it?).</p> <p>That is when it dawned to me - my alerting is effective only if container exits. This is usually the case when the init process of the container is OOMKilled. But there is no guarantee this will happen if a child of the init is OOMKilled. In the case where the container’s init tries to handle OOMKill by itself, my alerting is not triggering!</p> <h2>Trying the Existing Solutions</h2> <p>Given that OOMKills are as old as Unix, I thought: surely someone will have a solution for this already.</p> <p>I’ve ensued onto a frantic search for some kind of metric exporter for this. I just needed the number of OOMKill events in a pod, or at least in a Docker container. Here is what I’ve found:</p> <h3>cAdvisor</h3> <p>My first stop was cAdvisor itself. It turns out that cAdvisor is <a href="https://github.com/google/cadvisor/issues/1837" rel="nofollow">getting the OOMKill events, but not exporting them as a Prometheus metric and no one really seems to care.</a> So that was a dead-end.</p> <h3>kubernetes-oomkill-exporter</h3> <p>My second stop was <a href="https://github.com/sapcc/kubernetes-oomkill-exporter" rel="nofollow">kubernetes-oomkill-exporter</a>. A very promising-sounding project with two huge disadvantages:</p> <ul><li>There is really no documentation for it, literally anywhere.</li> <li>It does not work.</li></ul> <p>I’ve tried the latest version of <a href="https://hub.docker.com/layers/sapcc/kubernetes-oomkill-exporter/0.3.0/images/sha256-b80875b903635f0336ea0b122b332e086da51ec5cd797de5d682dd14c3910b9f?context=explore" rel="nofollow">the Docker image</a>, but once started it crashes and burns with:</p> <pre><code>standard_init_linux.go:211: exec user process caused "no such file or directory"</code></pre> <p>Going <a href="https://hub.docker.com/layers/sapcc/kubernetes-oomkill-exporter/0.2.0/images/sha256-5e1b57f4ac0b57406ef067da3e83f743d70ff89aa1db717d41af2c699dc12f3a?context=explore" rel="nofollow">back one minor version</a> one gets the following output:</p> <pre><code>F1120 22:04:21.571246       1 main.go:73] Could not create log watcher
I1120 22:04:21.572066       1 main.go:64] Starting prometheus metrics</code></pre> <p>As it seems no one has committed any code to in over a year. It has a low number of stars (14). All that meant that I was back to square one.</p> <h2>Rolling my Own: <code>missing-container-metrics</code></h2> <p>Having a hard time finding an existing solution meant only one thing: I will have to write my own.</p> <p>A cursory look at <a href="https://docs.docker.com/engine/reference/commandline/events/" rel="nofollow">Docker’s events</a> delivered everything I needed. There is an event called <code>oom</code>. Docker emits this event every time the OOMKiller process gets active in the container. Now I was only missing a piece of code that will listen to those events and export them as Prometheus metrics.</p> <p>This is how <a href="https://github.com/draganm/missing-container-metrics" rel="nofollow">missing-container-metrics</a> was born. What it does is to connect to a local Docker instance (via <code>/var/run/docker.sock</code>). It lists all existing containers as a starting point. And then it listens to Docker events. Using those events, it keeps track of the currently running containers. It also gathers the basic stats of each container it knows about:</p> <ul><li>Number of restarts</li> <li>Last exit code</li> <li>Number of OOMKills</li></ul> <p>By design, it is not Kubernetes specific. This means it can be used with a plain Docker. But it also has a couple of very convenient Kubernetes specific features.</p> <p>Whenever it finds a container label for the pod name or namespace, it adds them as a label to the exported metrics. Also, label naming is compatible with <code>kube-state-metrics</code>.</p> <p>This keeps things simple for metric joins in PromQL.</p> <h2>Running it in the Cluster</h2> <p>In a Kubernetes cluster, <code>missing-container-metrics</code> needs to run on every node. The simplest way to achieve this is to use a daemon-set. The source code comes with an example <a href="https://github.com/draganm/missing-container-metrics#kubernetes" rel="nofollow">daemon set</a> deployment.</p> <h2>An Interesting Find Using <code>missing-container-metrics</code></h2> <p>The most interesting issue I’ve found was where I’ve least expected it: Fluentd!</p> <p>Fluentd log forwarder for node/pod/kubelet logs to the log aggregator. When the volume of logs was very high, Fluentd is OOMKilled.</p> <p>Looking at the details of how Fluentd works, it becomes clear what is going on.</p> <p>Fluentd has one main process (that ends up being init process in the container). This main process forks a worker process that forwards the logs. When the worker process dies for some reason (for example OOMKill), the main process starts a new one. This leads to an endless loop of spawn/OOMKill.</p> <p>The fact that Fluentd is the log forwarder is very unfortunate. OOMKill loop would stop the log forwarding, so you could not ‘see’ what is going on by inspecting the logs.</p> <h2>Epilogue</h2> <p>If you want to make sure that your Kubernetes cluster is healthy, it is essential to alert on OOMKills. This enables you to know when processes hit their memory limits. Be it because of memory leaks or wrongly configured memory limits.</p> <p>It turns out that monitoring for OOMKills in Kubernetes is not as an easy task as one might think. Using <a href="https://github.com/draganm/missing-container-metrics" rel="nofollow">missing-container-metrics</a> makes it much easier though.</p> <p>So go ahead, deploy <a href="https://github.com/draganm/missing-container-metrics" rel="nofollow">missing-container-metrics</a> to your cluster. You might be surprised how many of OOMKills you have not been noticing.</p> <p>I hope that it will be useful to you, and will save you the time that I’ve spent searching for the solution.</p></article></div>]]>
            </description>
            <link>https://www.netice9.com/blog/guide-to-oomkill-alerting-in-kubernetes-clusters</link>
            <guid isPermaLink="false">hacker-news-small-sites-25192733</guid>
            <pubDate>Mon, 23 Nov 2020 22:31:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Walmart Exclusive Wi-Fi Router Contains Backdoor to Control Devices]]>
            </title>
            <description>
<![CDATA[
Score 281 | Comments 12 (<a href="https://news.ycombinator.com/item?id=25189673">thread link</a>) | @wikus
<br/>
November 23, 2020 | https://hfet.org/walmart-exclusive-wi-fi-router-contains-backdoor-to-control-devices/ | <a href="https://web.archive.org/web/*/https://hfet.org/walmart-exclusive-wi-fi-router-contains-backdoor-to-control-devices/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

			
	<main id="main" role="main">

		
<article id="post-1238">

	<!-- .entry-header -->

	
		<figure>
			<img width="1080" height="540" src="https://hfet.org/wp-content/uploads/2020/11/wallmart_router_backdoor_humans_for_ethical_technology_hfet-1080x540.jpg" alt="" loading="lazy" srcset="https://hfet.org/wp-content/uploads/2020/11/wallmart_router_backdoor_humans_for_ethical_technology_hfet-1080x540.jpg 1080w, https://hfet.org/wp-content/uploads/2020/11/wallmart_router_backdoor_humans_for_ethical_technology_hfet-20x11.jpg 20w" sizes="(max-width: 1080px) 100vw, 1080px">		</figure>

		
	
<div>

	<h4>A Walmart-exclusive Wi-Fi router, and others sold on Amazon &amp; eBay contain hidden backdoors to control devices <a href="https://cybernews.com/security/walmart-exclusive-routers-others-made-in-china-contain-backdoors-to-control-devices/" target="_blank" rel="noopener noreferrer">reports CyberNews</a>.</h4>
<ul>
<li>Researchers discovered that many low cost, Chinese-made Wi-Fi routers contain a hidden backdoor which is being actively exploited to create botnet attacks.</li>
</ul>
<p>CyberNews researchers discovered suspicious backdoors in a Chinese made router sold under the name ‘Jetstream’. This router is part of Walmart’s new line of affordable Wi-Fi routers.</p>
<blockquote><p>This backdoor would allow an attacker the ability to remotely control not only the routers, but also any devices connected to that network.</p></blockquote>
<p><img loading="lazy" src="https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-1024x681.jpg" alt="" width="800" height="532" srcset="https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-1024x681.jpg 1024w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-300x199.jpg 300w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-768x511.jpg 768w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-20x13.jpg 20w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-36x24.jpg 36w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-48x32.jpg 48w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232-272x182.jpg 272w, https://hfet.org/wp-content/uploads/2020/11/pexels-brett-sayles-2881232.jpg 1280w" sizes="(max-width: 800px) 100vw, 800px"></p>
<p>The researchers contacted Walmart to get a statement, and a Walmart spokesperson had this to say:</p>
<blockquote><p>“Thank you for bringing this to our attention. We are looking into the issue to learn more. The item in question is currently out of stock and we do not have plans to replenish it.”</p></blockquote>
<p>CyberNews researchers also discovered that ‘Wavlink’ branded routers, often sold on Amazon or eBay, <a href="https://cybernews.com/security/walmart-exclusive-routers-others-made-in-china-contain-backdoors-to-control-devices/" target="_blank" rel="noopener noreferrer">contain similar backdoors</a>.</p>
<p>Worryingly, they also discovered that these <strong>backdoors are being actively exploited</strong>, and there have been attempts to add the routers to a botnet with malware that allows them to be used in large scale DDoS attacks, which have <a href="https://cybernews.com/security/walmart-exclusive-routers-others-made-in-china-contain-backdoors-to-control-devices/" target="_blank" rel="noopener noreferrer">in the past taken down major websites</a> such as Reddit, Netflix, CNN, GitHub, Twitter, AirBnb and more.</p>
<h4><strong>Read more of the <a href="https://cybernews.com/security/walmart-exclusive-routers-others-made-in-china-contain-backdoors-to-control-devices/" target="_blank" rel="noopener noreferrer">full report on CyberNews</a>.</strong></h4>
<p><strong><a href="https://james-clee.com/2020/04/18/multiple-wavlink-vulnerabilities/" target="_blank" rel="noopener noreferrer">James Clee’s Report</a> on ‘Wavlink’ routers’ backdoors.<br>
</strong></p>
<p><a href="https://hfet.org/feed/"><img src="https://hfet.org/wp-content/uploads/2020/11/rss_button_hfet-1.png" alt="" width="219" height="30"></a><a href="https://hfet.org/support/"><img src="https://hfet.org/wp-content/uploads/2020/11/support_button_hfet.png" alt="" width="165" height="30"></a></p>
    <div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img width="100" height="100" src="https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-150x150.jpg" alt="" loading="lazy" srcset="https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-150x150.jpg 150w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-300x300.jpg 300w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-20x20.jpg 20w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-36x36.jpg 36w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-48x48.jpg 48w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-24x24.jpg 24w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel-96x96.jpg 96w, https://hfet.org/wp-content/uploads/2020/11/karl-swanepoel.jpg 640w" sizes="(max-width: 100px) 100vw, 100px"></p><div><p>I’m an AI &amp; Robotics Student interested in FOSS, Tech Sustainability and Data Rights. I’m also the founder of Humans For Ethical Technology.</p></div></div>	
</div><!-- .entry-content -->


</article>

	</main><!-- #main -->

	


	</div></div>]]>
            </description>
            <link>https://hfet.org/walmart-exclusive-wi-fi-router-contains-backdoor-to-control-devices/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25189673</guid>
            <pubDate>Mon, 23 Nov 2020 18:10:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Small Games]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 72 (<a href="https://news.ycombinator.com/item?id=25188542">thread link</a>) | @polm23
<br/>
November 23, 2020 | https://lorenzo.itch.io/on-small-games | <a href="https://web.archive.org/web/*/https://lorenzo.itch.io/on-small-games">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><em>I wanted to write a Small Games Manifesto for the Manifesto Jam, but I&nbsp;was too tired, so I collected&nbsp;other people's thoughts about small games instead.</em></p>

<p><em>See also: <a href="http://ebeth.itch.io/small-games-manifesto" target="_blank">Small Games Manifesto</a> by Ebeth.</em><br></p>

<p><em>Looking for some small games to play? Check out my <a href="https://itch.io/c/6160/small-is-beautiful" target="_blank">Small is Beautiful</a> and&nbsp;<a href="https://itch.io/c/232207/bitsy-faves-pt2-20192020" target="_blank">Bitsy Faves</a>&nbsp;collections.</em></p>

<p><em>Follow me on Twitter <a href="https://twitter.com/LorenzoPilia" target="_blank" rel="nofollow noopener">@LorenzoPilia</a></em></p>

<p>• • • • •</p>

<p>Make short and intense games:<br>think haiku, not epic.<br>Think poetry, not prose.<br><strong>— Auriea Harvey &amp; Michaël Samyn: Realtime Art Manifesto</strong><br><a href="http://tale-of-tales.com/tales/RAM.html" rel="nofollow noopener">http://tale-of-tales.com/tales/RAM.html</a></p>

<p>• • • • •<br></p>

<p>things i will never do in this lifetime:&nbsp;<br>play a game for a few straight hours<br>play a game with more than a few hours worth of content<br><strong>— @moshboy</strong><br><a href="https://twitter.com/moshboy/status/607408540496465922" rel="nofollow noopener">https://twitter.com/moshboy/status/607408540496465922</a></p>

<p>• • • • •<br></p>

<p>Hell, you'd be surprised at how many people buy games with a moderate length and never finish them. On PC over 50 percent of the people who bought the latest Wolfenstein, a game you can beat in under 15 hours, never earned the achievement for finishing the story. Only 31 percent of Dishonored players on the PC beat the game. People think game length is mandatory, but even shorter games aren't finished by the majority of players.
<br><strong>— Ben Kuchera: To hell with longer games, tell me how SHORT your game is</strong><br><a href="https://www.polygon.com/2014/10/14/6974791/short-games-review" rel="nofollow noopener">https://www.polygon.com/2014/10/14/6974791/short-games-review</a></p>

<p>• • • • •<br></p>

<p>Especially if you're starting out, try to do small projects and don't worry too much about polishing them, don't worry about shipping the perfect game, embrace the messiness of getting into games for the first time, embrace not knowing what you're doing exactly yet. (...) If you just put your heart into it in that way, and embrace the messiness of small games, people will really connect with that.<br><strong>— Nina Freeman: Keynote at A MAZE. / Johannesburg 2017<br></strong><a href="https://twitter.com/AMazeFest/status/908032352953217038" rel="nofollow noopener">https://twitter.com/AMazeFest/status/908032352953217038</a></p>

<p>• • • • •<br></p>

<p>Duration doesn't need to be a burden. It can be a tool to wield.<br><strong>— Thomas McMullan: Inside and the rise of short games</strong><br><a href="http://www.alphr.com/games/1003958/inside-and-the-rise-of-short-games" rel="nofollow noopener">http://www.alphr.com/games/1003958/inside-and-the-rise-of-short-games</a></p>

<p>• • • • •<br></p>

<p>Small-scale works are often derided for feeling embryonic or unfinished, throwaway motifs or fledgling ideas that the artist failed to integrate into a sufficiently ambitious whole. Game designer Jake Elliott, who drew the title of his Ruins from Schumann’s appraisal of Chopin’s preludes, defended their proportion in an interview: “Maybe [Chopin] felt like they were complete objects, but there wasn’t a vocabulary for talking about pieces of music that were short at the time. Their length is what drew me … there is a lot that’s unspoken.” Having conventionally privileged length, magnitude, and formal unity, games too have left critics bereft of a clear rubric for evaluating intentionally abbreviated, serialized, even disorderly exercises in interactive design.<br><strong>— Peter Lido: Undertale, one year later</strong><br><a href="https://killscreen.com/articles/undertale-one-year-later/" rel="nofollow noopener">https://killscreen.com/articles/undertale-one-year-later/</a></p>

<p>• • • • •<br></p>

<p>The final idea that we brought over as gamers, the final idea that we had to let go of, was that a longer game makes a better game. We felt that the sense of completion and catharsis that you get when you watch our ending was so critical to the experience, that we decided that we had to help as many people as possible to complete Monument Valley. And that was more important than making the game longer or more difficult.<br><strong>— Ken Wong: Games Without Gamers (#DICE2014 Europe)</strong><br><a href="http://youtu.be/YdSClYHDow0?t=13m37s" rel="nofollow noopener">https://youtu.be/YdSClYHDow0?t=13m37s</a></p>

<p>• • • • •<br></p>

<p>I value games being short, it makes them easier to fit into life, they get to the point sooner, it's possible to play them more times, trying out different possibilities, there's a clearer connection between decisions and outcome.<br><strong>— Michael Brough: imbroglio notes 6 - meditation</strong><br><a href="http://mightyvision.blogspot.de/2016/08/imbroglio-notes-6-meditation.html?m=1" rel="nofollow noopener">http://mightyvision.blogspot.de/2016/08/imbroglio-notes-6-meditation.html?m=1</a></p>

<p>• • • • •<br></p>

<p>Small games must be protected from their own defenders!! They must be defended against a rhetoric of convenience, as if fitting helpfully into the meagre free time allotted us by rentiers was something to be proud of rather than something to grind against - they must be defended against the meagre virtues of "minimalism", parsimony, elegance, the values of those with enough cultural cachet that they can afford to speak softly, and which hold the same relation to an actual human economy of wants and needs as does a millionaire who doesn't tip.<br><strong>— thecathamites: Small Game Manifesto (part of&nbsp;Buttertown, 10 manifestos for groups of no people)</strong><br><a href="https://thecatamites.itch.io/buttertown">https://thecatamites.itch.io/buttertown</a></p>


</div></div>]]>
            </description>
            <link>https://lorenzo.itch.io/on-small-games</link>
            <guid isPermaLink="false">hacker-news-small-sites-25188542</guid>
            <pubDate>Mon, 23 Nov 2020 16:38:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting Started with Firecracker on Raspberry Pi]]>
            </title>
            <description>
<![CDATA[
Score 82 | Comments 25 (<a href="https://news.ycombinator.com/item?id=25187965">thread link</a>) | @sairamkunala
<br/>
November 23, 2020 | https://dev.l1x.be/posts/2020/11/22/getting-started-with-firecracker-on-raspberry-pi/ | <a href="https://web.archive.org/web/*/https://dev.l1x.be/posts/2020/11/22/getting-started-with-firecracker-on-raspberry-pi/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2 id="abstract">Abstract</h2><p>Traditionally services were deployed on bare metal and in the last decades we have seen the rise of virtualisation (running additional operating systems in a operating system process) and lately containerisation (running an operating system process in a separate security context from the rest of processes on the same host). Virtualisation and containerisation offers different levels of isolation by moving some operating system functionality to the guest systems.</p><p>The following chart illustrates that pretty well:</p><p><img src="https://dev.l1x.be/img/isolation.png" alt="OS functionality location"></p><p>Source: <a href="https://research.cs.wisc.edu/multifacet/papers/vee20_blending.pdf">https://research.cs.wisc.edu/multifacet/papers/vee20_blending.pdf</a></p><p>In this article, I perform a deep dive into Firecracker and how it can be used for deploying services on Raspberry Pi (4B).</p><h2 id="getting-started">Getting started</h2><p>There are few paths to take here. First I am going to try the easy one, using Ubuntu. Later on we can investigate the use of Alpine Linux which is much more lightweight than Ubuntu, ideal for devices like RPI.</p><h3 id="installing-the-image-on-a-microsd-card">Installing the image on a microSD card</h3><p>We need a 64 bit Ubuntu image and a microsd card. For the imaging I use <a href="https://www.balena.io/etcher/">Balena Etcher</a> that makes the imaging process super easy.</p><p>Getting the pre-installed image:</p><div><pre><code data-lang="bash">wget https://cdimage.ubuntu.com/releases/20.04/release/<span>\
</span><span></span>ubuntu-20.04.1-preinstalled-server-arm64+raspi.img.xz
</code></pre></div><p>Preinstalled means that we get a fully working operating system and there is no need for additional installation steps after booting up. With Balena Etcher it is super easy to write the compressed image file to the sd card and boot the system up once ready. SSHD starts up after the installation and we can log in via ssh if we know the IP address that the DHCP server issues to our device (assuming DHCP server is present in our LAN).</p><p>There are few mildly annoying things with Ubuntu (snaps, unattended-upgrades) that I usually remove. I also prefer to use Chrony over the systemd equivalent. Ansible repo for these is available here: <a href="https://github.com/l1x/rpi/blob/main/ubuntu.20/ansible/roles/os/tasks/main.yml">https://github.com/l1x/rpi/blob/main/ubuntu.20/ansible/roles/os/tasks/main.yml</a></p><h3 id="installing-firecracker-jailer-and-firectl">Installing Firecracker, Jailer and Firectl</h3><ul><li>Firecracker: The main component, it is a virtual machine monitor (VMM) that uses the Linux Kernel Virtual Machine (KVM) to create and run microVMs.</li><li>Jailer: For starting Firecracker in production mode, applies a cgroup/namespace isolation barrier and then drops privileges. There</li><li>Firectl: A command line utility for convenience</li></ul><h4 id="getting-firecracker-and-jailer">Getting Firecracker and Jailer</h4><p>For the first two it is possible to download the release binaries from Github.</p><div><pre><code data-lang="bash"><span>version</span><span>=</span><span>'v0.23.0'</span>

wget https://github.com/firecracker-microvm/firecracker/<span>\
</span><span></span>releases/download/<span>${</span><span>version</span><span>}</span>/firecracker-<span>${</span><span>version</span><span>}</span>-aarch64
wget https://github.com/firecracker-microvm/firecracker/<span>\
</span><span></span>releases/download/<span>${</span><span>version</span><span>}</span>/jailer-<span>${</span><span>version</span><span>}</span>-aarch64

mv firecracker-<span>${</span><span>version</span><span>}</span>-aarch64 firecracker
mv jailer-<span>${</span><span>version</span><span>}</span>-aarch64 jailer

chmod +x firecracker jailer

./firecracker --help
./jailer --help
</code></pre></div><h4 id="firectl">Firectl</h4><p>Firectl is a bit trickier to install because there is no release binary and it requires Golang 1.14 to compile. We can do these in two steps.</p><div><pre><code data-lang="bash">wget https://golang.org/dl/go1.14.12.linux-arm64.tar.gz
tar xzvf go1.14.12.linux-arm64.tar.gz
</code></pre></div><p>After getting go we can get the source of firectl and compile it:</p><div><pre><code data-lang="bash">git clone https://github.com/firecracker-microvm/firectl.git
<span>cd</span> firectl/
 ~/go/bin/go build -x
</code></pre></div><p>Testing Firectl:</p><p>We have all the tools we need for running our first microVM the only thing is missing: something to run.</p><h3 id="downloading-our-first-image">Downloading our first image</h3><p>For a microVM there are two things necessary to have:</p><ul><li>an uncompressed linux kernel (vmlinux)</li><li>a filesystem</li></ul><p>Later on we are going to investigate how we could create our own version of these, but for now we are going to use images from</p><div><pre><code data-lang="bash">wget https://s3.amazonaws.com/spec.ccfc.min/<span>\
</span><span></span>img/aarch64/ubuntu_with_ssh/kernel/vmlinux.bin
wget https://s3.amazonaws.com/spec.ccfc.min/<span>\
</span><span></span>img/aarch64/ubuntu_with_ssh/fsfiles/xenial.rootfs.ext4
</code></pre></div><h3 id="configuring-network">Configuring network</h3><p>For the microVM to function properly we need a networking device. For this scenario we are going to use tap and create a device:</p><div><pre><code data-lang="bash">sudo ip tuntap add dev tap0 mode tap
sudo ip addr add 172.16.0.1/24 dev tap0
sudo ip link <span>set</span> tap0 up
ip addr show dev tap0
</code></pre></div><p>If we want to give access to our VM we have to enable IP forwarding:</p><div><pre><code data-lang="bash"><span>DEVICE_NAME</span><span>=</span>eth0
sudo sh -c <span>"echo 1 &gt; /proc/sys/net/ipv4/ip_forward"</span>
sudo iptables -t nat -A POSTROUTING -o <span>$DEVICE_NAME</span> -j MASQUERADE
sudo iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
sudo iptables -A FORWARD -i tap0 -o <span>$DEVICE_NAME</span> -j ACCEPT
</code></pre></div><h3 id="running-our-first-microvm">Running our first microVM</h3><p>This is how we can start up our first microVM. I usually start it in screen so I can open a new session easily because it will use the standard input and output for the newly started of console (unless you redirect it).</p><p>This is for debug mode, starting with sudo:</p><div><pre><code data-lang="bash">sudo ./firectl/firectl <span>\
</span><span></span>--firecracker-binary<span>=</span>./firecracker <span>\
</span><span></span>--kernel<span>=</span>vmlinux.bin <span>\
</span><span></span>--tap-device<span>=</span>tap0/aa:fc:00:00:00:01 <span>\
</span><span></span>--kernel-opts<span>=</span><span>\
</span><span></span><span>"console=ttyS0 reboot=k panic=1 pci=off \
</span><span>ip=172.16.0.42::172.16.0.1:255.255.255.0::eth0:off"</span> <span>\
</span><span></span>--root-drive<span>=</span>./xenial.rootfs.ext4
</code></pre></div><p>If everything went well you can see something like this:</p><pre><code>Ubuntu 18.04.2 LTS fadfdd4af58a ttyS0

fadfdd4af58a login:
</code></pre><p>User and password is root:root.</p><h3 id="testing-networking">Testing networking</h3><p>For this we need to have a bit bigger image.</p><div><pre><code data-lang="bash">dd <span>if</span><span>=</span>/dev/zero <span>bs</span><span>=</span>1M <span>count</span><span>=</span><span>800</span> &gt;&gt; xenial.rootfs.ext4
resize2fs -f xenial.rootfs.ext4
</code></pre></div><p>After starting up the usual way and logging in we need to fix few things:</p><p>Adding some working nameserver:</p><div><pre><code data-lang="bash"><span>echo</span> <span>'nameserver 1.1.1.1'</span> &gt;  /etc/resolv.conf
</code></pre></div><p>Now trying to update:</p><div><pre><code data-lang="bash">root@fadfdd4af58a:~# apt update
Get:1 http://ports.ubuntu.com/ubuntu-ports bionic InRelease <span>[</span><span>242</span> kB<span>]</span>
Get:2 http://ports.ubuntu.com/ubuntu-ports bionic-updates InRelease <span>[</span>88.7 kB<span>]</span>
Hit:3 http://ports.ubuntu.com/ubuntu-ports bionic-backports InRelease
Hit:4 http://ports.ubuntu.com/ubuntu-ports bionic-security InRelease
Get:5 http://ports.ubuntu.com/ubuntu-ports bionic/universe arm64 Packages <span>[</span>11.0 MB<span>]</span>
Get:6 http://ports.ubuntu.com/ubuntu-ports bionic/multiverse arm64 Packages <span>[</span><span>153</span> kB<span>]</span>
Get:7 http://ports.ubuntu.com/ubuntu-ports bionic/main arm64 Packages <span>[</span><span>1285</span> kB<span>]</span>
Get:8 http://ports.ubuntu.com/ubuntu-ports bionic/restricted arm64 Packages <span>[</span><span>572</span> B<span>]</span>
Get:9 http://ports.ubuntu.com/ubuntu-ports bionic-updates/universe arm64 Packages <span>[</span><span>1865</span> kB<span>]</span>
Get:10 http://ports.ubuntu.com/ubuntu-ports bionic-updates/restricted arm64 Packages <span>[</span><span>2262</span> B<span>]</span>
Get:11 http://ports.ubuntu.com/ubuntu-ports bionic-updates/main arm64 Packages <span>[</span><span>1431</span> kB<span>]</span>
Get:12 http://ports.ubuntu.com/ubuntu-ports bionic-updates/multiverse arm64 Packages <span>[</span><span>5758</span> B<span>]</span>
Fetched 16.1 MB in 6s <span>(</span><span>2543</span> kB/s<span>)</span>
Reading package lists... Error!
E: flAbsPath on /var/lib/dpkg/status failed - realpath <span>(</span>2: No such file or directory<span>)</span>
E: Could not open file  - open <span>(</span>2: No such file or directory<span>)</span>
E: Problem opening
E: The package lists or status file could not be parsed or opened.
</code></pre></div><p>Fixing the apt issues:</p><div><pre><code data-lang="bash">mkdir -p /var/lib/dpkg/<span>{</span>info,alternatives<span>}</span>
touch /var/lib/dpkg/status
apt install apt-utils -y
</code></pre></div><p>Enjoy!</p><p>Next time we can go through how to compile a new kernel and have a different rootfs (potentially using Alpine).</p></div></div>]]>
            </description>
            <link>https://dev.l1x.be/posts/2020/11/22/getting-started-with-firecracker-on-raspberry-pi/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25187965</guid>
            <pubDate>Mon, 23 Nov 2020 15:48:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Django refactoring game – can you fix all the Models anti-patterns?]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 53 (<a href="https://news.ycombinator.com/item?id=25187507">thread link</a>) | @rikatee
<br/>
November 23, 2020 | https://django.doctor/challenge | <a href="https://web.archive.org/web/*/https://django.doctor/challenge">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://django.doctor/challenge</link>
            <guid isPermaLink="false">hacker-news-small-sites-25187507</guid>
            <pubDate>Mon, 23 Nov 2020 15:11:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Tech Stack of a One-Man SaaS]]>
            </title>
            <description>
<![CDATA[
Score 432 | Comments 235 (<a href="https://news.ycombinator.com/item?id=25186342">thread link</a>) | @amzans
<br/>
November 23, 2020 | https://panelbear.com/blog/tech-stack/ | <a href="https://web.archive.org/web/*/https://panelbear.com/blog/tech-stack/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Being an engineer at heart, each time I see a company write about their tech stack, I brew a fresh cup of coffee, sit back and enjoy reading the newfound little treat.</p><p>There’s just something fascinating about getting to know what’s under the hood of other people’s businesses. It’s like gossip, but about software.</p><p>A couple of months ago I started working on <a href="https://panelbear.com/blog/why-panelbear/" target="_blank" rel="noopener">yet another private analytics service</a>, a project which has gone through numerous iterations, and I feel lucky that 400+ websites have already integrated with it, even though it's still in the early stages.</p><p>That’s why, in the same spirit as Jake Lazaroff’s <a href="https://jake.nyc/words/tools-and-services-i-use-to-run-my-saas/" target="_blank" rel="noopener">Tools and Services I Use to Run My SaaS</a>, I thought it’s now my turn to do a short write up of the technologies I’m using to run this new service.</p><h2>Languages</h2><p>Over the years I have added many programming languages to my toolbelt, but for solo projects I have converged to two in particular that strike a good balance of productivity and reliability.</p><ul><li><p><a href="https://python.org/" target="_blank" rel="noopener">Python</a>: Most of the backend code is in Python. Which has enabled me to ship features incredibly fast. Additionally, I use <a href="http://mypy-lang.org/" target="_blank" rel="noopener">mypy</a> for optional type hints, which helps keep the codebase manageable.</p></li><li><p><a href="https://www.typescriptlang.org/" target="_blank" rel="noopener">Typescript</a>: I used to avoid working on the frontend as much as I could. That is until I discovered Typescript about 4 years ago. It just makes the whole experience a lot better, and I now use it for all my projects together with React.</p></li></ul><h2>Frameworks and libraries</h2><p>This list could have been huge, as I stand on the shoulders of giants who have published the vast amount of open-source code which I rely on. But I'd like to highlight only a handful due to their major role in the stack:</p><ul><li><a href="https://www.djangoproject.com/" target="_blank" rel="noopener">Django</a>: It's like a superpower for solo developers. The longer you work in this industry, the more you appreciate not having to reinvent the wheel for the 100th time. A monolithic framework can get you <a href="https://instagram-engineering.com/web-service-efficiency-at-instagram-with-python-4976d078e366" target="_blank" rel="noopener">really</a>, <a href="https://github.com/getsentry/sentry" target="_blank" rel="noopener">really</a> <a href="https://djangostars.com/blog/10-popular-sites-made-on-django/" target="_blank" rel="noopener">far</a>. To me, it's about predictable software that's fast in every way that matters. In case you're interested, I talk more about this topic on <a href="https://panelbear.com/blog/boring-tech/" target="_blank" rel="noopener">Choose Boring Technology</a>.</li><li><a href="https://reactjs.org/" target="_blank" rel="noopener">React</a>: The web app for the dashboards is built using React + Webpack. After using Angular for a long time, I switched to React because it's just a pluggable view layer that doesn't get in the way. I use the fantastic <a href="https://github.com/Frojd/django-react-templatetags" target="_blank" rel="noopener">django-react-templatetags</a> to embed the React components in my Django templates.</li><li><a href="https://nextjs.org/" target="_blank" rel="noopener">NextJS</a>: I use it for the landing pages, documentation and the blog which you are currently reading. It enables me to re-use various React components, and still reap the performance and SEO benefits of a statically generated site.</li><li><a href="https://docs.celeryproject.org/" target="_blank" rel="noopener">Celery</a>: I use it for any kind of background/scheduled tasks. It does have a learning curve for more advanced use-cases, but it's quite reliable once you understand how it works, and more importantly when it fails.</li><li><a href="https://getbootstrap.com/" target="_blank" rel="noopener">Bootstrap 4</a>: I built a custom theme on top of Bootstrap. It has saved me a lot of time, and there's lots of documentation around it. That's why I picked it.</li></ul><h2>Databases</h2><p>I originally stored all data in a single SQLite database, doing backups meant making a copy of this file to an object storage like S3. At the time, it was more than enough for the small sites I tested Panelbear with. But as I added more features and websites, I needed more specialized software to support those features:</p><ul><li><a href="https://clickhouse.tech/" target="_blank" rel="noopener">Clickhouse</a>: I believe this is one of those technologies that over time will become ubiquitous. It's honestly a fantastic piece of software that enabled me to build features that initially seemed impossible on low-cost hardware. I do intend to write a future blog post on some lessons learned from running Clickhouse on Kubernetes. So stay tuned!</li><li><a href="https://www.postgresql.org/" target="_blank" rel="noopener">PostgreSQL</a>: My go-to relational database. Sane defaults, battle-tested, and deeply integrated with Django. For Panelbear, I use it for all application data that is not analytics related. For the analytics data, I instead wrote a simple interface for querying Clickhouse within Django.</li><li><a href="https://redis.io/" target="_blank" rel="noopener">Redis</a>: I use it for many things: caching, rate-limiting, as a task queue, and as a key/value store with TTL for various features. Rock-solid, and great documentation.</li></ul><h2>Deployment</h2><p>I treat my infrastructure as <a href="https://joachim8675309.medium.com/devops-concepts-pets-vs-cattle-2380b5aab313" target="_blank" rel="noopener">cattle instead of pets</a>, things like servers and clusters are meant to come and go. So if one server gets "sick", I just replace it with another one. That means everything is described as code in a git repo, and I do not change things by SSH'ing into the servers. You can think of it like a template to clone my entire infrastructure with one command into any AWS region/environment.</p><p>This also helps me in case of disaster recovery. I just run a few commands, and some minutes later my stack has been re-created. This was particularly useful when I moved from DigitalOcean, to Linode, and recently to AWS. Everything is described in code, so it's easy to keep track of what components I own, even years later (all companies have some AWS IAM policy or VPC subnet lurking around which was created via clicky-clicky on the UI, and now everyone depends on it).</p><ul><li><a href="https://www.terraform.io/" target="_blank" rel="noopener">Terraform</a>: I manage most of my cloud infrastructure with Terraform. Things like EKS clusters, S3 buckets, roles, and RDS instances are declared in my Terraform manifests. The state is synced to an encrypted S3 bucket to avoid getting in trouble in case something happens to my development laptop.</li><li><a href="https://www.docker.com/" target="_blank" rel="noopener">Docker</a>: I build everything as Docker images. Even stateful components like Clickhouse or Redis are packaged and shipped as Docker containers to my cluster. It also makes my stack very portable, as I can run it anywhere I can run Docker.</li><li><a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a>: Allowed me to simplify the operational aspects tremendously. However, I wouldn’t bindly recommend it to everyone, as I already felt comfortable working with it after having the pleasure of putting down multiple production fires for my employer over the years. I also rely on managed offerings, which helps reduce the burden too.</li><li><a href="https://github.com/features/actions" target="_blank" rel="noopener">GitHub Actions</a>: Normally I’d use <a href="https://circleci.com/" target="_blank" rel="noopener">CircleCI</a> in the past (which is also great), but for this project I prefer to use GitHub Actions as it removes yet another service which needs to have access to my repositories, and deployment secrets. However, CircleCI has plenty of good features, and I still recommend it.</li></ul><h2>Infrastructure</h2><p>I started in a single $5/mo instance in DigitalOcean, then moved to the managed Kubernetes offering as I was reinventing the wheel for a lot of things Kubernetes already gives me out of the box (service discovery, TLS certs, load balancing, log rotation, rollout, scaling, fault-tolerance, among others).</p><p>Unfortunately, I had <a href="https://www.digitalocean.com/community/questions/kubernetes-unable-to-connect-to-the-server" target="_blank" rel="noopener">reliability issues</a> with DigitalOcean's Kubernetes offering, even on larger instances. The cluster API would often go down randomly and no longer recover, this disrupted a lot of cluster services including the load balancer, which translated into downtime for me. I had to create a new cluster each time this happened, and while Terraform made it trivial, this was not something that inspired a lot of confidence about their managed service. I suspect their control plane was underprovisioned, which would be kind of understandable given the price tag.</p><p>Unfortunately I was not able to resolve the issue after several weeks. That's why I decided to move to <a href="https://www.linode.com/" target="_blank" rel="noopener">Linode</a>, and had exactly 0 problems during the 1.5 month-long honeymoon that followed.</p><p>However, I recently moved once again, this time to AWS due to a pretty good deal I received. It also enabled me to use managed services like RDS to offload managing PostgreSQL, which is a big plus. What made all these migrations relatively easy, was that all my infrastructure was described via Terraform and Kubernetes manifests. The migrations essentially consisted of an evening, some tea, and patience. But that's for another post.</p><ul><li><a href="https://aws.amazon.com/" target="_blank" rel="noopener">AWS</a>: Predictable, and lots of managed services. However, I use it at my full-time job, so I didn't have to spend too much time figuring things out. The main services I use are EKS, ELB, S3, RDS, IAM and private VPCs. I might also add Cloudfront and Kinesis in the future.</li><li><a href="https://www.cloudflare.com/" target="_blank" rel="noopener">Cloudflare</a>: I mainly use it for DDoS protection, serving DNS, and offloading edge caching of various static assets (currently shaves off 80% of the egress charges from AWS - their bandwidth pricing is insane!).</li><li><a href="https://letsencrypt.org/" target="_blank" rel="noopener">Let’s Encrypt</a>: Free SSL certificate authority. I use cert-manager in my Kubernetes cluster to automatically issue and renew certificates based on my ingress rules.</li><li><a href="https://www.namecheap.com/" target="_blank" rel="noopener">Namecheap</a>: My domain name registrar of choice. Allows MFA for login which is an important security feature. Unlike other registrars, they haven't surprised me with an expensive renewal every few years. I like them.</li></ul><h2>Kubernetes components</h2><p>The following components automate most of the devops work for me. I use several others too, but some of the main ones I use are:</p><ul><li><a href="https://github.com/kubernetes/ingress-nginx/" target="_blank" rel="noopener">ingress-nginx</a>: Rock-solid ingress controller for Kubernetes using NGINX as a reverse proxy, and load balancer. Sits behind the NLB which controls ingress to the cluster nodes.</li><li><a href="https://github.com/jetstack/cert-manager" target="_blank" rel="noopener">cert-manager</a>: Automatically issue/renew TLS certs as defined in my ingress rules.</li><li><a href="https://github.com/kubernetes-sigs/external-dns" target="_blank" rel="noopener">external-dns</a>: Synchronizes exposed Kubernetes Services and Ingresses with DNS providers (such as Cloudflare).</li><li><a href="https://github.com/prometheus-operator/prometheus-operator" target="_blank" rel="noopener">prometheus-operator</a>: Automatically monitors most of my services, and exposes dashboards via Grafana.</li><li><a href="https://fluxcd.io/" target="_blank" rel="noopener">flux</a>: GitOps way to do continuous delivery in Kubernetes. Basically pulls and deploys new Docker images when I release them.</li></ul><h2>CLI tools</h2><p>There’s plenty here, but frequently used include:</p><ul><li><a href="https://kubernetes.io/" target="_blank" rel="noopener">kubectl</a>: To interact with the Kubernetes cluster to watch logs, pods and services, SSH into a running container, and so on.</li><li><a href="https://github.com/wercker/stern" target="_blank" rel="noopener">stern</a>: Multi pod log tailing for Kubernetes. Really handy.</li><li><a href="https://htop.dev/" target="_blank" rel="noopener">htop</a>: Interactive system process viewer. Better than “top” if you ask me.</li><li><a href="https://curl.se/" target="_blank" rel="noopener">cURL</a>: Issue HTTP requests locally, inspect headers.</li><li><a href="https://httpie.io/" target="_blank" rel="noopener">HTTPie</a>: Like cURL, but simpler for JSON APIs.</li><li><a href="https://github.com/rakyll/hey" target="_blank" rel="noopener">hey</a>: Load testing HTTP endpoints. Gives a nice latency distribution summary.</li></ul><h2>Monitoring</h2><ul><li><a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a>: Efficient storage of time series data for monitoring. Tracks all the cluster and app metrics. It was a lot cheaper than using Cloudwatch for app metrics.</li><li><a href="https://grafana.com/" target="_blank" rel="noopener">Grafana</a>: Nice dashboards for the Prometheus monitoring data. All dashboards are described in JSON files and versioned in the …</li></ul></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://panelbear.com/blog/tech-stack/">https://panelbear.com/blog/tech-stack/</a></em></p>]]>
            </description>
            <link>https://panelbear.com/blog/tech-stack/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25186342</guid>
            <pubDate>Mon, 23 Nov 2020 13:06:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Use Netlify Functions and the Twitter API v2 as a CMS for Your Gatsby Blog]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 21 (<a href="https://news.ycombinator.com/item?id=25186006">thread link</a>) | @pauliescanlon
<br/>
November 23, 2020 | https://paulie.dev/posts/2020/11/gatsby-netlify-twitter/ | <a href="https://web.archive.org/web/*/https://paulie.dev/posts/2020/11/gatsby-netlify-twitter/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="https://res.cloudinary.com/www-paulie-dev/image/upload/v1605613346/paulie.dev/2020/11/gatsby-netlify-twitterjpg_ok1k0q.jpg"></p><div><div><div><p>Date published: </p><!-- --><p>17-Nov-2020</p></div></div></div><hr><p>JavaScript</p><p>React</p><p>Gatsby</p><p>Netlify Functions</p><p>Twitter API v2</p><hr><p>Apologies in advance for the rather long-winded blog title but as it suggests in this post i'm going to explain how you can use <a href="https://www.netlify.com/products/functions/">Netlify Functions</a> to access your Twitter profile data using the <a href="https://developer.twitter.com/en/docs/twitter-api/early-access">Twitter v2 API</a> and display it on your Gatsby blog.</p><h2>A rather unique requirement</h2><p>This might be a specific to me but I wanted to solve a little problem I was having with my "digital footprint". As you can see I have this blog: <a href="https://paulie.dev/">https://paulie.dev</a> and a commercial portfolio: <a href="https://www.pauliescanlon.io/">https://www.pauliescanlon.io</a></p><p>Both sites are built on top of my Gatsby theme: <a href="https://gatsby-theme-terminal.netlify.app/">gatsby-theme-terminal</a> which is Open source and can be found on my <a href="https://github.com/PaulieScanlon/gatsby-theme-terminal">GitHub</a></p><p>Using a Gatsby Theme solves one of my issues as I'm able to have two sites that look and work pretty much the same way and any changes I make to the theme are inherited by both my sites. It's kind of like managing your own multi brand design system, but just for yourself.</p><p>There was one other problem though. 🤔</p><p>I wanted both sites to have the same "intro" section, but every time I made a change to one I had to make the same change to the other site to ensure they were both displaying the same intro text.</p><p>This might be fine if I weren't a developer but doing something twice is one time too many IMO.</p><p>It was also a little frustrating because I also wanted my Twitter profile description to be in sync with both the sites so, again another place to remember to update my personal blurb.</p><p>One option I considered would have been to hook up a Content Management System, and this would have been fine and it would have kept both my sites in sync but it wouldn't have been able to update my Twitter profile blurb...</p><p>So, I've decided to reverse engineer the Twitter API and use that as a CMS to populate both my sites. The idea is quite simple. I'll use the Twitter profile description as though it were a field from a CMS. Naturally any changes I make to this will appear on my Twitter profile and below is how I pull that same info into both of my sites.</p><h2>Demo</h2><p>Here's what I'll be showing you how to build:</p><ul><li>App / API <a href="https://gatsby-netlify-twitter.netlify.app/">https://gatsby-netlify-twitter.netlify.app</a></li><li>GitHub repo <a href="https://github.com/PaulieScanlon/gatsby-netlify-twitter">https://github.com/PaulieScanlon/gatsby-netlify-twitter</a></li></ul><p>... but the actual API I use for my blog and site is here: <a href="https://paulie-api.netlify.app/">https://paulie-api.netlify.app</a></p><h2>Tech</h2><h3>Netlify Functions</h3><p>"Power your site without managing servers" is how Netlify describe Functions and for all intents and purposes thats exactly what they are. Similar to how you might create an <a href="https://expressjs.com/">Express</a> app and deploy it somewhere but without the hassle of having to setup server side environments and more crucially any really dweeby server uptime monitoring.</p><h3>Twitter API v2</h3><p>A set of endpoints that can be used to get data from Twitter. Any Twitter requests must be done server side and use a set of keys and tokens. You can't unfortunately hit the Twitter API from the browser so we need a "server" or as mentioned above, a Netlify Function</p><p>Using both of the above i've made my own API endpoint which goes off and hits the Twitter API and returns my Profile information which I can then display in the intro section of my blog and site. I've deployed this API to Netlify and it's completely de-coupled from either of my sites but will return data which can be fetched from client side "fetch" request from within my site and blog. That url again is here: <a href="https://paulie-api.netlify.app/">https://paulie-api.netlify.app</a></p><h2>Before we start</h2><p>Before we get started there's a couple of things you'll need to have in place.</p><h3>Twitter API v2</h3><p>Apply for access to the <a href="https://developer.twitter.com/en/products/twitter-api">Twitter API</a>. This is quite a lengthy process so strap in and also bookmark this post as it might take a few days for Twitter to accept your application.</p><p>Once you have access you can head over to the <a href="https://developer.twitter.com/en/portal/dashboard">Developer Portal</a> and create a new project, and within the project you can create an "app", I called mine "paulie-api".</p><p>In here you'll find all the API keys and tokens required to access the Twitter API. Make a note of them somewhere as we'll be using them later.</p><h3>Netlify CLI</h3><p>To run Netlify Functions we'll be using <code>netlify dev</code> rather than <code>gatsby develop</code> or <code>yarn develop</code> so you'll need to install the <a href="https://docs.netlify.com/cli/get-started/">Netlify CLI</a></p><h2>The Build</h2><p>In order to develop you own API I found it easiest to have some kind of "site" running at the same time which will access the API endpoint and render the response on the page. In the demo repo you'll see i've set up a really simple Gatsby Site with one page that uses "fetch" to, er fetch and then render the data.</p><p>I've used <a href="https://theme-ui.com/home">Theme UI</a> for the style but naturally you can choose whatever you like to do this.</p><p>Whether you're starting from scratch or adding Netlify Functions to an existing project you'll need to start by adding a <code>functions</code> dir to the root of your project.</p><hr><pre><p><span>|</span><span>-- functions</span></p><p><span>  </span><span>|</span><span>-- package.json</span></p><p><span></span><span>|</span><span>-- src</span></p><p><span>package.json</span></p></pre><hr><p><code>functions</code> is kind of it's own application so it'll need it's own <code>package.json</code> and will have one dependency on <a href="https://github.com/HunterLarco/twitter-v2">twitter-v2</a></p><hr><pre><p><span></span><span>{</span><span></span></p><p><span>  </span><span>"name"</span><span>:</span><span> </span><span>"gatsby-netlify-twitter-api"</span><span>,</span><span></span></p><p><span>  </span><span>"version"</span><span>:</span><span> </span><span>"1.0.0"</span><span>,</span><span></span></p><p><span>  </span><span>"description"</span><span>:</span><span> </span><span>"An api for the Twitter v2 api"</span><span>,</span><span></span></p><p><span>  </span><span>"main"</span><span>:</span><span> </span><span>"index.js"</span><span>,</span><span></span></p><p><span>  </span><span>"scripts"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"test"</span><span>:</span><span> </span><span>"echo \"Error: no test specified\" &amp;&amp; exit 1"</span><span></span></p><p><span>  </span><span>}</span><span>,</span><span></span></p><p><span>  </span><span>"keywords"</span><span>:</span><span> </span><span>[</span><span>]</span><span>,</span><span></span></p><p><span>  </span><span>"author"</span><span>:</span><span> </span><span>""</span><span>,</span><span></span></p><p><span>  </span><span>"license"</span><span>:</span><span> </span><span>"ISC"</span><span>,</span><span></span></p><p><span>  </span><span>"dependencies"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"twitter-v2"</span><span>:</span><span> </span><span>"^0.1.2"</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span></span><span>}</span></p></pre><hr><p>Next have a look at <a href="https://github.com/PaulieScanlon/gatsby-netlify-twitter/blob/main/.env.example">.env.example</a>. You'll need to create your own <code>.env</code> file and add the environment variables as seen in the <code>.env.example</code>. Naturally you'll want to change the <code>GATSBY_TWITTER_USERNAME</code> to your own Twitter username and the Twitter keys and tokens will be what I referenced earlier which are provided by the Twitter Developer Portal</p><hr><pre><p><span></span><span>GATSBY_API_URL</span><span>=</span><span>.</span><span>/</span><span>.</span><span>netlify</span><span>/</span><span>functions</span></p><p><span></span><span>GATSBY_TWITTER_USERNAME</span><span>=</span><span></span></p><p><span></span><span>TWITTER_API_KEY</span><span>=</span><span></span></p><p><span></span><span>TWITTER_API_KEY_SECRET</span><span>=</span><span></span></p><p><span></span><span>TWITTER_ACCESS_TOKEN</span><span>=</span><span></span></p><p><span></span><span>TWITTER_ACCESS_TOKEN_SECRET</span><span>=</span></p></pre><hr><p>Next create a Twitter client, this is what we'll use to pass the keys and tokens onto the Twitter API when we make a request</p><hr><pre><p><span></span><span>const</span><span> </span><span>Twitter</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>"twitter-v2"</span><span>)</span><span></span></p><p><span>module</span><span>.</span><span>exports</span><span> </span><span>=</span><span> </span><span>{</span><span></span></p><p><span>  client</span><span>:</span><span> </span><span>new</span><span> </span><span>Twitter</span><span>(</span><span>{</span><span></span></p><p><span>    consumer_key</span><span>:</span><span> process</span><span>.</span><span>env</span><span>.</span><span>TWITTER_CONSUMER_KEY</span><span>,</span><span></span></p><p><span>    consumer_secret</span><span>:</span><span> process</span><span>.</span><span>env</span><span>.</span><span>TWITTER_CONSUMER_KEY_SECRET</span><span>,</span><span></span></p><p><span>    access_token</span><span>:</span><span> process</span><span>.</span><span>env</span><span>.</span><span>TWITTER_ACCESS_TOKEN</span><span>,</span><span></span></p><p><span>    access_token_secret</span><span>:</span><span> process</span><span>.</span><span>env</span><span>.</span><span>TWITTER_ACCESS_TOKEN_SECRET</span><span>,</span><span></span></p><p><span>  </span><span>}</span><span>)</span><span>,</span><span></span></p><p><span></span><span>}</span></p></pre><hr><p>You should now be looking at something similar to the below</p><hr><pre><p><span>..</span><span>.</span></p><p><span></span><span>|</span><span>-- functions</span></p><p><span>  </span><span>|</span><span>-- client.js</span></p><p><span>  </span><span>|</span><span>-- package.json</span></p><p><span></span><span>|</span><span>-- src</span></p><p><span>package.json</span></p><p><span>.env</span></p><p><span></span><span>..</span><span>.</span></p></pre><hr><p>Now we need to create the "endpoint" that our frontend will hit, which in turn goes off and grabs the data from the Twitter API.</p><p>I created a dir called <code>twitter-user</code> and inside I create a new file and called it <code>twitter-user.js</code></p><hr><pre><p><span>..</span><span>.</span></p><p><span></span><span>|</span><span>-- functions</span></p><p><span>  </span><span>|</span><span>-- client.js</span></p><p><span>  </span><span>|</span><span>-- twitter-user</span></p><p><span>    </span><span>|</span><span>-- twitter-user.js</span></p><p><span>  </span><span>|</span><span>-- package.json</span></p><p><span></span><span>|</span><span>-- src</span></p><p><span>package.json</span></p><p><span>.env</span></p><p><span></span><span>..</span><span>.</span></p></pre><hr><p>It's in here where we can use the <code>client.js</code> to hit a Twitter API endpoint and pass with it the required keys and tokens from the <code>client</code></p><hr><pre><p><span></span><span>const</span><span> </span><span>{</span><span> client </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>"../client"</span><span>)</span><span></span></p><p><span>exports</span><span>.</span><span>handler</span><span> </span><span>=</span><span> </span><span>async</span><span> </span><span>(</span><span>event</span><span>,</span><span> context</span><span>,</span><span> callback</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> </span><span>{</span><span> data </span><span>}</span><span> </span><span>=</span><span> </span><span>await</span><span> client</span><span>.</span><span>get</span><span>(</span><span></span></p><p><span>    </span><span>`</span><span>users/by/username/</span><span>${</span><span>process</span><span>.</span><span>env</span><span>.</span><span>GATSBY_TWITTER_USERNAME</span><span>}</span><span>`</span><span>,</span><span></span></p><p><span>    </span><span>{</span><span></span></p><p><span>      user</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        fields</span><span>:</span><span></span></p><p><span>          </span><span>"created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld"</span><span>,</span><span></span></p><p><span>      </span><span>}</span><span>,</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>)</span><span></span></p><p><span>  </span><span>callback</span><span>(</span><span>null</span><span>,</span><span> </span><span>{</span><span></span></p><p><span>    headers</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>"Access-Control-Allow-Origin"</span><span>:</span><span> </span><span>"*"</span><span>,</span><span></span></p><p><span>    </span><span>}</span><span>,</span><span></span></p><p><span>    statusCode</span><span>:</span><span> </span><span>200</span><span>,</span><span></span></p><p><span>    body</span><span>:</span><span> </span><span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>{</span><span> user</span><span>:</span><span> data </span><span>}</span><span>)</span><span>,</span><span></span></p><p><span>  </span><span>}</span><span>)</span><span></span></p><p><span></span><span>}</span></p></pre><hr><p>In the above you can see we use our <code>client</code> to hit the <code>users/by/username</code> Twitter API endpoint which you can read more about <a href="https://developer.twitter.com/en/docs/twitter-api/users/lookup/introduction">here</a>, which returns a <code>data</code> object which I pass on to the callback body as <code>{ user: data }</code></p><p>This is the object that'll we receive in our frontend</p><p>The next bit will greatly depend on how you've set up your frontend but in the <a href="https://github.com/PaulieScanlon/gatsby-netlify-twitter/blob/main/src/pages/index.js">Demo</a> I have one <code>page</code> called <code>index.js</code> which uses a <code>useEffect</code> to "fetch" the data from the Netlify Function.</p><p>The example file contains a few extra bits for <code>isLoading</code> and <code>hasError</code> but the below should be enough to allow you hit to the Netlify Function which in turn hits the Twitter API and returns your profile information data.</p><hr><pre><p><span></span><span>import</span><span> </span><span>React</span><span>,</span><span> </span><span>{</span><span> useState </span><span>}</span><span> </span><span>from</span><span> </span><span>"react"</span><span></span></p><p><span></span><span>const</span><span> </span><span>IndexPage</span><span> </span><span>=</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> </span><span>[</span><span>response</span><span>,</span><span> setResponse</span><span>]</span><span> </span><span>=</span><span> </span><span>useState</span><span>(</span><span>{</span><span> user</span><span>:</span><span> </span><span>null</span><span> </span><span>}</span><span>)</span><span></span></p><p><span>  </span><span>useEffect</span><span>(</span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>fetch</span><span>(</span><span>`</span><span>${</span><span>process</span><span>.</span><span>env</span><span>.</span><span>GATSBY_API_URL</span><span>}</span><span>/twitter-user</span><span>`</span><span>)</span><span></span></p><p><span>      </span><span>.</span><span>then</span><span>(</span><span>(</span><span>response</span><span>)</span><span> </span><span>=&gt;</span><span> response</span><span>.</span><span>text</span><span>(</span><span>)</span><span>)</span><span></span></p><p><span>      </span><span>.</span><span>then</span><span>(</span><span>(</span><span>response</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>console</span><span>.</span><span>log</span><span>(</span><span>JSON</span><span>.</span><span>parse</span><span>(</span><span>response</span><span>)</span><span>)</span><span></span></p><p><span>        </span><span>setResponse</span><span>(</span><span>JSON</span><span>.</span><span>parse</span><span>(</span><span>response</span><span>)</span><span>)</span><span></span></p><p><span>      </span><span>}</span><span>)</span><span></span></p><p><span>      </span><span>.</span><span>catch</span><span>(</span><span>(</span><span>error</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>console</span><span>.</span><span>error</span><span>(</span><span>{</span><span> error </span><span>}</span><span>)</span><span></span></p><p><span>      </span><span>}</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span>,</span><span> </span><span>[</span><span>]</span><span>)</span><span></span></p><p><span>  </span><span>const</span><span> </span><span>{</span><span> user </span><span>}</span><span> </span><span>=</span><span> response</span></p><p><span>  </span><span>return</span><span> </span><span>(</span><span></span></p><p><span>    </span><span>&lt;</span><span>pre</span><span>&gt;</span><span></span></p><p><span>      </span><span>&lt;</span><span>code</span><span>&gt;</span><span>{</span><span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>user</span><span>,</span><span> </span><span>null</span><span>,</span><span> </span><span>2</span><span>)</span><span>}</span><span>&lt;</span><span>/</span><span>code</span><span>&gt;</span><span></span></p><p><span>    </span><span>&lt;</span><span>/</span><span>pre</span><span>&gt;</span><span></span></p><p><span>  </span><span>)</span><span></span></p><p><span></span><span>}</span><span></span></p><p><span></span><span>export</span><span> </span><span>default</span><span> </span><span>IndexPage</span></p></pre><hr><p><code>process.env.GATSBY_API_URL</code> is the path to the Netlify Function we added earlier to <code>.env</code> and i've hard-coded <code>/twitter-user</code> in the component / page as you might want to create different endpoints that return different data on different pages.</p><p>You might be wondering why this environment variable is prefixed with <code>GATSBY_</code>. This is so Gatsby can access it from the frontend. You can read more about Gatsby environment variables <a href="https://www.gatsbyjs.com/docs/environment-variables/#client-side-javascript">here</a></p><h3>IMPORTANT</h3><p>In order for Netlify Functions to work both locally and when deployed we need to ensure we've got <code>netlify-lambda</code> installed and have added both a <code>"start"</code> and <code>"postinstall"</code> script to the root <code>package.json</code> (not the <code>package.json</code> in <code>./functions</code>)</p><hr><pre><p><span>npm</span><span> </span><span>install</span><span> netlify-lambda --save -dev</span></p></pre><hr><pre><p><span>// ./package.json</span></p><p><span>...</span></p><p><span></span><span>  "scripts": {</span></p><p><span>    "develop": "gatsby develop",</span></p><p><span>    "build": "gatsby build",</span></p><p><span>    "clean": "gatsby clean",</span></p><p><span>    "serve": "gatsby serve",</span></p><p><span></span><span>+    "start": "npm run develop",</span></p><p><span>+    "postinstall": "netlify-lambda install"</span></p><p><span></span><span>  },</span></p><p><span>   "devDependencies": {</span></p><p><span></span><span>+   "netlify-lambda": "^1.6.3",</span></p><p><span></span><span>  }</span></p><p><span></span><span>...</span></p></pre><hr><p>Before we get too carried away, it's important to note that we'll no longer be using <code>gatsby develop</code> or <code>yarn develop</code> to start the Gatsby app, if you do that our Netlify Function won't be running and you'll get an error.</p><p>Instead, run <code>netlify dev</code> this is so both the Gatsby site and the Netlify Function are run at the same time.</p><p>Instead of visiting the usual <code>http://localhost:8000/</code> we'll now be visiting <code>http://localhost:8888/</code></p><p>And to ensure when we deploy everything works as it should you'll need to modify your <a href="https://github.com/PaulieScanlon/gatsby-netlify-twitter/blob/main/netlify.toml"><code>netlify.toml</code></a></p><p>For …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://paulie.dev/posts/2020/11/gatsby-netlify-twitter/">https://paulie.dev/posts/2020/11/gatsby-netlify-twitter/</a></em></p>]]>
            </description>
            <link>https://paulie.dev/posts/2020/11/gatsby-netlify-twitter/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25186006</guid>
            <pubDate>Mon, 23 Nov 2020 12:27:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are pixie fairies behind Bitcoin's latest bubble?]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 92 (<a href="https://news.ycombinator.com/item?id=25180563">thread link</a>) | @amycastor
<br/>
November 22, 2020 | https://amycastor.com/2020/11/21/are-pixie-fairies-behind-bitcoins-latest-bubble/ | <a href="https://web.archive.org/web/*/https://amycastor.com/2020/11/21/are-pixie-fairies-behind-bitcoins-latest-bubble/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4780">
		<div>
		
<p>Are the pixie fairies sprinkling gold dust on bitcoin’s market again? By the looks of things, you might think so. </p>



<p>Like in the bubble days of 2017, the price of bitcoin is headed ever upward. On Wednesday morning, it surpassed $18,000 — a number not seen since December 2017 when bitcoin, at its all-time peak, scratched $20,000.</p>



<p>Of course, the market <a href="https://www.bloomberg.com/news/articles/2018-09-12/crypto-s-crash-just-surpassed-dot-com-levels-as-losses-reach-80">crashed spectacularly</a> the following year, and retailers lost their shirts. But here we are once again, trying to unravel the mysteries of bitcoin’s latest price movements. </p>



<p>Several factors may explain it — Tether, PayPal, and China’s crackdown on over-the-counter desks — but before we get into that, let me reiterate how critical it is for bitcoin’s price to stay at or above a certain <em>magic number</em>.&nbsp;</p>



<p>Bitcoin miners — those responsible for securing the bitcoin network by “mining” the next block of transactions on the blockchain — need to sell their newly minted bitcoins for real money, so they can pay their <a href="https://news.bitcoin.com/the-bitcoin-network-now-consumes-7-nuclear-plants-worth-of-power/#:~:text=Today%2C%20the%20CBECI%20says%20the,terawatt%2Dhours%20of%20energy%20consumption.">massive energy bills.</a>&nbsp;&nbsp;</p>



<p>Roughly $8 million to $10 million in cash gets sucked out of the bitcoin ecosystem this way every day. So, in order for the miners — the majority of whom are in China — to turn a profit, bitcoin needs to be priced accordingly. Otherwise, if too many miners were to decide to call it quits and unplug from the network all at once, that would leave bitcoin vulnerable to attacks. The entire system, and its current $345 billion market cap, literally depends on keeping the miners happy.</p>



<p>Now let’s jump to May 11, an important day for bitcoin. That was the day of the “halvening,” an event hardwired into bitcoin’s code where the block reward gets slashed in half. A halvening occurs once every four years.</p>



<p>Before May 11, miners received 1,800 bitcoin a day in the form of block rewards, which meant they needed to cash in each bitcoin for $5,000. But <em>after</em> the halvening, the network would produce only 900 bitcoins per day, so miners knew they needed to sell each precious bitcoin for at least $10,000.&nbsp;&nbsp;</p>



<p>But trouble loomed. Just months before the halvening, the price of bitcoin went into free fall. Between February and March, when the world was first gripped by the COVID crisis, bitcoin lost half its value, sliding to $5,000 — barely enough to pay the system’s energy costs post-halvening. Miners were likely pacing, wringing their hands, wondering how they would stay in business. Who would guarantee their profits?</p>



<p>That is when Tether — a company that produces a dollar-pegged stablecoin of the same name — sprung into action and started issuing tethers in amounts far greater than it ever had before in its five years of existence.</p>



<p>Tethers, for the uninitiated, are the main source of liquidity for unbanked crypto exchanges, which account for most of bitcoin’s trading volume. Currently, there are $18 billion (notional value) worth of tethers sloshing around in the crypto markets. And nobody is quite sure what’s backing them.</p>



<p>Due to Tether’s lack of transparency, its failure to provide a long promised audit, and the fact that the New York Attorney General is <a href="https://www.wsj.com/articles/bitfinex-used-tether-reserves-to-mask-missing-850-million-probe-finds-11556227031">currently probing</a> the firm along with Tether’s sister company, crypto exchange Bitfinex, for fraud, a good guess is nothing. Tethers, many suspect, are being minted out of thin air.&nbsp;</p>



<p>(Tethers were initially promised as an IOU where one tether was supposed to represent a redeemable dollar. But that was long before the British Virgin Island-registered firm began issuing tethers in massive quantities. And no tethers, to anyone’s knowledge, have ever been redeemed—except for when Tether <a href="https://www.coindesk.com/tether-just-burned-500-million-usdt-stablecoin-tokens">burned 500 million tethers</a> in October 2018, following the <a href="https://amycastor.com/2019/04/26/new-york-attorney-general-bitfinex-is-hiding-850-million-in-losses/">seizure of $850 million</a> from its payment processor Crypto Capital.)</p>



<p>According to data from <a href="https://amycastor.com/2020/11/21/are-pixie-fairies-behind-bitcoins-latest-bubble/blank">Nomics</a>, at the beginning of 2020, there were only $4.3 billion worth of tethers in circulation. That number remained stable through January and February and into March. But starting on March 18, just five days after bitcoin dipped below $5,000, the tether printer kicked in.</p>



<figure><img src="https://lh4.googleusercontent.com/wquSWQQ2kEfLCnshYnCiE7RZY2tuh9JtkWwPvnSsImvegfwkMzdhboJKFhSdpxqc5CtbcOh-5xqaro5F5DQAupoThjDcw6DYUf9wQTPBgfSMOV2TObswcJbuZoiOi2z46e9AR4Wg" alt=""><figcaption><em>BTC price and USDT supply. </em><a href="https://nomics.com/"><em>Image: Nomics.com</em></a></figcaption></figure>



<p>Tether minted $1.9 billion worth of tethers in March, and another $1.5 billion worth in April — crypto’s own version of an economic stimulus package. The price of bitcoin rose in tandem back up to $10,000, just in time for the halvening. Yet the Tether printer kept printing, pushing the price of bitcoin ever skyward and giving bag holders an opportunity to cash out.&nbsp;</p>



<p>In May, June and July, Tether issued a combined total of $6 billion in tethers. In August, when the price of bitcoin reached $12,000, it spun out $2.5 billion in tethers. And in September, when BTC slid to $10,000, Tether infused the markets with another $2 billion in tethers, although, even that couldn’t lift bitcoin up to $12,000 again. It just hovered in the $10,000 range.&nbsp;</p>



<p>And then in October — just after US prosecutors <a href="https://www.justice.gov/usao-sdny/pr/founders-and-executives-shore-cryptocurrency-derivatives-exchange-charged-violation">charged the founders of BitMEX,</a> a Seychelles-registered, Hong Kong-based bitcoin derivatives exchange, for failing to maintain an adequate anti-money laundering program — the price of BTC started to soar. What happened?</p>



<h2><strong>Tether’s frenzied pumping</strong></h2>



<p>One theory is that Tether just kept issuing tethers, billions and billions of them, and those tethers were used to buy up bitcoin. A high demand drives up the price — even if it’s fake money.&nbsp;</p>



<p>Only unlike in 2017, the effort to drive up bitcoin’s price is requiring a lot more tethers than ever before. (At the end of 2017, before the last bitcoin bubble popped, there were only $1.3 billion worth of tethers in circulation, a fraction of what there are today.)</p>



<p>Nicholas Weaver, a bitcoin skeptic and a researcher at the International Computer Science Institute in Berkeley, is convinced&nbsp;bitcoin’s latest price moves are 100% synthetic.</p>



<p>“The amount of tether flooding into the system is more than enough explanation for the price as it is well more than the amount needed to buy up all the newly minted bitcoin,” he told me. “If it was organic, there would at least be some significant increase in the outstanding amount of non-fraudulent stablecoins.”</p>



<p>What he means is, if real money was behind tether, we’d be seeing a similar demand for regulated stablecoins. But that is not the case. Only one regulated stablecoin has seen substantial growth — <a href="https://www.theblockcrypto.com/linked/81422/stablecoin-supply-has-surged-past-20-billion-driven-by-derivatives-market">Circle’s USDC</a> — but that growth is far overshadowed by Tether, and mainly a result of the growing decentralized finance (DeFi) market — a topic for another time.</p>



<p>Jorge Stolfi, a professor of computer science at the State University of Campinas in Brazil, who in 2016 wrote a <a href="https://www.sec.gov/comments/sr-batsbzx-2016-30/batsbzx201630-2.htm">letter to the SEC</a> advising about the risks of a bitcoin ETF, which the SEC published, agrees.</p>



<p>“As long as fake money can be used to buy BTC, the price can be pumped to whatever levels to keep the miners happy,” he told me. He went on to <a href="https://twitter.com/JorgeStolfi/status/1329952095286472711">explain</a> in a Twitter thread that the higher the bitcoin price, the faster real money flows out of the system — assuming miners sell <em>all</em> their bitcoin for cash. Multiply bitcoin’s current price of $18,600 times 900, and that’s nearly $17 million a day. Investors will never get that money back, he said.</p>



<p>Klyith (not his real name) from Something Awful, a predecessor site to 4Chan, <a href="https://forums.somethingawful.com/showthread.php?noseen=0&amp;threadid=3838405&amp;perpage=40&amp;pagenumber=797#post505143737">explains Tether</a> this way:</p>



<p>“A bunch of pixies show up and start flooding the parchment market with fairy gold, driving prices to amazing new heights. But when any of the player characters try to spend the fairy gold in other towns or to pay tithes to the king, it turns into worthless rocks.</p>



<div><figure><a href="https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg"><img loading="lazy" data-attachment-id="4784" data-permalink="https://amycastor.com/fairy/" data-orig-file="https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg" data-orig-size="564,775" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fairy" data-image-description="" data-medium-file="https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg?w=218" data-large-file="https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg?w=564" src="https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg?w=564" alt="" width="291" height="400" srcset="https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg?w=291 291w, https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg?w=109 109w, https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg?w=218 218w, https://amyhcastor.files.wordpress.com/2020/11/fairy.jpg 564w" sizes="(max-width: 291px) 100vw, 291px"></a></figure></div>



<p>“If you denounce the pixies to the peasants or start using dispel magic to reveal that fairy gold is rocks, the price of parchments will collapse and the peasants may stop using them altogether. But if you ignore the pixies and keep the parchment economy going, you will end up with more and more worthless rocks instead of gold. The pixies can of course tell the difference between fairy gold and real gold at a glance. So they will quickly drain all the real gold from the whole township if you don’t act. What do you do?”</p>



<p>Still, it is hard to imagine that outside events don’t have some impact on bitcoin’s price. Two other events are being talked about right now as reasons behind bitcoin’s price gains—and they are getting a lot more media attention than Tether.</p>



<h2><strong>PayPal’s shilling</strong></h2>



<p>One of the biggest companies in the world is now <a href="https://www.ft.com/content/826eedac-b0cd-4591-897a-9e83cf697060">promoting crypto</a>, giving retail buyers the impression that bitcoin is a safe investment. After all, if bitcoin were a Ponzi or a scam, why would such a well-known, respectable company embrace it? I should add that <a href="https://www.coindesk.com/microstrategy-ceo-bitcoin-better-than-antiquated-gold">MicroStrategy</a>, <a href="https://squareup.com/us/en/press/2020-bitcoin-investment">Square</a>, <a href="https://decrypt.co/48252/wall-streets-fidelity-mounts-defense-for-bitcoin">Fidelity Investment</a> and Mexico’s third-richest person, <a href="https://www.bloomberg.com/news/articles/2020-11-18/billionaire-salinas-has-10-of-liquid-portfolio-in-bitcoin">Ricardo Salinas Pliego</a>, are also currently shilling bitcoin on the internet. </p>



<p>On Oct. 21, PayPal <a href="https://newsroom.paypal-corp.com/2020-10-21-PayPal-Launches-New-Service-Enabling-Users-to-Buy-Hold-and-Sell-Cryptocurrency">announced a new service</a> for its users to buy and sell crypto for cash. And on Nov. 12, the service <a href="https://techcrunch.com/2020/11/12/paypal-says-all-users-in-u-s-can-now-buy-hold-and-sell-cryptocurrencies/">became available to U.S. customers</a>, who can now buy and sell bitcoin, bitcoin cash, ether, and litecoin via their PayPal wallet.&nbsp;</p>



<p>If you are a PayPal user, you have already gone through the process of proving you are who you say you are. And that removes the hassle of having to sign up with an crypto exchange, like Coinbase in the U.S., and take selfies of yourself holding up your driver’s license or passport.</p>



<p>Of course, there are limitations. You can’t transfer crypto into or out of your wallet, like you can on a centralized exchange. But you can pay PayPal’s 26 million merchants with crypto — although, not really, because what they receive on their end is cash. And the transaction is subject to high fees, like <a href="https://techcrunch.com/2020/10/21/paypal-to-let-you-buy-and-sell-cryptocurrencies-in-the-us/">2.3% for anything under $100</a>, so what is the point? All you are doing is taking out a bet against PayPal that the price of bitcoin is going to rise.&nbsp;</p>



<p>Stolfi describes PayPal <a href="https://twitter.com/JorgeStolfi/status/1330207860484153347">on Twitter</a> as “a meta-casino where you can choose to use special in-house chips with a randomly variable value.”</p>



<p>The broader point is that PayPal makes it easy to buy crypto for people who are less likely to understand how crypto really works or know about …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://amycastor.com/2020/11/21/are-pixie-fairies-behind-bitcoins-latest-bubble/">https://amycastor.com/2020/11/21/are-pixie-fairies-behind-bitcoins-latest-bubble/</a></em></p>]]>
            </description>
            <link>https://amycastor.com/2020/11/21/are-pixie-fairies-behind-bitcoins-latest-bubble/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25180563</guid>
            <pubDate>Sun, 22 Nov 2020 20:03:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mark Zuckerberg's Ponzi Scheme (2019)]]>
            </title>
            <description>
<![CDATA[
Score 51 | Comments 22 (<a href="https://news.ycombinator.com/item?id=25180420">thread link</a>) | @annadane
<br/>
November 22, 2020 | http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/ | <a href="https://web.archive.org/web/*/http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>
Mark Zuckerberg's Ponzi Scheme<br>
<span color="#A0A0A0">Congress and the FTC brought a knife to a gun fight.</span></p><div>

<p>It's already a campaign issue for the next presidential election: should we, or should we not, break up the big tech companies? Elizabeth Warren says yes. Beto O'Rourke wants "stronger regulations."  Kamala Harris would rather talk about privacy. Everyone elseâ€”even Donald Trumpâ€”generally agrees that something needs to be done. But what?</p><p>There are plenty of law professors, think tanks and political consultants eager to share their ideas, but none of them are asking the right questions. In the case of Facebook, distractions are understandable when the company arguably has the worst track record of any major technology company in history (and will soon pay a record, if toothless, $5 billion fine). Yet the unspoken issue at the center of it all remains: although Wall Street, Congress and the Federal Trade Commission haven't figured it out, Mark is running a Ponzi scheme.</p><p>At this point it should come as no surprise to anyone paying attention that Mark is a bad-faith actor. He has no appreciation for the rule of law, or the role of a free press, and he has a dangerous tendency to view himself as infallible. After discovering a gaping security flaw in his product that revealed bulk information about friends of friends, exactly like Cambridge Analytica, I warned Mark in writing about the way his sloppy code would inevitably lead him to cross paths with the FTC and cause massive privacy and security concernsâ€”in <a href="http://www.thinkpress.com/authoritas/timeline.pdf" target="_new">April 2005</a>. His response: problems with the "Mark Zuckerberg production" were actually someone else's responsibility and "not worth arguing about."</p><p>Clearly, Mark can no longer argue that his decisions as Facebook's CEO are immaterial (though he has <a href="https://www.vox.com/2016/11/11/13596792/facebook-fake-news-mark-zuckerberg-donald-trump" target="_new">tried</a>). Many have already lost their lives, whether through avoidable suicides or avoidable genocidal acts in Myanmar, due to his string of increasingly tone-deaf and spectacularly dishonest decisions. Now, fifteen years and approximately as many false apologizes after my classmate started a grand social experiment that first captivated the media, then locked it in a profitless box, and then played a major supporting role in bringing fascism to America, the general consensus is that the best way to handle Mark and his tech brethren is through the Sherman Anti-Trust Act. But the consensus is wrong, based on a mountain of misapprehensions.</p><p>In a nutshell, the argument in favor of anti-trust action is that in the midst of the longest economic expansion in U.S. history, it's the Progressive Era all over again. A recent New York Times op-ed penned by Mark's former roommate and co-founder, Chris Hughes, made essentially this point, relying heavily on input from the Roosevelt Institute. The Open Markets Institute agrees. In a <a href="https://www.youtube.com/watch?v=xM9GMGDsKUU&amp;t=24m40s" target="_new">talk at Harvard Law School</a>, Matt Stoller argued that Facebook, Google and Amazon were "born as monopolists."</p><p>It's a compelling story, so long as one is willing to ignore the reality on the ground. For one thing, software products are not railroads, which require significant physical capital and labor to establish. Were he determined to do so, it would take Mark a few weeks to re-build Instagram and WhatsApp, and there really isn't any way the government could stop him. For another, I know that on this particular issue, Stoller is incorrect, because I was there when The Facebook was born on my hard drive on September 19, 2003, in Lowell House. It hardly resembled a monopoly. Monopolies are what happen as the result of prolonged neglect by law enforcement. They're not born; they're nourished by years and years of perverse incentives.</p><p>The biggest problem with treating Facebook as a monopoly, as opposed to the byproduct of what Jesse Eisenger calls "The Chickenshit Club," is that it wrongly affirms Mark's infallibility and fails to see through him and his scheme, let alone the reality that he's not even in control anymore because no one is. On October 26, 2012, Mark's friend and lieutenant, Sam Lessin, wrote, "we are running out of humans (and have run-out of valuable humans from an advertiser perspective)."  At the time, it was far from clear that Facebook even had a viable business model, and according to Frontline, Sheryl Sandberg was panicking due to the company's poor revenue numbers.</p><p>How times have changed; now there's a different source of panic. Facebook now has a market capitalization approaching $600 billion, making it nominally one of the most valuable companies on earth. It's a true business miracle: a company that was out of users in 2012 managed to find a wellspring of nearly infinite and sustained growth that has lasted it, so far, half of the way through 2019. So what is that magical ingredient, that secret sauce, that "genius" trade secret, that turned an over-funded money-losing startup into one of America's greatest business success stories? It's one that Bernie Madoff would recognize instantly: fraud, in the form of fake accounts.</p><p>Old money goes out, and new money comes in to replace it. That's how a traditional Ponzi scheme works. Madoff kept his going for decades, managing to attain the rank of Chairman of the NASDAQ while he was at it. Zuckerberg's version is slightly different, but only slightly: old users leave after getting bored, disgusted and distrustful, and new users come in to replace them. Except that as Sam Lessin told us, the "new users" part of the equation was already getting to be a problem in 2012. To balance it out and keep "growth" on the rise, all Facebook had to do was turn a blind eye. And did it ever.</p><p>In <a href="https://www.plainsite.org/dockets/3bvv82ier/california-northern-district-court/singer-v-facebook-inc/" target="_new"><i>Singer v. Facebook, Inc.</i></a>â€”a lawsuit filed in the Northern District of California alleging that Facebook has been telling advertisers that it can "reach" more people than actually exist in basically every major metropolitan areaâ€”the plaintiffs quote former Facebook employees, understandably identified only as Confidential Witnesses, as stating that Facebook's "Potential Reach" statistic was a "made-up PR number" and "fluff."  Also, that "those who were responsible for ensuring the accuracy â€˜did not give a shit.'"  Another individual, "a former Operations Contractor with Facebook, stated that Facebook was not concerned with stopping duplicate or fake accounts."</p><p>That's probably because according to its <a href="https://s21.q4cdn.com/399680738/files/doc_financials/2019/Q1/Q1-2019-Earnings-Presentation.pdf" target="_new">last investor slide deck</a> and basic subtraction, Facebook is not growing anymore in the United States, with zero million new accounts in Q1 2019, and only four million new accounts since Q1 2017. That leaves the rest of the world, where Facebook is growing fastest "in India, Indonesia, and the Philippines," according to Facebook CFO David Wehner. Wehner didn't mention the fine print on page 18 of the slide deck, which highlights the Philippines, Indonesia and Vietnam as countries where there are "meaningfully higher" percentages of, and "episodic spikes" in, fake accounts. In other words, Facebook is growing the fastest in the locations worldwide where one finds the most fraud. In other other words, Facebook isn't growing anymore at allâ€”it's shrinking. Even India, Indonesia and the Philippines don't register as many searches for Facebook as they used to. Many of the "new" users on Instagram are actually old users from the core platform looking to escape the deluge of fakery.</p><p>The last time Mark suggested that Facebook's growth heyday might be behind it, in July 2018, the stock took a nosedive that ended up being the single largest one-day fall of any company's stock in the history of the United States. In about an hour, it <a href="https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25" target="_new">plunged 20%</a> from around $220 per share to about $165. Needless to day, the loss of about $120 billion in market capitalization in an hour provided a sufficient disincentive for Mark to avoid a repeat performance.</p><p>Having narrowly escaped the ire of Wall Street, Mark knows he cannot get off the growth treadmill he set in motion years ago. The only solution: lying to investors about growth in an attempt to convince them that everything is fine. Yet signs that Mark's fake account problem is no different than Madoff's fake account statement problem are everywhere. Google Trends shows worldwide "Facebook" queries down 80% from their November 2012 peak. (Instagram doesn't even come close to making up for the loss.)  Mobile metrics measuring use of the Facebook mobile app are down. And the company's own disclosures about fake accounts stand out mostly for their internal inconsistencyâ€”one set of numbers, measured in percentages, is disclosed to the SEC, while another, with absolute figures, appears on its "transparency portal."  While they reveal a problem escalating at an alarming rate and are constantly being revised upwardâ€”Facebook claims that false accounts are at 5% and duplicate accounts at 11%, up from 1% and 6% respectively in Q2 2017â€”they don't measure quite the same things, and are <a href="https://www.plainsite.org/realitycheck/facebook.html" target="_new">impossible to reconcile</a>. At the end of 2017, Facebook decided to stop releasing those percentages on a quarterly basis, opting for an annual basis instead. Out of sight, out of mind.</p><p>One could argue that SEC disclosures are subject to strict regulations under the Securities Exchange Act and that Facebook would never be so bold as to lie to investors in black and white. That's true: it qualifies its fake account disclosures with the quizzical legal phrase "significant judgment" and it chose the color orange instead of black (insert Netflix joke here) for its transparency portal graph disclaimers that read, "These metrics are in development."  And one could further argue that the transparency portal metrics are reviewed by a team of academics, known as the Data Transparency Advisory Group (DTAG), who are supposed to vouch for their validity. But the DTAG academicsâ€”not one of whom is a statistician, despite Facebook's direct claim to the contrary, now erasedâ€”fully admit that they are paid by Facebook, and even after months of hard work, their <a href="https://law.yale.edu/system/files/area/center/justice/document/dtag_report_5.22.2019.pdf" target="_new">final report</a> released in April mentioned fake accounts only three times, and all three …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/">http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/</a></em></p>]]>
            </description>
            <link>http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25180420</guid>
            <pubDate>Sun, 22 Nov 2020 19:47:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Month of Terraform]]>
            </title>
            <description>
<![CDATA[
Score 99 | Comments 76 (<a href="https://news.ycombinator.com/item?id=25180355">thread link</a>) | @ingve
<br/>
November 22, 2020 | https://jeremywsherman.com/blog/2020/11/21/a-month-of-terraform/ | <a href="https://web.archive.org/web/*/https://jeremywsherman.com/blog/2020/11/21/a-month-of-terraform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>I took Heroku for granted, and a month into setting up my own infra, I now know how much it bought me.</p>
<p>A lot of my past work has been infrastructure-adjacent.
I often find myself filling in the Build &amp; Integration role - the person that gets continuous integration off the ground and keeps it actually continuing rather than falling flat on its face.
But often I’ve just been building one of a constellation of services, so the core infrastructure was already there,
or I’ve been targeting something like Heroku, where you basically pick your poison, git push, and bob’s your uncle.</p>
<p>This time, I’m putting the pieces together using the AWS toolkit.
And to smoosh them all together, I’m using Terraform,
because heck if I’m going to be hand-writing YAML or JSON and praying it’s formatted right.
Plus there’s more I want to orchestrate than just AWS, like, say, GitLab.</p>
<p>I don’t wanna talk about AWS just now.
It reminds me of learning Foundation &amp; Cocoa - you look at one piece, and it can do so much, and then you gotta put all those individually deep &amp; complex pieces together to do more stuff.
I figure if I put in the hours reading docs, learning what’s all there, and getting stabbed by the pointy bits, it’ll probably all come out fine in the end.</p>
<p>So, Terraform.</p>
<h2 id="the-good">The Good</h2>
<ul>
<li>It mostly works!</li>
<li>When it doesn’t, it generally fails in a useful way, and then I can fix it and try again.</li>
<li>There are docs for most things.</li>
<li>Autoformatting works great.</li>
<li>Linting works pretty well.</li>
<li><em>Terraform: Up &amp; Running</em> is excellent, and Terragrunt makes it even easier. Huge thanks to their team for providing the <a href="https://rachelbythebay.com/w/2018/03/23/ducttape/">duct tape</a> we need. 🙌</li>
</ul>
<h2 id="the-not-so-good">The Not So Good</h2>
<ul>
<li>terraform-lsp is supposed to provide autocomplete, but it mostly doesn’t, in my experience. First it flipped its lid that I dared to have a repo with multiple root modules in it, so I just aimed VS Code at the folder with a single root module. Then the language server says it’s all hunky dory AFAICT, and yet it autocompletes nothing beyond bare language syntax. As a result,  I’m manually referencing docs and writing stuff down and wasting tons of time that tools like autocomplete and integrated linting ought to be saving me from.</li>
<li>State files contain secrets in plaintext. (You might enjoy the <a href="https://github.com/hashicorp/terraform/issues/516">six-year-old GitHub issue about the plaintext secrets problem</a>.) You can mark outputs as secret, so they don’t get printed at the end of applying your infra spec, but run <code>terraform show</code> instead of <code>terraform apply</code>, and there they are, staring back at you. At least you can lock down and encrypt the S3 bucket holding the state.
<ul>
<li><a href="https://www.pulumi.com/docs/intro/concepts/config/#secrets">Pulumi’s secrets management</a> is far more satisfying. But Pulumi is even more cutting-edge than v0.whatever Terraform, and I expect Hashicorp to keep TF running for a good while, while I’m not so confident in Pulumi, so I’m using TF. (Hashicorp of course would recommend <a href="https://www.hashicorp.com/products/vault">Vault</a>.)</li>
</ul>
</li>
<li>Annoying asymmetries in the language about how you *<em>declare and reference</em> things in slightly variant ways - I trip over these over and over as a beginner:
<ul>
<li>You declare locals in a <code>locals</code> block, but you reference them as <code>local.thing</code>, not <code>locals.thing</code>.</li>
<li>You declare a variable in a <code>variable</code> block, but you reference it as <code>var.thing</code>.</li>
<li>You declare data sources as <code>data "provider_thingy" "my_name_for_this_data"</code>, and then you have to access it as <code>data.provider_thingy.my_name_for_this_data</code>. (This is actually pretty darn consistent, at least. Though, like, why the quotes around the provider thingy?)</li>
<li>You declare resources as <code>resource "provider_thingy" "my_name"</code>. But you do NOT reference them as <code>resource.provider_thingy.my_name</code>. Nope, you just reference them as bare <code>provider_thingy.my_name</code>.</li>
</ul>
</li>
<li>For that matter, there are other oddities as well. Pieces of syntax that seem like they should be orthogonal just aren’t. <code>for_each</code> stands out here:
<ul>
<li>You can generate multiple resources by just dropping a <code>for_each</code> in the block: <code>resource "provider_thing" "mine" {}</code> becomes <code>resource "provider_thing" "mine" { for_each = of_these }</code></li>
<li>But nested <em>argument</em> blocks require conversion from like <code>setting { namespace = "blah" }</code> to <code>dynamic "setting" { for_each = thingy; content { namespace = "blah" }}</code>. Have fun looking that up a few times.</li>
<li>And you can’t even use the <code>for_each</code> trick with module imports. It just isn’t supported. Sorry, sucks to be you.</li>
</ul>
</li>
<li>Annoying gaps in the docs:
<ul>
<li><strong>Required vs optional parameters</strong> are not very clearly called out and are not at all segregated. So you get to play the game of “what is the minimal skeleton to declare this resource”. Actually running it a few times to see what you screwed up takes longer than just looking at the docs and puzzling it out, due to the lengthy iteration times in infra-land (see below).</li>
<li><strong>Types are not shown in the docs!!!</strong> All the outputs and arguments are typed. You have to declare those types. It’s right there in the code. But the docs don’t say what any of the types are. You just hit a type error at runtime. Fun fun!</li>
<li><strong>The HCL language is doc’d under the CLI tool, not in and of itself.</strong> It was really hard to actually find the docs since my first thought when I have syntax questions isn’t “let’s look at the docs for the tool.” It’d be like pulling up the manpage for GCC (carefully draw your triangle of art first) when you have a question about C syntax.</li>
</ul>
</li>
<li>Annoying asymmetries in the AWS provider:
<ul>
<li><strong>Missing links:</strong> Sometimes you get into a “can’t get there from here” situation. Like trying to find the zone ID for an Elastic Beanstalk environment’s CNAME so you can aim a Route 53 alias at it. (Hint, you need a completely different resource, the <code>aws_elastic_beanstalk_hosted_zone</code>.)</li>
<li><strong>Irregular naming:</strong>
<ul>
<li>Sometimes something is <code>zone_id</code>, but other times it’s maybe just <code>id</code>.</li>
<li>Sometimes you can fish stuff out by <code>arn</code>, or maybe by <code>id</code>, or maybe it’s by <code>name</code> - good luck. Keep the docs close to hand.</li>
<li>(It’s totally possible this is inherited from the AWS APIs themselves, but the whole point of an abstraction layer is to make things better and more usable, dangit.)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="the-different">The Different</h2>
<ul>
<li><strong>Iteration times are way longer than with even mobile apps.</strong> Like, “you’re liable to task-switch while waiting to see plan output” longer.</li>
<li><strong>Testing is a pain.</strong> I haven’t pulled in <a href="https://terratest.gruntwork.io/">Terratest</a> yet, because anyone maintaining this after me is unlikely to have Go experience, and my focus here isn’t builing reusable infra anyway - it’s building <em>this</em> infra – so I’ve just been using <code>bats</code> and Bash shell scripts (with <a href="https://github.com/koalaman/shellcheck/blob/master/README.md">shellcheck</a>, which is amazing) for some after-the-fact sanity checking using the AWS CLI. (Pro tip: Use the community-maintained fork <a href="https://github.com/bats-core/bats-core"><code>bats-core</code></a> rather than the no-longer-maintained sstephenson original.)
<ul>
<li>Policy assertions feel like a different flavor of test, but the tooling here seems to be fairly immature, with perhaps the exception of if you’re targeting Kubernetes.</li>
</ul>
</li>
</ul>
<h2 id="summary">Summary</h2>
<p>I expect I’ll get used to most of the rough edges of the syntax in another month. And Terraform is stil v0, so hey, maybe some breaking changes will clear all this mess away. 🤞</p>
<p>I’m intentionally not getting sucked into hacking around the docs frustrations just now. Or even the <a href="https://github.com/gruntwork-io/terragrunt/issues/432#issuecomment-371467507">very tempting open issue about silencing all the Terragrunt logspew</a>.</p>
<p>I do plan to spend a bit of time trying to get autocomplete working for resource and data source types and their arguments/attributes from the language server, at least. That would be a huge help.</p>
<p>It still feels like magic to run a command and have infrastructure just…happen.
You hit return, wait a bit, and suddenly servers are serving and domains are aliasing and a whole constellation of systems are interoperating.
It kinda reminds me of the magic of home automation with blinkenlights, only without any of that messy “hardware” stuff to break on you.</p>

    </div></div>]]>
            </description>
            <link>https://jeremywsherman.com/blog/2020/11/21/a-month-of-terraform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25180355</guid>
            <pubDate>Sun, 22 Nov 2020 19:39:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We’re Optimizing Ourselves to Death (2019)]]>
            </title>
            <description>
<![CDATA[
Score 266 | Comments 180 (<a href="https://news.ycombinator.com/item?id=25180229">thread link</a>) | @thread_id
<br/>
November 22, 2020 | https://zandercutt.com/2019/02/18/were-optimizing-ourselves-to-death/ | <a href="https://web.archive.org/web/*/https://zandercutt.com/2019/02/18/were-optimizing-ourselves-to-death/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<h4>Burnout is the inevitable result of our endlessly accelerating pace of&nbsp;life</h4>



<figure><img data-attachment-id="552" data-permalink="https://zandercutt.com/screen-shot-2019-03-06-at-5-32-05-pm/" data-orig-file="https://zandernethercutt.files.wordpress.com/2019/03/screen-shot-2019-03-06-at-5.32.05-pm.png" data-orig-size="564,744" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2019-03-06-at-5.32.05-pm" data-image-description="" data-medium-file="https://zandernethercutt.files.wordpress.com/2019/03/screen-shot-2019-03-06-at-5.32.05-pm.png?w=227" data-large-file="https://zandernethercutt.files.wordpress.com/2019/03/screen-shot-2019-03-06-at-5.32.05-pm.png?w=564" src="https://zandernethercutt.files.wordpress.com/2019/03/screen-shot-2019-03-06-at-5.32.05-pm.png" alt=""><figcaption>Illustration: Jutta Kuss/Getty Images</figcaption></figure>



<p><strong>Author’s Note</strong>: I’ve recently partnered with <a href="https://diginthere.com/">Project DigInThere</a>, an online project to help people get more out of the articles they read. <a href="https://diginthere.com/">Project DigInThere</a> enables authors (in this case, me) to create 3-4 question “quests” that readers (in this case, you) can review prior to reading an article. Doing so primes you with what to look for in the article, and then you can take the quest at the end of the article to test your recall. If you’re interested, you can review the quest I’ve built by <a href="https://diginthere.com/quests/zandercutt-2019-02-17-were-optimizing-ourselves-to-death-2447316663cd/view">clicking here</a>, then take it after reading the article and see how you do. Or you can just read the article — it’s up to you ;).</p>



<hr>



<p><strong>pro·cel·er·a·tion</strong></p>



<p>/prōˌseləˈrāSH(ə)n/</p>



<p><em>noun</em></p>



<ol><li>The acceleration of acceleration</li></ol>



<p>— excerpt from <em>The Age of Earthquakes</em>, by Shannon Basar, Douglas Coupland, and Hans Ulrich Obrist</p>



<hr>



<p>There’s a famous thought experiment in economics known as the “prisoner’s dilemma.” In it, two men have been caught committing a crime. Each of them is placed in a separate interrogation room and effectively has two options: confess or lie. There are three possible outcomes (the payoffs of which are illustrated in the payoff matrix below):</p>



<p><strong>Outcome 1</strong>: Both confess, and both serve eight years in prison (illustrated by payoff “-8, -8” in Figure A).</p>



<figure><img src="https://zandernethercutt.files.wordpress.com/2019/02/cec19-1vvzfhu09tthxuusxda1zeg.png" alt=""><figcaption>Figure A: The Prisoner’s Dilemma. Credit:&nbsp;Author</figcaption></figure>



<p><strong>Outcome 2</strong>: Both men lie, and both serve one year in prison (illustrated by payoff “-1, -1” in Figure A).</p>



<p><strong>Outcome 3</strong>: One man confesses while the other lies. The liar serves the longest possible sentence, 10 years, while the confessor goes free (illustrated by payoff “-10, 0” in Figure A).</p>



<p>So, if both men lie, they both get off with a lighter sentence. That appears to be the full story — except it isn’t.</p>



<p>The importance of the prisoner’s dilemma is understanding that in selecting a strategy, each player should account for the effectiveness of that strategy given what the other player might do.</p>



<p>Knowing this, consider the game from the perspective of Prisoner 1. If he thinks Prisoner 2 will lie, he should confess, because serving zero years in prison is better than serving one. If he thinks Prisoner 2 will confess, he should also confess, because serving eight years in prison is better than serving 10. In this situation, confessing is both players’ dominant strategy, the strategy they should play regardless of what the other player does.</p>



<p>This thought experiment illustrates how two self-interested individuals with a clear way to maximize their collective utility fail to do so. It also happens to be a fantastic way to understand our current moment. Millennials — not all of us, but many of us — are burned out, and the prisoner’s dilemma can shed light on why.</p>



<p>Unfortunately, it also sheds light on a distressing conclusion: Barring some miracle of human coordination, our quest to optimize our lives will never slow, let alone stop. If anything, it will accelerate.</p>



<hr>



<p>Imagine a two-player labor market represented by the prisoner’s dilemma matrix. Now imagine both players encountered a service that would help optimize their lives. For a real-world example (and one I use), let’s take the premade meal delivery service Freshly.</p>



<p>Freshly claims to save people approximately two hours a week in the time they don’t have to spend grocery shopping, meal prepping, or cooking. Now imagine that both players had two choices of how they could spend those hours: either on extra leisure (e.g., sleep, Netflix, a book, etc.,) or on productivity (e.g., optimization/work).</p>



<p>What would each player choose?</p>



<p>Well, if wealth is considered freedom from busyness, or freedom to spend your time as you wish, the hour would be best spent on leisure. When forming a strategy, however — like with the prisoner’s dilemma — players must consider those strategies in the context of what the other players in the game might do. Consider the adjusted payoff matrix below:</p>



<p><strong>Outcome 1</strong>: Both players use the time afforded by the service’s convenience to optimize/work harder and thus remain in a state of constant acceleration (illustrated by payoff “1, 1” in Figure B).</p>



<figure><img src="https://zandernethercutt.files.wordpress.com/2019/02/92958-1qccvg4lbzz4zmcjpbuhs6w.png" alt=""><figcaption>Figure B: The Millennial Dilemma (leisure vs. work). Credit:&nbsp;Author.</figcaption></figure>



<p><strong>Outcome 2</strong>: Both players use the time afforded by the service’s convenience to relax (illustrated by payoff “8, 8” in Figure A).</p>



<p><strong>Outcome 3</strong>: Player 1 uses the time afforded by the service’s convenience to optimize/work harder, while Player 2 uses it to relax. Player 1 reaps the benefits of being the only provider of labor in a market and corners it. Player 2 languishes as the world accelerates endlessly and leaves him behind (illustrated by payoff “10, 0” in Figure A).</p>



<p>Borrowing earlier analysis, it’s clear that given the payoffs, both players have a dominant strategy: work. If Player 2 relaxes, Player 1 should work because a payoff of 10 is better than a payoff of 8. If the Player 2 works, Player 1 should also work because a payoff of 1 is better than a payoff of zero.</p>



<p>Now, remember, these payoffs — and their explanations — are completely made up. In the modern era, there is no reason to be convinced that torturing yourself with additional employment is associated with any improvement in your lifestyle. And yet this is exactly how most people behave.</p>



<p>Thus, we arrive at our new Nash equilibrium: Both players use a service — mind you, a service built to supposedly make their lives easier and more relaxing — that ends up making their lives more stressful and complex. Put another way, both players burn out.</p>



<hr>



<p>In a recent viral BuzzFeed article, “<a href="https://www.buzzfeednews.com/article/annehelenpetersen/millennials-burnout-generation-debt-work" rel="noreferrer noopener" target="_blank">How Millennials Became The Burnout Generation</a>,” Anne Helen Petersen notes this seeming paradox of leisure, specifically as it pertains to freed up time. She writes:</p>



<blockquote><p>Attempts [by companies] to discourage working “off the clock” misfire, as millennials read them not as permission to stop working, but a means to further distinguish themselves by being available anyway.</p></blockquote>



<p>In other words: Attempts by companies like Google or Freshly to create services that save you time misfire, as millennials see them not as services that will give them more time to relax, but as services that will increase the amount of time they’re available to work.</p>



<p>As employees in a hyperproductive, work-obsessed world, we’ve become acutely aware of any opportunity for optimization. Our Instagram feeds are filled with every possible combination of meal delivery service and online shopper that exists. Startups emerge daily to automate every mundane activity ever scrawled on and scratched off a legal pad.</p>



<p>The escalators I take to work are filled with the same desperate faces and vacant eyes I feel staring through me on the subway, except instead of standing still, they’re bounding up it, subconsciously aware that below their feet is yet another opportunity to optimize on an existing convenience. This, if anything, is a symptom of our current moment: People ignoring the luxury of a moving staircase in favor of whatever sprinting up it can transport them to faster.</p>



<p>There’s a kind of sick satisfaction derived from optimizing one’s own life, and there’s a good reason: Being able to do so is a status symbol. Only the most successful are free enough to spend their time finding better ways to spend their time. For those at the very top, I imagine these methods of optimization can actually exist in a vacuum; billionaires can optimize for the sake of optimizing, rather than to keep their head above water. For the rest of the world, optimization is a survival mechanism. To them, the tools that are luxuries to those at the top are good for one thing and one thing only: freeing up time that is only ever used to get more done.</p>



<hr>



<p>The one bright side to all this productivity should be that everyone makes more money, but that’s all too often not the case. The popular narrative is that we’re all working harder, but “wages haven’t risen in 40 years,” and “purchasing power is lower now than any point in recent memory.” The economist in me has always struggled with this line of thinking. Wages are only truly relevant indicators of wealth in the sense that they allow you increased control over how you spend your time. If you’re earning a wage and a service comes along that saves you the time and effort you’d normally have to expend to access a certain good (read: Freshly for meals), that service effectively increases the value of your existing wage. Thus, even though you’re not earning any more money, you’re now wealthier.</p>



<p>For consumers, services like Google and Freshly do exactly this.</p>



<p>The media, though — and a select few politicians — prefer a different narrative. “There’s a finite amount of money in the world,” they effectively claim, “and since we’re making less, and tech companies are making more, it follows that tech companies are to blame for wage stagnation, which is a net bad, always.”</p>



<p>Reality, though, isn’t that simple.</p>



<p>Though companies like Google and Amazon do generate healthy — and yes, <a href="https://tradingeconomics.com/united-states/corporate-profits" rel="noreferrer noopener" target="_blank">quite frankly absurd</a> — returns for their executive teams and shareholders, they’re valuable because people find whatever they offer to be worth more than whatever they’re being asked to pay for it. In the case of Google, that offering is time (via frictionless access to information), and its price is effectively zero. The partial rationalization I make for stagnant wages, then, is that Google and services like it allow people to get more out of the same wage.</p>



<p>In this world, Google and its contemporaries are to blame for wage stagnation, but only because they’re creating a world where wages are no longer necessarily synonymous with wealth. Ergo, wage stagnation at the hands of tech companies — everyone’s favorite narrative — is a feature, not a bug.</p>



<p>The problem with this line of thinking, though, gets at the root of both the millennial …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://zandercutt.com/2019/02/18/were-optimizing-ourselves-to-death/">https://zandercutt.com/2019/02/18/were-optimizing-ourselves-to-death/</a></em></p>]]>
            </description>
            <link>https://zandercutt.com/2019/02/18/were-optimizing-ourselves-to-death/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25180229</guid>
            <pubDate>Sun, 22 Nov 2020 19:25:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Makes a Great Product Manager]]>
            </title>
            <description>
<![CDATA[
Score 157 | Comments 50 (<a href="https://news.ycombinator.com/item?id=25179296">thread link</a>) | @laybak
<br/>
November 22, 2020 | https://informedpm.com/posts/great-product-manager | <a href="https://web.archive.org/web/*/https://informedpm.com/posts/great-product-manager">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><span>What sets great product managers apart from merely good ones? </span></p> <p><span>It is a difficult question to answer because there are so many things that "great" product managers do. And because the discipline itself is changing very quickly. There is no universal answer. But it is a question worth asking.</span></p> <p><span>I tackled this question by interviewing industry leaders, drawing on my own experience, and research. </span></p> <p><span>Here are the top traits, principles, and tactics I collected, that great product managers tend to share.</span></p>  <p><h3><span>Data + Intuition</span></h3></p> <p><span>As product managers, we face many decisions every day. But what does "effective decision making" mean?</span></p> <p><span>Being "data-driven" is a good start. But the term is so overused that it has lost its meaning. It also has its blind spots. </span></p> <p><span>Here is how </span> <a href="https://twitter.com/trevin" target="_blank"><span>Trevin Chow</span></a> <span>, Director of Product at Nike, put it:</span></p> <div><blockquote><span>Great product managers are able to find the right balance between data and intuition at any given moment to inform and drive their decision making. This could be something as small a bug fix, or something much larger such as which features to include in a v1 vs what to cut.

The best product managers I've worked with are able to do this very well, balancing all sorts of inputs of "data" and combining it with their intuition to find the right balance of any given moment on what to index on.</span></blockquote></div>  <p><h3><span>Truth-Seeking</span></h3></p> <p><span>Both scientists and product managers play a truth-seeking game — scientists test their ideas in the natural world; product managers test theirs in the market.</span></p> <p><span>﻿</span> <a href="https://www.linkedin.com/in/danielfalabella" target="_blank"><span>Daniel Falabella</span></a> <span>, Director of Product at Duolingo, believes that "90% of a Product Manager’s job is to find 'truth'". And this is one reason why Daniel thinks that entrepreneurs make the best PMs. He added,</span></p> <div><blockquote><span>Lots of PMs go through the motions: they talk to teammates, become familiar with vanity metrics, talk to 2 users, or assume that their managers are right. They give the illusion of having the right inputs, only to pay up the price when it's time to discuss outputs.</span></blockquote></div>  <p><h3><span>Adaptable</span></h3></p> <p><span>Product managers are responsible for the success of their products. But the path to get there is rarely straight-forward.  </span></p>  <div><blockquote><span>Great PMs are adaptable. They know when to go into execution mode, and when to step back and work on strategy. They make everyone around them feel heard. At the end of the day, they make their customers and business successful, and their teams and cross functional partners want to work with them over and over again.</span></blockquote></div>   <div><blockquote><span>Underrated skill for founders: Altitude shifting. 

People are often either good at high-level strategy or atomic-level execution, but rarely both.

It's the ability to zoom out &amp; paint the 5 yr vision, and then drop down into the weeds of day-to-day, &amp; see how the two connect.</span></blockquote></div>  <p><h3><span>First Principles</span></h3></p> <p><span>﻿</span> <a href="https://blackboxofpm.com/" target="_blank"><span>Brandon Chu</span></a> <span>, VP Product &amp; GM of Platform at Shopify, </span> <a href="https://blackboxofpm.com/the-first-principles-of-product-management-ea0e2f2a018c" target="_blank"><span>wrote in an article</span></a> <span> that some of the best PMs he knows make their decisions based on first principles. He explained,</span></p> <div><blockquote><span>First principle thinking helps PMs because as companies scale, communicating the rationale behind historical, current, and future decisions can be simplified in a way that their team and stakeholders can rally around. </span></blockquote></div> <div><blockquote><span>This enables people around the PM to move quickly in the same direction, decouple, and make smart trade offs without their presence.</span></blockquote></div>    <p><h3><span>Deep Understanding of the Problem</span></h3></p> <p><span>It is tempting, especially for the technically-minded folks, to start with an idea for a solution and run with it. But building a great product requires understanding the problem, and understanding the person behind the problem.</span></p>  <div><blockquote><span>If you get on the ground and hear what people are suffering from, then you can have a deeper understanding of what needs to be done. It’s not just empathy. It’s being specific and zoning in on the areas of improvement based on people’s real experiences."</span></blockquote></div>  <p><span>﻿</span> <a href="https://twitter.com/lissijean" target="_blank"><span>Melissa Perri</span></a> <em>, </em> <span>CEO and founder of Produx Labs, </span> <a href="https://roadmunk.com/blog/melissa-perri/" target="_blank"><span>said in an interview</span></a> <span> that "
what sets a decent product manager apart from a really great product manager, is really the way they think and approach problems". She continued,</span></p> <div><blockquote><span>To me, thinking like a product manager is about problem solving. It’s about synthesizing a lot of information, understanding the system, trying to piece together what is the problem, breaking it down into small manageable chunks so you can analyze it, and then figuring out what is the right solution from there.</span></blockquote></div>  <p><h3><span>Humble and Coachable</span></h3></p> <p><span>Being humble/coachable is what Daniel (from earlier) considers the other important trait of a great product manager.</span></p> <p><span>Related to the truth-seeking point, you will be wrong a lot throughout your career. This may be a tough pill to swallow for the smart and ambitious ones. But it is an important part of growing as a person and learning to make better decisions over time.</span></p>  <p><h3><span>Evangelize</span></h3></p> <p><span>Your work isn't done when the requirements are defined, or even when the product gets shipped. </span></p>  <div><blockquote><span>If you’re not sick of saying it, you probably aren’t saying it enough. Constant communication might feel like “fluff,” but it isn’t. Evangelism is a critical part of the role—and it’s your job to make sure the organization is aligned and swimming in the same direction.</span></blockquote></div> <p><span>And that,</span></p> <div><blockquote><span>For you [product managers], communication&nbsp;</span> <em>is</em> <span>&nbsp;a primary “output,” and it should be exceptional.</span></blockquote></div>  <p><h3><span>Resourceful</span></h3></p> <p><span>There are always more things to build than there is time for. And often, you don't get as many people working on your product as you would like.</span></p> <p><span>Mastering the art of stretching resources given a tight budget is valuable.</span></p> <p><span>This could mean finding cheaper technical workarounds. This could mean ruthlessly focusing on the small number of things that will really move the needle.</span></p>  <p><h3><span>Love of Making Things</span></h3></p> <p><span>﻿</span> <a href="https://www.ellenchisa.com/" target="_blank"><span>Ellen Chisa</span></a> <span>, Co-founder of Dark and former VP of Product at Lola, </span> <a href="https://mokriya.com/blog/the-new-product-manager-a-conversation-with-ellen-chisa/" target="_blank"><span>said in an interview</span></a> <span> that, "You have to really love making things, but you have to be okay with not being the person actually making the thing."</span></p> <p><span>Having a genuine love for making things is powerful. This intrinsic motivation can keep you going, especially when things get difficult.</span></p> <p><span>This genuine interest also translates to you noticing other cool products in your life and building things outside of work.</span></p>  <p><h3><span>At the End of the Day</span></h3></p> <p><span>Ultimately, your responsibility as a product manager to ship successful products. To help your business and customers succeed. </span></p> <p><span>Being on top of things. Delivering results. These are attributes of a great PM that few would disagree.</span></p>  <p><h3><span>Further Readings</span></h3></p>  <p><span>﻿</span> <a href="https://blackboxofpm.com/the-first-principles-of-product-management-ea0e2f2a018c" target="_blank"><span>The First Principles of Product Management</span></a> <span> by Brandon Chu. It boils down product management to maximizing impact to the mission given a set of inputs and accomplishing everything through others.&nbsp;</span></p>  <p><span>﻿</span> <a href="https://www.lewis-lin.com/blog/2019/4/11/introducing-the-esteem-method" target="_blank"><span>ESTEEM Method</span></a> <span> by Lewis C. Lin: Execution, Superior communication skills, Tactical awareness, Extraordinary mental toughness, Exceptional team builder, Moonshot vision.</span></p>        
          
          
          <p><em>Every two weeks, I send out a newsletter with the latest product learnings and insights.</em></p>
          <p><em>Enter your email below to subscribe.</em></p>

          
        </div></div>]]>
            </description>
            <link>https://informedpm.com/posts/great-product-manager</link>
            <guid isPermaLink="false">hacker-news-small-sites-25179296</guid>
            <pubDate>Sun, 22 Nov 2020 17:41:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When science was the best show in America]]>
            </title>
            <description>
<![CDATA[
Score 134 | Comments 158 (<a href="https://news.ycombinator.com/item?id=25177334">thread link</a>) | @CapitalistCartr
<br/>
November 22, 2020 | http://m.nautil.us/issue/93/forerunners/when-science-was-the-best-show-in-america | <a href="https://web.archive.org/web/*/http://m.nautil.us/issue/93/forerunners/when-science-was-the-best-show-in-america">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
			<p><span>O</span>n May 29, 1810, Katherine Fritsch, a sister in the Moravian Church, boarded a coach in Lititz, Pennsylvania, along with a group of her friends and began the 75-mile trek to Philadelphia. Fritsch noted in her diary the one city site she most wished to see: Peale’s Museum. On the grounds of the museum, whose two buildings sat on State House Square, with rows of trees and manicured lawns, Fritsch passed through a menagerie that included a large cage with a live eagle sitting “right majestically on his perch—above his head a placard with this petition on it: feed me daily for 100 years.”</p><figure data-alt="Dugatkin_BREAKER-1"><img src="http://static.nautil.us/17940_1221132d8390ea66832cf2eabd8eb668.png" width="733" alt=""><figcaption><span><strong>THE MET OF ITS TIME:</strong> Charles Willson Peale painted this self-portrait to celebrate his pioneering museum. Its goal, he wrote his friend Thomas Jefferson, was to collect subjects in nature and “enlighten the minds of my countrymen.”</span><span>Charles Willson Peale; <i>The Artist in His Museum</i>, 1822; Oil on canvas; Courtesy of the Pennsylvania Academy of the Fine Arts, Philadelphia.&nbsp;Gift of Mrs. Sarah Harrison (The Joseph Harrison, Jr. Collection), 1878.1.2</span></figcaption></figure><p>From the yard, Fritsch went into the Peale Museum proper, through a door with “Whoso would learn Wisdom, let him enter here!” posted above. Fritsch walked past a turnstile that rang chimes to announce visitors. She walked up the stairs and into the Quadruped Room, which included a moose, llama, bear, bison, prong-horned antelope, hyena, and a jackal. She explored the Marine Room, overflowing with fish, amphibians, lizards, sponges, and corals. In the Long Room, glass cases were filled with hundreds of birds set against backdrops matching their natural environments; she saw insect cases in which the specimens could be rotated under a microscope. Fritsch didn’t get to see the museum’s mammoth skeleton, but noted in her diary that “all our talk was of how delightful had been our visit to the museum.”<br></p><p>Fritsch was not the only one who felt that way. From the time Peale’s Museum had opened its doors in 1786, annual attendance had averaged more than 10,000 people. Born both of science and art, it was the first true museum in the fledgling United States and the first must-see attraction not only for Philadelphians but for visitors from around the U.S. and the world. The museum’s creator, Charles Willson Peale, saw the museum as a national good. The “very sinews of government are made strong by a diffused knowledge of this science,” he wrote. The museum’s success made Peale a proud man for many years. It embodied the age of Enlightenment in the new world. After a visit, the French philosopher Comte de Volney proclaimed the museum housed “nothing but truth and reason.” But national funding for truth and reason foundered on the shore of politics. And then the circus came to town.</p><p><span>P</span>eale was born in Maryland in 1741. When he was 9, his father, a schoolteacher, died, leaving the family in poverty. Peale was apprenticed to a saddler at age 13, but spent almost as much time tinkering with mechanical devices of all sorts as he did saddling. His other interest lay in paint brushes and sketching pads. Ambitious as could be, Peale became the Colony’s most famous portrait painter. In 1771, Peale met Martha Washington and convinced her that Colonel Washington should sit for him—the first of 25 portraits, miniatures, mezzotints, or sculptures he would do of the soon-to-be general. Peale also painted portraits of Thomas Jefferson, Benjamin Franklin, and Alexander Hamilton. He named most of his 17 children after famous painters, including Rembrandt, Rubens, and Titian.</p><p>A true autodidact, Peale saw himself as a naturalist and scientist. And any self-respecting deist of the Enlightenment should have a museum. Fortunately, Peale knew all the right people. Robert Patterson, professor of mathematics at the University of the State of Pennsylvania, gave Peale his first specimen for the museum, “a curious fish called the paddle fish caught in the Allegheny River,” Peale wrote. Ben Franklin sent his friend the body of an angora cat that Madame Helvétius had given him when he departed Paris, and Washington sent the body of a just-deceased golden pheasant from the aviary of Louis XVI that the general had received as a gift from the Marquis de Lafayette. Other specimens soon came flooding in.</p><blockquote><p>Peale wrote that society raised roadblocks to women, which didn’t allow them to pursue science.</p> </blockquote><p>In a letter to Jefferson, Peale explained that his goal for the museum was to bring together “a variety of interesting subjects of Nature … collected in one view as would enlighten the minds of my countrymen, and, demonstrate the importance of diffusing a knowledge of the wonderful and various beauties of Nature, more powerful to humanize the mind, promote harmony, and aid virtue than any … yet imagined.”<br></p><p>Peale, a former member of the Philadelphia Militia, was a true-blooded patriot. He created an effigy of a double-faced model of Benedict Arnold in a carriage, dressed in a red coat and holding a letter to Beelzebub with the devil standing behind him shaking a purse full of money. When it came to his museum, he had no intent of curating the sort of European hall that catered only to “particular classes of society only, or open at such turns or at such portions of time, as effectually to debar the mass of society, from participating in the improvement, and the pleasure resulting from a careful visitation,” he wrote. His museum would be open to all—“the unwise as well as the learned.”</p><p>Peale held progressive views on women and children, and because he knew that a family-friendly venue would attract more visitors, he reached out to bring women and children into his museum. Women were not only encouraged to visit the museum, Peale wanted them to contribute to the enterprise, sending in samples and sharing ideas. Society, he believed, raised roadblocks to women “which allow no time for them to devote in the arduous pursuits of science,” but, he was quick to point out, “when females have devoted themselves to these pursuits they have given every demonstration of the intensity and depth of their intellectual powers.” He wanted to tap into those powers to better the museum and the plight of women.</p><figure data-alt="Dugatkin_BREAKER"><img src="http://static.nautil.us/17932_bbc90218e55a81732f0f78c16cbf2b6f.png" width="733" alt=""><figcaption><span><strong>TOM THUMB’S BLUES:</strong> Charles Stratton (right) was a child when P.T. Barnum (left) first hired him to perform in his museum. Barnum publicized Stratton as “General Tom Thumb,” a character who became a major attraction for the circus impresario for decades.</span><span>Wikimedia</span></figcaption></figure><p>Reverend Manasseh Cutler, a respected naturalist of the day, who had gained fame for his bravery as a chaplain during the Revolution, was an early visitor to Peale’s Museum in 1787 and was struck by the exhibits “arranged in a most romantic and amusing manner.” He describes two dioramas—a mound with trees and an artificial pond, each the result of Peale having spent many a morning “dressing the museum in moss.” The pond was stocked with fish, geese, ducks, cranes, and herons, “all having the appearance of life, for their skins were admirably preserved.” On the beach around the pond Cutler was dazzled by an assortment of “shells of different kinds, turtles, frogs, toads, lizards, water snakes, etc.” Cutler’s diary ends: “Mr. Peale’s animals reminded me of Noah’s Ark, into which was received every kind of beast and creeping thing in which there was life. But I can hardly conceive that even Noah could have boasted of a better collection.”<br></p><p>From the outset, the museum was meant to be a collaborative effort. Jefferson, Hamilton, James Madison, Gouverneur Morris, famed astronomer David Rittenhouse, and naturalists Benjamin Smith Barton and William Barton, sat on the museum’s board of directors. But this museum was not to be some highfalutin society club. Peale turned more often to his fellow citizens than to his board of directors to contribute what they could, be it specimen or knowledge. Each year, he would publish dozens of newspaper advertisements that included not just a call for specimens, but lists of new specimens received of late, information about new exhibits, changes to museum hours of operation, and perhaps strangest to our eyes, praise or admonitions of the way the public was responding to activities at the museum.</p><p>A few years after it opened, in a series of parades, young boys (and older men) moved all the exhibits to the museum’s new home in the American Philosophical Society’s Philosophical Hall. Peale and his family moved their own home to the basement of the museum, so as to better manage the ever-growing enterprise. Soon, the museum outgrew even Philosophical Hall and moved to the State House (what we now call Independence Hall), above the rooms where the Declaration of Independence were signed, and below where a soon to be rather famous bell rang each day.</p><p><span>I</span>n 1801, Peale and a team undertook the first major paleontological excavation in the U.S. Near Newburgh, New York they dug up, and then painstakingly reconstructed, the complete skeleton of a mammoth (technically, it was a mastodon, but that distinction did not yet exist). It was quite the sight and caught the fancy of locals. “Every farmer with his wife and children, for twenty miles round in every direction flocked to see the operation,” wrote Peale’s son Rembrandt. The <i>Mercantile Advertiser</i> soon ran tantalizing headlines like “Bones of a Mammoth or some other Wonderful Animal,” titillating readers with tales of “a monster so vastly disproportionate to every creature; as to induce a momentary suspension of every animal faculty but admiration and wonder.”</p><blockquote><p>Ben Franklin sent his friend the body of an angora cat and George Washington sent a golden pheasant.</p> </blockquote><p>In <i>Skeleton of the Mammoth</i>, a broadside that Peale posted across Philadelphia in 1802, he informed readers that though “numerous have been the attempts of scientific characters of all nations to procure a satisfactory collection of bones,” he and his museum had at last done just that. Peale even had one of the museum employees distribute the broadside throughout the city while on horseback …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://m.nautil.us/issue/93/forerunners/when-science-was-the-best-show-in-america">http://m.nautil.us/issue/93/forerunners/when-science-was-the-best-show-in-america</a></em></p>]]>
            </description>
            <link>http://m.nautil.us/issue/93/forerunners/when-science-was-the-best-show-in-america</link>
            <guid isPermaLink="false">hacker-news-small-sites-25177334</guid>
            <pubDate>Sun, 22 Nov 2020 13:37:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sengi – A FLOSS multi-account Mastodon and Pleroma desktop client]]>
            </title>
            <description>
<![CDATA[
Score 51 | Comments 11 (<a href="https://news.ycombinator.com/item?id=25177330">thread link</a>) | @blindm
<br/>
November 22, 2020 | https://nicolasconstant.github.io/sengi/ | <a href="https://web.archive.org/web/*/https://nicolasconstant.github.io/sengi/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <section>

            <p>
                    Sengi will let you use all your accounts<br> easily and seamlessly<br>
                </p>

        </section>

        

        <section>
            <h2>Quick Overview</h2>

            <video controls="">
                <source src="https://nicolasconstant.github.io/sengi/videos/Quick_overview.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </section>

        

        <section>
            <h2>Main Functionalities</h2>

            <h4>Seamless account switch</h4>
            <div>
                <p>
                        Just click on the account's avatar, <br>
                        and all your next actions will be performed by it.
                    </p>
                <p>
                    <video width="326" height="260" controls="">
                        <source src="https://nicolasconstant.github.io/sengi/videos/Clip_account_switch.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </p>
            </div>

            <h4>All instances timelines in one place</h4>
            <div>
                <p>
                        Add timelines and lists from all your accounts in the same
                        interface.
                    </p>
                <p><img src="https://nicolasconstant.github.io/sengi/images/timelines.png">
                </p>
            </div>

            <h4>Don't lose your focus</h4>
            <div>
                <p>
                        Opening a profile, thread, hashtag or even just replying to someone will always take place in the
                        current Timeline.
                    </p>
                <p>
                    <video width="326" height="260" controls="">
                        <source src="https://nicolasconstant.github.io/sengi/videos/Clip_timelines.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </p>
            </div>

            <h4>Labels</h4>
            <div>
                <p>
                        Get a quick insight if a status is part of a thread, has replies, is from a bot, is old, is
                        cross-posted (limited to local TL) or is remotely fetched.<br>
                        <a href="https://github.com/NicolasConstant/sengi/wiki/Labels">more details</a>
                    </p>
                <p><img src="https://nicolasconstant.github.io/sengi/images/labels.png">
                </p>
            </div>

            <h4>Auto-remove Thread's Content-Warnings</h4>
            <div>
                <p>
                        Easily remove all CW from a thread<br>
                        with one single click!
                    </p>
                <p>
                    <video width="326" height="260" controls="">
                        <source src="https://nicolasconstant.github.io/sengi/videos/Clip_cw_button.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </p>
            </div>

            <h4>And many more!</h4>

            <p>
                    There is a lot more things to discover<br> and more to come too!
                </p>

        </section>
        
    </div></div>]]>
            </description>
            <link>https://nicolasconstant.github.io/sengi/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25177330</guid>
            <pubDate>Sun, 22 Nov 2020 13:36:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a roam-like, networked, heavily-customized realtime editor, part 1]]>
            </title>
            <description>
<![CDATA[
Score 122 | Comments 33 (<a href="https://news.ycombinator.com/item?id=25177290">thread link</a>) | @namiheike
<br/>
November 22, 2020 | https://namiwang.github.io/2020/11/12/building-a-roam-like-networked-heavily-customized-realtime-editor-part-1.html | <a href="https://web.archive.org/web/*/https://namiwang.github.io/2020/11/12/building-a-roam-like-networked-heavily-customized-realtime-editor-part-1.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
        <header>
          
          

  <p>
    
      
      <span>
        
        <time datetime="2020-11-12T12:21:42+00:00">November 12, 2020</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section itemprop="text">
        
        

<blockquote>
  <p>I can build this.</p>

  <p>— <cite>every developer at least once</cite></p>
</blockquote>

<p>Knowledge is hard to manage, as mind is hard to materialize and visualize.</p>

<p>Bi-directional networked tools like <code>roam-research</code> and <code>obsidian</code> are on the trend for a while now. <a href="https://en.wikipedia.org/wiki/Knowledge_graph">The idea behind them</a> is not brand new, yet the much evolved web-based tech makes them possible.</p>

<h2 id="what-i-want-to-build">what I want to build</h2>

<p>I record my building of <code>fiber-note</code> in this series of dev posts, what I want to build is:</p>

<ul>
  <li>tag-based bi-directional networked note-taking</li>
  <li>web-based real-time experience
    <ul>
      <li>constantly auto-saving</li>
      <li>real-time reactive interface
        <ul>
          <li>updating components other than the editor (related notes, navigation, tags network, calendar, etc.)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>a highly-customized editor
    <ul>
      <li>enforce a custom schema to control the layout of the documents
        <ul>
          <li>say everything is a list item, there may be only text and tags in a paragraph, etc.</li>
        </ul>
      </li>
      <li>handle meta-data like tags, assigning unique ids for the block</li>
      <li>complex UI like inline drop-down menu</li>
      <li>…</li>
    </ul>
  </li>
  <li>visualize data as a network and a calendar</li>
  <li>open-sourced and self-hosted</li>
</ul>

<h2 id="code--demo">code &amp; demo</h2>

<p><a href="https://github.com/namiwang/fiber-note" target="_blank">
  <img src="https://namiwang.github.io/assets/images/fiber-note.gif" width="480" alt="fiber note screenshot">
</a></p>

<p>I placed the source <a href="https://github.com/namiwang/fiber-note">here</a>, which could be hosted by yourself. There’s even a configured deploy-to-heroku button for one-click hosting.</p>

<p>I also put up a public demo running at <a href="https://fiber-note-demo.herokuapp.com/session/new">fiber-note-demo.herokuapp.com</a> (password is <code>password</code>).</p>

<h2 id="naming-the-project">naming the project</h2>

<p>I always use rails as my first choice for web projects, and the first code name for this project is <code>roam-on-rails</code>, which is a bad joke since I’ve already got a project called <a href="https://github.com/ruby-on-rust/ruby-on-rust">ruby on rust</a>.</p>

<p>I’m bad at this, so I just picked the word <code>fiber</code> as a synonym for <code>network</code> from the thesaurus.</p>



<h2 id="a-prototype-on-paper">a prototype on paper</h2>

<p><a href="https://namiwang.github.io/assets/images/fiber-note-series/fiber-note-diagram.png" target="_blank">
  <img src="https://namiwang.github.io/assets/images/fiber-note-series/fiber-note-diagram.png" alt="fiber note data structure">
</a></p>

<h2 id="a-data-structure-in-mind">a data structure in mind</h2>

<p>The whole database is structured as a directed graph. The basic unit is a <code>block</code>, a node in the graph, representing a paragraph, bearing data like its content and optional tags.</p>

<p>Every block-node may have one parent. Thus a note, spawning from the root (a parent-free block-node), forms a tree and the whole database forms a forest.</p>

<p>There’re some edge cases to consider</p>

<ul>
  <li>have to avoid cycles in the graph</li>
  <li>a tag may points to the same note</li>
</ul>

<h2 id="a-data-structure-on-disk">a data structure on disk</h2>

<p>Apparently we need to maintain a graph, yet I didn’t choose a graph-oriented database like <code>neo4j</code>. Using good old SQL to simulate one is good enough for now.</p>

<p>There’re both plugins to do graph on database level (<a href="https://www.postgresql.org/about/news/announcing-age-a-multi-model-graph-database-extension-for-postgresql-2050/">AGE</a> for postgresql), and rails level (like <a href="https://github.com/jackc/edge">edge</a>). For the initial implementation, I chose to hand-written everything from the grounded up for faster iteration because I was constantly changing things.</p>



<p>This is gonna be a front-end-heavy project, I have to choose an editor as one of the first steps.</p>

<h2 id="requirements">requirements</h2>

<ul>
  <li>restrict the doc to a special set of content types
    <ul>
      <li>e.g. allow list, list item, and inline tags; disallow individual paragraphs or images</li>
    </ul>
  </li>
  <li>render existing data into the editor</li>
  <li>inspect and manipulate input from the user
    <ul>
      <li>enforce input rules like properly wrapping/indenting list items</li>
      <li>assign unique ids to created blocks</li>
    </ul>
  </li>
  <li>implement drop-down menu to auto-complete tags</li>
  <li>send updated content to the server</li>
  <li>features we may need in the future
    <ul>
      <li>copy-and-paste, drag-and-drop, image, etc.</li>
    </ul>
  </li>
</ul>

<h2 id="comparision">comparision</h2>

<table>
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>trix</th>
      <th>quill</th>
      <th>prose-mirror</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>integrating</td>
      <td>easy</td>
      <td>moderate</td>
      <td>moderate-to-high</td>
    </tr>
    <tr>
      <td>customization</td>
      <td>minimal</td>
      <td>moderate</td>
      <td>high</td>
    </tr>
    <tr>
      <td>custom schema</td>
      <td>no</td>
      <td>no</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>plugins</td>
      <td>no</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>themes</td>
      <td>no</td>
      <td>yes</td>
      <td>no</td>
    </tr>
    <tr>
      <td>docs</td>
      <td>minimal</td>
      <td>detailed</td>
      <td>detailed</td>
    </tr>
    <tr>
      <td>typescript</td>
      <td>no</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
    <tr>
      <td>scenario</td>
      <td>adding rich-text editing to your rails app in 10 minutes</td>
      <td>building editor email client with some cool features like markdown syntax</td>
      <td>building editor for an collaborative encyclopaedia with custom schema.</td>
    </tr>
  </tbody>
</table>

<p>I tried a few options. At the end I settled with <a href="https://prosemirror.net/">prose-mirror</a> to build fiber-note due to thorough guides and references, up-to-date maintaining, and <a href="https://discuss.prosemirror.net/">a friendly forum</a>.</p>

<h2 id="trix">trix</h2>

<p>As a rails user, my first thought are <code>actiontext</code> and <a href="https://trix-editor.org/">trix</a>. Trix is perfect for adding out-of-box rich-text editing to a normal rails app, like rails, it just works.</p>

<p>It’s just hard to tweak for more custom features.</p>

<ul>
  <li>it completely intertwined with rails’ components like <code>actionview</code> (rendering) and <code>activestorage</code> (image uploading), it’s hard to mutate the saved content without hacking into hidden methods.</li>
  <li>it saves content as raw HTML fragment, which is bad because
    <ul>
      <li>unique content have multiple legit representations</li>
      <li>it’s slow to parse and manipulate the content (say tags detection, image processing, table mutation, etc.)</li>
    </ul>
  </li>
  <li>it doesn’t come with detailed docs/specs about the format of generated HTML docs, which is not reliable when you system relies on processing the content on-the-fly.</li>
  <li>minimal events not enough to compose complex logic around the user’s operation</li>
</ul>

<h2 id="quill">quill</h2>

<p><a href="https://quilljs.com/">quill</a> is another competitive  candidate, regarding elaborated docs, data format specs, themes, and typescript support.</p>

<p>It’s easy to integrate the library, tweak some configurations, and apply different themes. You can add markdown syntax support in like 10 minutes.</p>

<p>It’s not easy to limit what kind of content is allowed in the document, or what would happen if a special formatted text is pasted.</p>

<p>It’s hard to pragmatically control the mutation of the data to manually implement functions like “create another list item with the same indent when <em>return</em> is pressed and current cursor on the end of a list item, including the end of an inline span of a list item”.</p>

<p>It’s not impossible, yet it will get over-complicated if you need many mutations like this.</p>

<h2 id="prose-mirror">prose-mirror</h2>

<p><code>prose-mirror</code> has the most complicated structure.  You have to import <strong>at least ten packages</strong> to build a simple demo, each managing a single aspect of the editor (model, view, schema definitions, keymaps, etc.).</p>

<p>There’s a starter kit kind of package (<code>prosemirror-example-setup</code>) which makes the journey a little bit easier. I’d recommend that kit to users who demand only basic features, while for complex functions you’ll have to compose each part and piece them together for better control. It’s like how advanced users almost never want the pre-set default preferences or <code>rails scaffold</code>.</p>

<p>Verbosity and redundancy means total control and vice versa, it just have to be like this. You posses the ability to custom the schema, listen to each key press, oversee every transaction of the model, and to arrange how to create (also parse, and wrap, and truncate during drag-n-drop, etc.) different kind of element.</p>

<p>You’ll spend a lot of time jumping between the <a href="https://prosemirror.net/docs/guide/">guides</a>, the <a href="https://prosemirror.net/docs/ref/">references for individual packages</a>, and even the source code. I can promise you that the guide will be a great read about the designing of a complicated modular system.</p>



<p>Thanks for reading. In the next post, I’ll discuss how I built and tweaked the editor.</p>

        
      </section>

      

      

      
  

    </div></div>]]>
            </description>
            <link>https://namiwang.github.io/2020/11/12/building-a-roam-like-networked-heavily-customized-realtime-editor-part-1.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25177290</guid>
            <pubDate>Sun, 22 Nov 2020 13:31:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Booting from a vinyl record]]>
            </title>
            <description>
<![CDATA[
Score 940 | Comments 156 (<a href="https://news.ycombinator.com/item?id=25177045">thread link</a>) | @ruik
<br/>
November 22, 2020 | http://boginjr.com/it/sw/dev/vinyl-boot/ | <a href="https://web.archive.org/web/*/http://boginjr.com/it/sw/dev/vinyl-boot/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>http://boginjr.com/it/sw/dev/vinyl-boot/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25177045</guid>
            <pubDate>Sun, 22 Nov 2020 12:51:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[S230 is a censorship law masquerading as a friend of free speech]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 70 (<a href="https://news.ycombinator.com/item?id=25177003">thread link</a>) | @mikerthomsen
<br/>
November 22, 2020 | https://mikethomsen.github.io/posts/2020/11/16/s230-the-two-faced-free-speech-law | <a href="https://web.archive.org/web/*/https://mikethomsen.github.io/posts/2020/11/16/s230-the-two-faced-free-speech-law">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://mikethomsen.github.io/posts/2020/11/16/s230-the-two-faced-free-speech-law</link>
            <guid isPermaLink="false">hacker-news-small-sites-25177003</guid>
            <pubDate>Sun, 22 Nov 2020 12:44:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How much is YouTube worth today?]]>
            </title>
            <description>
<![CDATA[
Score 137 | Comments 161 (<a href="https://news.ycombinator.com/item?id=25176451">thread link</a>) | @elephant_burger
<br/>
November 22, 2020 | https://mannhowie.com/youtube-valuation | <a href="https://web.archive.org/web/*/https://mannhowie.com/youtube-valuation">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><blockquote>
<p><strong>How much is YouTube worth today?</strong>
<img src="https://images.ctfassets.net/vwq10xzbe6iz/6LQGeFMMp2z9YMRiOurO4g/9489b18efefac13f387cdc116c7b5ff1/IMG_CB7ACB71DC8B-1.jpeg" alt="howmuchyoutubeworth"></p>
</blockquote>
<p>Google acquired YouTube for US$1.6 billion back in 2006. We estimate YouTube to be worth up to US$170 billion in 2020, delivering Google a +100x return in under 15 years.
<img src="https://images.ctfassets.net/vwq10xzbe6iz/6YrHqLoRpqCr5w4JwDJvtI/b5a86d3af5149953770bf1814a9c473c/IMG_18657804259D-1.jpeg" alt="youtube-valuation-comparison"></p>
<p>This article will cover in-depth how we arrive at this valuation and the steps you can follow to build your own model for YouTube and to value any technology company:</p>
<p><strong>Key Sections:</strong></p>
<ol>
<li><a href="#1">5 key questions to understand YouTube’s business model</a></li>
<li><a href="#2">Building a YouTube financial forecast model</a></li>
<li><a href="#3">Preparing a final YouTube valuation</a></li>
</ol>
<p>Skip to the following links for a valuation checklist and complete financial model:</p>
<ul>
<li><a href="#4">6 Step Valuation Checklist</a></li>
<li><a href="#5">Download Free YouTube Valuation Model</a></li>
<li><a href="#alphabet">Alphabet SOTP Valuation</a></li>
</ul>
<h2><a name="1"></a>5 Key Questions to Answer</h2>
<p>To reach a valuation for YouTube, we must firstly answer 5 questions about the business:</p>
<ol>
<li><strong>Industry</strong>: What industry does it compete in?</li>
<li><strong>Customer</strong>: Who are its customers and how does it deliver them value?</li>
<li><strong>Revenue</strong>: What is its revenue model and key drivers?</li>
<li><strong>Costs</strong>: What are its major costs and how are they managed?</li>
<li><strong>Growth</strong>: What is the growth thesis?</li>
</ol>
<h3>1. What industry does YouTube compete in?</h3>
<p>YouTube is the world’s largest online video service and competes for user viewership in order to compete for digital advertising spend from businesses. YouTube’s parent is Google/ Alphabet and operates globally except China where Google is banned.</p>
<p>Global ad spend (ex China) is over half a trillion dollars annually, of which half is spent on digital (web and mobile). The lion’s share of this is dominated by Google and Facebook which capture over 80% of digital ad spend.</p>
<p><img src="https://images.ctfassets.net/vwq10xzbe6iz/4kzjexfgsxFPhJOyyo2jS0/9b06d17b7cc5d7e0a6b080ec5beb7bf2/globaladshare.png" alt="globaladshare"></p>
<p>Splitting up Google and Facebook’s major ad platforms we reveal YouTube is the 5th largest digital ad platform with 6% share, behind Google Search, Facebook, Google Display and Instagram.</p>
<p><img src="https://images.ctfassets.net/vwq10xzbe6iz/2EbkvWiatOFXQYBVoqNlE5/03b1f3088e0b11f783116028d2682985/globaladfbyoutube.png" alt="globaladbyproduct"></p>
<p>Future growth of the digital ad market is any oracle’s guess. But if we assume digital remains a dominant advertising medium industry, then digital spend could reach over half a trillion by 2030 (assumes 7% annual growth).</p>
<blockquote>
<p>When looking at the industry a company operates in, focus on how the company generates revenue and compare against competitors and alternative mediums. Research industry statistics and competitor annual reports to get a rough estimate of how large the current industry is.</p>
</blockquote>
<h3>2. Who are YouTube’s customers and how do they deliver them value?</h3>
<p>YouTube is an online video platform that allows content creators (individuals and businesses) to publish videos for users to watch. YouTube sells ads to businesses placed within video content and shares the revenue with publishers.</p>
<p>YouTube delivers value to 3 key stakeholders:</p>
<ul>
<li><strong>Creators</strong>: monetize their video content on the platform with the widest reach. Allows creators to focus on producing quality content and outsource hosting and marketing costs to YouTube</li>
<li><strong>Users</strong>: free, targeted and relevant video content and entertainment (for better or worse)</li>
<li><strong>Advertisers</strong>: brand impressions and targeted reach across a global audience</li>
</ul>
<p>YouTube ultimately competes for global viewership and monetizes this by selling ads to businesses. YouTube has over 2 billion monthly active users and is the second largest social media platform in the world. The chart below compares the monthly active users across major ad supported social media platforms and video platforms:</p>
<p><img src="https://images.ctfassets.net/vwq10xzbe6iz/yQISXiQ5Gh5jgiQWcYKLu/61903f1edd297535079c73c0d7b17d22/mausocial.png" alt="mausocial"></p>
<p>Whilst each player competes for viewership, their target audience and monetization models differ:</p>
<ul>
<li>Facebook, Snapchat, Instagram and Twitter  sell ads placed in a user’s social media feed of images, video, news, articles</li>
<li>Netflix sells subscription services to users for its original and purchased content</li>
<li>TikTok is early in its monetization strategy</li>
</ul>
<blockquote>
<p>When understanding how a company generates revenue, ask yourself: Who is the customer and why would they pay for the offering? For technology platform companies, understand the ecosystem of stakeholders and the benefit each of them receive from using the platform.</p>
</blockquote>
<h3>3. What is YouTube’s revenue model and key drivers?</h3>
<p>YouTube has two primary revenue streams:</p>
<ul>
<li><strong>Ad sales</strong> (90% revenue): YouTube sells targeted ads within video content across its 2 billion monthly active users. It shares ad revenue with content publishers via a 45/55 split in favour of publishers; and</li>
<li><strong>Subscription</strong> (10% revenue): YouTube premium offers an ad free video experience and music service offered at ~$10 per month. There are currently 20 million premium subscribers representing ~1% of YouTube’s total monthly active user base</li>
</ul>
<p>YouTube’s key revenue driver is its ability to monetize its monthly active user base. We can assess its performance vs peers by comparing the average annual ad spend generated by each provider divided by its monthly active user base.</p>
<p><img src="https://images.ctfassets.net/vwq10xzbe6iz/5SzsrFRvxXSiFttWKHFSxm/596bb1ecbda955f39031a46059e20178/arputitle.png" alt="arpu"></p>
<p>Facebook currently generates 2x higher ad revenue per active user than YouTube across both Facebook and Instagram platforms. This could be explained by Facebook’s greater variety of ad formats and granular audience targeting (interests, age, behaviour, lookalike audience) and being further along its journey of ad monetization.</p>
<p>The biggest driver of revenue for YouTube will likely be its ability to drive higher ad spend to its already large audience via product innovation (e.g. new ad formats, improved targeting) rather than growing its audience. YouTube is also earlier in its monetization strategy and has also invested significant resources to clean up the quality of content published.</p>
<blockquote>
<p>The key to understanding any revenue model is to identify the key revenue drivers: 1) volume (e.g. customers, users, businesses), 2) pricing (ARPU, new products, upsell) and 3) churn (% customers leaving each month, non-renewals). Try and understand the narrative of how the company can influence these drivers.</p>
</blockquote>
<h3>4. What are YouTube’s biggest cost drivers?</h3>
<p>YouTube’s major costs can be separated across 3 categories:</p>
<ul>
<li><strong>Cost of Revenue</strong>: variable costs associated with delivery of services and revenue. Includes ad share revenue paid to creators and data-centre operation and hosting costs;</li>
<li><strong>Operating Costs</strong>: largely staff and support related costs to drive platform and product development (R&amp;D), revenue growth and creator support (Sales &amp; Marketing), content moderation, finance, admin (General &amp; Admin); and</li>
<li><strong>Capex &amp; Acquisitions</strong>: investments in data-centre infrastructure, acquisitions of businesses and supporting tech. These are cash outflows not captured in the income statement</li>
</ul>
<p><img src="https://images.ctfassets.net/vwq10xzbe6iz/7yjEplwqk3bj6K9lfULGQm/2fa7a97f21a0ff6821ead4e0fb0a5918/opextitle.png" alt="youtubeopex"></p>
<p><strong>Cost of Revenue</strong>
YouTube’s largest cost base is the revenue it shares with content publishers. It splits ad and subscription revenue attached to content viewership in the ratio 45/ 55% in the favour of publishers.</p>
<p>YouTube’s other major variable costs are the delivery and data-centre operation costs for the 500 hours of video content that gets uploaded every minute. These costs largely include depreciation charges following major upfront capex investments in data-centres. We estimate 7% in line with Alphabet’s overall depreciation charge as % of revenue.</p>
<p><strong>Operating Costs</strong>
Alphabet does not publicly disclose YouTube’s other operating expenses so we will need to estimate.</p>
<p>YouTube’s major fixed costs are its operating costs which are largely staff related and associated with sales and marketing, supporting creators, research &amp; development (software and product roles), content moderation (outsourced contractors), finance, admin and support.</p>
<p>We can estimate YouTube’s opex margins by comparing against peers. Netflix has a total opex/ revenue of 20% and Facebook at ~35-45%. We can estimate that YouTube may currently operate within the range at 35% opex/ revenue. We can assume given scale benefits that this will lower over time in line with Netflix at 20% opex as % of revenue.</p>
<p>This analysis highlights that YouTube’s profitability is lower than other social media platforms and Google’s other ad businesses. This is largely due to YouTube’s reliance on ad sharing with content publishers.</p>
<p><strong>Capex &amp; Acquisitions</strong>
Alphabet does not separate YouTube’s cashflow financials so we must estimate capex figures.</p>
<p>In our methodology, depreciation is already accounted for in the operating expenses hence we must estimate the incremental net capital expenditure (e.g. capex less depreciation) in order to estimate the total cash outflows associated with investment in infrastructure.</p>
<p>We will adopt the Godfather of Valuation’s (Aswath Damodaran) approach of using the <a href="https://www.informit.com/articles/article.aspx?p=2928207&amp;seqNum=5">sales to capital ratio</a> to estimate the required net capex to support revenue growth. This is calculated as the net change in revenue divided by the net capex of each year.</p>
<p>Alphabet has an average sales to capital ratio of 1.9x (annual change in revenue divided by change in invested capital (debt + equity less cash)). This implies that for every $100 of incremental revenue Alphabet generates each year it reinvests $50 as capital to support growth.</p>
<blockquote>
<p>When estimating costs, identify the major drivers across cost of revenue, operating costs and capex costs. Go through annual financial reports to identify the major line items and review notes to understand what is included and not included and understand the main drivers.</p>
</blockquote>
<h3>5. What is the growth thesis for YouTube?</h3>
<p>Based on our above analysis and understanding we can base our growth thesis for YouTube on two key drivers: ad monetization and stable user growth.</p>
<p><strong>Ad Monetization</strong>
YouTube is early in its monetization strategy and has largely relied on ad revenue from high quality in-video ad formats driven by impressions. It has begun to experiment with new formats including direct response display ads within homepage recommendation feeds and direct text search ads. These new formats support greater ability to sell more ads to its user base.</p>
<p>We can estimate that YouTube over the next 10 years will increase its ad monetization from $8 ARPU to $23 ARPU in line with Facebook currently.</p>
<p><strong>Stable User Growth</strong>
YouTube shares a majority of ad revenue to content publishers which is negative for gross profit margins but positive for supporting high quality content creation. This will likely attract amateur and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mannhowie.com/youtube-valuation">https://mannhowie.com/youtube-valuation</a></em></p>]]>
            </description>
            <link>https://mannhowie.com/youtube-valuation</link>
            <guid isPermaLink="false">hacker-news-small-sites-25176451</guid>
            <pubDate>Sun, 22 Nov 2020 10:44:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Birth of Unix with Brian Kernighan]]>
            </title>
            <description>
<![CDATA[
Score 369 | Comments 78 (<a href="https://news.ycombinator.com/item?id=25176318">thread link</a>) | @rodrigo975
<br/>
November 22, 2020 | https://corecursive.com/058-brian-kernighan-unix-bell-labs/ | <a href="https://web.archive.org/web/*/https://corecursive.com/058-brian-kernighan-unix-bell-labs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><span>When you work on your computer, there are so many things you take for granted: operating systems, programming languages, they all have to come from somewhere. </span></p><p><span>In the late 1960s and 1970s, that somewhere was Bell Labs, and the operating system they were building was UNIX. </span></p><p><span>They were building more than just an operating system though. They were building a way to work with computers that had never existed before.&nbsp; </span></p><p><span>In today’s episode I talk to Brian Kernighan about the history of Unix.</span></p><h3><b>Quotes</b></h3><p><em><span>“If you wanted, you could go sit in your office and think deep thoughts or program, or write on your own blackboard or whatever, but then come back to the common space when you wanted to.“&nbsp;</span></em></p><p><em><span>“I found it easier to program when I was trying to figure out the logic for myself rather than trying to figure out where in the infinite stack of documentation was the function I needed. So for me, programming is more like creating something rather than looking it up, and too much of today’s programming is more like looking it up.”</span></em></p><p><em><span>“If what I find challenging or hard or whatever is also something that other people find hard or challenging or whatever, then if I do something that will improve my lot, I’m perhaps improving their lot at the same time.”</span></em></p><h2><b>Transcript:</b></h2><p><i><span>Note:&nbsp; This podcast is designed to be heard. If you are able, we strongly encourage you to listen to the audio, which includes emphasis that’s not on the page.&nbsp; The podcast page for</span></i><a href="https://corecursive.com/brian-kernighan-unix-bell-labs/" target="_blank" rel="noopener noreferrer"><i><span> this episode is here</span></i></a></p><p><strong>Adam:&nbsp;</strong></p><p><span>When you work on your computer, there are so many things you take for granted: operating systems, programming languages, they all have to come from somewhere. In the 1960s, that somewhere was Bell Labs, and the operating system they were building was Unix. They were building more than just an operating system though. They were building a way to work with computers that had never existed before. To find out more, I reached out to this guy.</span></p><p><strong>Brian:&nbsp;</strong></p><p><span>I’m Brian Kernighan, and at the moment, I teach computer science at Princeton University.</span></p><p><strong>Adam:&nbsp;</strong></p><p><span>He’s the K in K&amp;R, the famous book about C that still tops most recommended book lists. He was part of this computer science research group at Bell Labs for 30 years. He’s going to share the story of the creation of Unix, and hopefully, I’m going to try to figure out some of their secrets to being so impactful. Along the way, we’re going to have to learn about the Unix philosophy and printing patent applications, but we’re also going to have to learn about 10-kilo chocolate bars and fake demos to the CIA, and of course, British satirical magazines.</span></p><p><strong>Adam:</strong></p><p><span>The story of Unix is a story about Bell Labs, so let’s start at the beginning when Brian is a grad student and he gets an internship to work there for the summer.</span></p><p><strong>Brian:</strong></p><p><span>Bell Labs is a very big building, a sequence of connected buildings, and probably 3,000 people working over these long multi-story buildings. The thing that I remember most clearly about the first day, and I think it was the first day of the first internship, so call it the summer of 1967, and I got an office, and if I recall correctly, I had an office to myself. So this is something that’s unheard of in the modern era.</span></p><p><strong>Brian:</strong></p><p><span>But I had an office to myself, and I was sitting there in my office at probably 11:00 or something like that in my first morning, I wondered, “What the heck do I do? I have no idea what’s going on.” And this older gentleman came past my office and he said, “Hi, I’m Dick… Let’s go to lunch.” I thought, “Well, okay.” I went off to lunch with Dick…, whose name I hadn’t caught. We had a good lunch, he was an interesting kind of curmudgeonly, but intriguing guy. Then after lunch, he went off somewhere else.</span></p><p><strong>Brian:</strong></p><p><span>I snuck past my office to his office on the same corridor to see who the heck he was because everybody had name tags on the doors. It turns out it was Dick Hamming, the inventor of error-correcting codes.</span></p><p><strong>Adam:</strong></p><p><span>Dick Hamming is aka Richard Hamming. His Wikipedia page is huge. He worked on the Manhattan project programming computers to calculate the equations needed to develop nuclear weapons. One year after this lunch with Brian, he would win the Turing Award, the so-called Nobel Prize of Computing for his work on error-correcting codes. Hamming is also famous for this talk he gave on the secret to having impact in your professional life.</span></p><p><strong>Brian:</strong></p><p><span>The talk was called You and Your Research, and it was basically a retrospective on his career, thinking whether there were general lessons that would help other people in some way to have a better career. He was very, very interesting, and I think a good example of somebody with clearly lots of talent, but not a super genius type, who made the most of what he had. Who in every way, amplified so that he compounded his effect on the world. The other thing that’s maybe is appropriate for today, he used to say that he would reserve Friday afternoons for thinking great thoughts.</span></p><p><strong>Brian:</strong></p><p><span>He would sit in his office, he would put his feet on the desk, and he would think great thoughts, whatever that might be. It was usually introspection on himself or on where was the field going, or what might happen in the future? What might you do to take advantage of that or deal with it in some way or other? This is Friday morning when we’re talking, and I don’t get that luxury on Friday afternoons very often, but it’s a useful way to think of it. You say, “I’m going to stop and do it regularly to take stock of what’s going on, and in some way, think about, ‘What could I be doing that in some way would be better, that would be more useful for me or my family or the world or whatever?'”</span></p><p><strong>Brian:</strong></p><p><span>He did that quite religiously, you went in after lunch on Friday, you’d find him sitting in his office thinking great thoughts. So he’s fun.</span></p><p><strong>Adam:</strong></p><p><span>I love this advice, it presupposes that if I just had my Fridays free, and I wrote thinking great thoughts on my calendar, I would upgrade thoughts. I mean, maybe that’s the case, I’ll give it a try. There’s one concept though that Hamming is most famous for, and that is about how you choose what to work on.</span></p><p><strong>Brian:</strong></p><p><span>The way he told it to me and probably lots of others was that he used to eat with some group of people like chemists, I think the specific thing was, and he would eat at their table at lunchtime, big cafeteria setting. He would sit down with chemists and talk to them and he would ask them what they were working on, and whether what they’re working on could possibly lead to a Nobel Prize. The answer was often no, not a chance, and that was the point where he’d say, “Well, then why are you working on it? Because if it couldn’t at least potentially lead to a Nobel Prize, it isn’t important. Why are you wasting your time on something that isn’t important?”</span></p><p><strong>Adam:</strong></p><p><span>Whether intentionally or not, Brian followed this advice. When he returned to Princeton to work on his thesis, he was working on graph partitioning, which we now know is in some sense, equivalent to the traveling salesman problem. You have to find an optimum route that the salesman would travel from city to city minimizing travel distance. To complete his thesis, Brian had to work on the computers of Princeton at the time. Computers today are a lot different than they were in 1967 and ’68 at Princeton. At the time, computers were all about Fortran and punch cards.</span></p><p><strong>Brian:</strong></p><p><span>Fortran was designed in a card environment very definitely, and I assume the cards came before Fortran, but in my mind, they’re very strongly linked. And so yes, it was basically one statement per line, which was, therefore, one physical card. And so, when you wrote a program, you had to punch it on these punch cards, and then make sure you kept them in order and things like that and then you handed them to somebody who operated a very big, expensive machine. And a while later, back would come to your results, very often where it’s just something like there was a syntax error somewhere, and you had to find the cards that were wrong, replace them with new cards that were right and repeat the process, but with a very, very long latency that could be often measured in hours or sometimes even days.</span></p><p><strong>Brian:</strong></p><p><span>It’s not exactly like an instant compilation. And Fortran itself is a kind of clunky language as well in part reflecting those early days in computing, and partly just the fact that we didn’t understand a lot, and the computers themselves were not particularly sophisticated. Then finally, Fortran was intended for scientific computing. It was not intended for, let’s say, general-purpose system programming or anything like that. All of those things meant that although the program was a lot of fun, it’s not the same as it became five or 10 years later, and it has continued to evolve.</span></p><p><strong>Adam:</strong></p><p><span>I had to watch a couple of YouTube videos to get a sense of this punch card world. A punch card is like an index card, but it’s wider because it has 80 columns. And each of these columns corresponds to a single character. You punch holes in that column to indicate what letters should go there, and so each punch card represents one line of Fortran code. People build the programs this way, punching these cards, putting them into big boxes in order that they would carry around, then you take someplace to give them to a computer operator who would give them to the computer that would read all this in and run the program.</span></p><p><strong>Adam:</strong></p><p><span>So if you had a 1,000-line program, you would have 1,000 cards. There were no screens, no interactive output. You gave your cards to the computer operator and waited for your printout that was the result of your program. Computers were expensive and giant, so they wanted to maximize the throughput. Your program might be doing expensive mathematical calculations, but you could also just be doing word processing. One card might say, “In bold, print my thesis,” and the next would say, “Print, by Brian Kernighan,” and so on. It’s like a verbose way of using a typewriter, except the advantage is you could change the cards around and have it reprinted.</span></p><p><strong>Brian:</strong></p><p><span>T…</span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://corecursive.com/058-brian-kernighan-unix-bell-labs/">https://corecursive.com/058-brian-kernighan-unix-bell-labs/</a></em></p>]]>
            </description>
            <link>https://corecursive.com/058-brian-kernighan-unix-bell-labs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25176318</guid>
            <pubDate>Sun, 22 Nov 2020 10:14:19 GMT</pubDate>
        </item>
    </channel>
</rss>
