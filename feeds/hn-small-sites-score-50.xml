<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 11 Sep 2020 08:24:57 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Fri, 11 Sep 2020 08:24:57 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why Bayesian Stats Needs Monte-Carlo Methods]]>
            </title>
            <description>
<![CDATA[
Score 107 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24416908">thread link</a>) | @laplacesdemon48
<br/>
September 8, 2020 | https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods | <a href="https://web.archive.org/web/*/https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site">
		<div id="canvas">

			<!-- / headerWrapper -->

			<div id="pageWrapper" role="main">
				<section id="page" data-content-field="main-content">
					<article id="article-5f3999dd551f15457784cec9" data-item-id="5f3999dd551f15457784cec9">

	<div>
  <!--SPECIAL CONTENT-->

    

    <div>

    <!--POST HEADER-->

			<header>
				
				
			</header>

    <!--POST BODY-->

      <div><div data-layout-label="Post Body" data-type="item" data-updated-on="1597610547002" id="item-5f3999dd551f15457784cec9"><div><div><div data-block-type="2" id="block-ce72701dbc5d55959e37"><div><div><p>This post emerged from a series of question surrounding a Twitter comment that brought up some very interesting points about how Bayesian Hypothesis testing works and the inability of analytic solutions to solve even some seemingly trivial problems in Bayesian statistics. </p><p>Comparing Beta distributed random variables is something that comes up pretty frequently on this blog (and in my book as well). The set up is fairly straight forward: model an A/B test as sampling from two beta distributions, sample from each distribution a lot, then compare the results.</p><p>This simulation approach often first appears as a clever little trick to solve a more complex math problem, but in fact is a primative form of Monte-Carlo Integration and turns out to one of the only ways to really solve this problem. By exploring this topic deeper in this post we'll see some of the myths that many people have about analytic solutions as well as demonstrating why Monte-carlo methods are so essential to Bayesian statistics.</p></div><h2>Background: A conversation about election results</h2><div><p>An interesting conversation happened on Twitter recently. It started with a retweet of mine regarding Nate Silver (well know author and election forecaster) posting his latest predictions for the 2020 presidential election showing that Biden has a 71% probability of winning versus Trump's 29%</p></div></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_4659"><div><div><p>After the last election there was quite a lot of criticism about Nate Silver's forecasting since his company (538) predicted that <a href="https://projects.fivethirtyeight.com/2016-election-forecast/">Hilary Clinton would win with a probability of 71%</a> in the 2016 presidential election.</p><p>This criticism has always annoyed me personally since, in statistical terms, 71% is generally not considered a strong belief in anything. So it is not inconsistent, nor suprising for someone to believe a candidate has 71% chance of success and they still lose. Even when looking at typical p-values, we wait for 95% percent certainty before making claims (and many feel this is a pretty weak belief). But for some reason whenever election polls come up, it seems even very statistically minded people suddenly think that 51% chance is a high probability.</p><p>I retweeted Nate Silver's forecast, mentioned my annoyance and provided an example of another case with a similar probability of winning:</p></div></div></div><div data-block-json="{&quot;cache_age&quot;:&quot;3153600000&quot;,&quot;authorUrl&quot;:&quot;https://twitter.com/willkurt&quot;,&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;resolveObject&quot;:&quot;Tweet&quot;,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p dir=\&quot;ltr\&quot; lang=\&quot;en\&quot;>This time can we all remember that rarely in statistics would we judge P(H|D) = 0.71 as a strong belief in anything. <br><br>For comparison if in an A/B test we had these results:<br><br>A has 2 successes in 15 trials<br>B has 3 successes in 14 trials<br><br>This roughly how strong our belief in B is <a href=\&quot;https://t.co/bB4PiB5Tao\&quot;>https://t.co/bB4PiB5Tao</a></p>\u2014 Will Kurt (@willkurt) <a href=\&quot;https://twitter.com/willkurt/status/1293575032975884288?ref_src=twsrc%5Etfw\&quot;>August 12, 2020</a></blockquote>\n<script async=\&quot;\&quot; src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/willkurt/status/1293575032975884288&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;authorName&quot;:&quot;Will Kurt&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;resolved&quot;:true,&quot;type&quot;:&quot;rich&quot;,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;providerUrl&quot;:&quot;https://twitter.com&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1597610689295_6372"><div><blockquote><div dir="ltr" lang="en"><p>This time can we all remember that rarely in statistics would we judge P(H|D) = 0.71 as a strong belief in anything. </p><p>For comparison if in an A/B test we had these results:</p><p>A has 2 successes in 15 trials<br>B has 3 successes in 14 trials</p><p>This roughly how strong our belief in B is <a href="https://t.co/bB4PiB5Tao">https://t.co/bB4PiB5Tao</a></p></div>— Will Kurt (@willkurt) <a href="https://twitter.com/willkurt/status/1293575032975884288?ref_src=twsrc%5Etfw">August 12, 2020</a></blockquote>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_6436"><div><div><p>If you were running an A/B test and your A variant had 2 success in 15 trials and your B variant had 3 successes in 14 trails, you would be roughly 71% confident that B was the superior variant.</p><p>Even someone without much statistical training would likely be very skeptical of such a claim, but somehow during election forecasts even experience statisticians can look at Silver's post and think that Biden winning is a sure thing.</p></div><h2> How do we arrive at P(B &gt; A)?</h2><p><br>Twitter user <a href="https://twitter.com/mbarras_ing">@mbarras_ing</a> ask a really important follow up question, asking to explain this result:</p></div></div><div data-block-json="{&quot;cache_age&quot;:&quot;3153600000&quot;,&quot;authorUrl&quot;:&quot;https://twitter.com/mbarras_ing&quot;,&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;resolveObject&quot;:&quot;Tweet&quot;,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p dir=\&quot;ltr\&quot; lang=\&quot;en\&quot;>I may be a bit slow but could you elaborate on how that is? How would one compute 0.71, from the info \&quot;A has 2 successes in 15 trials, B has 3 successes in 14 trials\&quot;?</p>\u2014 Matthew Rhys Barras (@mbarras_ing) <a href=\&quot;https://twitter.com/mbarras_ing/status/1293928326579589121?ref_src=twsrc%5Etfw\&quot;>August 13, 2020</a></blockquote>\n<script async=\&quot;\&quot; src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/mbarras_ing/status/1293928326579589121&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;authorName&quot;:&quot;Matthew Rhys Barras&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;resolved&quot;:true,&quot;type&quot;:&quot;rich&quot;,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;providerUrl&quot;:&quot;https://twitter.com&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1597610689295_7791"><div><blockquote><p dir="ltr" lang="en">I may be a bit slow but could you elaborate on how that is? How would one compute 0.71, from the info "A has 2 successes in 15 trials, B has 3 successes in 14 trials"?</p>— Matthew Rhys Barras (@mbarras_ing) <a href="https://twitter.com/mbarras_ing/status/1293928326579589121?ref_src=twsrc%5Etfw">August 13, 2020</a></blockquote>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_7855"><div><div><p>I very strongly believe that all statistics, even quick off the cuff estimates, should be reproducible and explainable.</p><p>I've written a fair bit about approaching similar problems both <a href="https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing">on this blog</a> and <a href="https://www.amazon.com/Bayesian-Statistics-Fun-Will-Kurt/dp/1593279566">in my book</a>. The big picture is that we're going to come up with parameter estimates for the rate that A and B convert users and then compute the probability that B is greater than A. </p><p>Since we're estimating conversion rates we're going to use <a href="https://www.countbayesie.com/blog/2015/3/17/interrogating-probability-distributions">the Beta distribution</a> as the distribution of our parameter estimate. In this example I'm also assume a \(\text{Beta}(1,1)\) prior for our A and B variants.</p><p>The likelihood for A is \(\text{Beta}(2,13)\) and for B is \(\text{Beta}(3,11)\) so we can represent A and B as two random variables samples form these posteriors:</p><p>$$A \sim \text{Beta}(2+1, 13+1)$$<br>$$B \sim \text{Beta}(3+1,11+1)$$</p></div><p>We can now represent this in R, and sample from these distributions:</p></div></div><div data-block-type="23" id="block-yui_3_17_2_1_1597610689295_9136"><div><pre><span>N</span> <span>&lt;</span><span>-</span> <span>10000</span>
<span>a_samples</span> <span>&lt;</span><span>-</span> <span>rbeta</span>(<span>N</span>,<span>2</span><span>+</span><span>1</span>,<span>13</span><span>+</span><span>1</span>)
<span>b_samples</span> <span>&lt;</span><span>-</span> <span>rbeta</span>(<span>N</span>,<span>3</span><span>+</span><span>1</span>,<span>11</span><span>+</span><span>1</span>)</pre></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_9202"><p>And finally we can look at the results of this to compute the probability that B is greater than A:</p></div><div data-block-type="23" id="block-yui_3_17_2_1_1597610689295_10612"><div><pre><span>sum</span>(<span>b_samples</span> <span>&gt;</span> <span>a_samples</span>)<span>/</span><span>N</span></pre></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_10678"><div><p>In this case we get 0.7028 pretty close to 71% for Nate Silver’s problem.</p><h2><p>Can we solve this without R?</p></h2><p><br>This explains where we get our probabilities from, but there is an obvious question that comes up when you see this result, one raised by <a href="https://twitter.com/little_rocko">@little_rocko</a></p></div></div><div data-block-json="{&quot;cache_age&quot;:&quot;3153600000&quot;,&quot;authorUrl&quot;:&quot;https://twitter.com/little_rocko&quot;,&quot;width&quot;:550,&quot;height&quot;:null,&quot;hSize&quot;:null,&quot;resolveObject&quot;:&quot;Tweet&quot;,&quot;html&quot;:&quot;<blockquote class=\&quot;twitter-tweet\&quot;><p dir=\&quot;ltr\&quot; lang=\&quot;en\&quot;>this is an awesome example. is there an easy way (as in non-brute force) to finding the beta parameters that'll match a probability?</p>\u2014 Rocko (@little_rocko) <a href=\&quot;https://twitter.com/little_rocko/status/1294938572299018242?ref_src=twsrc%5Etfw\&quot;>August 16, 2020</a></blockquote>\n<script async=\&quot;\&quot; src=\&quot;https://platform.twitter.com/widgets.js\&quot; charset=\&quot;utf-8\&quot;></script>&quot;,&quot;url&quot;:&quot;https://twitter.com/little_rocko/status/1294938572299018242&quot;,&quot;resolvedBy&quot;:&quot;twitter&quot;,&quot;floatDir&quot;:null,&quot;authorName&quot;:&quot;Rocko&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;resolved&quot;:true,&quot;type&quot;:&quot;rich&quot;,&quot;providerName&quot;:&quot;Twitter&quot;,&quot;providerUrl&quot;:&quot;https://twitter.com&quot;}" data-block-type="22" id="block-yui_3_17_2_1_1597610689295_12117"><div><blockquote><p dir="ltr" lang="en">this is an awesome example. is there an easy way (as in non-brute force) to finding the beta parameters that'll match a probability?</p>— Rocko (@little_rocko) <a href="https://twitter.com/little_rocko/status/1294938572299018242?ref_src=twsrc%5Etfw">August 16, 2020</a></blockquote>
</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_12181"><div><div><p>This question is interesting because it asks two questions that don't necessarily have to be related:</p><p>- is there an easy way to solve this<br>- is there a non-"brute force" way solve this</p><p>Before moving on I want to mention that this little snippet of R involves a lot more abstraction then it seems at first glance. What we are really doing here is equalivant to using a Monte-Carlo simulation to integrate over the distribution of the difference between two Beta distributed random variables. After the next two sections it will be more clear that what's happening here is a surprisingly sophisticated operation that is, in my opinion, the easiest method of solving this problem as well as not truly a brute force solution.</p></div><h2><br>Analytic versus Easy</h2><div><p>When we see computational solutions to mathematical problems our first instinct is typically to feel that we are avoiding solving the problem <em>analytically.</em> An analytical solution is one that uses mathematical analysis to find a closed form solution.</p><p>A strivial example, suppose I wanted to find the value that minimized \(f(x) = (x+3)^2\)</p><p>In R I could brute force this by looking over a range of answers like this:</p></div></div></div><div data-block-type="23" id="block-yui_3_17_2_1_1597610689295_13626"><div><pre><span>f</span> <span>&lt;</span><span>-</span> <span>function</span>(<span>x</span>){  
  (<span>x</span><span>+</span><span>3</span>)<span>^</span><span>2</span>
}
<span>xs</span> <span>&lt;</span><span>-</span> <span>seq</span>(<span>-</span><span>6</span>,<span>6</span>,<span>by</span><span>=</span><span>0.031</span>)
<span>xs</span>[<span>which</span>.<span>min</span>(<span>f</span>(<span>xs</span>))]</pre></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1597610689295_13692"><div><div><p>As expected we get our answer of -3, but this solution takes a bit of work: we need to know how to code and we need a computer. It's also a bit messy because if we had iterated by an incredment that didn't include -3 exactly (say by 0.031) we would not get the exact answer.</p><p>If we know some basic calculus we know that our minimum has to be where the derivative is at 0. We can very easily work out that</p><p>$$f'(x) = 2(x+3)$$<br>And that</p><p>$$2(x + 3) = 0 $$</p><p>When</p><p>$$ x = -3 $$</p><p>Knowning basic calculus this later solution becomes much easier. </p><p>But even with the calculus is part is hard, often solving it once makes future solutions much easier. Take for example if you wanted to find the maximum likelihood for a normal distribution with a mean of \(\mu\) and standard deviation of \(\sigma\)</p><p>To solve this we start with our PDF for the normal distribution \(\varphi\):</p><p>$$\varphi(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$</p><p>Now computing the derivative of this is not necessarily "easy" but it's certainly something we can do. All we really care about is when</p><p>$$\varphi'(x) = 0$$</p><p>Which we can find out happens when (computing the deriviative of course is left as an exercise for the reader):</p><p>$$\frac{\mu-x}{\sigma}$$</p></div><div><p>This allows us to reallize the amazing fact that for any normal distribution we come across, we know that the maximum likelihood estimate is the sample is when \(x = \mu\)!</p><p>Even though our calculus might take us a bit of work, once this is done the problem of doing maximum likelihood estimation for any Normal distribution truly does become easy!</p></div><h3><br>Proposing an Analytic solution to our problem</h3><div><p>Let's revisit our original problem this time attempting to find an analytic solution. This is a very interesting case because arguably this is the simplest Bayesian hypothesis test you can imagine.</p><p>Recall that we have two random variable representing our beliefs in each test. These are distributed according the the posterior which we described earlier.</p><p>$$A \sim \text{Beta}(2+1, 13+1)$$<br>$$B \sim \text{Beta}(3+1,11+1)$$</p><p>Here is where I skipped some steps in reasoning. What we want to know is:</p><p>$$P(B &gt;A)$$</p></div><div><p>Which is not expressed in a particularly useful mathematical way. A better way to solve this is to consider this as the sum (or difference in this case) of two random variables. What we really want to know is:</p><p>$$P(B - A &gt; 0)$$</p><p>In order to solve this problem we can think of a new random variable \(X\) which is going to be the difference between B and A:</p><p>$$X = B - A$$</p><p>Finally we'll suppose we have a probability density function for \(X\) we'll call \(\text{pdf}_X\). If we know \(\text{pdf}_X\) our solution is pretty close, we just need to integrate between 0 and the max domain of this distribution:</p><p>$$P(B &gt; A) = P(B - A &gt; 0) = \int_{x=0}^{\text{max}}\text{pdf}_{X}(x)$$</p><p>Already this is starting to look a bit complicated, but there's one big problem ahead. Unlike Normally distributed random variables, we have no equivalent of the Normal sum theorem (we'll cover this in a bit) for Beta distributed random variables. </p><p>What does \(\text{pdf}_X\) look like? For starters we know it's not a Beta distribution itself. We can see this because we know the domain (or support) of this distribution is not \([0,1]\). Because they are Beta distributed, A and B can both take on values from 0 to 1, which means the maximum result of this difference is 1 but the minimum is -1. So whatever this distribution is, its domain is \([-1,1]\) meaning it cannot be a Beta distribution.</p><p>We can use various rules about sum of random variables to determine the mean and variance of this distribution, but without knowing the exact form of this distribution we are unable to solve the integral analytically. </p><p>Here we can see that even in this profoundly simple problem the analytical solution is frustratingly …</p></div></div></div></div></div></div></div></div></div></article></section></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods">https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods</a></em></p>]]>
            </description>
            <link>https://www.countbayesie.com/blog/2020/8/16/why-bayesian-stats-need-monte-carlo-methods</link>
            <guid isPermaLink="false">hacker-news-small-sites-24416908</guid>
            <pubDate>Wed, 09 Sep 2020 03:54:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AMD PSB Vendor Locks EPYC CPUs for Enhanced Security at a Cost]]>
            </title>
            <description>
<![CDATA[
Score 273 | Comments 166 (<a href="https://news.ycombinator.com/item?id=24416005">thread link</a>) | @virgulino
<br/>
September 8, 2020 | https://www.servethehome.com/amd-psb-vendor-locks-epyc-cpus-for-enhanced-security-at-a-cost/ | <a href="https://web.archive.org/web/*/https://www.servethehome.com/amd-psb-vendor-locks-epyc-cpus-for-enhanced-security-at-a-cost/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover.jpg" data-caption="AMD Platform Secure Boot Feature Cover"><img width="696" height="465" src="https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-696x465.jpg" srcset="https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-696x465.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-400x268.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover-628x420.jpg 628w, https://www.servethehome.com/wp-content/uploads/2020/09/AMD-Platform-Secure-Boot-Feature-Cover.jpg 800w" sizes="(max-width: 696px) 100vw, 696px" alt="AMD Platform Secure Boot Feature Cover" title="AMD Platform Secure Boot Feature Cover"></a><figcaption>AMD Platform Secure Boot Feature Cover</figcaption></figure></div>
            <!-- content --><p>Today we are going to discuss a change to server security that is going to make waves in the home lab and secondary markets for servers and components in the future. During our recent <a href="https://www.servethehome.com/dell-emc-poweredge-c6525-review-2u4n-amd-epyc-kilo-thread-server/">Dell EMC PowerEdge C6525 review</a> we briefly mentioned that AMD EPYC CPUs in the system are vendor locked to Dell EMC systems. This is not a Dell-specific concern. We have confirmed that other vendors are supporting the feature behind this. For the large vendors, their platform security teams are pushing to build more secure platforms for their customers, and that is going to have future impacts on the secondary server market and home labs.</p>
<p>In this article, we are going to cover the basics of what is happening. We are going to discuss the motivations, and why this is going to be more common in the future. Finally, we are going to discuss what those in the industry can do to keep the secondary server market operating well. If you work with partners or resellers who dip into used parts bins or even have the potential to purchase grey market CPUs, send them this article or accompanying video. The current market has a large disconnect between what some large customers are asking for, and large vendors are delivering on and what others in the market know is happening.<span id="more-46716"></span></p>
<h2>Accompanying Video</h2>
<p>This is an important topic. To ensure that we can cover those who like to read/ skim and those who like to get information via audio, we have an accompanying video:</p>
<p><iframe title="Vendor Locking AMD EPYC CPUs Great for Security at a Cost" width="696" height="392" src="https://www.youtube.com/embed/kNVuTAVYxpM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>Feel free to pop open that video on YouTube and check it out or to send to those who prefer not to read.</p>
<h2>Background: How we learned this was a “thing”</h2>
<p>In 2018 we did a <a href="https://www.servethehome.com/dell-emc-poweredge-r7415-review/">Dell EMC PowerEdge R7415 review</a> and as part of that review, we started our normal process of trying different CPUs in the system. Early in that process, we used an AMD EPYC 7251 CPU, the low-end 8-core model, and noticed something curious. It would not work in our other test systems after.</p>
<p>After a bit of research, we found it was because Dell EMC was vendor locking the chips to Dell systems. We did not know exactly why, but we were told was a security feature. At this point, and even to this day two years later, not every vendor takes advantage of all of the AMD EPYC security features. What that practically means is that what we saw with the Dell EMC system is not what we saw with other systems. For example, we were able to interchangeably use CPUs in Supermicro and Tyan systems, but we could not use those systems once they went into a Dell EMC server.</p>
<figure id="attachment_24514" aria-describedby="caption-attachment-24514"><a href="https://www.servethehome.com/dual-amd-epyc-7251-linux-benchmarks-least-expensive-2p-epyc/amd-epyc-7251-in-socket-and-carrier/" rel="attachment wp-att-24514"><img src="https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier.jpg" alt="AMD EPYC In Socket And Carrier" width="800" height="533" srcset="https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier.jpg 800w, https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier-400x267.jpg 400w, https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier-696x464.jpg 696w, https://www.servethehome.com/wp-content/uploads/2017/09/AMD-EPYC-7251-in-Socket-and-Carrier-630x420.jpg 630w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption id="caption-attachment-24514">AMD EPYC In Socket And Carrier</figcaption></figure>
<p>We found we were not alone. Laboratories, VARs, and other organizations were finding that transferring AMD EPYC CPUs from one vendor’s system to another, was not the simple process it was on the Intel Xeon side. It did not always work.</p>
<p>We knew it was a security feature and thought that most who are buying servers would be informed of this by their sales reps or channel partners. After I personally got a lot of texts, e-mails, instant messaging, and comments on our C6525 video and article, I realized that this actually may be a situation where many people do not know what is going on.</p>
<figure id="attachment_46531" aria-describedby="caption-attachment-46531"><a href="https://www.servethehome.com/dell-emc-poweredge-c6525-review-2u4n-amd-epyc-kilo-thread-server/dell-emc-poweredge-c6525-internal-view-nodes-partially-out/" rel="attachment wp-att-46531"><img src="https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out.jpg" alt="Dell EMC PowerEdge C6525 Internal View Nodes Partially Out" width="800" height="519" srcset="https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out-400x260.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out-696x452.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/08/Dell-EMC-PowerEdge-C6525-Internal-View-Nodes-Partially-Out-647x420.jpg 647w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption id="caption-attachment-46531">Dell EMC PowerEdge C6525 Internal View Nodes Partially Out</figcaption></figure>
<p>This experience that we had, apparently is one that is not overly common yet. That makes sense because the systems that utilize the enhanced security levels are still largely new, and being used by their first buyers. Also, AMD still has a smaller market share than Intel. A big reason, by the way, that Intel Xeon does not have this issue is that they do not have the security feature that AMD has. Vendors have come out and stated that their AMD EPYC systems are more secure than their Intel Xeon systems, and this behavior is a byproduct of that enhanced security.</p>
<p>Next, we are going to dive into the feature of AMD processors (and what will be more common in future CPUs from other vendors.)</p>
<h2>AMD EPYC Secure Processor Platform Secure Boot (PSB)</h2>
<p>Let us start with the high-level slide. This is effectively the same slide on the AMD Secure Processor that we saw with the AMD EPYC 7001 series launch, but this is from the EPYC 7002 series. AMD EPYC CPUs may be x86, but they have an embedded Arm Cortex-A5 microcontroller that runs its own OS that is independent of the main system. This AMD Secure Processor is the backbone of AMD’s security push as it provides features such as key management and hardware root of trust for the platform.</p>
<figure id="attachment_36705" aria-describedby="caption-attachment-36705"><a href="https://www.servethehome.com/amd-epyc-7002-series-rome-delivers-a-knockout/amd-epyc-7002-platform-secure-processor/" rel="attachment wp-att-36705"><img src="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor.jpg" alt="AMD EPYC 7002 Platform Secure Processor" width="1792" height="918" srcset="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor.jpg 1792w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-400x205.jpg 400w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-800x410.jpg 800w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-696x357.jpg 696w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-1068x547.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Processor-820x420.jpg 820w" sizes="(max-width: 1792px) 100vw, 1792px"></a><figcaption id="caption-attachment-36705">AMD EPYC 7002 Platform Secure Processor</figcaption></figure>
<p>AMD spends time <a href="https://www.servethehome.com/amd-confirms-cts-labs-exploits-requiring-admin-access/">patching this solution</a>&nbsp;to make it more secure, but it is generally fairly hard to reach without some extremely low-level access in a system. We are going to come back to the “Enables hardware validated boot” line shortly, but it is important to understand that this secure processor underpins many of AMD’s best security features.</p>
<p>For example, at STH we use EPYC’s Secure Memory Encryption and Secure Encrypted Virtualization heavily. With AMD EPYC, we do not have to manually manage keys. Instead, the ephemeral keys are managed for us by the AMD Secure Processor. This is the basis for what is really the building wave of confidential computing offerings such as&nbsp;<a href="https://www.servethehome.com/google-cloud-confidential-computing-enabled-by-amd-epyc-sev/">Google Cloud Confidential Computing Enabled by AMD EPYC SEV</a>. Intel has its secure boot features and SGX that will be enhanced greatly with <a href="https://www.servethehome.com/the-2021-intel-ice-pickle-how-2021-will-be-crunch-time/">Ice Lake Xeons</a>, but for now, AMD has this capability while Intel does not. When big vendors say AMD is more secure, the AMD Secure Processor is a cornerstone of those offerings.</p>
<figure id="attachment_36704" aria-describedby="caption-attachment-36704"><a href="https://www.servethehome.com/amd-epyc-7002-series-rome-delivers-a-knockout/amd-epyc-7002-platform-secure-memory-encryption/" rel="attachment wp-att-36704"><img src="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption.jpg" alt="AMD EPYC 7002 Platform Secure Memory Encryption" width="1769" height="890" srcset="https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption.jpg 1769w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-400x201.jpg 400w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-800x402.jpg 800w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-696x350.jpg 696w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-1068x537.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2019/08/AMD-EPYC-7002-Platform-Secure-Memory-Encryption-835x420.jpg 835w" sizes="(max-width: 1769px) 100vw, 1769px"></a><figcaption id="caption-attachment-36704">AMD EPYC 7002 Platform Secure Memory Encryption</figcaption></figure>
<p>Let us discuss that “Enables hardware validated boot” line. While traditionally CPUs just fire up in whatever platform they are in, AMD has intelligence in their CPU due to the Arm-based AMD Secure Processor. EPYC CPUs are designed to be a bit more intelligent about the platforms they are in, and interact with server platform security to act as this root of trust that would not be possible if they effectively just booted up in any system.</p>
<p>Here is a statement from AMD describing the AMD Platform Secure Boot.</p>
<p><em>The AMD Platform Secure Boot Feature (PSB) is a mitigation for firmware Advanced Persistent Threats. It is a defense-in-depth feature. PSB extends AMD’s silicon root of trust to protect the OEM’s BIOS.&nbsp; This allows the OEM to establish an unbroken chain of trust from AMD’s silicon root of trust to the OEM’s BIOS using PSB, and then from the OEM’s BIOS to the OS Bootloader using UEFI secure boot. This provides a very powerful defense against remote attackers seeking to embed malware into a platform’s firmware.</em></p>
<p><em>An OEM who trusts only their own cryptographically signed BIOS code to run on their platforms will use a PSB enabled motherboard and set one-time-programmable fuses in the processor to bind the processor to the OEM’s firmware code signing key. AMD processors are shipped unlocked from the factory, and can initially be used with any OEM’s motherboard. But once they are used with a motherboard with PSB enabled, the security fuses will be set, and from that point on, that processor can only be used with motherboards that use the same code signing key. (<strong>Source</strong>: AMD statement to STH)</em></p>
<p>That is a lot to take in. We asked HPE about this. Their response mirrored what the above was describing. HPE firmware, when a system is first turned on, performs this binding process where the AMD EPYC CPU expects to see HPE signed firmware. If you alter the HPE firmware on the system, the check fails and the system will not work. That means if your HPE motherboard fails, you can replace it and put your CPU in another HPE motherboard with signed HPE firmware. It also means if the server platform’s firmware is not signed by HPE, the processor will see it as evidence of tampering and not work.</p>
<p><strong>Edit: 2020-09-09</strong> – HPE clarified that they are doing this in a different manner than Dell after initially confirming that they were using the AMD PSB feature. After this went live, HPE sent us the following:</p>
<p><em>HPE does not use the same security technique that Dell is using for a BIOS hardware root of trust. HPE does not burn, fuse, or permanently store our public key into AMD processors which ship with our products. HPE uses a unique approach to authenticate our BIOS and BMC firmware: HPE fuses our hardware – or silicon – root of trust into our own BMC silicon to ensure only authenticated firmware is executed.&nbsp; Thus, while we implement a hardware root of trust for our BIOS and BMC firmware, the processors that ship with our servers are not locked to our platforms. (<strong>Source</strong>: HPE)</em></p>
<p>What is at least interesting there is that HPE was initially claiming feature parity with Dell to us, and from the comments on this article were saying they used this feature in sales pitches, but now are saying they are not blowing the eFuses.</p>
<figure id="attachment_39258" aria-describedby="caption-attachment-39258"><a href="https://www.servethehome.com/pcie-gen4-hpe-proliant-dl325-gen10-plus-and-dl385-gen10-plus-amd-epyc-7002/hpe-proliant-dl325-gen10-plus-at-sc19-cpu-cover/" rel="attachment wp-att-39258"><img src="https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover.jpg" alt="HPE ProLiant DL325 Gen10 Plus At SC19 CPU Cover" width="800" height="600" srcset="https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover.jpg 800w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-400x300.jpg 400w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-80x60.jpg 80w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-265x198.jpg 265w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-696x522.jpg 696w, https://www.servethehome.com/wp-content/uploads/2019/11/HPE-ProLiant-DL325-Gen10-Plus-at-SC19-CPU-Cover-560x420.jpg 560w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption id="caption-attachment-39258">HPE ProLiant DL325 Gen10 Plus At SC19 CPU Cover</figcaption></figure>
<p>Here is where the concern develops, and not necessarily for AMD, the OEM, or most of the initial customer base. Customers want more security. The OEMs want to create a secure hardware environment because that is what their customers want. AMD is implementing an advanced security solution beyond what Intel Xeons have giving the OEMs and end-customers what they want. Effectively, when these are sold as new systems, this is exactly what everyone involved wants.</p>
<p>If everyone is getting what they want, then where is the concern, that is what we are going to cover next.</p>
        </div></div>]]>
            </description>
            <link>https://www.servethehome.com/amd-psb-vendor-locks-epyc-cpus-for-enhanced-security-at-a-cost/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24416005</guid>
            <pubDate>Wed, 09 Sep 2020 02:02:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is the web getting slower?]]>
            </title>
            <description>
<![CDATA[
Score 223 | Comments 202 (<a href="https://news.ycombinator.com/item?id=24413705">thread link</a>) | @oedmarap
<br/>
September 8, 2020 | https://www.debugbear.com/blog/is-the-web-getting-slower | <a href="https://web.archive.org/web/*/https://www.debugbear.com/blog/is-the-web-getting-slower">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <div>
    <div>
    
      
      

      

      <div>
        
        

        <p>A story on Hacker News recently argued that webpage speeds haven't improved, even as internet speeds have gone up.</p>
<p>This article explains why that conclusion can't be drawn from the original data.</p>
<p>We'll also look at how devices and the web have changed over the past 10 years, and what those changes have meant for web performance.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/hn-article.png" alt="Webpage speeds article on Hacker News"></p>
<ol>
<li><a href="#interpreting-the-http-archive-data">Interpreting the HTTP Archive data</a></li>
<li><a href="#how-have-mobile-networks-and-devices-changed-over-the-last-10-years">How have mobile networks and devices changed over the last 10 years?</a></li>
<li><a href="#how-have-websites-changed">How have websites changed?</a></li>
<li><a href="#data-from-the-chrome-user-experience-report">Data from the Chrome User Experience Report</a></li>
<li><a href="#modelling-page-load-times">Modelling page load times</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>
<h2 id="interpreting-the-http-archive-data">Interpreting the HTTP Archive data</h2>
<p>This chart from the <a href="https://www.nngroup.com/articles/the-need-for-speed/">Nielsen Norman Group article</a> suggested that increasing mobile network bandwidth hasn't resulted in faster page load times.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/nngroup-mobile-webperf-chart.png" alt="Chart showing increasing bandwidth along increasing page load times"></p>
<p>However, <strong>the connection speed used by HTTP Archive has not actually increased over time.</strong></p>
<p>Instead it went down in 2013, <a href="https://httparchive.org/faq#what-changes-have-been-made-to-the-test-environment-that-might-affect-the-data">switching from wifi to an emulated 3G connection</a>.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/nngroup-mobile-webperf-chart-annotated.png" alt="Annotation for page loads time showing when methodology changed"></p>
<p>The onLoad metric has increased 55% since 2013, from 12.7s to 19.7s. If you bought a phone in 2013 and have been on a 3G connection ever since, then the web has become slower for you.</p>
<p>Before looking at how devices and the web have changed over the last 10 years, here are a few notes on how to think about this data.</p>
<h3 id="why-look-at-on-load">Why look at onLoad?</h3>
<p>The <code>load</code> event is emitted by the page when all page resources like scripts or images have been downloaded.</p>
<p>If the top of a page renders quickly, but the page also loads 20 images further down, then the onLoad metric will suggest that the page is slow.</p>
<p>A different page might not initially render anything useful at all, and only start loading additional resources and rendering content long after the onLoad event. Yet this page will appear fast.</p>
<p>As a result, onLoad doesn't do a good job measuring whether a user experiences the page as fast.</p>
<p>So why do we even look at this metric? <strong>Because it's been around for a long time</strong>, and HTTP Archive has been tracking it since 2010. Newer metrics like <a href="https://www.debugbear.com/docs/metrics/first-contentful-paint">First Contentful Paint</a> or Time to Interactive were only added to HTTP Archive in 2017.</p>
<h3 id="should-we-expect-increasing-bandwidth-to-result-in-faster-page-load-times">Should we expect increasing bandwidth to result in faster page load times?</h3>
<p>Increasing bandwidth will only make a page load faster if bandwidth is the bottleneck at some point. It won't help if you're on a Gigabit connection with a 1s network roundtrip time.</p>
<p>However, the 1.6Mbps 3G connection emulated by HTTP Archive is very slow, so we should expect significant performance improvements as bandwidth improves. The average website downloads 1.7MB of data in 2020, which will take at least 9s to download on the HTTP Archive connection.</p>
<h3 id="some-more-http-archive-caveats">Some more HTTP Archive caveats</h3>
<p>I'll talk a lot about "the average website" in this article. It's worth noting that HTTP Archive only collects data on homepages, not pages deeper down in the site. The corpus of tested domains has also grown over time.</p>
<p>The tests weren't always run on the same device. Initially a physical iPhone 4 was used, today the tests are run on an emulated Android device.</p>
<p>We'll look at median metric values in this article. If most websites are fast but one in five websites freeze your phone for 20s we won't be able to pick this up.</p>
<h3 id="performance-on-desktop">Performance on desktop</h3>
<p>This article will focus on mobile performance in the US. However, if you're looking at the desktop data from the original article, it's worth noting that the test bandwidth was increased and latency was reduced in 2013.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/desktop-performance.png" alt="Chart showing desktop connection speeds and when emulated connection speed changed"></p>
<h2 id="how-have-mobile-networks-and-devices-changed-over-the-last-10-years">How have mobile networks and devices changed over the last 10 years?</h2>
<p>Let's look at 4 factors:</p>
<ul>
<li>network bandwidth</li>
<li>network latency</li>
<li>processor speeds</li>
<li>browser performance</li>
</ul>
<h3 id="mobile-network-bandwidth-in-the-us">Mobile network bandwidth in the US</h3>
<p>This chart shows average mobile bandwidth in the US by year, according to different sources. It increased from 1 Mbps to around 30 Mbps.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/us-mobile-bandwidth.png" alt="Mobile bandwidth in the US by year"></p>
<p>(I've not been very careful when collecting this data. For example, I didn't consistently distinguish when data was collected from when it was published. <a href="https://docs.google.com/spreadsheets/d/1ifZ_ngADpT3YzezNQLpXKCsr74-BvaBJUkYm6PGpv1g/edit?usp=sharing">You can find my sources here</a>.)</p>
<h3 id="mobile-network-latency-in-the-us">Mobile network latency in the US</h3>
<p>This was harder to find data on, but the results indicate that latency dropped from around 200ms in 2011 to 50ms in 2020.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/us-mobile-latency.png" alt="Mobile latency in the US by year, going down from 200ms in 2011 to 50ms in 2020"></p>
<h3 id="mobile-device-cpu-speeds">Mobile device CPU speeds</h3>
<p>I've not been able to find data on average mobile device speeds in the US. But <a href="https://infrequently.org/">Alex Russel</a> and <a href="https://surma.dev/">Surma</a> have published a <a href="https://twitter.com/slightlylate/status/1233275220275818498">chart showing GeekBench 4 scores alongside the release years of different phones</a>.</p>
<p>Even budget phones have become 4x faster, with iPhones now being up to 20x more powerful.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/mobile-cpu-benchmark.jpeg" alt="Mobile CPU performance over time"></p>
<h3 id="how-have-browsers-changed">How have browsers changed?</h3>
<p>A lot of work has been done on browsers over the last 10 years. JavaScript has become a larger part of the web, so many improvements have focussed here.</p>
<p>Looking at <a href="https://v8.dev/blog/10-years">this chart from the V8 blog</a>, page CPU usage for gone down by a factor of 4.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/v8-performance.png" alt="V8 Speedometer 1 benchmark results 2013 to 2018"></p>
<h4 id="networking">Networking</h4>
<p>Browser networking has also improved, for example with the introduction of HTTP/2 in 2015. 64% of requests are now served over HTTP/2.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/http2.png" alt="HTTP/2 adoption over time"></p>
<h2 id="how-have-websites-changed">How have websites changed?</h2>
<p>Let's look at some data from HTTP Archive to see how websites have changed.</p>
<h3 id="page-weight">Page weight</h3>
<p><a href="https://httparchive.org/reports/page-weight">Mobile page weight</a> increased by 337% between 2013 and 2020. This is primarily driven by an increase in images and JavaScript code.</p>
<p>Other resources also increased a lot –&nbsp;I suspect these are mostly videos.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/page-weight.png" alt="Page weight by resource type over time"></p>
<p>The chart starts in 2013 as HTTP Archive changed its methodology in October 2012. Before page weight was undercounted, as the test stopped when the page load event was triggered, even if more data was still being loaded.</p>
<h3 id="java-script-execution-time">JavaScript execution time</h3>
<p>JavaScript would be the most likely culprit if pages are getting slower despite faster mobile networks. Unfortunately, HTTP Archive only started collecting this data in late 2017, and it seems to have been mostly stable since then.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/javascript-execution-time.png" alt="HTTP Archive JavaScript execution time chart"></p>
<p>The drop in mid-2018 can probably be attributed to a URL corpus change.</p>
<p>Note that the absolute run duration (0.5s) is less than what you'd normally see in a tool like Lighthouse. These tools normally slow down JavaScript execution to emulate a mobile device, but <a href="https://almanac.httparchive.org/en/2019/methodology#lighthouse">this was broken for the HTTP Archive tests</a>. So while this number might be realistic for mid-range phones, a common assumption is that budget phones are around 4x slower.</p>
<h2 id="answering-whether-the-web-has-become-slower">Answering whether the web has become slower</h2>
<p>Has the web become slower? Well, it depends on what your device, network connection, and most-used websites are.</p>
<p>We'd need to weigh real-world performance data to get a distribution that shows how different users experienced the web over time. And should the experience of someone opening thousands of pages a day count as much as someone who only visits Facebook once a week?</p>
<p>I don't have detailed per-user data, but we can take a look at the question in a few different ways:</p>
<ol>
<li>Real-user data from the <a href="https://developers.google.com/web/tools/chrome-user-experience-report">Chrome UX Report (CrUX)</a></li>
<li>Naive modelling based on how websites and devices have changed</li>
</ol>
<p>I also tried downloading old page versions from archive.org and testing them with Lighthouse, but wasn't able to get meaningful results in the time I had available. For example, often some images are missing from the page archive.</p>
<h2 id="data-from-the-chrome-user-experience-report">Data from the Chrome User Experience Report</h2>
<p>The big limitation of CrUX data is that it's only been collected since late 2017. But we can still use it to see if the web has become slower in the last two and a half years.</p>
<p>Note that, unlike HTTP Archive, CrUX looks at the whole domain instead of just homepages.</p>
<p>The data we'll look at is the 75th percentile, meaning pages load at least this fast for 75% of users.</p>
<p>(I'm taking the average across websites rather than the median, which is not great.)</p>
<h3 id="us-page-load-times">US page load times</h3>
<p>CrUX data for the US does not show page performance getting worse.</p>
<p>The onLoad metric shows a slight improvement, maybe due to an increase in bandwidth. Or maybe more activity is now happening after the initial page load.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/crux-us.png" alt="Page load speeds in the US"></p>
<p>The paint metrics seem fairly stable. Largest Contentful Paint is a new metric that has only been collected since mid-2019.</p>
<h3 id="the-rest-of-the-world">The rest of the world</h3>
<p>The downward trend in the US onLoad metric is matched by the global data. There are however signifianct differences in page load times across countries, with onLoad timings in India being almost twice those in South Korea.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/crux-global.png" alt="Page load speeds globally, in the US, UK, Korea, and India"></p>
<p>We can use CrUX data to put HTTP Archive data into perspective. In January 2020 HTTP Archive reported a median (50% percentile) load time of 18.7s, based on its synthetic data.</p>
<p>In contrast, CrUX suggests a load time of just 5.8s –&nbsp;and this is the 75th percentile.</p>
<p>(Note that the Global values here just take an average and are not weighed by population.)</p>
<h2 id="modelling-page-load-times">Modelling page load times</h2>
<p>We can create a theoretical model of how changes in devices, networks, and websites might affect overall performance.</p>
<p>This won't be a great model, but hopefully it will still provide some insight.</p>
<h3 id="theoretical-page-download-time">Theoretical page download time</h3>
<p>Page weight has increased over time, but so has bandwidth. Round-trip latency has also gone down.</p>
<p>Downloading a file the size of the median mobile website would have taken 1.7s in 2013. If your connection hasn't improved since then downloading this much data would now take 4.4s. But with an average connection today it would only take 0.9s.</p>
<p><img src="https://www.debugbear.com/public/blog/is-the-web-getting-slower/minimum-page-download-time.png" alt="TCP download time"></p>
<p>In practice, a website wouldn't consist of just a single request, and other factors like CPU processing or server latency would also affect how quickly the page loads. The onLoad times reported by HTTP Archive are 2-3 times this lower bound.</p>
<p>But we can still use this as an indicator that reduced latency and increased bandwidth have helped make websites load faster overall.</p>
<p>(I'm starting in 2013 rather than 2011, as the HTTP Archive page weight metric has only been measured consistently since then.)</p>
<h3 id="cpu">CPU</h3>
<p>I'm not quite sure how to think about this, but I'll make some guesses anyway.</p>
<p>Someone who used a Galaxy S4 in 2013 and now uses a Galaxy S10 will have seen their CPU processing power go up by a factor of 5. Let's assume that browsers have become 4x more efficient since then. If we naively multiply these numbers we get an overall 20x improvement.</p>
<p>Since 2013, JavaScript page weight has increased 3.7x from 107KB to 392KB. Maybe minification and compression have improved a bit …</p></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.debugbear.com/blog/is-the-web-getting-slower">https://www.debugbear.com/blog/is-the-web-getting-slower</a></em></p>]]>
            </description>
            <link>https://www.debugbear.com/blog/is-the-web-getting-slower</link>
            <guid isPermaLink="false">hacker-news-small-sites-24413705</guid>
            <pubDate>Tue, 08 Sep 2020 21:33:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Non-Posix File Systems]]>
            </title>
            <description>
<![CDATA[
Score 342 | Comments 126 (<a href="https://news.ycombinator.com/item?id=24412970">thread link</a>) | @nsm
<br/>
September 8, 2020 | https://weinholt.se/articles/non-posix-filesystems/ | <a href="https://web.archive.org/web/*/https://weinholt.se/articles/non-posix-filesystems/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
      <div>
        <article>
          <section><p>Operating systems and file systems have traditionally been developed
hand in hand. They impose mutual constraints on each other. Today we
have two major leaders in file system semantics: Windows and <span>POSIX</span>.
They are very close to each other when compared to the full set of
possibilities. Interesting things happened before POSIX monopolized
file system&nbsp;semantics.</p>

<p>When you use a file system through a library instead of going through
the operating system there are some extra possibilities. You are no
longer required to obey the host operating system’s semantics for
filenames. You get to decide if you use <code>/</code> or <code>\</code> to separate
directory components (or something else altogether). Maybe you don’t
even use strings for filenames. The <a href="https://gitlab.com/weinholt/fs-fatfs">fs-fatfs</a> library uses
a list of strings, so it’s up to the caller to define a directory
separator for themselves. While working on that library, I was driven
to write down some ideas that I’ve previously run across and found&nbsp;inspirational.</p>

<p>The very first hierarchical file system was developed for Multics. It
is described in the
paper
<a href="https://www.multicians.org/fjcc4.html">A General-Purpose File System For Secondary Storage</a> (1965)
by Robert C. Daley and Peter G. Neumann. There are several things that I find
astounding about this&nbsp;paper:</p>
<ul>
<li>There were apparently no hierarchical file systems before Multics.
The references do no cite any previous work on this and I haven’t
found&nbsp;any.</li>
<li>Privacy in computing was a big consideration even back&nbsp;then.</li>
<li>Modern hierarchical file system are essentially unchanged from 1965,
but some features disappeared along the&nbsp;way.</li>
<li>There’s a <em>very</em> interesting backup system described in this&nbsp;paper.</li>
</ul>
<p>The paper describes these concepts that we still use&nbsp;today:</p>
<ul>
<li>Files.</li>
<li>Directories.</li>
<li>Root&nbsp;directory.</li>
<li>Working&nbsp;directory.</li>
<li>Tree name, i.e. the name of a file or directory in the tree
structure. Akin to&nbsp;realpath(3).</li>
<li>Path name, which is like a tree name, but it is conceptually
different because components can be links. The paper used <code>":"</code>
instead of <code>"/"</code> to separate&nbsp;directories.</li>
<li>Relative paths. They used an initial <code>":"</code> in place of <code>"./"</code> (or
empty) and <code>"*"</code> in place of <code>".."</code>. Instead of <code>"cd .."</code> you used
<code>"CHANGEDIRECTORY :*"</code>.</li>
<li>Symbolic&nbsp;links.</li>
<li>Access control using access control lists (showed up relatively
recently in&nbsp;<span>POSIX</span>).</li>
<li>Read, execute and write modes on files and directories. Execute
on a directory means permission to search it, just as in&nbsp;<span>POSIX</span>.</li>
<li>Segments and segment numbers (file descriptions and file
descriptors, respectively, if I’m not&nbsp;mistaken).</li>
</ul>
<p>It is said that Unix took inspiration from Multics and simplified
things. What was lost? Whoever implemented the file system for Unix
stopped reading the paper somewhere around subsection 2.3. There are
at least these features we’re&nbsp;missing:</p>
<ul>
<li><p>Access control lists can define a <span>TRAP</span> for a user. Users can refuse
to use TRAPs and instead get an error. Otherwise TRAPs call a named
function at the time of the access. They can be used to implement
smart files that can e.g. monitor file usage or apply password-based
file&nbsp;locks.</p>
<p>(Someone will undoubtedly insert eBPF into the Linux <span>VFS</span> soon and we
will have invented TRAPs&nbsp;again).</p>
</li>
<li><p>Multics fully separated the append mode and the write mode and made
it available to users. You could actually have files and
<em>directories</em> that were writable but not appendable, or appendable
but not writable (or&nbsp;both).</p>
<p>(Append-only <em>files</em> exist on Linux through file attributes settable
by the super user or processes with <code>CAP_LINUX_IMMUTABLE</code>. This is
very weak sauce. Immutable directories exist, but not append-only
directories. And of course there is no way to have a writable file
or directory that can’t&nbsp;grow).</p>
</li>
<li>Directory entries that point to secondary storage. <em>This is a game
changer for file system management.</em> More on this&nbsp;below.</li>
</ul>
<h2 id="multiple-level-storage-and-backups">Multiple-level storage and&nbsp;backups</h2>
<p>Files in the Multics file system could have directory entries that
point to a secondary storage medium. At the time the secondary media
were tapes, but today they could be mechanical hard drives, <span>USB</span> drives
or network servers. But what is the point of having a file that isn’t
there? It’s really quite clever and it only gets more clever the more
you look at&nbsp;it.</p>
<p><strong>When a Multics system was running out of disk space it could remove
unused files from the primary medium, but keep the directory entries.</strong>
If you then tried to open such a file it would ask you to wait while
the file is being made available. It would issue instructions to the
operator (perhaps later a tape robot) to have them retrieve the
required tapes for restoring the file. Very useful when disks were
small and tapes were&nbsp;plentiful.</p>
<p>But it gets even better when you consider backups. Today if you need
to restore a Linux system from backups you do it this way: find the
oldest full backup and restore it, then restore each incremental
backup, going from oldest to newest. This means that the system can’t
be used until you’ve completely restored it from the backups. With
today’s disk sizes that could take a very, very long&nbsp;time.</p>
<p>Multics backups are done differently. To restore a completely hosed
system you first restore the system files, which will allow you to
boot. Then you restore the <em>latest</em> incremental backup. After this the
file system will contain the most current directory entries. <strong>Just
restore one incremental backup tape and all the files are already in
their right places. This is <em>much</em> faster than waiting for everything
to be&nbsp;restored!</strong></p>
<p>Why is only the latest incremental backup required to get everything
back in its place and working? The clever part is that the files might
not yet be on the disk. In fact, most files will probably be on
another backup medium. But the most recently used files have been
restored, so you can most likely do useful work already, and all other
files have their directory entries. If you try to open a file that
hasn’t been restored then you’re asked to wait and the operator is
told which tape to put in to get the file&nbsp;back.</p>
<p>Is something like this offered on <em>any</em> <span>POSIX</span>-compatible file&nbsp;system?</p>

<p>The Xerox Alto was also a system of many firsts. It is most famous for
having the first windowing <span>GUI</span>, which inspired Apple’s design, which
then inspired Microsoft’s design. Of course it had a hierarchical file
system with version numbers, and even a network file&nbsp;system.</p>
<p>I would like to highlight the interesting disk structure of the Alto
file system, which is described in the
paper
<a href="https://www.microsoft.com/en-us/research/publication/an-open-operating-system-for-a-single-user-machine/">An Open Operating System for a Single-User Machine</a> (1979)
by Butler Lampson and Robert F.&nbsp;Sproull.</p>
<p>The designers of the system were concerned about data loss caused by
hardware issues and buggy software. They came up with a feature to
protect the user’s data against loss of any block on the disk. (Unless
the user data was in that lost block, of&nbsp;course).</p>
<p>All blocks on the disk have a label that contains these&nbsp;fields:</p>
<ul>
<li>A file&nbsp;identifier</li>
<li>A version&nbsp;number</li>
<li>A page number (block&nbsp;index)</li>
<li>A&nbsp;length</li>
<li>A next link (pointer to the next block in the&nbsp;file)</li>
<li>A previous&nbsp;link</li>
</ul>
<p>The label means that each block is self-identifying. By my count this
type of disk had an additional 14 bytes of label per each 512 byte&nbsp;block.</p>
<p><strong>If the disk structure is damaged then a special program called the
<em>scavenger</em> will iterate over all blocks and recover the structure.</strong>
Contrast this to a file system like ext4 where a damaged structure can
mean that huge swathes of data are rendered&nbsp;inaccessible.</p>
<p><strong>The scavenger was also used to upgrade the disk structure.</strong> Imagine
converting in-place between ext4, btrfs or <span>ZFS</span> by just running a
scavenger for the new disk structure. It would e.g. regard the
existing ext4 metadata as nonsense and build a new disk structure
based on btrfs by using the labels. (It will not work with these
specific file systems, but it demonstrates the&nbsp;principle).</p>
<p>Why don’t we have this today? Nobody wants to build a <span>POSIX</span> file
system that uses labels because the disks we’re using have 512-byte or
4096-byte blocks and no room for a label. A label would have a very
small overhead for each block, but even a very small overhead like
this is not acceptable to file system designers. More seriously, it
would mess up mmap(2). There are disks with slightly larger blocks
meant to store a label, but they are more expensive and not very
common. So we’re left with fragile file system structures and the sage
advice that you’re supposed to have backups anyway (for which, see
previous&nbsp;section).</p>
<p>The best modern alternative is something like Btrfs or <span>ZFS</span>. Blocks are
checksummed and those checksums are stored next to the pointers in the
disk structure. A modern version of the Alto file system should
certainly have checksums, but not using labels gives weaker
protections than what the Alto file system provided. If you want to
see how relevant the Alto file system design is today, just read what
using ZFS protects against, but then remember that it doesn’t use&nbsp;labels.</p>

<p>Hydra is another historical operating system with innovations in the
file system. It is described in the
paper
<a href="https://research.cs.wisc.edu/areas/os/Qual/papers/hydra.pdf"><span>HYDRA</span>: The Kernel of a Multiprocessor Operating System</a> (1974)
by W. Wulf, E. Cohen, W. Corwin, A. Jones, R. Levin, C. Pierson, and
F. Pollack at Carnegie-Mellon&nbsp;University.</p>
<p>Hydra does away with the idea of ownership and a hierarchy of ever
more privileged components. It uses a fundamentally different concept
of protection and security than what <span>POSIX</span> is based&nbsp;on.</p>
<blockquote>
<p>“Technologists like hierarchical structures – they are elegant; but
experience from the real world shows they are not viable security
structures. The problem, then, which <span>HYDRA</span> attempts to face
squarely, is to maintain order in a nonhierarchical&nbsp;environment.”</p>
<p>– <span>HYDRA</span>: The Kernel of a Multiprocessor Operating System&nbsp;(1974)</p>
</blockquote>
<p>Resources that need protection, such as files, are called <em>objects</em>.
You can apply an <em>operation</em> to an object, such as reading or writing.
But to do so you need a reference to that object, which is called a
<em>capability</em>. You can even know a file’s identity, but without the
reference you can’t do&nbsp;anything.</p>
<p>This is similar, but not identical, to a safe programming …</p></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://weinholt.se/articles/non-posix-filesystems/">https://weinholt.se/articles/non-posix-filesystems/</a></em></p>]]>
            </description>
            <link>https://weinholt.se/articles/non-posix-filesystems/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24412970</guid>
            <pubDate>Tue, 08 Sep 2020 20:32:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Verne Edquist – Glenn Gould’s Piano Man]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24410113">thread link</a>) | @bookofjoe
<br/>
September 8, 2020 | https://www.glenngould.ca/verne-edquist/ | <a href="https://web.archive.org/web/*/https://www.glenngould.ca/verne-edquist/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				 <!-- .et_pb_column --><div>
				
				
				<div>
				
				
				<div><p><span>In the winter of 1938, deep into the Depression,&nbsp;eight-year-old Verne was put on a train and sent 2,000 miles east, to the Ontario School for the Blind. It was when Verne was in the school’s hospital for six weeks with scarlet fever that he first heard the sound of a piano being tuned – the intervals being stretched and shortened. It was a pinging sound, not quite musical, and hardly melodic.</span></p>
<p><span>Once he had recovered,Verne sought out the school’s tuning shop. As it happened, he had a near perfect ear, and he studied the craft of piano tuning with rigor and determination.</span></p>
<p><span>Years later, Verne often took to quoting his tuning teacher, J. D. Ansell, whose favorite aphorism was “The only place where success comes before work is in the dictionary.” To give Verne experience, Ansell started taking his young protégé into town to tune pianos in private homes. Verne was allowed to keep the money – $2.50 per piano, and sometimes, when he got lucky, $3.00 – which he put toward some basic tools: a tuning wrench, a tuning fork, needle-nose pliers, gauges for measuring the diameter of piano wire, and rubber wedges for muting strings.</span></p>
<p><span>After graduating from high school at age 19, Verne knew his best chance at a job was to find work as a chipper in one of the many piano factories in Toronto (in the early 20th century the city directory listed no fewer than sixteen piano manufacturers). The chippers were the first tuners to work on a newly strung piano, so incomplete in construction that it still had no keys. Those initial tunings are done, first with a pitch fork, then by ear, by plucking the strings with a small chip of wood.</span></p>
<p><span>Verne started out as an apprentice chipper in the factory of the Winter Company, one of piano manufacturers.</span></p>
<p><span>He had a fishing tackle box that he had converted to a tuner’s toolbox. Over the years Verne collected dozens of tools. Some he bought from old-timers, and others he adapted from other trades. He had surgical forceps and dental explorers, which made dandy hooks, opticians’ screwdrivers for adjusting harpsichords, barber scissors for trimming felt, and shoemaker pegs for plugging holes. From the welding trade he took soapstone, a dry lubricant for the buckskin that can squeak in the action of older pianos.</span></p>
<p><span>After being laid off, Verne decided to go door-to-door, tuning pianos. He was systematic, choosing a sequence of different neighborhoods around Toronto that he could reach by street car. Verne soon grew bolder and began cold-calling at sergeants’ messes,&nbsp;asylums, and prisons. He would walk a mile and a half in a snowstorm to tune a piano, and charge $3 for a tuning. He was lucky to get one tuning a day. “That’s how things were,” he recalled years later. “I was glad to get the work. During that time you did what you could.”</span></p>
<p><span>In 1952, at the age of 21, Verne was hired as a fine-tuner in the Heintzman factory, then the largest piano manufacturer in Canada.&nbsp;Verne’s tuning always stood out, even when he was tuning pianos for the first time. To trained ears, the quality of Verne’s tuning was always superior.</span></p>
<p><span>In 1961, he moved to the T. Eaton Co., to take over as the concert tuner. Verne liked to think he could take a piano beyond mere sound, into realms of color. And he liked to think he was giving people a glimpse of that color every time he tuned a piano. Muriel Mussen, who ran the concert department, sat in a small office off the showroom floor,&nbsp;and&nbsp;came to recognize Verne’s spare style of tuning whenever she heard it. Unlike many tuners, who banged a key as hard as they could, he kept it gentle, careful not to hit the key any harder than he had to. Halfway into a piano, Verne would often hear Miss Mussen call out, “I know it’s you tuning out there, Verne.”</span></p>
<p><span>One afternoon about a year after Verne started at Eaton’s,&nbsp;Miss Mussen sent him across town to Glenn Gould’s apartment to tune Gould’s old Chickering. All Gould wanted, he told Verne, was for the tuner to do what had been done hundreds of times before: get the piano into playable condition, if only for the time being. But Verne refused, telling Gould that the tuning pins were so loose they needed to be replaced.</span></p>
<p><span>Verne’s stubborn insistence on doing things his way had endeared him to Gould, and the encounter galvanized what was to become a decades-long association between a pianist and his technician.</span></p>
<p><span>Verne tuned for many famous musicians over the years, including Duke Ellington, Arthur Rubinstein, Rudolf Serkin, Victor Borge, and Liberace. But it was the business he got from Gould that eventually enabled him to quit Eaton’s employ and sustain his family for two decades.</span></p>
<p><span>Each tolerated the other’s idiosyncracies, which were in ample evidence in both men. Gould’s quirks, of course, were legion and legendary. One of their earliest conversations was about Verne’s physical limitations. “I can’t see very well, but I get the job done,” Verne told Gould. And Gould replied that of this he had no doubt. Nothing further on the topic was ever said.</span></p>
</div>
			</div> <!-- .et_pb_text -->
			</div> <!-- .et_pb_column --> <!-- .et_pb_column -->
				
				
			</div></div>]]>
            </description>
            <link>https://www.glenngould.ca/verne-edquist/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24410113</guid>
            <pubDate>Tue, 08 Sep 2020 16:40:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: TerminusHub, Distributed Revision Control for Structured Data]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24409334">thread link</a>) | @LukeEF
<br/>
September 8, 2020 | https://terminusdb.com/hub/ | <a href="https://web.archive.org/web/*/https://terminusdb.com/hub/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://terminusdb.com/hub/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24409334</guid>
            <pubDate>Tue, 08 Sep 2020 15:25:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vitamin D, part 3 – The Evidence]]>
            </title>
            <description>
<![CDATA[
Score 283 | Comments 198 (<a href="https://news.ycombinator.com/item?id=24408511">thread link</a>) | @usefulcat
<br/>
September 8, 2020 | https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3 | <a href="https://web.archive.org/web/*/https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="7.19.2"><div dir="ltr"><div><div id="viewer-26bnd"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_9d834b06083a45db9984155f34157ea8~mv2.jpg/v1/fit/w_814,h_363,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_9d834b06083a45db9984155f34157ea8~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div><h3 id="viewer-4bpvv">Vitamin D deficiency is linked to cancer, heart disease, respiratory infection, stroke, diabetes, and death. </h3><p id="viewer-4ds7a">It might seem reasonable that we should all be taking this supplement. But hold on. The argument for taking Vitamin D assumes a few things. First, that low Vitamin D actually causes all of the conditions it is associated with. This is not necessarily true. Low Vitamin D could cause (or contribute to) heart disease, but it could also be that heart disease causes low Vitamin D. Another possibility is that some third factor causes both low Vitamin D and heart disease. </p><p id="viewer-9g65f">Even if we can prove some causation, we then need to assume that correcting the Vitamin D deficiency will prevent the disease. This may not be true either. Low Vitamin D may be an indicator of overall poor health, which increases the risk for cardiac disease. It could appear that low Vitamin D contributes to the condition. But raising the Vitamin D will not help because it will just fix the symptom, not the underlying problem. </p><h3 id="viewer-399rj"><strong>Randomized Controlled Trials</strong></h3><p id="viewer-4p68a">Fortunately we have studies that can help figure this out. Currently the best way to answer a question of clinical relevance is with a large, well-executed <strong>randomized controlled trial (RCT).</strong> An RCT works like this: enroll the participants you want to study and randomly assign half of them to receive a treatment and the other half to receive a placebo. Those receiving the treatment are called the intervention group. The other group is the control group. Ideally, the two groups will be similar in every way except the treatment – they will have the same distribution in age, income, race, gender, and health status. They will even receive pills that look identical, though half will be placebos. If, at the end of the study, there are differences between the two groups, we can attribute those differences to the treatment. </p><div id="viewer-3aoj5"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img" aria-label="Simple randomized controlled trial "><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_70919e7b7a7d480c815f451bdce6cb09~mv2.png/v1/fit/w_1674,h_920,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_70919e7b7a7d480c815f451bdce6cb09~mv2.png/v1/fit/w_300,h_300,al_c,q_5/file.png" alt="Simple randomized controlled trial "></p></div><p><span dir="auto">Simple randomized controlled trial</span></p></div></div></div><p id="viewer-fvrr">RCTs are costly and time intensive, so generally we will only be able to study the interventions and outcomes that have the highest chance of showing a benefit. We base this on prior studies; if several large observational studies show a correlation between Vitamin D and heart disease, then this is something we should study with an RCT. </p><p id="viewer-1ofc5">
We now have results for over 1500 RCTs on Vitamin D supplementation. Some of the largest and most significant trials are listed and summarized at the end of this article. When tested on its ability to prevent disease, <strong>Vitamin D has failed to live up to expectations</strong>. </p><div id="viewer-b7fav"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img" aria-label="Results of Vitamin D Trials "><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_a25ba8bbf26b47a096d2dc2ee7e41fcd~mv2.png/v1/fit/w_1206,h_878,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_a25ba8bbf26b47a096d2dc2ee7e41fcd~mv2.png/v1/fit/w_300,h_300,al_c,q_5/file.png" alt="Results of Vitamin D Trials "></p></div><p><span dir="auto">For more details, see the list of high-quality RCTs at the bottom of this page</span></p></div></div></div><p id="viewer-en75e">One of the largest and best studies we have is called <a href="https://www.vitalstudy.org/" target="_blank" rel="noopener"><u>VITAL</u></a>, led by Dr. JoAnn Manson at Brigham and Women's Hospital in Boston. Over 25,000 adults (men age 50 and older, women age 55 and older) with no history of cardiovascular disease or cancer participated in the study. They received either 2,000 IUs of Vitamin D3 or placebo daily for five years. Researchers evaluated the rates of cancer and major cardiovascular events including stroke, heart attack, and cardiovascular death. </p><p id="viewer-f6k24"><strong>The result: Vitamin D failed to prevent cancer, stroke, heart attack, or cardiovascular death. </strong>These conditions occurred at the same rate in those receiving Vitamin D and those receiving placebo. There was one positive finding, though: in a post hoc analysis, it seemed that Vitamin D decreased the mortality from cancer when the initial two years were excluded. Post hoc (“after the event”) analyses can be quite useful, but need to be viewed with some skepticism. In high-quality RCTs (like VITAL), the outcome measures and statistical analyses are specified prior to data collection. Sometimes, as in VITAL’s case, an unexpected outcome may appear at the time the data are evaluated, prompting a post hoc analysis. In this case, when results from the first two years were excluded, the rate of cancer death was 1.2% in the placebo group, and 0.9% in the Vitamin D group - 25% reduction in cancer mortality, though the absolute numbers were low. It is not possible to draw too many conclusions from this, but it is something to note for future studies. </p><p id="viewer-3cooc"><strong>Even in studies of bone health, Vitamin D has been disappointing.</strong> We already know that Vitamin D is essential in calcium metabolism, and Vitamin D deficiency can lead to bone disorders like rickets and osteomalacia. Taking Vitamin D to treat severe Vitamin D deficiency is necessary for the treatment of these conditions. But in the large trials looking at Vitamin D's ability to prevent bone loss, the participants were taken from the general population, many of whom had normal Vitamin D levels. For severely deficient individuals (only a small percentage of the population), there are clear benefits to supplementation. These benefits do not appear to extend to those with normal levels or mild deficiencies.</p><h3 id="viewer-ct6r5"><strong>Meta-analyses of randomized controlled trials</strong></h3><p id="viewer-ek5t0">Large randomized controlled trials are currently the best way to evaluate a treatment, but they are difficult to do. If done well, an RCT is expensive, time intensive, and challenging to coordinate. It is worth it if we can answer our question adequately, but sometimes we realize after the trial that we needed to account for something else, or that we need more people in the study before we can really answer our question. In addition, RCTs will not always give definitive answers. Sometimes two or more small trials of the same treatment will show opposite results, and we need to reconcile those. Simply redoing an RCT is usually not an option. Instead, scientists have developed a method of combining similar RCTs and analyzing them together, in what is called a meta-analysis. </p><p id="viewer-ab05u">Meta-analyses have exploded in popularity over the last few decades.<span> If done well, a meta-analysis of a group of trials can reveal insights that cannot be seen in the individual trials. Unfortunately there are potential problems. The quality of the meta-analysis depends on the quality of the studies it includes, which can lead to erroneous results when poor quality studies are present (garbage in, garbage out).</span></p><p id="viewer-9nlvf"><span>In addition, <strong>publication bias</strong> may incorrectly amplify a treatment effect. Publication bias occurs when researchers decide to publish only “positive” findings – studies that show a treatment effect. Suppose ten small trials of Vitamin D are performed. Five show a benefit to Vitamin D and five do not. Researchers and journals are much more inclined to publish positive results than negative, so it is possible that only the five positive studies will make it into the medical literature. A meta-analysis on the five published positive studies will show a much stronger effect of Vitamin D than a meta-analysis that included all ten studies. </span></p><div id="viewer-9ul9t"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3" data-pin-media="https://static.wixstatic.com/media/3e0600_1afb8b809c0446029010b53b0b7cdf14~mv2.jpg/v1/fit/w_900,h_373,al_c,q_80/file.png" src="https://static.wixstatic.com/media/3e0600_1afb8b809c0446029010b53b0b7cdf14~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div><p id="viewer-a6n1n"> <em>Where negative studies go to die. </em></p><p id="viewer-a6bj6"><a href="https://en.wikipedia.org/wiki/Robert_Rosenthal_(psychologist)" target="_blank" rel="noopener"><em><u>Robert Rosenthal </u></em></a><em>called publication bias "the file drawer problem" because important negative studies often end up here. Photo by </em><a href="https://www.bigstockphoto.com/search/?contributor=nirat" target="_blank" rel="noopener"><em><u>nirat</u></em></a><em> on </em><a href="http://bigstockphoto.com/" target="_blank" rel="noopener"><em><u>bigstockphoto.com.</u></em></a><em> </em></p><p id="viewer-7700d"><span>Not every related trial should be included in a meta-analysis, though. One of the major challenges in performing these analyses is deciding which studies to include or exclude. Ideally only high-quality, well-designed, well-executed studies will be included. Researchers do not always agree on which studies to include, and differences in inclusion criteria have led to similar meta-analyses producing opposite results. </span><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0115934" target="_blank" rel="noopener"><u>This has occurred</u></a> <span>in studies of Vitamin D and fracture risk in older adults, with some meta-analyses showing zero benefit to Vitamin D and others showing a decreased risk of fracture, even though they included many of the same studies. </span></p><p id="viewer-donuf"><span>In addition to being high quality, the included studies should all try to answer the same clinical question. Many meta-analyses can only be done by combining trials that differ in important details, like the age of participants, treatment dosages, or definition of endpoints. Combining these may be feasible, but the clinical relevance may change when disparate groups are lumped together. For example, suppose we want to know if Vitamin D prevents asthma attacks in children. If a meta-analysis shows a small benefit to Vitamin D when all ages are combined, but the individual small studies in children did not show a benefit, what do I advise a 10 year old with asthma?</span></p><p id="viewer-9le66">Despite these challenges, meta-analyses of RCTs can provide meaningful insights. Unfortunately, most high-quality meta-analyses show similar results to our individual randomized controlled trials: <strong>Vitamin D does not appear to prevent disease in healthy adults.</strong> </p><p id="viewer-9hssp">But there are a few promising areas identified in meta-analyses. Note that some of these findings are not consistent with the large RCTs and will need to be studied further before definitive recommendations can be made. </p><p id="viewer-25ike">The areas in which meta-analyses have identified benefits from Vitamin D:</p><ol><li id="viewer-f3qjq"><p><strong>Fracture prevention in elderly nursing home residents </strong>(when also given with calcium):   This is not surprising. We know that Vitamin D and calcium can prevent bone loss in severe Vitamin D deficiency. Elderly adults who are not getting outside are more likely to be severely deficient in Vitamin D, and supplements likely help.</p></li><li id="viewer-fa0a9"><p><strong>Asthma and respiratory infection</strong>: Vitamin D seems to reduce asthma attacks in adults with mild to moderate disease, and daily or weekly Vitamin D seems to prevent acute respiratory infection in those with Vitamin D less than 10 ng/ml (25 nmol/l). There is considerable excitement around Vitamin D's potential role in Covid treatment, though we do not yet have enough evidence to make a definitive conclusion. </p></li><li id="viewer-d7fj2"><p><strong>Cancer mortality</strong>: Vitamin D does not appear to prevent cancer, but may reduce death rates from cancer overall (when all cancers are combined) when Vitamin D is taken for several years. It is not known whether Vitamin D itself fights cancer. It could also be that individuals with cancer are more prone to developing severe Vitamin D deficiency, which leads to bone loss, fragility, and fractures, which increase mortality.</p></li><li id="viewer-cvias"><p><strong>Atopic dermatitis (eczema)</strong>: Vitamin D …</p></li></ol></div></div></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3">https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3</a></em></p>]]>
            </description>
            <link>https://www.devaboone.com/post/vitamin-d-part-3-the-evidence?postId=5f4e8bf673d853002ded6cd3</link>
            <guid isPermaLink="false">hacker-news-small-sites-24408511</guid>
            <pubDate>Tue, 08 Sep 2020 14:10:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Free hosted open-source alternative to Zapier/Airflow]]>
            </title>
            <description>
<![CDATA[
Score 263 | Comments 74 (<a href="https://news.ycombinator.com/item?id=24407706">thread link</a>) | @newcrobuzon
<br/>
September 8, 2020 | https://cloud.titanoboa.io/index.html | <a href="https://web.archive.org/web/*/https://cloud.titanoboa.io/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
        <div>
            <div>
                <p><span>public βeta</span></p><h2><b>Try a <u>free</u> hosted instance of Titanoboa!</b></h2>


                <p>You don't have to install Titanoboa on your computer - just try it straight away in your browser!</p>
                
                <dev>
                     <br>
                    <b>We are sorry, but Titanoboa server's GUI is not fully optimized for mobile devices yet. Feel free to watch the demo below or read our <a href="https://github.com/mikub/titanoboa/wiki">wiki</a> to learn more!</b>
                </dev>
                <p><br>
                If you are not familiar with Titanoboa you can watch a short demo here:<br>

                <video controls="">
                    <source src="https://www.titanoboa.io/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                </p>

 </div>
        </div>
    </div>
</div></div>]]>
            </description>
            <link>https://cloud.titanoboa.io/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407706</guid>
            <pubDate>Tue, 08 Sep 2020 12:26:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ad Fraud on LinkedIn]]>
            </title>
            <description>
<![CDATA[
Score 460 | Comments 265 (<a href="https://news.ycombinator.com/item?id=24407432">thread link</a>) | @sbachman
<br/>
September 8, 2020 | https://www.samueljscott.com/2020/09/08/linkedin-ad-fraud/ | <a href="https://web.archive.org/web/*/https://www.samueljscott.com/2020/09/08/linkedin-ad-fraud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<p>The LinkedIn Ads network is likely awash in mistaken clicks and bot traffic that make the platform’s value extremely dubious, according to a new study from global digital agency RMG that the company shared exclusively with me.</p>
<p>Three weeks ago, RMG founding partner Ryan Gellis <a href="https://www.linkedin.com/posts/ryan-gellis_theory-the-linkedin-ad-network-is-full-of-activity-6700077824360910848-LysO/" target="_blank" rel="noopener noreferrer">posted a thread</a> wondering if LinkedIn “is full of fraudulent accounts and unchecked misclicks that artificially inflate the cost of advertising and result in poor performance compared to other ad networks.” The ensuing discussion inspired him to look into the issue further.</p>
<center><iframe src="https://www.youtube.com/embed/jKuyxgWuiRM" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></center><p>Gellis goes through his findings <a href="https://www.youtube.com/watch?v=jKuyxgWuiRM" target="_blank" rel="noopener noreferrer">in this YouTube video</a>. For the test, RMG promoted a webinar by targeting c-certain c-suite executives at large companies in the United States based on their job titles. The ad encouraged them to click to sign up for for event for free.</p>
<p>The first issue the agency found was the projected versus actual cost per click. LinkedIn estimated a $25 average CPC. The actual campaign CPC was nearly $45.</p>
<p>“What’s the point of forecasting if you project one cost and then the actual cost ends up being 200% the projection?” Gellis asks in the video.</p>
<p>But the real issue was the source of the clicks themselves. LinkedIn’s ad platform reported 11 clicks. RMG’s logs showed ten clicks. <a href="https://www.fullstory.com/" target="_blank" rel="noopener noreferrer">FullStory</a>, the digital experience analytics platform that RMG uses, stated that eight people registered for the webinar. However, the true number of registrations was zero.</p>
<p>“Zero people signed up for the webinar from that specific campaign,” Gellis told me in an interview. “That’s kind of the punch line and why I believe this is newsworthy. Nobody signed up. In fact, nobody even exhibited what I believe is real user behavior on the site after clicking the ad. That is also true of all the other ads we ran on LinkedIn. We had a total of 0 people ever “sign up” on any ad we ran during our testing on LinkedIn — regardless of the ad format or messaging.”</p>
<p><a href="https://www.rmgmedia.com/" target="_blank" rel="noopener noreferrer">RMG</a> suspects bot activity or misattributed misclicks had been the cause because visitors would bounce away from the webinar’s landing page before it even had a chance to render — in other words, they would click the LinkedIn advertisement and then leave the website in less than 1.3 seconds. In addition, FullStory reported mouse movements that were nothing like what an actual human would do.</p>
<p>“As far as we’re concerned, the LinkedIn ad network basically has fraudulent clicks and/or misclicks that don’t get attributed to the users that are being passed through as leads in campaigns,” Gellis says in the video.</p>
<p>Of course, the simple size in RMG’s one-day test was extremely small. The target audience size was 16,000. The number of impressions would be between 680 and 2,000. The cost was $250. But Gellis told me that the results match a prior, larger test.</p>
<p><img src="https://www.samueljscott.com/wp-content/uploads/2020/09/linkedin-ads.png" alt="linkedin ads" width="660" height="197" data-eio="l" data-old-src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 660 197'%3E%3C/svg%3E" data-src="https://www.samueljscott.com/wp-content/uploads/2020/09/linkedin-ads.png"></p>
<p>During a $3,000 campaign in March, LinkedIn reported 256 clicks with an average CPC of $11. The company received zero leads from all of the clicks.</p>
<p>“You’ll see the call to action is ‘Request Demo,'” Gellis said. “So, arguably, a user who sees this ad and clicks legitimately on it will be looking to get a demo of the analytics tool we were marketing. If the ads were giving us behavior like this on a $3,000-per-month budget, it’s unlikely they would have performed any better with additional budget.”</p>
<p>RMG charges that LinkedIn’s advertising is on a closed network and does not follow the Interactive Advertising Bureau’s <a href="https://www.iab.com/guidelines/iab-measurement-guidelines/" target="_blank" rel="noopener noreferrer">standards for digital advertising</a>. (The IAB did not respond to a request for comment.)</p>
<p>After reviewing RMG’s findings, a LinkedIn spokesperson sent me the following company statement.</p>
<p>“LinkedIn is a members-first organization and our&nbsp;<u><a title="https://www.linkedin.com/legal/ads-policy" href="https://www.linkedin.com/legal/ads-policy" target="_blank" rel="noreferrer noopener" data-saferedirecturl="https://www.google.com/url?q=https://www.linkedin.com/legal/ads-policy&amp;source=gmail&amp;ust=1599566456486000&amp;usg=AFQjCNFFsmfrYAHwHhV6tM-nG54kogFWEw">Ads program</a></u>&nbsp;is designed to ensure that only high-quality, relevant ads are served to our members. We prohibit the use of bots or other automated fraudulent methods to access our services, as they are in violation of the&nbsp;<u><a title="https://www.linkedin.com/legal/user-agreement" href="https://www.linkedin.com/legal/user-agreement" target="_blank" rel="noreferrer noopener" data-saferedirecturl="https://www.google.com/url?q=https://www.linkedin.com/legal/user-agreement&amp;source=gmail&amp;ust=1599566456486000&amp;usg=AFQjCNErEgCF6J2mC2B2u6Rnd32Ialafiw">User Agreement.</a></u>&nbsp;Additionally, LinkedIn is a&nbsp;member of the IAB&nbsp;and works closely with the organization and its members to help develop standards for the digital advertising industry.”</p>
<p>But when summarizing his findings in the video, Gellis is skeptical of LinkedIn.</p>
<p>“As far as we can tell, LinkedIn is basically a money pit,” he says. “You’re not going to get the performance that you would expect out of the network.”</p>
<p><strong>(Note: For more, <a href="https://www.samueljscott.com/2019/02/26/fake-online-fake-internet/" target="_blank" rel="noopener noreferrer">see my recent talk on how much of the Internet is fake</a>.)</strong></p>
<p><b><i>Thanks for reading! Follow me on <a href="http://twitter.com/samueljscott">Twitter</a> and see my <a href="https://www.samueljscott.com/marketing-speaker/">marketing speaker</a> page to have me visit your conference or company.</i></b></p>
					</div></div>]]>
            </description>
            <link>https://www.samueljscott.com/2020/09/08/linkedin-ad-fraud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407432</guid>
            <pubDate>Tue, 08 Sep 2020 11:44:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chef to be acquired by Progress]]>
            </title>
            <description>
<![CDATA[
Score 158 | Comments 126 (<a href="https://news.ycombinator.com/item?id=24407323">thread link</a>) | @snorlaxhugsy
<br/>
September 8, 2020 | https://blog.chef.io/the-fourth-chapter-of-chef-has-arrived-progress-to-purchase-chef/ | <a href="https://web.archive.org/web/*/https://blog.chef.io/the-fourth-chapter-of-chef-has-arrived-progress-to-purchase-chef/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<div id="primary">
		<main id="main" role="main">

		
<article id="post-24840">
	<!-- .entry-header -->

	<div>
					
<p><span>Chefs,</span></p>
<p><span>We are excited to announce that Chef has signed a definitive agreement to be acquired by </span><a href="http://www.progress.com/"><span>Progress</span></a><span>. </span><span>We expect the transaction&nbsp;to be finalized over the next 30 days&nbsp;or so and, once completed,&nbsp;Chef&nbsp;will become an integral part of Progress.&nbsp;</span></p>
<p><span>Progress is a trusted provider of some of the best products to develop, deploy and manage high-impact business applications. Chef will bolster that position by providing continuous delivery, application delivery, dependable compliance, remediation automation, desktop management and more.</span> <span>You, our customers and community, will benefit from working with an established, publicly traded company with significantly greater resources that will provide extensive support for our open source projects and product roadmap.&nbsp;</span></p>
<p><span>Until the deal closes, Chef will remain independent and continue its normal business operations.&nbsp; </span><span>Once the deal is complete, Progress will be focused on continuing and executing on Chef’s business model and product roadmaps and supporting your business</span><span> and our combined communities. Our unwavering commitment to your success will continue and remains a top priority, as Chef enters this new phase as part of the global Progress Software family.&nbsp;</span></p>
<p><span>We look forward to the opportunities that lie ahead and to the benefits the acquisition will provide.</span></p>
<p><span>For more information on the transaction, please see <a href="https://www.progress.com/blogs/progress-to-acquire-chef">this great post</a></span><span>&nbsp;from Progress CEO Yogesh Gupta.</span></p>
<p><span>Barry Crist<br></span><span>Chief Executive Officer, Chef</span></p>
			</div><!-- .entry-content -->

	
	
				
	
		
<div>
							<div>
				<p><a href="#"><img src="https://secure.gravatar.com/avatar/4d50999463b9ae09ae36afd69f82397d?s=98&amp;d=mm&amp;r=g" width="98" height="98" alt="Avatar"></a></p><div>
					<p><strong>Barry Crist</strong></p><p>				
						Barry Crist has more than 20 years of experience in driving enterprise customer success with open source and DevOps software solutions and is a recognized leader in driving a culture of innovation. Barry joined Chef as CEO in 2013 and has been a leading force behind the company’s business operations, culture and technology innovation.					</p>
				</div>
			</div>
			</div>

		<a id="merch-banner" href="https://pages.chef.io/cloudmigrationwp.html" target="_blank">
			<img src="https://blog.chef.io/wp-content/uploads/2019/09/Blog-Merch-Bottom-Banner.jpg">
		</a>
	
	
	

</article><!-- #post-## -->

		</main><!-- #main -->
	</div><!-- #primary -->


<!-- #secondary -->

	</div></div>]]>
            </description>
            <link>https://blog.chef.io/the-fourth-chapter-of-chef-has-arrived-progress-to-purchase-chef/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24407323</guid>
            <pubDate>Tue, 08 Sep 2020 11:25:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PandaDoc employees arrested in Belarus after founders protest against violence]]>
            </title>
            <description>
<![CDATA[
Score 435 | Comments 184 (<a href="https://news.ycombinator.com/item?id=24406366">thread link</a>) | @perch56
<br/>
September 8, 2020 | https://savepandadoc.org/en/ | <a href="https://web.archive.org/web/*/https://savepandadoc.org/en/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://savepandadoc.org/en/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24406366</guid>
            <pubDate>Tue, 08 Sep 2020 08:29:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I link to Wayback Machine instead of original web content]]>
            </title>
            <description>
<![CDATA[
Score 548 | Comments 234 (<a href="https://news.ycombinator.com/item?id=24406193">thread link</a>) | @puggo
<br/>
September 8, 2020 | https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/ | <a href="https://web.archive.org/web/*/https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main">
      <div>
        <div id="content">
          <article>
    
    

    
    <div>
      <p><img src="https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/wayback.jpg" alt="WayBackMachine"></p>
<hr>
<p>When linking to a page for the <strong>purpose of reference</strong>, it seems better to me to <em><strong>link to the <a href="https://archive.org/">archive</a> of a given page</strong></em>, rather than to the original site itself.</p>
<p>This ensures that after some years have gone by, <strong>my article is guaranteed to be consistent</strong>. Due the  changing nature of the web, there is a chance that after some years, the link could lead to a:</p>
<ul>
<li>404 /  Not Found (most common)</li>
<li>Changed or edited content, or entirely replaced content</li>
<li>Content that, due to a rise in popularity, is now shielded, demanding the user to make an account to read the entire article.</li>
</ul>
<hr>

<p>Take defensive measures. To future-proof your content, rather than reference the general web, its far more reliable to link to an archive.</p>
<h2 id="example">Example:</h2>
<p>The Epoch Times wrote an article on Smartphones data-mining their users. This is the archived article here:</p>
<p><a href="https://web.archive.org/web/20190214015500/https://www.theepochtimes.com/smartphone-app-users-are-data-mined-even-when-not-using-the-apps_2787202.html">The Archive Version (fully readable)</a></p>
<h3 id="article-content-before">Article Content Before</h3>
<p>You can see its perfectly “normal” readable useful  content.</p>
<p><img src="https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/link-content-before.png" alt="Article Link Content Before"></p>
<h3 id="article-content-after">Article Content After</h3>
<p>Now it’s spam from a site suffering financial need.</p>
<p><img src="https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/link-content-after.png" alt="Article Link Content Before"></p>
<p>So in Feb 14 2019 your users would have seen the content you intended. However in Sep 07 2020, your users are being asked to support independent Journalism instead.</p>
<h2 id="if-an-archive-record-doesnt-exist-make-one">If an Archive Record Doesn't Exist, Make One</h2>
<p>Its worth the extra moment, in referencing a site, to make an archive of the page you wish to reference, if one does not exist. After that, immediately use the link from the archive.org entry, rather than the blog, news, info, or forum site you wish to refer to.</p>
<h2 id="in-unstable-times-take-measures-for-stability">In Unstable Times, Take Measures for Stability</h2>
<p>The web is a fast changing place. Even more during the Covid pandemic and suffering financial markets. Since times are financially harder, websites are disappearing, heaping up advertising, demanding user response, and things like this.</p>
<p>To avoid your content losing quality due to these things, linking to a solid, unchanging static copy of the page is far more reliable.</p>

    </div>

    <div>
  <p>
    <span>Author</span>
    <span>Leo Blanchette</span>
  </p>
  <p>
    <span>LastMod</span>
    <span>
        2020-09-07
        
    </span>
  </p>
  
  
</div>

  </article>
        </div>
        

  

  

      </div>
    </div></div>]]>
            </description>
            <link>https://hawaiigentech.com/post/commentary/why-i-link-to-waybackmachine-instead/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24406193</guid>
            <pubDate>Tue, 08 Sep 2020 08:03:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Escape from Creek Fire]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 66 (<a href="https://news.ycombinator.com/item?id=24405981">thread link</a>) | @twohey
<br/>
September 8, 2020 | https://www.jmeshe.co/escape-from-creek-fire | <a href="https://web.archive.org/web/*/https://www.jmeshe.co/escape-from-creek-fire">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<p><span>
By subscribing to the mailing list of
<strong>
Jaymie Shearer
</strong>
your email address is stored securely, opted into new post notifications and related communications. We respect your inbox and privacy, you may unsubscribe at any time.
</span></p><div>
<p><a href="https://www.exposure.co/privacy" rel="noopener" target="_blank" title="Link to Jaymie Shearer Privacy Policy">
Privacy Policy
</a>
<a href="https://www.exposure.co/terms" rel="noopener" target="_blank" title="Link to Jaymie Shearer Terms of Service">
Terms of Service
</a>
<a href="https://www.exposure.co/report" rel="noopener" target="_blank" title="Report a story or story">
Report
</a>
</p></div>

</div>
</div></div>]]>
            </description>
            <link>https://www.jmeshe.co/escape-from-creek-fire</link>
            <guid isPermaLink="false">hacker-news-small-sites-24405981</guid>
            <pubDate>Tue, 08 Sep 2020 07:22:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[URL query parameters and how laxness creates de facto requirements on the web]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 72 (<a href="https://news.ycombinator.com/item?id=24404814">thread link</a>) | @oftenwrong
<br/>
September 7, 2020 | https://utcc.utoronto.ca/~cks/space/blog/web/DeFactoQueryParameters | <a href="https://web.archive.org/web/*/https://utcc.utoronto.ca/~cks/space/blog/web/DeFactoQueryParameters">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>URL query parameters and how laxness creates de facto requirements on the web</h2>

	<p><small>September  7, 2020</small></p>
</div><div><p>One of the ways that <a href="https://utcc.utoronto.ca/~cks/space/dwiki/DWiki">DWiki</a> (the code behind <a href="https://utcc.utoronto.ca/~cks/space/blog/">Wandering Thoughts</a>) is unusual is that it strictly validates the query parameters
it receives on URLs, including on HTTP <code>GET</code> requests for ordinary
pages. If a HTTP request has unexpected and unsupported query
parameters, such a <code>GET</code> request will normally fail. When I made
this decision it seemed the cautious and conservative approach, but
<a href="https://utcc.utoronto.ca/~cks/space/blog/web/CautionIsAMistakeToday">this caution has turned out to be a mistake on the modern web</a>. In practice, all sorts of sites will
generate versions of your URLs with all sorts of extra query
parameters tacked on, give them to people, and expect them to work.
If your website refuses to play along, (some) people won't get to
see your content. <strong>On today's web, you need to accept (and then
ignore) arbitrary query parameters on your URLs</strong>.</p>

<p>(Today's new query parameter is 's=NN', for various values of NN
like '04' and '09'. I'm not sure what's generating these URLs, but
it may be Slack.)</p>

<p>You might wonder how we got here, and that is a story of lax behavior
(or, if you prefer, being liberal in what you accept). In the
beginning, both Apache (for static web pages) and early web
applications often ignored extra query parameters on URLs, at least
on <code>GET</code> requests. I suspect that other early web servers also
imitated Apache here, but I have less exposure to their behavior
than Apache's. My guess is that this behavior wasn't deliberate,
it was just the simplest way to implement both Apache and early
web applications; you paid attention to what you cared about and
didn't bother to explicitly check that nothing else was supplied.</p>

<p>When people noticed that this behavior was commonplace and widespread,
they began using it. I believe that one of the early uses was for
embedding 'where this link was shared' information for your own web
analytics (<a href="https://utcc.utoronto.ca/~cks/space/blog/web/AnalyticsVsSecurity">cf</a>), either based on your logs
or using JavaScript embedded in the page. In the way of things,
once this was common enough other people began helpfully tagging
the links that were shared through them for you, which is why I
began to see various 'utm_*' query parameters on inbound
requests to <a href="https://utcc.utoronto.ca/~cks/space/blog/">Wandering Thoughts</a> even though I never
published such URLs.
Web developers don't leave attractive nuisances alone for long, so
soon enough people were sticking on extra query parameters to your
URLs that were mostly for them and not so much for you. Facebook
may have been one of the early pioneers here with their 'fbclid'
parameter, but other websites have hopped on this particular train
since then (as I saw recently with these 's=NN' parameters).</p>

<p>At this point, the practice of other websites and services adding
random query parameters to your URLs that pass through them is so
wide spread and common that accepting random query parameters is
pretty much a practical requirement for any web content serving
software that wants to see wide use and not be irritating to the
people operating it. If, like <a href="https://utcc.utoronto.ca/~cks/space/dwiki/DWiki">DWiki</a>, you stick to your guns and
refuse to accept some or all of them, you will drop some amount of
your incoming requests from real people, disappointing would be
readers.</p>

<p>This practical requirement for URL handling is not documented in
any specification, and it's probably not in most 'best practices'
documentation. People writing new web serving systems that are
tempted to be strict and safe and cautious get to learn about it
the hard way.</p>

<p>In general, any laxness in actual implementations of a system can
create a similar spiral of de facto requirements. Something that
is permitted and is useful to people will be used, and then supporting
that becomes a requirement. This is especially the case in a
distributed system like the web, where any attempt to tighten the
rules would only be initially supported by a minority of websites.
These websites would be 'outvoted' by the vast majority of websites
that allow the lax behavior and support it, because that's what
happens when the vast majority work and the minority don't.</p>
</div></div>]]>
            </description>
            <link>https://utcc.utoronto.ca/~cks/space/blog/web/DeFactoQueryParameters</link>
            <guid isPermaLink="false">hacker-news-small-sites-24404814</guid>
            <pubDate>Tue, 08 Sep 2020 02:50:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fast.ai and why Python is not the future of ML with Jeremy Howard]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 94 (<a href="https://news.ycombinator.com/item?id=24404002">thread link</a>) | @tosh
<br/>
September 7, 2020 | https://www.wandb.com/podcast/jeremy-howard | <a href="https://web.archive.org/web/*/https://www.wandb.com/podcast/jeremy-howard">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Jeremy Howard is a founding researcher at fast.ai, a research institute dedicated to making Deep Learning more accessible. Previously, he was the CEO and Founder at Enlitic, an advanced machine learning company in San Francisco, California. </p><p>Howard is a faculty member at Singularity University, where he teaches data science. He is also a Young Global Leader with the World Economic Forum, and spoke at the World Economic Forum Annual Meeting 2014 on "Jobs For The Machines." </p><p>Howard advised Khosla Ventures as their Data Strategist, identifying the biggest opportunities for investing in data-driven startups and mentoring their portfolio companies to build data-driven businesses. Howard was the founding CEO of two successful Australian startups, FastMail and Optimal Decisions Group. Before that, he spent eight years in management consulting, at McKinsey &amp; Company and AT Kearney.</p><p><strong>TOPICS COVERED:</strong></p><p>0:00 Introduction</p><p>0:52 Dad things</p><p>2:40 The story of Fast.ai</p><p>4:57 How the courses have evolved over time</p><p>9:24 Jeremy’s top down approach to teaching</p><p>13:02 From Fast.ai the course to Fast.ai the library</p><p>15:08 Designing V2 of the library from the ground up</p><p>21:44 The ingenious type dispatch system that powers Fast.ai</p><p>25:52 Were you able to realize the vision behind v2 of the library</p><p>28:05 Is it important to you that Fast.ai is used by everyone in the world, beyond the context of learning</p><p>29:37 Real world applications of Fast.ai, including animal husbandry</p><p>35:08 Staying ahead of the new developments in the field</p><p>38:50 A bias towards learning by doing</p><p>40:02 What’s next for Fast.ai</p><p>40.35 Python is not the future of Machine Learning</p><p>43:58 One underrated aspect of machine learning</p><p>45:25 Biggest challenge of machine learning in the real world</p><p>Follow Jeremy on Twitter:</p><p><a href="https://twitter.com/jeremyphoward" target="_blank">https://twitter.com/jeremyphoward</a><br></p><p>Links:</p><p>Deep learning R&amp;D &amp; education: <a target="_blank" href="https://t.co/ZvDGNlehRt?amp=1">http://fast.ai</a></p><p>Software: <a target="_blank" href="https://t.co/GMYkPDXNW3?amp=1">http://docs.fast.ai</a></p><p>Book: <a target="_blank" href="https://t.co/1YSqXvWW87?amp=1">http://up.fm/book</a></p><p>Course: <a target="_blank" href="https://t.co/Q2qMl59EfH?amp=1">http://course.fast.ai</a></p><p>Papers:</p><p><a target="_blank" href="https://dl.acm.org/doi/10.1145/2487575.2491127"><strong>The business impact of deep learning</strong></a></p><p><a target="_blank" href="https://dl.acm.org/doi/10.1145/2487575.2491127">https://dl.acm.org/doi/10.1145/2487575.2491127</a></p><p><a target="_blank" href="https://www.linkedin.com/redir/redirect?url=http%3A%2F%2Fwww%2Ejmir%2Eorg%2F2012%2F1%2Fe33%2F&amp;amp;urlhash=gLU-&amp;trk=public_profile_publication-title"><strong>De-identification Methods for Open Health Data</strong></a></p><p><a href="https://www.jmir.org/2012/1/e33/">https://www.jmir.org/2012/1/e33/</a><br></p><p>Visit our podcasts homepage for transcripts and more episodes!</p><p><a target="_blank" href="https://www.wandb.com/podcast">www.wandb.com/podcast</a></p><p> Get our podcast on Soundcloud, Apple, and Spotify!</p><p>Soundcloud: <a target="_blank" href="https://bit.ly/2YnGjIq">https://bit.ly/2YnGjIq</a></p><p>Apple Podcasts: <a target="_blank" href="https://bit.ly/2WdrUvI">https://bit.ly/2WdrUvI</a></p><p>Spotify: <a target="_blank" href="https://bit.ly/2SqtadF">https://bit.ly/2SqtadF</a></p><p>We started Weights and Biases to build tools for Machine Learning practitioners because we care a lot about the impact that Machine Learning can have in the world and we love working in the trenches with the people building these models. One of the most fun things about these building tools has been the conversations with these ML practitioners and learning about the interesting things they’re working on. This process has been so fun that we wanted to open it up to the world in the form of our new podcast called Gradient Dissent. We hope you have as much fun listening to it as we had making it!</p><p>Weights and Biases:</p><p>We’re always free for academics and open source projects. Email carey@wandb.com with any questions or feature suggestions.</p><ul role="list"><li>Blog: <a target="_blank" href="https://www.wandb.com/articles">https://www.wandb.com/articles</a></li><li>Gallery: See what you can create with W&amp;B - <a target="_blank" href="https://app.wandb.ai/gallery">https://app.wandb.ai/gallery</a></li><li>Continue the conversation on our slack community - <a href="http://bit.ly/wandb-forum" target="_blank">http://bit.ly/wandb-forum</a><br></li></ul><p>Host: Lukas Biewald - <a href="https://twitter.com/l2k" target="_blank">https://twitter.com/l2k</a></p><p>Producer: Lavanya Shukla - <a href="https://twitter.com/lavanyaai" target="_blank">https://twitter.com/lavanyaai</a></p><p>TRANSCRIPT:</p><p><strong>Lukas: </strong>You're listening to Gradient Dissent, a show where we learn about making machine learning models work in the real world. I'm your host Lukas Biewald. Jeremy Howard created the Fast.ai course, which is maybe the most popular course to learn machine learning and there are a lot out there. He's also the author of the book Deep Learning for Coders with Fast.ai and PyTorch and in that process, he made the Fast.ai library which lots of people use independently to write deep learning. Before that, he was the CEO and co-founder of Enlitic, an exciting startup that applies deep learning to health care applications. And before that, he was the president of Kaggle, one of the most exciting earliest machine learning companies. I'm super excited to talk to him. So Jeremy, it's nice to talk to you. And in preparing the questions, I realized that every time I've talked to you there have been a few gems that I've remembered that I would never think to ask about. Like one time you told me about how you learned Chinese and another time you gave me Dad parenting advice, very specific advice and it's been actually super helpful. </p><p><strong>Jeremy: </strong>Oh great. Tell me what Dad parenting advice worked out?</p><p><strong>Lukas: </strong>Well, what you told me was when you change diapers, use a blow dryer to change a really frustrating experience to a really joyful experience and it's like such good advice. I don't know how you.. I guess I can imagine how you thought of it, but it's...</p><p><strong>Jeremy: </strong>Yeah, yeah, I know they love the whooshing sound, they love the warmth. I'm kind of obsessed about Dad things. So I'm always happy to talk about Dad things. That is this podcast.</p><p><strong>Lukas: </strong>Can we start with that? Now that my daughter is eight months old. Do you have any suggestions for her?</p><p><strong>Jeremy: </strong>Oh my goodness! Eight months old. You know, it's like the same with any kind of learning. It's all about consistency. So I think that the main thing we did right with Claire was just, you know, this delightful child now is we were just super consistent. Like if we said you can't have X unless you do Y, we would never give her X if she didn't do Y. If you want to take your scooter down to the bottom of the road, you have to carry it back up again. We read this great book that was saying if you're not consistent, it becomes like this thing, it's like a gambler. It's like sometimes you get the thing you want, so you just have to keep trying so that's my number one piece of advice. It's the same with teaching machine learning. We always tell people that tenacity is the most important thing for students. To stick with it, do it every day.</p><p><strong>Lukas: </strong>I guess just in the spirit of questions, I'm genuinely curious about, you know, you've built this amazing framework and teaching thing that I think is maybe the most popular and most appreciated framework. I was wondering if you could start by telling me the story of what inspired you to do that and what was the journey to making Fast.ai, the curriculum and Fast.ai, the ML framework.</p><p><strong>Jeremy: </strong>So it was something that my wife Rachel and I started together. Rachel has a math PhD, super technical background, early data scientist and engineer, Uber. I don't. I have just scraped by a philosophy undergrad and have no technical background. But from both of our different directions, we both had this frustration that neural networks in 2012 were super important, clearly going to change the world, but super inaccessible and so we would go to meetups and try to figure out like how do we... Like I knew the basic idea, I'd coded neural networks 20 years ago, but how do you make them really good? There wasn't any open source software at the time for running on GPUs. You know, Dan Seresen's thing was available, but you had to pay for it. There was no source code and we just thought, oh, we've got to change this, because the history of technology leaps has been that it generally increases inequality because the people with resources can access the new technology and then that leads to societal upheaval and a lot of unhappiness. So we thought, well, we should just do what we can. So we thought how are we going to fix this? Basically the goal was, and still is, to be able to use deep learning without requiring any code so that, you know, because the vast majority of the world can't code, we kind of thought, well, to get there, we should, first of all, see what exists right now? Learn how to use it as best as we can ourselves, teach people how to best use it as we can and then make it better, which requires doing research and then turning that into software and then changing the course to teach the hopefully slightly easier version and repeat that again and again for a few years. And so we're kind of in that process.</p><p><strong>Lukas: </strong>That's so interesting. Do you worry that the stuff you're teaching, you're sort of trying to make it obsolete, right? Because you're trying to build higher level abstractions? Like I think one of the things that people really appreciate your course is that it's really clear, in-depth explanations of how these things work. Do you think that that's eventually going to be not necessary or how do you think about that?</p><p><strong>Jeremy: </strong>Yeah, to some extent. I mean, so if you look at the new book and the new course, chapter one starts with really, really foundational stuff around what is a machine learning algorithm? What do we mean to learn an algorithm? What's the difference between traditional programming and machine learning to solve the same problem? And those kinds of basic foundations I think will always be useful, even at the point you're not using any code. I feel like even right now, if somebody is using like PlatformAI or some kind of code-free framework, you still need to understand these basics of an algorithm can only learn based on the data you provide. It's generally not going to be able to extrapolate to patterns it's not seen yet, stuff like that. Um, but yeah, I mean, we have so far released two new courses every year, you know, a part one and part two every year because every year, it's totally out of date. And we always say to our students at the start of part one, Look, you know, none of the details you're learning are going to be of any use in a year or two's time. There's a time when we're doing Piano and then TensorFlow and Keras, and then playing PyTorch. We always say, look, don't worry too much about the software we're using because none of it's still any good, you know, it's goal changing rapidly, you know, faster than JavaScript frameworks, but the concepts are important and yeah, you can pick up a new library and I don't know by weekend, I …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.wandb.com/podcast/jeremy-howard">https://www.wandb.com/podcast/jeremy-howard</a></em></p>]]>
            </description>
            <link>https://www.wandb.com/podcast/jeremy-howard</link>
            <guid isPermaLink="false">hacker-news-small-sites-24404002</guid>
            <pubDate>Tue, 08 Sep 2020 00:10:21 GMT</pubDate>
        </item>
    </channel>
</rss>
