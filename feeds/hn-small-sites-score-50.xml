<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Tue, 07 Jul 2020 20:16:56 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Tue, 07 Jul 2020 20:16:56 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[CoreBGP – Plugging in to BGP]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 8 (<a href="https://news.ycombinator.com/item?id=23744167">thread link</a>) | @jordanwhited
<br/>
July 5, 2020 | https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/ | <a href="https://web.archive.org/web/*/https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<hr>
<p><img src="https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/cover.png" alt="cover"></p>
<hr>

<p><a href="https://tools.ietf.org/html/rfc4271" target="_blank">BGP</a> is one of many protocols that powers the Internet. Chances are you have heard of it, even if you don’t work in or around the computer networking space. If you aren’t familiar, I’ll try to provide some quick background:</p>
<ul>
<li>BGP is a <a href="https://en.wikipedia.org/wiki/Distance-vector_routing_protocol" target="_blank">distance-vector routing protocol</a> used to disseminate routing information.</li>
<li>A BGP speaker implements a <a href="https://en.wikipedia.org/wiki/Finite-state_machine" target="_blank">finite state machine</a> with 6 states:
<ul>
<li>Idle</li>
<li>Active</li>
<li>Connect</li>
<li>OpenSent</li>
<li>OpenConfirm</li>
<li>Established</li>
</ul>
</li>
<li>Inputs to the BGP FSM include messages, timer events, and administrative events.</li>
<li>Routing information is exchanged via UPDATE messages in the Established state.</li>
<li>BGP is extensible; speakers communicate their capabilities via OPEN messages.</li>
</ul>
<p>Expanding on that last bullet point, it’s difficult to summarize exactly how/where BGP is used due to its flexibility and extensibility. Various <a href="https://ietf.org/about/" target="_blank">IETF</a> Working Groups continue to publish BGP-related RFCs for a protocol that took shape in the early 90s. As the BGP landscape and application widens, we need software that enables us to keep up.</p>
<p>In this post I’ll provide some of my personal experience and history working with BGP, and introduce a new BGP library, <a href="https://github.com/jwhited/corebgp" target="_blank">CoreBGP</a>, which can be used to build the next generation of BGP-enabled applications.</p>

<p>In October of 2010 I attended my first <a href="https://www.nanog.org/" target="_blank">NANOG</a> meeting in Atlanta, GA after accidentally falling into the position of Network Operations Engineer at work. I worked for a modest-sized hosting provider at the time, and was intrigued with BGP. Upon arriving in Atlanta, I vaguely remember some confusion after telling a cab driver that the hotel I needed to be dropped at was on Peachtree St. I later learned that there are 71 streets in Atlanta with a variant of “Peachtree” in their name, according to <a href="https://en.wikipedia.org/wiki/Peachtree_Street#Nomenclature" target="_blank">Wikpedia</a>.</p>
<p>I got where I needed to go, eventually, and the first talk I attended was <a href="https://archive.nanog.org/meetings/nanog50/presentations/Sunday/NANOG50.Talk33.NANOG50-BGP-Techniques.pdf" target="_blank">BGP techniques for Internet Service Providers</a> by <a href="http://www.bgp4all.com.au/" target="_blank">Philip Smith</a>. Philip started with the basics before getting into the techniques used at ISPs. So many light bulbs went off for me during this talk. I have yet to see any other BGP presentation cover such a breadth of information but still do it in a way that is beginner-friendly, useful as a refresher for any expert, and just downright interesting.</p>
<p>Fast-forward 10 years and I’ve gained a fair share of experience operating networks that use BGP. In more recent years I’ve shifted to software engineering where I’ve had the opportunity to implement various BGP-enabled applications for network observability, data analytics, and SDN purposes.</p>
<p>Each time I started a new BGP-enabled app, I had to answer the following question – which existing BGP implementation should be its foundation?</p>

<p>Of the handful of open source BGP implementations out there, I’ve had hands-on experience with projects making use of:</p>
<ul>
<li><a href="https://bird.network.cz/" target="_blank">BIRD</a></li>
<li><a href="https://osrg.github.io/gobgp/" target="_blank">GoBGP</a></li>
<li><a href="https://www.opendaylight.org/what-we-do/odl-platform-overview" target="_blank">OpenDaylight</a></li>
<li><a href="https://www.quagga.net/" target="_blank">Quagga</a></li>
</ul>
<p>BIRD shines where a <a href="https://bird.network.cz/?get_doc&amp;v=20&amp;f=bird-5.html" target="_blank">rich policy language</a> is needed. GoBGP has a <a href="https://github.com/osrg/gobgp/tree/master/api" target="_blank">feature-rich gRPC API</a>, and can be embedded as a library. OpenDaylight’s BGP implementation is part of a larger SDN controller solution and has extensive support for <a href="https://docs.opendaylight.org/en/stable-oxygen/user-guide/bgpcep-guide/bgp/bgp-user-guide-linkstate-family.html" target="_blank">BGP-LS</a>. Quagga can reliably produce <a href="https://tools.ietf.org/html/rfc6396" target="_blank">MRT</a> dumps and has been around a long time, though I believe <a href="https://frrouting.org/" target="_blank">FRRouting</a> is now considered its successor.</p>
<p>These are all mature, established implementations. Some of them are in production at large ISPs, <a href="https://www.digitalocean.com/blog/scaling-droplet-public-networking/" target="_blank">Cloud Providers</a>, and <a href="https://joinup.ec.europa.eu/collection/open-source-observatory-osor/document/bird-manages-routing-worlds-largest-internet-exchanges-bird" target="_blank">Internet Exchange Points</a>. They are purpose-built and make various tradeoffs to suit their use cases (programming language, threading model, data structures, API, etc…).</p>
<p>But what if we are building something that doesn’t line up with the primary use cases of these widely used implementations? We may be locked in to decisions that are ultimately burdensome if we choose to build around them. Swapping in our own data structures for routing tables, or adding a new NLRI is non-trivial. Even if an implementation is intended to be embedded as library, it can still back us into a corner with resource consumption. There’s clearly a need to plug in or hook into specific parts of the BGP FSM, without inheriting decisions that went into a full-blown BGP daemon.</p>
<p>At the 27th IEEE International Conference On Network Protocols (ICNP), a group from the Université catholique de Louvain presented a paper on <code>The Case for Pluginized Routing Protocols</code>:</p>
<blockquote>
<p>Abstract—Routing protocols such as BGP and OSPF are key components of Internet Service Provider (ISP) networks. These protocols and the operator’s requirements evolve over time, but it often takes many years for network operators to convince their different router vendors and the IETF to extend routing protocols. Some network operators, notably in enterprise and datacenters have adopted Software Defined Networking (SDN) with its centralised control to be more agile. We propose a new approach to implement routing protocols that enables network operators to innovate while still using distributed routing protocols and thus keeping all their benefits compared to centralised routing approaches. We extend a routing protocol with a virtual machine that is capable of executing plugins. These plugins extend the protocol or modify its underlying algorithms through a simple API to meet the specific requirements of operators. We modify the OSPF and BGP implementations provided by FRRouting and demonstrate the applicability of our approach with several use cases.</p>
<p>— <!-- raw HTML omitted -->The Case for Pluginized Routing Protocols<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><!-- raw HTML omitted --></p>
</blockquote>
<p>In their paper they present a method for plugging into a previously mentioned open-source BGP implementation, FRRouting. Plugins exist at a function level, either prior to invocation (PRE), as a replacement (REPLACE), or just before returning (POST). Much of their BGP plugin focus is around the reception of messages, and decisions made shortly after:</p>
<blockquote>
<p>The BGP daemon is also extended similarly. We add insertion points on functions receiving BGP messages from neighbours, on filters and inside the decision process. We also expose specific functions to the plugins that are executed by the uBPF VM.</p>
<p>— <!-- raw HTML omitted -->The Case for Pluginized Routing Protocols<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><!-- raw HTML omitted --></p>
</blockquote>
<p>They take a clever approach with plugin sandboxing by leveraging a user space eBPF VM (<a href="https://github.com/iovisor/ubpf" target="_blank">uBPF</a>) linked to the FRRouting protocol implementation. Each plugin compiles to eBPF bytecode and runs inside of said VM. Plugins can be loaded and unloaded without impacting the primary protocol implementation. Using an eBPF VM also allowed them to utilise all the pre-existing Linux Kernel tooling.</p>
<p>I found this approach inspiring, but still not quite a match for my use cases:</p>
<ul>
<li>Plugins appear to be built around “incoming” events, or messages. What if I want to inject an UPDATE message to a peer irrespective of what FRRouting wants to send?</li>
<li>FRRouting was not built with this plugin model in mind. Changes/Updates to FRRouting will result in a maintenance headache for the VM hook points.</li>
<li>eBPF bytecode is typically compiled from C. Writing C can be time-consuming in comparison to more modern languages.</li>
<li>I need to be an FRRouting expert to do anything non-trivial.</li>
</ul>
<p>This experience and research led me to create CoreBGP, a BGP library that I could re-use across my BGP-enabled applications.</p>

<p>CoreBGP is a BGP library written in Go that implements the BGP FSM with an event-driven, pluggable model. It exposes an API that empowers the user to:</p>
<ul>
<li>send and validate OPEN message capabilities</li>
<li>handle “important” state transitions</li>
<li>handle incoming UPDATE messages</li>
<li>send outgoing UPDATE messages</li>
</ul>
<p>CoreBGP does not decode UPDATE messages (besides header validation), manage a routing table, or send its own UPDATE messages. These responsibilities are all passed down to the user. Therefore, the intended user is someone who wants that responsibility.</p>
<p>The primary building block of CoreBGP is a Plugin, defined by the following interface:</p>
<div><pre><code data-lang="go"><span>// Plugin is a BGP peer plugin.
</span><span></span><span>type</span> Plugin <span>interface</span> {
	<span>// GetCapabilities is fired when a peer's FSM is in the Connect state prior
</span><span></span>	<span>// to sending an Open message. The returned capabilities are included in the
</span><span></span>	<span>// Open message sent to the peer.
</span><span></span>	<span>GetCapabilities</span>(peer <span>*</span>PeerConfig) []<span>*</span>Capability

	<span>// OnOpenMessage is fired when an Open message is received from a peer
</span><span></span>	<span>// during the OpenSent state. Returning a non-nil Notification will cause it
</span><span></span>	<span>// to be sent to the peer and the FSM will transition to the Idle state.
</span><span></span>	<span>//
</span><span></span>	<span>// Per RFC5492 a BGP speaker should only send a Notification if a required
</span><span></span>	<span>// capability is missing; unknown or unsupported capabilities should be
</span><span></span>	<span>// ignored.
</span><span></span>	<span>OnOpenMessage</span>(peer <span>*</span>PeerConfig, capabilities []<span>*</span>Capability) <span>*</span>Notification

	<span>// OnEstablished is fired when a peer's FSM transitions to the Established
</span><span></span>	<span>// state. The returned UpdateMessageHandler will be fired when an Update
</span><span></span>	<span>// message is received from the peer.
</span><span></span>	<span>//
</span><span></span>	<span>// The provided writer can be used to send Update messages to the peer for
</span><span></span>	<span>// the lifetime of the FSM's current, established state. It should be
</span><span></span>	<span>// discarded once OnClose() fires.
</span><span></span>	<span>OnEstablished</span>(peer <span>*</span>PeerConfig, writer UpdateMessageWriter) UpdateMessageHandler

	<span>// OnClose is fired when a peer's FSM transitions out of the Established
</span><span></span>	<span>// state.
</span><span></span>	<span>OnClose</span>(peer <span>*</span>PeerConfig)
}
</code></pre></div><p>Here’s an example Plugin that logs when a peer enters/leaves an established state and when an UPDATE message is received:</p>
<div><pre><code data-lang="go"><span>type</span> plugin <span>struct</span>{}

<span>func</span> (p <span>*</span>plugin) <span>GetCapabilities</span>(c <span>*</span>corebgp.PeerConfig) []<span>*</span>corebgp.Capability {
	caps <span>:=</span> <span>make</span>([]<span>*</span>corebgp.Capability, <span>0</span>)
	<span>return</span> caps
}

<span>func</span> (p <span>*</span>plugin) <span>OnOpenMessage</span>(peer <span>*</span>corebgp.PeerConfig, capabilities []<span>*</span>corebgp.Capability) <span>*</span>corebgp.Notification {
	<span>return</span> <span>nil</span>
}

<span>func</span> (p <span>*</span>plugin) <span>OnEstablished</span>(peer <span>*</span>corebgp.PeerConfig, writer corebgp.UpdateMessageWriter) corebgp.UpdateMessageHandler {
	log.<span>Println</span>(<span>"peer established"</span>)
	<span>// send End-of-Rib
</span><span></span>	writer.<span>WriteUpdate</span>([]<span>byte</span>{<span>0</span>, <span>0</span>, <span>0</span>, <span>0</span>})
	<span>return</span> p.handleUpdate
}

<span>func</span> (p <span>*</span>plugin) <span>OnClose</span>(peer <span>*</span>corebgp.PeerConfig) {
	log.<span>Println</span>(<span>"peer closed"</span>)
}

<span>func</span> (p <span>*</span>plugin) <span>handleUpdate</span>(peer <span>*</span>corebgp.PeerConfig, u []<span>byte</span>) <span>*</span>corebgp.Notification {
	log.<span>Printf</span>(<span>"got update message of len: %d"</span>, <span>len</span>(u))
	<span>return</span> <span>nil</span>
}
</code></pre></div><p>Plugins are attached to peers when they are added to the Server, which manages their lifetime:</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/">https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/</a></em></p>]]>
            </description>
            <link>https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23744167</guid>
            <pubDate>Mon, 06 Jul 2020 02:39:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust for JavaScript Developers – Functions and Control Flow]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 39 (<a href="https://news.ycombinator.com/item?id=23743363">thread link</a>) | @rkwz
<br/>
July 5, 2020 | http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/ | <a href="https://web.archive.org/web/*/http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>This is the third part in a series about introducing the Rust language to JavaScript developers. Here are the past chapters:</p>
<ol>
<li><a href="http://www.sheshbabu.com/posts/rust-for-javascript-developers-tooling-ecosystem-overview/">Tooling Ecosystem Overview</a></li>
<li><a href="http://www.sheshbabu.com/posts/rust-for-javascript-developers-variables-and-data-types/">Variables and Data Types</a></li>
</ol>
<h2 id="Functions"><a href="#Functions" title="Functions"></a>Functions</h2><p>Rust’s function syntax is pretty much similar to the one in JavaScript.</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> income <span>=</span> <span>100</span><span>;</span>
  <span>let</span> tax <span>=</span> <span>calculate_tax</span><span>(</span>income<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> tax<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>calculate_tax</span><span>(</span>income<span>:</span> i32<span>)</span> <span>-&gt;</span> i32 <span>{</span>
  <span>return</span> income <span>*</span> <span>90</span> <span>/</span> <span>100</span><span>;</span>
<span>}</span></code></pre>
<p>The only difference you might see above is the type annotations for arguments and return values.</p>
<p>The <code>return</code> keyword can be skipped and it’s very common to see code without an explicit return. If you’re returning implicitly, make sure to remove the semicolon from that line. The above function can be refactored as:</p>
<pre><code>fn main() {
  let income = 100;
  let tax = calculate_tax(income);
  println!("{}", tax);
}

fn calculate_tax(income: i32) -&gt; i32 {
<span>- return income * 90 / 100;</span>
<span>+ income * 90 / 100</span>
}</code></pre>
<h2 id="Arrow-Functions"><a href="#Arrow-Functions" title="Arrow Functions"></a>Arrow Functions</h2><p>Arrow functions are a popular feature in modern JavaScript - they allow us to write functional code in a concise way.</p>
<p>Rust has something similar and they are called “Closures”. The name might be a bit confusing and would require getting used to because in JavaScript, closures can be created using both normal and arrow functions.</p>
<p>Rust’s closure syntax is very similar to JavaScript’s arrow functions:</p>
<p><strong>Without arguments:</strong></p>
<pre><code>
<span>let</span> greet <span>=</span> <span>(</span><span>)</span> <span>=</span><span>&gt;</span> console<span>.</span><span>log</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>

<span>greet</span><span>(</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> greet <span>=</span> <span>||</span> <span>println!</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>

<span>greet</span><span>(</span><span>)</span><span>;</span> </code></pre>
<p><strong>With arguments:</strong></p>
<pre><code>
<span>let</span> greet <span>=</span> <span>(</span>msg<span>)</span> <span>=</span><span>&gt;</span> console<span>.</span><span>log</span><span>(</span>msg<span>)</span><span>;</span>

<span>greet</span><span>(</span><span>"good morning!"</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> greet <span>=</span> <span>|</span>msg<span>:</span> <span>&amp;</span>str<span>|</span> <span>println!</span><span>(</span><span>"{}"</span><span>,</span> msg<span>)</span><span>;</span>

<span>greet</span><span>(</span><span>"good morning!"</span><span>)</span><span>;</span> </code></pre>
<p><strong>Returning values:</strong></p>
<pre><code>
<span>let</span> add <span>=</span> <span>(</span>a<span>,</span> b<span>)</span> <span>=</span><span>&gt;</span> a <span>+</span> b<span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> add <span>=</span> <span><span>|</span>a<span>:</span> i32<span>,</span> b<span>:</span> i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span> a <span>+</span> b <span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<p><strong>Multiline:</strong></p>
<pre><code>
<span>let</span> add <span>=</span> <span>(</span>a<span>,</span> b<span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
  <span>let</span> sum <span>=</span> a <span>+</span> b<span>;</span>
  <span>return</span> sum<span>;</span>
<span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> add <span>=</span> <span><span>|</span>a<span>:</span> i32<span>,</span> b<span>:</span> i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span>
  <span>let</span> sum <span>=</span> a <span>+</span> b<span>;</span>
  <span>return</span> sum<span>;</span>
<span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<p>Here’s a cheatsheet:<br><img src="http://www.sheshbabu.com/images/2020-rust-for-javascript-developers-3/image-2.png" alt=""></p>
<p>Closures don’t need the type annotations most of the time, but I’ve added them here for clarity.</p>
<h2 id="If-Else"><a href="#If-Else" title="If Else"></a>If Else</h2><pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> income <span>=</span> <span>100</span><span>;</span>
  <span>let</span> tax <span>=</span> <span>calculate_tax</span><span>(</span>income<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> tax<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>calculate_tax</span><span>(</span>income<span>:</span> i32<span>)</span> <span>-&gt;</span> i32 <span>{</span>
  <span>if</span> income <span>&lt;</span> <span>10</span> <span>{</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> income <span>&gt;=</span> <span>10</span> <span>&amp;&amp;</span> income <span>&lt;</span> <span>50</span> <span>{</span>
    <span>return</span> <span>20</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>return</span> <span>50</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<h2 id="Loops"><a href="#Loops" title="Loops"></a>Loops</h2><p>While loops:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> <span>mut</span> count <span>=</span> <span>0</span><span>;</span>

  <span>while</span> count <span>&lt;</span> <span>10</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> count<span>)</span><span>;</span>
    count <span>+=</span> <span>1</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Normal <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for" target="_blank" rel="noopener">for loops</a> don’t exist in Rust, we need to use <code>while</code> or <code>for..in</code> loops. <code>for..in</code> loops are similar to the <code>for..of</code> loops in JavaScript and they loop over an iterator.</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>for</span> n <span>in</span> numbers<span>.</span><span>iter</span><span>(</span><span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> n<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Notice that we’re not iterating directly over the array but instead using the <code>iter</code> method of the array.</p>
<p>We can also loop over <a href="https://doc.rust-lang.org/reference/expressions/range-expr.html" target="_blank" rel="noopener">ranges</a>:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>for</span> n <span>in</span> <span>1</span><span>..</span><span>5</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> n<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<h2 id="Iterators"><a href="#Iterators" title="Iterators"></a>Iterators</h2><p>In JavaScript, we can use array methods like map/filter/reduce/etc instead of <code>for</code> loops to perform calculations or transformations on an array.</p>
<p>For example, here we take an array of numbers, double them and filter out the elements that are less than 10:</p>
<pre><code><span>function</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>let</span> double <span>=</span> <span>(</span>n<span>)</span> <span>=</span><span>&gt;</span> n <span>*</span> <span>2</span><span>;</span>
  <span>let</span> less_than_ten <span>=</span> <span>(</span>n<span>)</span> <span>=</span><span>&gt;</span> n <span>&lt;</span> <span>10</span><span>;</span>

  <span>let</span> result <span>=</span> numbers<span>.</span><span>map</span><span>(</span>double<span>)</span><span>.</span><span>filter</span><span>(</span>less_than_ten<span>)</span><span>;</span>

  console<span>.</span><span>log</span><span>(</span>result<span>)</span><span>;</span> 
<span>}</span></code></pre>
<p>In Rust, we can’t directly use map/filter/etc over vectors, we need to follow these steps:</p>
<ol>
<li>Convert the vector into an iterator using <code>iter</code>, <code>into_iter</code> or <code>iter_mut</code> methods</li>
<li>Chain <code>adapters</code> such as map/filter/etc on the iterator</li>
<li>Finally convert the iterator back to a vector using <code>consumers</code> such as <code>collect</code>, <code>find</code>, <code>sum</code> etc</li>
</ol>
<p>Here’s the equivalent Rust code:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>vec!</span><span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>let</span> double <span>=</span> <span><span>|</span>n<span>:</span> <span>&amp;</span>i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span> n <span>*</span> <span>2</span> <span>}</span><span>;</span>
  <span>let</span> less_than_10 <span>=</span> <span><span>|</span>n<span>:</span> <span>&amp;</span>i32<span>|</span></span> <span>-&gt;</span> bool <span>{</span> <span>*</span>n <span>&lt;</span> <span>10</span> <span>}</span><span>;</span>

  <span>let</span> result<span>:</span> Vec<span>&lt;</span>i32<span>&gt;</span> <span>=</span> numbers<span>.</span><span>iter</span><span>(</span><span>)</span><span>.</span><span>map</span><span>(</span>double<span>)</span><span>.</span><span>filter</span><span>(</span>less_than_10<span>)</span><span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span>

  <span>println!</span><span>(</span><span>"{:?}"</span><span>,</span> result<span>)</span><span>;</span> 
<span>}</span></code></pre>
<p>You should be able to understand most of the code above but you might notice few things off here:</p>
<ul>
<li>The usage of <code>&amp;</code> and <code>*</code> in the closure</li>
<li>The <code>Vec&lt;i32&gt;</code> type annotation for the <code>result</code> variable</li>
</ul>
<p>The <code>&amp;</code> is the reference operator and the <code>*</code> is the dereference operator. The <code>iter</code> method instead of copying the elements in the vector, it passes them as references to the next adapter in the chain. This is why we use <code>&amp;i32</code> in the map’s closure (double). This closure returns <code>i32</code> but <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter" target="_blank" rel="noopener">filter</a> calls its closure (less_than_10) with reference so that’s why we need to use <code>&amp;i32</code> again. To dereference the argument, we use the <code>*</code> operator. We’ll cover this in more detail in future chapters.</p>
<p>Regarding <code>Vec&lt;i32&gt;</code>, so far we haven’t added type annotations to variables as Rust can infer the types automatically, but for <code>collect</code>, we need to be explicitly tell Rust that we expect a <code>Vec&lt;i32&gt;</code> output.</p>
<p>Aside from map and filter, there are ton of other <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html" target="_blank" rel="noopener">useful adapters</a> that we can use in iterators.</p>
<p>Thanks for reading! Feel free to follow me in <a href="https://twitter.com/sheshbabu" target="_blank" rel="noopener">Twitter</a> for updates :)</p>
</div></div>]]>
            </description>
            <link>http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23743363</guid>
            <pubDate>Mon, 06 Jul 2020 00:07:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I’m Writing a Book on Cryptography]]>
            </title>
            <description>
<![CDATA[
Score 171 | Comments 23 (<a href="https://news.ycombinator.com/item?id=23743218">thread link</a>) | @gedigi
<br/>
July 5, 2020 | https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/ | <a href="https://web.archive.org/web/*/https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<p>I’ve now been writing a book on <strong>applied cryptography</strong> for a year and a half.
I’m nearing the end of my journey, as I have one last ambitious chapter left to write: next-generation cryptography (a chapter that I’ll use to talk about cryptography that will become more and more practical: post-quantum cryptography, homomorphic encryption, multi-party computation, and zk-SNARKs).</p>
<p>I’ve been asked multiple times <strong>why write a new book about cryptography?</strong> and <strong>why should I read your book?</strong>.
To answer this, you have to understand when it all started…</p>
<h2>Diagrams are everything</h2>
<p>Today if you want to learn about almost anything, you just google it.
Yet, for cryptography, and depending on what you're looking for, resources can be quite lacking.</p>
<p>It all started a long time ago.
For a class, I had to implement a <a href="https://www.paulkocher.com/doc/DifferentialPowerAnalysis.pdf">differential power analysis attack</a>, a breakthrough in cryptanalysis as it was the first side-channel attack to be published.
A differential power analysis uses the power consumption of a device during an encryption to leak its private key.
At the time, I realized that great papers could convey great ideas with very little emphasis on understanding.
I remember banging my head against the wall trying to figure out what the author of the white paper was trying to say.
Worse, I couldn’t find a good resource that explained the paper.
So I banged my head a bit more, and finally I got it.
And then I thought I would help others.
So I drew some diagrams, animated them, and recorded myself going over them.
That was <a href="https://www.youtube.com/watch?v=gbqNCgVcXsM">my first screencast</a>.</p>
<p>This first step in education was enough to make me want to do more.
I started making more of these videos, and started writing more articles about cryptography on this blog (today totaling more than 500 articles).</p>
<p><img alt="we want to know" src="https://www.cryptologie.net/upload/we_want_to_know.png"></p>
<p>I realized early that diagrams were extremely helpful to understand complicated concepts, and that strangely most resources in the field shied away from them.</p>
<p>For example, anyone in cryptography who thinks about AES-CBC would immediately think about the following wikipedia diagram:</p>
<p><img alt="aes cbc" src="https://www.cryptologie.net/upload/600px-CBC_encryption.svg_.png"></p>
<p>So here I was, trying to explain everything I learned, and thinking hard about what sorts of simple diagrams could easily convey these complex ideas.
That’s when I started thinking about a book, years and years before <a href="https://manning.com/">Manning Publications</a> would reach out to me with a book deal.</p>
<h2>The applied cryptographer curriculum</h2>
<p> I hadn’t started cryptography due to a long-life passion.
I had finished a bachelor in theoretical mathematics and didn’t know what was next for me.
I had also been programming my whole life, and I wanted to reconcile the two.
Naturally, I got curious about cryptography, which seemed to have the best of both world, and started reading the different books at my disposal.
I quickly discovered my life's calling.</p>
<p>Some things were annoying me though. In particular, the long introductions that would start with history.
I was only interested in the technicalities, and always had been.
I swore to myself, if I ever wrote a book about cryptography, I would not write a single line on Vigenère ciphers, Caesar ciphers, and others.</p>
<p>And so after applying to the masters of Cryptography at the university of Bordeaux, and obtaining a degree in the subject, I thought I was ready for the world.
Little did I know.
What I thought was a very applied degree actually lacked a lot on the real world protocols I was about to attack.
I had spent a lot of time learning about the mathematics of elliptic curves, but nothing about how they were used in cryptographic algorithms.
I had learned about LFSRs, and ElGamal, and DES, and a series of other cryptographic primitives that I would never see again.</p>
<p>When I started working in the industry at Matasano, which then became NCC Group, my first gig was to audit <a href="https://www.openssl.org/">OpenSSL</a> (the most popular TLS implementation).
Oh boy, did it hurt my brain.
I remember coming back home every day with a strong headache.
What a clusterfuck of a library.
I had no idea at the time that I would years later become a co-author of TLS 1.3.</p>
<p><img alt="sign" src="https://www.cryptologie.net/upload/7._Note_that_digital_signatures_are_specified_with_a_hash_function,_allowing_you_to_.png"></p>
<p>But at that point I was already thinking: this is what I should have learned in school.
The knowledge I’m getting now is what would have been useful to prepare me for the real world.
After all, I was now a security practitioner specialized in cryptography.
I was reviewing real-world cryptographic applications.
I was doing the job that one would wish they had after finishing a cryptography degree.
I implemented, verified, used, and advised on what cryptographic algorithms to use.</p>
<p>This is the reason I’m the first reader of the book I’m writing.
This is what I would have written to my past self in order to prepare me for the real world.</p>
<h2>The use of cryptography is where most of the bugs are</h2>
<p>My consulting job led me to audit many real world cryptographic applications like the <a href="https://www.nccgroup.com/us/about-us/newsroom-and-events/blog/2015/may/openssl-audit/">OpenSSL</a>, the <a href="https://www.nccgroup.trust/globalassets/our-research/us/public-reports/2018/final_public_report_ncc_group_google_encryptedbackup_2018-10-10_v1.0.pdf">encrypted backup system of Google</a>, the <a href="https://blog.cloudflare.com/ncc-groups-cryptography-services-audit-of-tls-1-3/">TLS 1.3 implementation of Cloudflare</a>, the <a href="https://letsencrypt.org/2015/04/14/ncc-group-audit.html">certificate authority protocol of Let’s Encrypt</a>, the <a href="https://www.nccgroup.com/us/our-research/zcash-overwinter-consensus-and-sapling-cryptography-review/">sapling protocol of Zcash</a>, the <a href="https://blog.nucypher.com/security-audits--round-1--3/">threshold proxy re-encryption scheme of NuCypher</a> and dozens and dozens of other real-world cryptographic applications that I unfortunately cannot mention publicly.</p>
<p>Early in my job, I was tasked to audit the custom protocol a big corporation (that I can’t name) had written to encrypt their communications.
It turns out that, they were signing everything but the ephemeral keys, which completely broke the whole protocol (as one could have easily replaced the ephemeral keys).
A rookie mistake from anyone with some experience with secure transport protocols, but something that was missed by people who thought they were experienced enough to roll their own crypto.
I remember explaining the vulnerability at the end of the engagement, and a room full of engineers turning silent for a good 30 seconds.</p>
<p>This story repeated itself many times during my career.
There was this time where while auditing a cryptocurrency for another client, I found a way to forge transactions from already existing ones (due to some ambiguity of what was being signed).
Looking at TLS implementations for another client, I found some subtle ways to break an RSA implementation, which in turned transformed into a white paper (with one of the inventor of RSA) leading to a number of <a href="https://eprint.iacr.org/2018/1173">Common Vulnerabilities and Exposures (CVEs) reported to a dozen of open source projects</a>.
More recently, reading about Matrix as part of writing my book, I realized that their authentication protocol was completely broken, <a href="https://matrix.org/security-disclosure-policy/">leading to a complete break of their end-to-end encryption</a>.</p>
<p><img alt="comic" src="https://www.cryptologie.net/upload/HEY_MERE_S_AN.png"></p>
<p>There’s so many details that can unfortunately collapse under you, when making use of cryptography.
At that point, I knew I had to write something about it.
This is why my book contains many of these anecdotes.</p>
<p>As part of the job, I would review cryptography libraries and applications in a multitude of programming languages.
I discovered bugs (for example <a href="https://cryptologie.net/article/347/my-first-cve-o/?utm_content=buffer5c408&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">CVE-2016-3959</a> in Golang’s standard library), I researched ways that libraries could fool you into misusing them (for example see my paper <a href="https://eprint.iacr.org/2016/644">How to Backdoor Diffie-Hellman</a>), and I advised on what libraries to use.
Developers never knew what library to use, and I always found the answer to be tricky.</p>
<p>I went on to invent the <a href="https://discocrypto.com/">disco protocol</a>, and wrote a fully-featured cryptographic library in less than 1,000 lines of code in several languages.
Disco only relied on two cryptographic primitives: the permutation of SHA-3 and curve25519.
Yes, from only these two things in 1,000 lines of code a developer could do any type of authenticated key exchange, signatures, encryption, MACs, hashing, key derivation, etc.
This gave me a unique perspective as to what a good cryptographic library was supposed to be.</p>
<p>I wanted my book to contain these kind of practical insights.
So naturally, the different chapters contain examples on how to do crypto in different programming languages, using well-respected cryptographic libraries.</p>
<h2>A need for a new book?</h2>
<p>As I was giving <a href="https://www.blackhat.com/us-17/training/beyond-the-beast-a-broad-survey-of-crypto-vulnerabilities.html">one of my annual cryptography training at Black Hat</a>, one student came to me and asked if I could recommend a good book or online course on cryptography.
I remember advising the student to read <a href="http://toc.cryptobook.us/">the book from Boneh &amp; Shoup</a> and <a href="https://crypto.stanford.edu/~dabo/courses/OnlineCrypto/">Cryptography I from Boneh on Coursera</a>.</p>
<p>The student told me “<em>Ah, I tried, it’s too theoretical!</em>”.
This answer stayed with me.
I disagreed at first, but slowly realized that they were right.
Most of these resources were pretty heavy in math, and most developers interacting with cryptography don’t want to deal with math.
 What else was there for them?
The other two somewhat respected resources at the time were Applied Cryptography and Cryptography Engineering (both from Schneier).
But these books were starting to be quite outdated.
Applied Cryptography spent 4 chapters on block ciphers, with a whole chapter on cipher modes of operation but none on authenticated encryption.
Cryptography Engineering had a single mention of elliptic curve cryptography (in a footnote).</p>
<p>On the other hand, many of my videos or blog posts were becoming good primary references for some cryptographic concepts.</p>
<p><strong>I knew I could do something special</strong>.</p>
<p>Gradually, many of my students started becoming interested in cryptocurrencies, asking more and more questions on the subject.
At the same time, I started to audit more and more cryptocurrency applications.
I finally moved to a job at Facebook to work on <a href="https://libra.org/">Libra</a>.
Cryptocurrency was now one of the hottest field to work on, mixing a multitude of extremely interesting cryptographic primitives that so far had seen no real-world use case (zero knowledge proofs, aggregated signatures, threshold cryptography, multi-party computations, consensus protocols, cryptographic accumulators, verifiable random functions, verifiable delay functions, ... the list goes on)</p>
<p><strong>I was now in a unique position</strong>.</p>
<p>I knew I could write something that would tell students, developers, consultants, security engineers, and others, what modern applied cryptography was all about.</p>
<p><img alt="book" src="https://www.cryptologie.net/upload/needs_to_send_a_let.png"></p>
<p>This was going to be a book with very little …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/">https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/</a></em></p>]]>
            </description>
            <link>https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23743218</guid>
            <pubDate>Sun, 05 Jul 2020 23:45:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Modern Object Pascal Introduction for Programmers]]>
            </title>
            <description>
<![CDATA[
Score 96 | Comments 36 (<a href="https://news.ycombinator.com/item?id=23742999">thread link</a>) | @eatonphil
<br/>
July 5, 2020 | http://newpascal.org/assets/modern_pascal_introduction.html | <a href="https://web.archive.org/web/*/http://newpascal.org/assets/modern_pascal_introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
<div>
<h2 id="_why">1. Why</h2>
<div>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
This is a modified version of the original document from Michalis, because we (authors of <a href="http://newpascal.org/">http://newpascal.org/</a> and <a href="http://synopse.info/">http://synopse.info/</a> ) prefer the "mode delphi" without the "generic" / "specialize" keywords.
</td>
</tr>
</tbody></table>
</div>
<p>There are many books and resources about Pascal out there, but too many of them talk about the old Pascal, without classes, units or generics.</p>
<p>So I wrote this quick introduction to what I call <strong>modern Object Pascal</strong>. Most of the programmers using it don’t really call it <em>"modern Object Pascal"</em>, we just call it  <em>"our Pascal"</em>. But when introducing the language, I feel it’s important to emphasize that it’s a modern, object-oriented language. It evolved a <strong>lot</strong> since the old (Turbo) Pascal that many people learned in schools long time ago. Feature-wise, it’s quite similar to C++ or Java or C#.</p>
<div>
<ul>
<li>
<p>It has all the modern features you expect — classes, units, interfaces, generics…​</p>
</li>
<li>
<p>It’s compiled to a fast, native code,</p>
</li>
<li>
<p>It’s very type safe,</p>
</li>
<li>
<p>High-level but can also be low-level if you need it to be.</p>
</li>
</ul>
</div>
<p>It also has excellent, portable and open-source compiler called the <em>Free Pascal Compiler</em>, <a href="http://freepascal.org/">http://freepascal.org/</a> . And an accompanying IDE (editor, debugger, a library of visual components, form designer) called <em>Lazarus</em> <a href="http://lazarus.freepascal.org/">http://lazarus.freepascal.org/</a> . Myself, I’m the creator of <em>Castle Game Engine</em>, <a href="https://castle-engine.io/">https://castle-engine.io/</a> , which is a cool portable 3D and 2D game engine using this language to create games on many platforms (Windows, Linux, MacOSX, Android, iOS, web plugin).</p>
<p>This introduction is mostly directed at programmers who already have experience in other languages. We will not cover here the meanings of some universal concepts, like <em>"what is a class"</em>, we’ll only show how to do them in Pascal.</p>
</div>
</div>
<div>
<h2 id="_basics">2. Basics</h2>
<div>
<div>
<h3 id="__hello_world_program">2.1. "Hello world" program</h3>
<div>
<div>
<pre><code data-lang="pascal"><span>{$mode delphi}</span> 

<span>program</span> MyProgram; 
<span>begin</span>
  Writeln(<span><span>'</span><span>Hello world!</span><span>'</span></span>);
<span>end</span>.</code></pre>
</div>
</div>
<p>This is a complete program that you can <em>compile</em> and <em>run</em>.</p>
<div>
<ul>
<li>
<p>If you use the command-line FPC, just create a new file <code>myprogram.lpr</code> and execute <code>fpc myprogram.lpr</code>.</p>
</li>
<li>
<p>If you use <em>Lazarus</em>, create a new project (menu <em>Project</em> → <em>New Project</em> → <em>Simple Program</em>). Save it as <code>myprogram</code> and paste this source code as the main file. Compile using the menu item <em>Run → Compile</em>.</p>
</li>
<li>
<p>This is a command-line program, so in either case — just run the compiled executable from the command-line.</p>
</li>
</ul>
</div>
<p>The rest of this article talks about the Object Pascal language, so don’t expect to see anything more fancy than the command-line stuff. If you want to see something cool, just create a new GUI project in <em>Lazarus</em> (<em>Project</em> → <em>New Project</em> → <em>Application</em>).
Voila — a working GUI application, cross-platform, with native look everywhere, using a comfortable visual component library. The <em>Lazarus</em> and <em>Free Pascal Compiler</em> come with lots of ready units for networking, GUI, database, file formats (XML, json, images…​), threading and everything else you may need. I already mentioned my cool <em>Castle Game Engine</em> earlier:)</p>
</div>
<div>
<h3 id="_functions_procedures_primitive_types">2.2. Functions, procedures, primitive types</h3>
<div>
<div>
<pre><code data-lang="pascal"><span>{$mode delphi}</span>

<span>program</span> MyProgram;

<span>procedure</span> MyProcedure(<span>const</span> A: Integer);
<span>begin</span>
  Writeln(<span><span>'</span><span>A + 10 is: </span><span>'</span></span>, A + <span>10</span>);
<span>end</span>;

<span>function</span> MyFunction(<span>const</span> S: <span>string</span>): <span>string</span>;
<span>begin</span>
  Result := S + <span><span>'</span><span>strings are automatically managed</span><span>'</span></span>;
<span>end</span>;

<span>var</span>
  X: Single;
<span>begin</span>
  Writeln(MyFunction(<span><span>'</span><span>Note: </span><span>'</span></span>));
  MyProcedure(<span>5</span>);

  
  X := <span>15</span> / <span>5</span>;
  Writeln(<span><span>'</span><span>X is now: </span><span>'</span></span>, X); 
  Writeln(<span><span>'</span><span>X is now: </span><span>'</span></span>, X:<span>1</span>:<span>2</span>); 
<span>end</span>.</code></pre>
</div>
</div>
<p>To return a value from a function, assign something to the magic <code>Result</code> variable. You can read and set the <code>Result</code> freely, just like a local variable.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> MyFunction(<span>const</span> S: <span>string</span>): <span>string</span>;
<span>begin</span>
  Result := S + <span><span>'</span><span>something</span><span>'</span></span>;
  Result := Result + <span><span>'</span><span> something more!</span><span>'</span></span>;
  Result := Result + <span><span>'</span><span> and more!</span><span>'</span></span>;
<span>end</span>;</code></pre>
</div>
</div>
<p>You can also treat the function name (like <code>MyFunction</code> in example above) as the variable, to which you can assign. But I would discourage it in new code, as it looks "fishy" when used on the right side of the assignment expression. Just use <code>Result</code> always when you want to read or set the function result.</p>
<p>If you want to call the function itself recursively, you can of course do it. If you’re calling a parameter-less function recursively, be sure to specify the parenthesis (even though in Pascal you can usually omit the parentheses for a parameter-less function), this makes a recursive call to a parameter-less function different from accessing this function’s current result. Like this:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> ReadIntegersUntilZero: <span>string</span>;
<span>var</span>
  I: Integer;
<span>begin</span>
  Readln(I);
  Result := IntToStr(I);
  <span>if</span> I &lt;&gt; <span>0</span> <span>then</span>
    Result := Result + <span><span>'</span><span> </span><span>'</span></span> + ReadIntegersUntilZero();
<span>end</span>;</code></pre>
</div>
</div>
<p>You can call <code>Exit</code> to end the execution of the procedure or function before it reaches the final <code>end;</code>. If you call parameter-less <code>Exit</code> in a function, it will return the last thing you set as <code>Result</code>. You can also use <code>Exit(X)</code> construct, to set the function result and exit <strong>now</strong> — this is just like <code>return X</code> construct in C-like languages.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> AddName(<span>const</span> ExistingNames, NewName: <span>string</span>): <span>string</span>;
<span>begin</span>
  <span>if</span> ExistingNames = <span><span>'</span><span>'</span></span> <span>then</span>
    Exit(NewName);
  Result := ExistingNames + <span><span>'</span><span>, </span><span>'</span></span> + NewName;
<span>end</span>;</code></pre>
</div>
</div>
</div>
<div>
<h3 id="_testing_if">2.3. Testing (if)</h3>
<p>Use <code>if .. then</code> or <code>if .. then .. else</code> to run some code when some condition is satisfied. Unlike in the C-like languages, in Pascal you don’t have to wrap the condition in parenthesis.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A: Integer;
  B: boolean;
<span>begin</span>
  <span>if</span> A &gt; <span>0</span> <span>then</span>
    DoSomething;

  <span>if</span> A &gt; <span>0</span> <span>then</span>
  <span>begin</span>
    DoSomething;
    AndDoSomethingMore;
  <span>end</span>;

  <span>if</span> A &gt; <span>10</span> <span>then</span>
    DoSomething
  <span>else</span>
    DoSomethingElse;

  
  B := A &gt; <span>10</span>;
  <span>if</span> B <span>then</span>
    DoSomething
  <span>else</span>
    DoSomethingElse;
<span>end</span>;</code></pre>
</div>
</div>
<p>The <code>else</code> is paired with the last <code>if</code>. So this works as you expect:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> A &lt;&gt; <span>0</span> <span>then</span>
  <span>if</span> B &lt;&gt; <span>0</span> <span>then</span>
    AIsNonzeroAndBToo
  <span>else</span>
    AIsNonzeroButBIsZero;</code></pre>
</div>
</div>
<p>While the example with nested <code>if</code> above is correct, it is often better to place the nested <code>if</code> inside a <code>begin</code> …​ <code>end</code> block in such cases. This makes the code more obvious to the reader, and it will remain obvious even if you mess up the indentation. The improved version of the example is below. When you add or remove some <code>else</code> clause in the code below, it’s obvious to which condition it will apply (to the <code>A</code> test or the <code>B</code> test), so it’s less error-prone.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> A &lt;&gt; <span>0</span> <span>then</span>
<span>begin</span>
  <span>if</span> B &lt;&gt; <span>0</span> <span>then</span>
    AIsNonzeroAndBToo
  <span>else</span>
    AIsNonzeroButBIsZero;
<span>end</span>;</code></pre>
</div>
</div>
</div>
<div>
<h3 id="_logical_relational_and_bit_wise_operators">2.4. Logical, relational and bit-wise operators</h3>
<p>The <em>logical operators</em> are called <code>and</code>, <code>or</code>, <code>not</code>, <code>xor</code>. Their meaning is probably obvious (search for <em>"exclusive or"</em> if you’re unsure what <em>xor</em> does:). They take <em>boolean arguments</em>, and return a <em>boolean</em>. They can also act as <em>bit-wise operators</em> when both arguments are integer values, in which case they return an integer.</p>
<p>The <em>relational (comparison)</em> operators are <code>=</code>, <code>&lt;&gt;</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>. If you’re accustomed to C-like languages, note that in Pascal you compare two values (check are they equal) using a single equality character <code>A = B</code> (unlike in C where you use <code>A == B</code>). The special <em>assignment</em> operator in Pascal is <code>:=</code>.</p>
<p>The <em>logical (or bit-wise) operators have a higher precedence than relational operators</em>. So you may need to use parenthesis around some expressions.</p>
<p>For example this is a compilation error:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A, B: Integer;
<span>begin</span>
  <span>if</span> A = <span>0</span> <span>and</span> B &lt;&gt; <span>0</span> <span>then</span> ... </code></pre>
</div>
</div>
<p>The above fails to compile, because the compiler sees the bit-wise <code>and</code> inside: <code>(0 and B)</code>.</p>
<p>This is correct:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A, B: Integer;
<span>begin</span>
  <span>if</span> (A = <span>0</span>) <span>and</span> (B &lt;&gt; <span>0</span>) <span>then</span> ...</code></pre>
</div>
</div>
<p>The <em>short-circuit evaluation</em> is used. Consider this expression:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> MyFunction(X) <span>and</span> MyOtherFunction(Y) <span>then</span>...</code></pre>
</div>
</div>
<div>
<ul>
<li>
<p>It’s guaranteed that <code>MyFunction(X)</code> will be evaluated first.</p>
</li>
<li>
<p>And if <code>MyFunction(X)</code> returns <code>false</code>, then the value of expression is known (the value of <code>false and whatever</code> is always <code>false</code>), and <code>MyOtherFunction(Y)</code> will not be executed at all.</p>
</li>
<li>
<p>Analogous rule is for <code>or</code> expression. There, if the expression is known to be <code>true</code> (because the 1st operand is <code>true</code>), the 2nd operand is not evaluated.</p>
</li>
<li>
<p>This is particularly useful when writing expressions like</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> (A &lt;&gt; <span>nil</span>) <span>and</span> A.IsValid <span>then</span>...</code></pre>
</div>
</div>
<p>This will work OK, even when <code>A</code> is <code>nil</code>.</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_testing_single_expression_for_multiple_values_case">2.5. Testing single expression for multiple values (case)</h3>
<p>If a different action should be executed depending on the value of some expression, then the <code>case .. of .. end</code> statement is useful.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>case</span> SomeValue <span>of</span>
  <span>0</span>: DoSomething;
  <span>1</span>: DoSomethingElse;
  <span>2</span>: <span>begin</span>
       IfItsTwoThenDoThis;
       AndAlsoDoThis;
     <span>end</span>;
  <span>3</span>..<span>10</span>: DoSomethingInCaseItsInThisRange;
  <span>11</span>, <span>21</span>, <span>31</span>: AndDoSomethingForTheseSpecialValues;
  <span>else</span> DoSomethingInCaseOfUnexpectedValue;
<span>end</span>;</code></pre>
</div>
</div>
<p>The <code>else</code> clause is optional. When no condition matches, and there’s no <code>else</code>, then nothing happens.</p>
<p>In you come from C-like languages, and compare this with <code>switch</code> statement in these languages, you will notice that there is no automatic <em>fall-through</em>. This is a deliberate blessing in Pascal. You don’t have to remember to place <code>break</code> instructions. In every execution, <em>at most one</em> branch of the <code>case</code> is executed, that’s it.</p>
</div>
<div>
<h3 id="_enumerated_and_ordinal_types_and_sets_and_constant_length_arrays">2.6. Enumerated and ordinal types and sets and constant-length arrays</h3>
<p>Enumerated type in Pascal is a very nice, opaque type. You will probably use it much more often than enums in other languages:)</p>
<div>
<div>
<pre><code data-lang="pascal"><span>type</span>
  TAnimalKind = (akDuck, akCat, akDog);</code></pre>
</div>
</div>
<p>The convention is to prefix the enum names with a two-letter shortcut of type name, hence <code>ak</code> = shortcut for <em>"Animal Kind"</em>. This is a useful convention, since the enum names are in the unit (global) namespace. So by prefixing them with <code>ak</code> prefix, you minimize the chances of collisions with other identifiers.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
The collisions in names are not a show-stopper. It’s Ok for different units to define the same identifier. But it’s a good idea to try to avoid the collisions anyway, to keep code simple to understand and grep.
</td>
</tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
You can avoid placing enum names in the global namespace by directive <code>{$scopedenums on}</code>. This means you will have to access them qualified by a type name, like <code>TAnimalKind.akDuck</code>. The need for <code>ak</code> prefix disappears in this situation, and you will probably just call the enums <code>Duck, Cat, Dog</code>. This is …</td></tr></tbody></table></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://newpascal.org/assets/modern_pascal_introduction.html">http://newpascal.org/assets/modern_pascal_introduction.html</a></em></p>]]>
            </description>
            <link>http://newpascal.org/assets/modern_pascal_introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742999</guid>
            <pubDate>Sun, 05 Jul 2020 23:15:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing a winning 4K intro in Rust]]>
            </title>
            <description>
<![CDATA[
Score 266 | Comments 63 (<a href="https://news.ycombinator.com/item?id=23742870">thread link</a>) | @Dowwie
<br/>
July 5, 2020 | https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html | <a href="https://web.archive.org/web/*/https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-4371838969983872321" itemprop="description articleBody">
<div><p><span>I recently wrote my first 4K intro in Rust and released it at the Nova 2020 where it took first place in the new school intro competition. Writing a 4K intro is quite involved and requires you to master many different areas at the same time. Here I will focus on what I learned about making Rust code as small as possible.</span></p><p><iframe allowfullscreen="" height="322" src="https://www.youtube.com/embed/SIkkYRQ07tU" width="387" youtube-src-id="SIkkYRQ07tU"></iframe></p><p>You can view the demo on<span>&nbsp;</span><a href="https://www.youtube.com/watch?v=SIkkYRQ07tU">youtube</a>, download the executable at<span>&nbsp;</span><a href="https://www.pouet.net/prod.php?which=85924">pouet</a><span>&nbsp;</span>or get the source code from<span>&nbsp;</span><a href="https://github.com/janiorca/sphere_dance">github</a></p><p>A 4K intro is a demo where the entire program ( including any data ) has two be 4096 bytes or less so it is important that the code is as space efficient as possible. Rust has a bit of a reputation for creating bloated executables so I wanted to find out if is possible to create very space efficient code with it.</p><p>The entire intro is written in a combination of Rust and glsl. Glsl is used for rendering everything on screen but Rust does everything else; world creation, camera and object control, creating instruments and playing music etc.</p><p>Some of the features I depend on, such as xargo, are not yet part of stable Rust so I use the nightly rust toolchain. To install and use the nightly toolchain as default you need the following rustup commands.</p><pre data-info="" data-role="codeBlock"><code>rustup toolchain install nightly
rustup default nightly
</code></pre><p>I use<span>&nbsp;</span><a href="http://crinkler.net/">crinkler</a><span>&nbsp;</span>to compress the object file generated by the rust compiler.</p><p>I also used<span>&nbsp;</span><a href="https://github.com/laurentlb/Shader_Minifier">shader minifier</a><span>&nbsp;</span>for pre-processing the<span>&nbsp;</span><code>glsl</code><span>&nbsp;</span>shader to make it smaller and more crinkler friendly. The shader minifier doesn't support output into<span>&nbsp;</span><code>.rs</code><span>&nbsp;</span>files so I ended up using its raw output and manually copying it into my<span>&nbsp;</span><a href="http://shader.rs/">shader.rs</a><span>&nbsp;</span>file. (In hindsight, I should have written something to automate that stage. Or even created a PR for shader minifier)</p><p>The starting point was the proof of concept code I developed earlier (<a href="https://www.codeslow.com/2020/01/writing-4k-intro-in-rust.html">https://www.codeslow.com/2020/01/writing-4k-intro-in-rust.html</a>) which I thought was pretty lean at the time. That article also goes into but more detail about setting up the<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file and how to use xargo for compiling tiny executable.</p><p>Many of the most effective size optimizations have nothing to do with clever hacks but are the result of rethinking the design.</p><p>My initial design had one part of the code creating the world, including placing the spheres and another part was responsible for moving the spheres. At some point I realized that the sphere placement and sphere moving code were doing very similar things and I could merge them into one sightly more complicated function that did both. Unfortunately, this type of optimization can make the code less elegant and readable.</p><p>At some point you have to look at the compiled assembly code to understand what the code gets compiled into and what size optimizations are worth it. The Rust compiler has a very useful option,<span>&nbsp;</span><code>--emit=asm</code><span>&nbsp;</span>for outputting assembler code. The following command creates a<span>&nbsp;</span><code>.s</code><span>&nbsp;</span>assembly file;</p><pre data-info="" data-role="codeBlock"><code>xargo rustc --release --target i686-pc-windows-msvc -- --emit=asm
</code></pre><p>It is not necessary to be an expert in assembler to benefit from studying the assembler output but it definitely helps to have a basic understanding assembler syntax. The release version uses<span>&nbsp;</span><code>opt-level = "z</code><span>&nbsp;</span>which causes the compiler to optimize for the smallest possible size. This can make it a bit tricky to work out which part of the assembly code corresponds to which part of the Rust code.</p><p>I discovered that the Rust compiler can be surprisingly good at minimizing code; getting rid of unused code and unnecessary parameters and folding code. It can also do some strange things which is why it is essential to occasionally study the resulting assembly code.</p><p>I worked with two versions of the code; one version does logging and allows the viewer to manipulate the camera which is used for creating interesting camera paths. Rust allows you to define<span>&nbsp;</span><strong>features</strong><span>&nbsp;</span>that you can use to optionally include bits of functionality. The<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file has a<span>&nbsp;</span><strong>[features]</strong><span>&nbsp;</span>section that lets you declare the available features and their dependencies. My 4K intro has the following section in the<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file;</p><pre data-info="toml" data-role="codeBlock"><span>[</span><span>features</span><span>]</span>
<span>logger</span> <span>=</span> <span>[</span><span>]</span>
<span>fullscreen</span> <span>=</span> <span>[</span><span>]</span>
</pre><p>Neither of the optional features has dependencies so they effectively work as being conditional compilation flags. The conditional blocks of code are preceded by<span>&nbsp;</span><code>#[cfg(feature)]</code><span>&nbsp;</span>statement. Using features in itself does not make the code smaller but it makes development process much nicer when you easily switch between different feature sets.</p><pre data-info="rust" data-role="codeBlock">        <span>#[cfg(feature = "fullscreen")]</span>
        <span>{</span>
            
        <span>}</span>

        <span>#[cfg(not(feature = "fullscreen"))]</span>
        <span>{</span>
            
        <span>}</span>
</pre><p>Having inspected the compiled code I am certain that only the selected features get included in the compiled code.</p><p>One of the main uses of<span>&nbsp;</span><strong>features</strong><span>&nbsp;</span>was to enable logging and error checking for the debug build. The code loading and compiling the glsl shader failed frequently and without useful error messages it would have been extremely painful to find the problems.</p><p>When putting code inside an<span>&nbsp;</span><code>unsafe{}</code><span>&nbsp;</span>block I sort of assumed that all safety checks would be disabled within this block but this is not true, all the usual checks are still applied and these checks can be expensive.</p><p>By default Rust range checks all array accesses. Take the following Rust code</p><pre data-info="rust" data-role="codeBlock">    delay_counter <span>=</span> sequence<span>[</span> play_pos <span>]</span><span>;</span>
</pre><p>Before doing the table look up the compiler would insert code that checks that play_pos is not indexing past the end of sequence and panic if that was the case. This adds considerable size to the code as there can be a lot of table look-ups like this.</p><p>Converting the above code into</p><pre data-info="rust" data-role="codeBlock">    delay_counter <span>=</span> <span>*</span>sequence<span>.</span><span>get_unchecked</span><span>(</span> play_pos <span>)</span><span>;</span>
</pre><p>tells the compiler to not perform any range checks and just do the table look-up. This is clearly a potentially dangerous operation and can thus only be performed within an<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code block</p><p>Initially all my loops used the idiomatic rust way of doing loops, using the<span>&nbsp;</span><code>for x in 0..10</code><span>&nbsp;</span>syntax which I just assumed would be compiled into tightest possible loop. Surprisingly, this was not the case. The simplest case;</p><pre data-info="rust" data-role="codeBlock"><span>for</span> x <span>in</span> <span>0</span><span>..</span><span>10</span> <span>{</span>
    
<span>}</span>
</pre><p>would get translated into assembly code that did the following;</p><pre data-info="" data-role="codeBlock"><code>    setup loop variable
loop:
    check for loop condition    
    if loop finished, jump to end
    // do code inside loop
    unconditionally jump to loop
end:
</code></pre><p>whereas if did the following rust code</p><pre data-info="rust" data-role="codeBlock"><span>let</span> x <span>=</span> <span>0</span><span>;</span>
<span>loop</span><span>{</span>
    
    x <span>+=</span> <span>1</span><span>;</span>
    <span>if</span> i <span>==</span> <span>10</span> <span>{</span>
        <span>break</span><span>;</span>
    <span>}</span>
<span>}</span>
</pre><p>would get directly compiled into;</p><pre data-info="" data-role="codeBlock"><code>    setup loop variable
loop:
    // do code inside loop
    check for loop condition    
    if loop not finished, jump to loop
end:
</code></pre><p>Note that the loop condition is checked at the end of each loop which makes the unconditional jump unnecessary. This is small space saving for one loop but they do add up when there are 30 loops in the program.</p><p>The other, much harder to understand, problem with the idiomatic Rust loop is that in some cases it the compiler would add some additional iterator setup code that really bloated the code. I never fully understood what triggered this additional iterator setup as it was always trivial to replace the<span>&nbsp;</span><code>for {}</code><span>&nbsp;</span>constructs with a<span>&nbsp;</span><code>loop{}</code><span>&nbsp;</span>construct.</p><p>I spent a lot of time optimizing the<span>&nbsp;</span><code>glsl</code><span>&nbsp;</span>code and one of the best class of optimizations ( which also usually made the code run faster) was to operate on an entire vector at a time instead of operating at a component at a time.</p><p>For example, the ray tracing code use a fast<span>&nbsp;</span><a href="http://www.cse.yorku.ca/~amana/research/grid.pdf">grid traversal algorithm</a><span>&nbsp;</span>to check which parts of the map each ray visits. The original algorithm considers each axis separately but it is possible to rewrite the algorithm so it considers all axes at the same time and does not need any branches. Rust doesn't really have a native vector type like glsl but you can use intrinsics to tell it to use SIMD instructions.</p><p>To use intrinsics I would convert the following code</p><pre data-info="rust" data-role="codeBlock">        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>0</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>0</span> <span>]</span><span>*</span>camera_speed<span>;</span>
        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>1</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>1</span> <span>]</span><span>*</span>camera_speed<span>;</span>
        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>2</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>2</span> <span>]</span><span>*</span>camera_speed<span>;</span>
</pre><p>into</p><pre data-info="rust" data-role="codeBlock">        <span>let</span> <span>mut</span> dst<span>:</span>x86<span>:</span><span>:</span>__m128 <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_load_ps</span><span>(</span>global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>let</span> <span>mut</span> src<span>:</span>x86<span>:</span><span>:</span>__m128 <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_load_ps</span><span>(</span>camera_rot_speed<span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>)</span><span>;</span>
        dst <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_add_ps</span><span>(</span> dst<span>,</span> src<span>)</span><span>;</span>
        core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_store_ss</span><span>(</span> <span>(</span><span>&amp;</span><span>mut</span> global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>)</span><span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>,</span> dst <span>)</span><span>;</span>
</pre><p>which would be quite a bit smaller ( but a lot less readable ). Sadly, for some reason this broke the debug build while working perfectly on the release build. Clearly, this is a problem with my intrinsics knowledge and not a problem with Rust. This is something I would spend more time on for my next 4K intro as the space saving were significant.</p><p>There are lot of standard Rust crates for loading OpenGL functions but by default they all load a very large set of OpenGL functions. Each loaded function takes up some space because the loader has to know its name. Crinkler does a very good job of compressing this kind of code but it is not able to completely get rid of the overhead so I had to create my own version<span>&nbsp;</span><code>gl.rs</code><span>&nbsp;</span>that only includes the OpenGL functions that are used in the code.</p><p>My first objective was to write a competitive proper 4K intro to prove that language was suitable for scenarios where every single byte counts and you really need low level control. Typically this has been the sole domain of assembler and C. The secondary objective was to write it using idiomatic Rust as much possible.</p><p>I think I was fairly successful on the first objective. At no point during the development did I feel that Rust was holding me back in any way or that I was sacrificing performance or capabilities because I was using Rust rather than C.</p><p>I was less successful on the second objective. There is far too much<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code that doesn't really need to be there.<span>&nbsp;</span><code>Unsafe</code><span>&nbsp;</span>has a corrupting effect; it is very easy to use<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code to quickly accomplish something (like using mutable statics) but once the unsafe code is there it begets more unsafe code and suddenly it …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html">https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html</a></em></p>]]>
            </description>
            <link>https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742870</guid>
            <pubDate>Sun, 05 Jul 2020 23:00:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Triplebyte data download doesn’t give you all your data]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23742473">thread link</a>) | @wolfgang42
<br/>
July 5, 2020 | https://www.linestarve.com/blog/post/triplebyte-data-download/ | <a href="https://web.archive.org/web/*/https://www.linestarve.com/blog/post/triplebyte-data-download/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="<%= page.layout %>-<%= page.slug %>" itemscope="" itemprop="blogPost">
	<div>
		<div>
			<div>
				<p>In May of last year I decided to start looking for a new job, and started by taking <a href="https://triplebyte.com/">Triplebyte</a>’s quiz. Having passed that, I spent the next three months going through the rest of their process, from a <a href="https://triplebyte.com/interview_guide">two-hour remote interview</a> all the way through to final negotiations with the company whose offer I selected. Throughout the process they were extremely competent and helpful, and at the end of it all I had only good things to say about them. They made the whole process go extremely smoothly, answered all the questions I had and gave me a ton of advice on the whole process, and their screening process was not only great from the my perspective but also gave me confidence in the quality of all their candidates.</p>

<p>Then, a little over a month ago, I got an email announcing the upcoming launch of Triplebyte’s new public profiles. I thought they were was a neat idea, and made a note that I should turn mine on next time I started a job hunt. Then someone <a href="https://news.ycombinator.com/item?id=23279837">posted the email on Hacker News</a>, pointing out that buried in the middle of the email was the fact that these new profiles were going to be opt-<em>out,</em> and unless I turned it off in the next week my profile would become public. This understandably caused an uproar, which the Triplebyte CEO Ammon <a href="https://news.ycombinator.com/item?id=23280460">completely misinterpreted</a>, posting a series of <a href="https://news.ycombinator.com/item?id=23280120">inflammatory comments</a> that <a href="https://news.ycombinator.com/item?id=23280472">misunderstood what people were upset about</a> before vanishing. A few days later he came back with a very apologetic email explaining that they weren’t going to go through with it after all, though it received <a href="https://news.ycombinator.com/item?id=23303037">mixed reactions</a>, with a lot of people being concerned that the idea had been considered at all.</p>

<p>In the midst of all this, I submitted a request through the <a href="https://triplebyte.com/privacy-center">Triplebyte privacy center</a> to download my data. (I considered deleting my account, but decided to give them the benefit of the doubt until things settled down.) After approving the request by clicking an email link, I was informed that it might take up to 30 days to complete my request, so I settled down to wait. As the weeks passed, I thought that the sudden influx of requests must have overwhelmed whoever was responsible for gathering the data from all the systems it was stored in.</p>

<p>Then, 36 days after I first submitted the request, I got an email informing me that my data was now ready to be downloaded. I clicked the link in the email, and then another link on the next page, and finally I got—</p>

<p>A 2,917 byte JSON blob.</p>

<p><em>Odd,</em> I thought, <em>that seems like an awfully long time for so little data.</em> (It’s just over 81 bytes per day, in fact, though I realize that’s a silly metric.) Still, I was relieved to know that they hadn’t been gathering reams of data about me behind my back. Scanning over the minified data, it looked like all they had was my address, some information I’d given them about my past jobs and preferred languages, and a couple of recent IP addresses. Seemingly they hadn’t even kept any information at all about my job search with them.</p>

<p>Then I opened up the file in a JSON viewer and gradually realized: <em>this was not all the information they had.</em> It wasn’t even all of the information they were <em>willing to admit</em> they had—it was missing some obvious things, like the text descriptions on the <code>education</code> and <code>work experience</code> objects, which were prominently displayed on my profile page. As far as I can tell, all I got was a sloppy attempt at making it look at a casual glance like they’d given me what I asked for.</p>

<p>This raises serious concerns for me about Triplebyte, even more so than their plan to make profiles public by default, which started this all. That may well have been born of overenthusiastic naïvité, and was quickly rescinded after being exposed to public comment. After that fiasco, though, I would have expected them to double down on making sure that they were taking privacy seriously. They had over a month before sending me this data to fix any issues with the system, and instead they sent me some slapdash attempt at maybe giving me a whiff of my data.</p>

<p>Triplebyte (as they explain in their privacy center) “care deeply about how your personal information is used and shared,” but apparently not enough to actually put effort into getting it right when you ask for it.</p>

<p>(I’ve sent them an email asking what happened to the rest of the data, and will update this post when I get a response. As it’s the weekend I may not hear back for a few days.)</p>

			</div>
		</div>
	</div>
</article></div>]]>
            </description>
            <link>https://www.linestarve.com/blog/post/triplebyte-data-download/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742473</guid>
            <pubDate>Sun, 05 Jul 2020 22:02:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contrarian view on closing files]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 90 (<a href="https://news.ycombinator.com/item?id=23742390">thread link</a>) | @coady
<br/>
July 5, 2020 | https://coady.github.io/posts/closing-files/ | <a href="https://web.archive.org/web/*/https://coady.github.io/posts/closing-files/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <div>
<div>

<div>
<div>
<h2 id="Contrarian-view-on-closing-files.">Contrarian view on closing files.<a href="#Contrarian-view-on-closing-files.">¶</a>
</h2>
<p>It has become conventional wisdom to always explicitly close file-like objects, via context managers.
The <a href="https://google.github.io/styleguide/pyguide.html#311-files-and-sockets">google style guide</a>
is representative:</p>
<blockquote>
<p>Explicitly close files and sockets when done with them.
Leaving files, sockets or other file-like objects open unnecessarily has many downsides, including:</p>
<p>They may consume limited system resources, such as file descriptors.</p>
<ul>
<li>Code that deals with many such objects may exhaust those resources unnecessarily if they're not returned to the system promptly after use.</li>
<li>Holding files open may prevent other actions being performed on them, such as moves or deletion.</li>
<li>Files and sockets that are shared throughout a program may inadvertantly be read from or written to after logically being closed. If they are actually closed, attempts to read or write from them will throw exceptions, making the problem known sooner.</li>
</ul>
<p>Furthermore, while files and sockets are automatically closed when the file object is destructed, tying the life-time of the file object to the state of the file is poor practice, for several reasons:</p>
<ul>
<li>There are no guarantees as to when the runtime will actually run the file's destructor. Different Python implementations use different memory management techniques, such as delayed Garbage Collection, which may increase the object's lifetime arbitrarily and indefinitely.</li>
<li>Unexpected references to the file may keep it around longer than intended (e.g. in tracebacks of exceptions, inside globals, etc).</li>
</ul>
<p>The preferred way to manage files is using the "with" statement:</p>

<pre><code>with open("hello.txt") as hello_file:
    for line in hello_file:
        print line</code></pre>
</blockquote>
<h3 id="In-theory">In theory<a href="#In-theory">¶</a>
</h3>
<p>Good points, and why limit this advice to file descriptors?  Any resource may be limited or require exclusivity;  that's why they're called resources.  Similarly one should always explicitly call <code>dict.clear</code> when finished with a <code>dict</code>.  After all, "there are no guarantees as to when the runtime will actually run the &lt;object's&gt; destructor.  And "code that deals with many such objects may exhaust those resources unnecessarily", such as memory, or whatever else is in the <code>dict</code>.</p>
<p>But in all seriousness, this advice is applying a notably higher standard of premature optimization to file descriptors than to any other kind of resource.  There are plenty of Python projects that are guaranteed to run on CPython for a variety of reasons, where destructors are immediately called.  And there are plenty of Python projects where file descriptor usage is just a non-issue.  It's now depressingly commonplace to see this in <code>setup.py</code> files:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>with</span> <span>open</span><span>(</span><span>"README.md"</span><span>)</span> <span>as</span> <span>readme</span><span>:</span>
    <span>long_description</span> <span>=</span> <span>readme</span><span>.</span><span>read</span><span>()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<p>Let's consider a practical example: a <code>load</code> function which is supposed to read and parse data given a file path.</p>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>import</span> <span>csv</span>
<span>import</span> <span>json</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""the supposedly bad way"""</span>
    <span>return</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>filepath</span><span>))</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""the supposedly good way"""</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>return</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>file</span><span>)</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""with a different file format"""</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>return</span> <span>csv</span><span>.</span><span>reader</span><span>(</span><span>file</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>Which versions work correctly?  Are you sure?  If it's not immediately obvious why one of these is broken, that's the point.  In fact, it's worth trying out before reading on.</p>
<p>...</p>
<p>The <code>csv</code> version returns an iterator over a closed file.  It's a violation of procedural abstraction to know whether the result of <code>load</code> is lazily evaluated or not; it's just supposed to implement an interface.  Moreover, according to this best practice, it's <em>impossible</em> to write the <code>csv</code> version correctly.  As absurd as it sounds, it's just an abstraction that can't exist.</p>
<p>Defiantly clever readers are probably already trying to fix it.  Maybe like this:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>yield from</span> <span>csv</span><span>.</span><span>reader</span><span>(</span><span>file</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>No, it will not be fixed.  This version only appears to work by <em>not</em> closing the file until the generator is exhausted or collected.</p>
<p>This trivial example has deeper implications.  If one accepts this practice, then one must also accept that storing a file handle anywhere, such as on an instance, is also disallowed.  Unless of course that object then virally implements it owns context manager, ad infinitum.</p>
<p>Furthermore it demonstrates that often the context is not being managed locally.  If a file object is passed another function, then it's being used outside of the context.  Let's revisit the <code>json</code> version, which works because the file is fully read.  Doesn't a json parser have some expensive parsing to do after it's read the file?  It might even throw an error.  And isn't it desirable, trivial, <a href="https://github.com/python/cpython/blob/master/Lib/json/__init__.py#L274">and likely</a> that the implementation releases interest in the file as soon as possible?</p>
<p>So in reality there are scenarios where the supposedly good way could keep the file open <em>longer</em> than the supposedly bad way.  The original inline version does exactly what it's supposed to do: close the file when all interested parties are done with it.  Python uses garbage collection to manage shared resources.  Any attempt to pretend otherwise will result in code that is broken, inefficient, or reinventing reference counting.</p>
<p>A true believer now has to accept that <code>json.load</code> is a useless and dangerous wrapper, and that the only correct implementation is:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>contents</span> <span>=</span> <span>file</span><span>.</span><span>read</span><span>()</span>
    <span>return</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>contents</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>This line of reasoning reduces to the absurd: a file should never be passed or stored anywhere.  Next an example where the practice has caused real-world damage.</p>
<h3 id="In-practice">In practice<a href="#In-practice">¶</a>
</h3>
<p><a href="https://requests.readthedocs.io/en/master/">Requests</a> is one of the most popular python packages, and <a href="https://docs.python.org/3/library/http.client.html#module-http.client">officially recommended</a>.  It includes a <a href="http://requests.readthedocs.org/en/latest/user/advanced/#session-objects">Session</a> object which supports closing via a context manager.  The vast majority of real-world code uses the the top-level functions or single-use sessions.</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>...</span><span>)</span>

<span>with</span> <span>requests</span><span>.</span><span>Session</span><span>()</span> <span>as</span> <span>session</span><span>:</span>
    <span>response</span> <span>=</span> <span>session</span><span>.</span><span>get</span><span>(</span><span>...</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>Sessions manage the connection pool, so this pattern of usage is establishing a new connection every time.  There are popular standard API clients which seriously do this, for every single request to the same endpoint.</p>
<p>Requests' documentation prominently states that "Keep-alive and HTTP connection pooling are 100% automatic".  So part of the blame may lay with that phrasing, since it's only "automatic" if sessions are reused.  But surely a large part of the blame is the dogma of closing sockets, and therefore sessions, explicitly.
The whole point of a connection pool is that it may leave connections open, so users who genuinely need this granularity are working at the wrong abstraction layer.  <code>http.client</code> is already builtin for that level of control.</p>
<p>Tellingly, requests' own top-level functions didn't always close sessions.  There's a long history to that code, including a <a href="https://github.com/kennethreitz/requests/commit/3155bc99362a8c6ab136b6a3bb999732617cd2e5">version that only closed sessions on success</a>.  An older version was <a href="https://github.com/kennethreitz/requests/issues/1882">causing warnings</a>, when run to check for such warnings, and was being blamed for the <em>appearance</em> of <a href="https://github.com/kennethreitz/requests/issues/1685">leaking memory</a>.  Those threads are essentially debating whether a resource pool is "leaking" resources.</p>

</div>
</div>
</div>
<div>

<div>
<div>
<h3 id="Truce">Truce<a href="#Truce">¶</a>
</h3>
<p>Prior to <code>with</code> being introduced in Python 2.5, it was <em>not</em> recommended that inlined reading of a file required a <code>try... finally</code> block.  Far from it, in the past idioms like <code>open(...).read()</code> and <code>for line in open(...)</code> were lauded for being succinct and expressive.  But if all this orphaned file descriptor paranoia was well-founded, it would have been a problem back then too.</p>
<p>Finally, let's address readability.  It could be argued (though it rarely is) that showing the reader when the file is closed has inherent value.  Conveniently, that tends to align with having opened the file for writing anyway, thereby needing an reference to it.  In which case, the readability is approximately equal, and potential pitfalls are more realistic.  But readability is genuinely lost when the file would have been opened in a inline expression.</p>
<p>The best practice is unjustifiably special-casing file descriptors, and not seeing its own reasoning through to its logical conclusion.  This author proposes advocating for <em>anonymous read-only</em> <code>open</code> expressions.  Your setup script is not going to run out of file descriptors because you wrote <code>open("README.md").read()</code>.</p>

</div>
</div>
</div>
</div>
    </div></div>]]>
            </description>
            <link>https://coady.github.io/posts/closing-files/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742390</guid>
            <pubDate>Sun, 05 Jul 2020 21:50:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Transport makes up only 6% of the greenhouse gas emissions from food]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 69 (<a href="https://news.ycombinator.com/item?id=23741040">thread link</a>) | @shafyy
<br/>
July 5, 2020 | https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/ | <a href="https://web.archive.org/web/*/https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>There's a common misconception that eating locally produced foods is important from an environmental point of view. Even the <a href="https://twitter.com/UN/status/1188622911080415235">UN tweeted about it.</a> This is wrong.</p><p>Transport makes up only 6% of the greenhouse gas emissions from food:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/How-much-of-GHGs-come-from-food-1-.png" alt="" width="3889" height="3935" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/How-much-of-GHGs-come-from-food-1-.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/How-much-of-GHGs-come-from-food-1-.png 1000w, https://blog.yeticheese.com/content/images/size/w1600/2020/07/How-much-of-GHGs-come-from-food-1-.png 1600w, https://blog.yeticheese.com/content/images/size/w2400/2020/07/How-much-of-GHGs-come-from-food-1-.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://ourworldindata.org/environmental-impacts-of-food">Our World in Data</a>.</figcaption></figure><p>The reason for this is that most foods are transported by ship and not plane. Only about 0.16% of food miles are done by plane:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/share-food-miles-by-method.png" alt="" width="3400" height="2400" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/share-food-miles-by-method.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/share-food-miles-by-method.png 1000w, https://blog.yeticheese.com/content/images/size/w1600/2020/07/share-food-miles-by-method.png 1600w, https://blog.yeticheese.com/content/images/size/w2400/2020/07/share-food-miles-by-method.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://ourworldindata.org/grapher/share-food-miles-by-method">Our World in Data</a>.</figcaption></figure><p>It makes sense to try and avoid foods that are transported by air. Typically, those are foods which are highly perishable, such as asparagus, green beans and berries.</p><p>In some cases, eating local food even has a more negative impact on the environment than buying something that has been produced half way around the world. For example, heated greenhouses are energy intensive and can produce more greenhouse gases than transporting something for thousands of kilometers by water or road.</p><p>It's clear that avoiding meat and dairy has a much bigger impact on reducing greenhouse gas emissions.</p><p>So, why do people keep saying that we should eat local?</p><p>It could just be ignorance. However, I think that it's often a straw man argument pushed by interest groups that want to keep selling meat and dairy. It is something that is easy to do and seems to make sense on the surface to many people. Let's take another look at that UN tweet from before:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png" alt="" width="1194" height="634" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 1000w, https://blog.yeticheese.com/content/images/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 1194w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://twitter.com/UN/status/1188622911080415235">Tweet from @UN</a> on Oct 28, 2019.</figcaption></figure><p>In addition to eating local food, they also recommend unplugging unused appliances and using less hot water. Like avoiding plastic bags or plastic straws, this is good advice but a long shot from making a meaningful impact on climate change.</p><p>Arguments like these try to shift away the spot light from big companies who collectively make up a large chunk of the greenhouse gas emissions to individuals. People think that they did something meaningful by buying local food, which, as we have seen, is not the case.</p><p>I'm not saying that we shouldn't do those things. We absolutely should, but it shouldn't be the main talking points of organizations like the UN or WWF.</p><p>To make real change, we must eat less meat and dairy, move to more renewable energy sources and reduce air and road travel significantly.</p><p>PS: I'm only talking about the impact on climate change in this article. Eating local and organic food has other benefits such as supporting the local economy and in most cases it's a good idea to do it.</p><p>Comments or questions? Join in on the discussion on <a href="https://twitter.com/yeticheeseparty/status/1279850824378781697?s=20">this Twitter thread</a>. </p><!--kg-card-begin: html--><!-- Begin Mailchimp Signup Form -->


<div id="mc_embed_signup">
<p>
    Our plant-based Yeti Feta will be available to order soon. Leave your email below and we'll let you know when it's ready. (No newsletters or other shenanigans)
</p>

</div>

<!--End mc_embed_signup--><!--kg-card-end: html-->
			</section></div>]]>
            </description>
            <link>https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741040</guid>
            <pubDate>Sun, 05 Jul 2020 19:05:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Purpose of Persuasion]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 40 (<a href="https://news.ycombinator.com/item?id=23740669">thread link</a>) | @apsec112
<br/>
July 5, 2020 | https://www.persuasion.community/p/the-purpose-of-persuasion | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/the-purpose-of-persuasion">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg&quot;,&quot;height&quot;:972,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6649732,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p>Friends,</p><p>I'm floored by the response of the past three days.</p><p>Once I hit <em>send</em>, this article will land in the inboxes of over 15,000 people. When we launched, my main fear was that the world would not be interested in a community that pledges to defend the values of a free society; now, my main fear is that we won't be able to live up to the hype.</p><p>So, here's my promise: We will do our very best to earn your trust. Some great articles will be coming your way soon. We are getting ready to announce more events and high-level members of the community. I hope you will join us for our inaugural townhall, which will take place next Sunday, July 12th, at 4pm EST. (Watch this space for the invite.) I'm very, very excited about what lies ahead. But I also know that it is hard to build this kind of community from the ground up, and that we will undoubtedly make mistakes along the way. Please bear with us when we do.</p><p>In the meanwhile, I want to take you a little deeper into the thinking that went into creating <em>Persuasion</em>. Why this project? Why now? And how can just a bunch of us—even if we are a much larger bunch than I could possibly have dreamed a few days ago—really make a difference to the future of free societies in the United States and around the world? The key to an answer lies in a short (and necessarily schematic) history of American intellectual life over the past half century. </p><p>Fifty years ago, the most important American institutions enjoyed a degree of legitimacy that is now hard to fathom. Nearly every American watched the news on one of the three network television stations. Nearly every American had a positive opinion of Princeton and Stanford. Nearly every Member of Congress believed that the advice of the Brookings Institution or the Council on Foreign Relations was to be taken seriously.&nbsp;</p><p>These institutions had much to recommend them: They gave the public a shared set of facts and assumptions, which could form the basis of political debate. And, though they never thought of their primary goal as fighting for the ideals of a free society, their operating system was philosophically liberal: From CBS to Harvard to Brookings, senior decision makers instinctively believed in values like free speech and due process.</p><p>However, these institutions also suffered from two important shortcomings. First, the people they admitted into their gilded halls only represented a small slice of America's population: sexism, racism and homophobia were <em>far</em> more prevalent in these institutions than they are today. The views they considered serious sometimes included the morally abhorrent.&nbsp;</p><p>Second, the realm of the “reasonable" was rather narrow. And, though this narrowness of debate constituted the lesser injustice, it was—at least in the short term—the cause of greater instability: Having come to believe that they could never quite speak in their own voices in the halls of the Brookings Institution or the column inches of the <em>New York Times</em>, a few assorted bands of malcontents started to cast around for an alternative. </p><p>Of these, the group that had the biggest impact on public life in America was a band of devoted conservatives, determined to create an ideological counter-establishment that could rival the mainstream.&nbsp;What started with <em>National Review</em>, an ideological fighting magazine, quickly grew into a sprawling and immensely powerful network of conservative institutions. The Heritage Foundation was set up to rival the influence of Brookings. The Federalist Society sought to change the ideological composition of America's judiciary. Fox News did its dismal best to spread the ideas of the conservative movement beyond the Beltway. A whole network of activist groups provided conservatives with an ideological foundation, a group of friends, and a professional home. Measured by its own ambitions,&nbsp;the movement was a staggering success.</p><p>Other minoritarian ideological movements took a page out of the same playbook. In 1960, a libertarian was a person with idiosyncratic views and no obvious political home. Then, the Institute for Humane Studies started to advocate libertarian ideas on college campuses, <em>Reason </em>took up their public defense, and a reinvigorated American Enterprise Institute set out to influence legislators on Capitol Hill. By 1980, the influence and intellectual self-confidence of libertarians had increased enormously.</p><p>The further left has always had its share of counter-establishment institutions. <em>The Nation</em>, after all, is one of the oldest magazines in the country, and some academic disciplines have long been at the forefront of leftist thought. But the left, too, has of late succeeded in building a more cohesive network of fighting institutions, as universities have become much more progressive, movements like the Democratic Socialists of America have awakened from decades of peaceful slumber, and publications like <em>Jacobin </em>have infused the movement with fresh intellectual energy.</p><p>Five or ten years ago, our potted history might have concluded here. Ideological movements from conservative to libertarian to leftist had fighting institutions of their own. Though philosophical liberals did not have a comparable home, they could confidently express their views within mainstream institutions.&nbsp;</p><p>But then those institutions started to change.</p><p>The story of that change has attracted an immense amount of attention over the past months. I won't bore you with a detailed recap of its most worrying manifestations, from the firing of James Bennet to the uncritical celebration of Robin DiAngelo. Nor do I want to suggest that these changes have completely delegitimized the mainstream: These institutions have not yet become wholly illiberal, and the advocates of a free society would be foolish to stop fighting for them.</p><p>But the erosion of values like free speech and due process within mainstream institutions does put philosophical liberals at a unique disadvantage. It is difficult to convey just how many amazing writers, journalists, and think-tankers—some young and some old, some relatively obscure and others very famous—have privately told me that they can no longer write in their own voices; that they are counting the days until they get fired; and that they don't know where to turn if they do. (Astonishingly, a number of them are far enough to the left to have supported Bernie Sanders in the primaries.)</p><p>This, to me, is a huge part of the reason why the defenders of the free society have seemed to lack conviction in recent months and years. Feeling, at best, begrudgingly tolerated by the institutions that employ them, they are always on the back foot: writing and speaking with one eye on Twitter, one eye on a hostile editor, and one eye on the attacks being shared on their own company’s Slack channel. (As you may have noticed, that requires too many eyes.)</p><p>But, if this situation helps to explain the collective lack of confidence among the advocates of a free society, it also points the way to an obvious solution. <strong>Instead of lamenting our loss of control over the establishment, we should follow the lead of other movements that have successfully built their own counter-establishment institutions.</strong><em>&nbsp;</em></p><p><em>That </em>is the goal I had in mind in starting <em>Persuasion</em>.</p><p>One core element of this project is a publishing platform explicitly devoted to debating, articulating, and defending the values of a free society. Emulating what <em>Reason</em>, <em>Jacobin,</em> and the <em>National Review</em> have accomplished within their own ideological traditions, I hope to create a space in which philosophical liberals can ask hard questions and come up with compelling answers. This requires both a commitment to a set of shared aspirations and enough diversity of opinion to force us to think very hard about how we can make the world a better place. This is a space for people who are open to changing their minds, but not their fundamental values.</p><p>But creating a modern reinvention of a fighting magazine, devoted to defending the ideals of a free society, is not my only ambition. If places like the <em>National Review</em> had a tremendous influence on our society, it is also because they became the nucleus of a cohesive community, which seeded a much wider archipelago of allied institutions. This is why I take the community element of <em>Persuasion</em>—all the live events, book clubs and social gatherings we'll experiment with over the coming months—so seriously. And it is also why I hope that this particular venture will spawn many formally independent organizations that share our founding values.</p><p>Before I close, let me say two quick words about some of the establishment institutions whose recent fate I have been lamenting. The first is that we must do what we can to preserve those universities, publications, and think tanks that still operate with fundamentally (small l) liberal assumptions. For example, I deeply love <em>The Atlantic</em>, and will continue to write for it. A small fighting institution that primarily addresses a devoted crowd of philosophical liberals neither is nor should be in competition with a large general interest magazine whose readership will always span a much broader ideological range. Part of the reason why we should articulate these values as clearly, forcefully, and persuasively as possible within these pages is to maximize the likelihood that they will continue to form the implicit operating system of vitally important publications like <em>The Atlantic</em>.&nbsp;</p><p>The second thing is that our ambition needs to extend beyond nostalgia. There is much to lament about the changes that have taken place in some of the country's most important institutions over the past years. But there is also much to criticize in what these institutions looked like at their supposed best. Our goal is not to return to a golden age that has, sadly, never existed; it is to build societies that live up to the noble and ambitious values of freedom and justice better than any society of the past.</p><p>The examples I have used here&nbsp;are very …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.persuasion.community/p/the-purpose-of-persuasion">https://www.persuasion.community/p/the-purpose-of-persuasion</a></em></p>]]>
            </description>
            <link>https://www.persuasion.community/p/the-purpose-of-persuasion</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740669</guid>
            <pubDate>Sun, 05 Jul 2020 18:16:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Deep Introduction to JIT Compilers: JITs are not very Just-in-time]]>
            </title>
            <description>
<![CDATA[
Score 278 | Comments 89 (<a href="https://news.ycombinator.com/item?id=23740655">thread link</a>) | @chrisseaton
<br/>
July 5, 2020 | https://carolchen.me/blog/jits-intro/ | <a href="https://web.archive.org/web/*/https://carolchen.me/blog/jits-intro/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <header>
    
    
  </header>

  <main id="main">
    
<article>
  <header>
    
    
  </header>
  <section id="js-article">
    
<p><em>If you are familiar with how JITs generally work (if you get what the title is referring to), I recommend skimming this or going straight to reading <a href="https://carolchen.me/blog/jits-impls">How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More</a></em> </p>
<p>My mentor, <a href="https://chrisseaton.com/">Chris</a>, who took me from “what is a JIT” to where I am now once told me that compilers were just bytes in bytes out and not at all low-level and scary. This is actually fairly true, and it's fun to learn about compiler internals and often useful for programmers everywhere!</p>
<p>This blog post gives background on how programming languages are implemented and how JITs work. It'll introduce the implementation details of the Julia language, though it won't talk about specific implementation details or optimizations made by more traditional JITs. Check out <a href="https://carolchen.me/blog/jits-impls">How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More</a> to read about how meta-tracing is implemented, how Graal supports C extensions, the relationship of JITs with LLVM and more!</p>
<h2 id="how-programming-languages-are-implemented">How Programming Languages are Implemented<a href="#how-programming-languages-are-implemented" aria-label="Anchor link for: how-programming-languages-are-implemented"> <i></i></a>
</h2>
<p>When we run a program, it’s either interpreted or compiled in some way. The compiler/interpreter is sometimes referred to as the "implementation" of a language, and one language can have many implementations. You may have heard things like "Python is interpreted", but that really means the reference(standard/default) implementation of Python is an interpreter. Python is a language specification and <em>CPython</em> is the interpreter and implementation of Python. </p>
<p>An interpreter is a program that directly executes your code. Well-known interpreters are usually written in C. Ruby, Python and PHP are written in C. Below is a function that loosely models how an interpreter might work:</p>
<pre><code><span>func </span><span>interpret</span><span>(</span><span>code </span><span>string</span><span>) {
  </span><span>if </span><span>code </span><span>== </span><span>"print('Hello, World!')" </span><span>{
    </span><span>print</span><span>(</span><span>"Hello, World"</span><span>);
  } </span><span>else if </span><span>code </span><span>==</span><span> “</span><span>x </span><span>= </span><span>0</span><span>; </span><span>x </span><span>+= </span><span>4</span><span>; </span><span>print</span><span>(</span><span>x</span><span>)” {
    variable_x </span><span>:= </span><span>0 
    </span><span>variable_x </span><span>+= </span><span>4
    </span><span>print</span><span>(</span><span>x</span><span>)
  }
}
</span></code></pre>
<p>A compiler is a program that translates code from some language to another language, though it usually refers to a destination language that is a machine code. Examples of compiled languages are C, Go and Rust.</p>
<pre><code><span>func </span><span>compile</span><span>(</span><span>code </span><span>string</span><span>) {
  []</span><span>byte </span><span>compiled_code </span><span>= </span><span>get_machine_code</span><span>(</span><span>code</span><span>);
  </span><span>write_to_executable</span><span>(</span><span>compiled_code</span><span>);
}
</span></code></pre>
<p>The difference between a compiled and interpreted language is actually much more nuanced. C, Go and Rust are clearly compiled, as they output a machine code file - which can be understood natively by the computer. The compile and run steps are fully distinct.</p>
<p>However, compilers can translate to any target language (this is sometimes called transpiling). Java for example, has a two-step implementation. The first is compiling Java source to bytecode, which is an Intermediate Representation (IR). The bytecode is then JIT compiled - which involves interpretation.</p>
<p>Python and Ruby also execute in two steps. Despite being known as interpreted languages, their reference implementations actually compile the source down to a bytecode. You may have seen .pyc files (not anymore in Python3) which contain Python bytecode! The bytecode is then interpreted by a virtual machine. These interpreters use bytecode because programmers tend to care less about compile time, and creating a bytecode language allows the engineers to specify a bytecode that is as efficient to interpret as possible. </p>
<p>Having bytecode is how languages check syntax before execution (though they could technically just do a pass before starting the interpreter). An example below shows why you would want to check syntax before runtime.</p>
<pre><code><span>sleep</span><span>(</span><span>1000</span><span>)
bad syntax beep boop beep boop
</span></code></pre>
<p>Another important note is that interpreted languages are typically slower for various reasons, the most obvious being that they're executed in a higher level language that has overhead execution time. The main reason is that the dynamic-ness of the languages they tend to implement means that they need many extra instructions to decide what to do next and how to route data. People still choose to build interpreters over compilers because they're easier to build and are more suited to handle things like dynamic typing, scopes etc (though you could build a compiler that has the same features). </p>
<h3 id="so-what-is-a-jit">So What is a JIT?<a href="#so-what-is-a-jit" aria-label="Anchor link for: so-what-is-a-jit"> <i></i></a>
</h3>
<p>A JIT compiler doesn't compile code Ahead-Of-Time (AOT), but still compiles source code to machine code and therefore is not an interpreter. JITs compile code at runtime, while your program is executing. This gives the JITs flexibility for dynamic language features, while maintaining speed from optimized machine code output. JIT-compiling C would make it slower as we'd just be adding the compilation time to the execution time. JIT-compiling Python would be fast, as compilation + executing machine code can often be faster than interpreting, especially since the JIT has no need to write to a file (disk writing is expensive, memory/RAM/register writing is fast). JITs also improve in speed by being able to optimize on information that is only available at runtime.</p>
<h3 id="julia-a-jit-compiler-that-s-just-in-time">Julia: a JIT Compiler that's Just-in-time<a href="#julia-a-jit-compiler-that-s-just-in-time" aria-label="Anchor link for: julia-a-jit-compiler-that-s-just-in-time"> <i></i></a>
</h3>
<p>A common theme between compiled languages is that they're statically typed. That means when the programmer creates or uses a value, they’re telling the computer what type it is and that information is guaranteed at compile time.</p>
<p>Julia is dynamically typed, but internally Julia is much closer to being statically typed.</p>
<pre><code><span>function </span><span>multiply</span><span>(x, y)
  x </span><span>*</span><span> y
</span><span>end
</span></code></pre>
<p>Here is an example of a Julia function, which could be used to multiply integers, floats, vectors, strings etc (Julia allows operator overloading). Compiling out the machine code for <em>all</em> these cases is not very productive for a variety of reasons, which is what we'd have to do if we wanted Julia to be a compiled language. Idiomatic programming means that the function will probably only be used by a few combinations of types and we don't want to compile something that we don't use yet since that's not very jitty (this is not a real term).</p>
<p>If I were to code <code>multiply(1, 2)</code>, then Julia will compile a function that multiplies integers. If I then wrote <code>multiply(2, 3)</code>, then the already-compiled code will be used. If I then added <code>multiply(1.4, 4)</code>, another version of the function will be compiled. We can observe what the compilation does with <code>@code_llvm multiply(1, 1)</code>, which generates LLVM Bitcode (not quite machine code, but a lower-level Intermediate Representation).</p>
<pre><code><span>define i64 @julia_multiply_17232(i64, i64) {
top</span><span>:</span><span>
; ┌ @ int</span><span>.</span><span>jl</span><span>:</span><span>54</span><span> within `*'
   </span><span>%</span><span>2 </span><span>=</span><span> mul i64 </span><span>%</span><span>1</span><span>, </span><span>%</span><span>0</span><span>
; └
  ret i64 </span><span>%</span><span>2</span><span>
}
</span></code></pre>
<p>And with <code>multiply(1.4, 4)</code>, you can see how complicated it can get to compile even one more function. In AOT compiled Julia, all (some optimizations can be made to reduce) of these combinations would have to live in the compiled code even if only one was used, along with the control flow to delegate. </p>
<pre><code><span>define double @julia_multiply_17042(double, i64) {
top</span><span>:</span><span>
; ┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>312</span><span> within `*'
; │┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>282</span><span> within `promote'
; ││┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>259</span><span> within `_promote'
; │││┌ @ number</span><span>.</span><span>jl</span><span>:</span><span>7</span><span> within `convert'
; ││││┌ @ float</span><span>.</span><span>jl</span><span>:</span><span>60</span><span> within `</span><span>Float64</span><span>'
       </span><span>%</span><span>2 </span><span>=</span><span> sitofp i64 </span><span>%</span><span>1</span><span> to double
; │└└└└
; │ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>312</span><span> within `*' @ float</span><span>.</span><span>jl</span><span>:</span><span>405
   </span><span>%</span><span>3 </span><span>=</span><span> fmul double </span><span>%</span><span>2</span><span>, </span><span>%</span><span>0</span><span>
; └
  ret double </span><span>%</span><span>3</span><span>
}
</span></code></pre>
<p>The general strategy of “assume a type and compile/behave based on that” is called type inferencing, which Julia mildly uses in the examples above. There are a lot of other compiler optimizations that are made, though none of them are very specific to JITs as Julia may be better described as a lazy AOT compiler.</p>
<p>The simplicity of this kind of jitting makes it easy for Julia to also supply AOT compilation. It also helps Julia to benchmark very well, definitely a tier above languages like Python and comparable to C (I'd cite numbers, but those are always nuanced and I don't want to get into that).</p>
<h3 id="so-what-is-a-jit-take-two">So What is a JIT? Take Two.<a href="#so-what-is-a-jit-take-two" aria-label="Anchor link for: so-what-is-a-jit-take-two"> <i></i></a>
</h3>
<p>Julia is actually the jittiest JIT I'll discuss, but not the most interesting as a "JIT". It actually compiles code right before the code needs to be used -- just in time. Most JITs however (Pypy, Java, JS Engines), are not actually about compiling code just-in-time, but compiling <em>optimal code</em> at an optimal time. In some cases that time is actually never. In other cases, compilation occurs more than once. In a vast majority of the cases compilation doesn't occur until after the source code has been executed numerous times, and the JIT will stay in an interpreter as the overhead to compilation is too high to be valuable.</p>
<p><img src="https://carolchen.me/blog/img/jits/jitbrr.jpg" alt=""></p>
<p>The other aspect at play is generating <em>optimal code</em>. Assembly instructions are not created equal, and compilers will put a lot of effort into generating well-optimized machine code. Usually, it is possible for a human to write better assembly than a compiler (though it would take a fairly smart and knowledgeable human), because the compiler cannot dynamically analyze your code. By that, I mean things like knowing the possible range of your integers or what keys are in your map, as these are things that a computer could only know after (partially) executing your program. A JIT compiler can actually do those things because it interprets your code first and gathers data from the execution. Thus, JITs are expensive in that they interpret, and add compilation time to execution time, but they make it up in highly optimised compiled code. With that, the timing of compilation is also dependent on whether the JIT has gathered enough valuable information.</p>
<p>The cool part about JITs is that I was sort of lying when I said a JIT implementation of C could not be faster than existing compiled implementations. It would not be feasible to try, but jit-compiling C in the way I just described is not a strict superset of compiling a language and thus it is not logically impossible to compile code fast enough to make up for the compile+profile+interpreting time. If I "JIT compiled" C similarly to how Julia does it (statically compile each function as it's called), it would be impossible to …</p></section></article></main></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://carolchen.me/blog/jits-intro/">https://carolchen.me/blog/jits-intro/</a></em></p>]]>
            </description>
            <link>https://carolchen.me/blog/jits-intro/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740655</guid>
            <pubDate>Sun, 05 Jul 2020 18:14:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Choosing a Rust web framework]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 30 (<a href="https://news.ycombinator.com/item?id=23740028">thread link</a>) | @LukeMathWalker
<br/>
July 5, 2020 | https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/ | <a href="https://web.archive.org/web/*/https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    

<blockquote>
<p><em>This post was originally meant as a section of <a href="https://www.lpalmieri.com/posts/2020-05-24-zero-to-production-0-foreword/"><strong>Zero To Production</strong></a> to explain the reasoning behind our technology choice. It eventually grew so large to be its own article!</em></p>

<p><em>You can discuss the article on <a href="https://news.ycombinator.com/item?id=23740028">HackerNews</a> or <a href="https://www.reddit.com/r/rust/comments/hlpsw5/choosing_a_rust_web_framework_2020_edition/">r/rust</a></em>.</p>
</blockquote>

<p>As of July 2020, the main web frameworks in the Rust ecosystem are:</p>

<ul>
<li><a href="https://actix.rs/"><code>actix-web</code></a>;<br></li>
<li><a href="https://rocket.rs/"><code>rocket</code></a>;<br></li>
<li><a href="https://github.com/http-rs/tide"><code>tide</code></a>;<br></li>
<li><a href="https://github.com/seanmonstar/warp"><code>warp</code></a>.</li>
</ul>

<p>Which one should you pick if you are about to start building a new <strong>production-ready</strong> API in Rust?</p>

<p>I will break down where each of those web frameworks stands when it comes to:</p>

<ul>
<li><a href="#1-comprehensiveness">Comprehensiveness</a>;<br></li>
<li><a href="#2-community-and-adoption">Community and adoption</a>;<br></li>
<li><a href="#3-sync-vs-async">Sync vs Async</a>, as well as their choice of <a href="#3-1-futures-runtime">futures runtime</a>;<br></li>
<li><a href="#4-documentation-tutorials-and-examples">Documentation, tutorials and examples</a>;<br></li>
<li><a href="#5-api-and-ergonomics">API and ergonomics</a>.</li>
</ul>

<p>I will in the end make <a href="#6-our-choice">my recommendation</a>.<br>
Worth remarking that there are no absolutes: different circumstances (and taste) might lead you to a different pick.</p>

<h2 id="1-comprehensiveness">1. Comprehensiveness</h2>

<p><code>actix-web</code>, <code>tide</code> and <code>warp</code> are <em>slim</em> web frameworks: they offer you an HTTP web server, routing logic, middleware infrastructure and basic building blocks and abstractions to parse, manipulate and respond to HTTP requests.</p>

<p><code>rocket</code> takes a different approach - it aims to be batteries-included: the most common needs should be covered by functionality provided out-of-the-box by <code>rocket</code> itself, with hooks for you to extend <code>rocket</code> if your usecase needs it.<br>
It should not come as a surprise then that <code>rocket</code> ships an easy-to-use <a href="https://rocket.rs/v0.4/guide/state/#databases">integration to manage connection pools</a> for several popular database (e.g. Postgres, Redis, Memcache, etc.) as well as its own <a href="https://rocket.rs/v0.4/guide/configuration/">configuration system</a> in <a href="https://api.rocket.rs/v0.4/rocket_contrib/"><code>rocket-contrib</code></a>, an ancillary crate hosted in <code>rocket</code>’s own repository.</p>

<p>We can compare them to frameworks available in other ecosystems:</p>

<ul>
<li><code>actix-web</code>, <code>tide</code> and <code>warp</code> are closer in spirit to <a href="https://palletsprojects.com/p/flask/"><code>Flask</code></a> from Python or <a href="https://expressjs.com/"><code>Express</code></a> from Javascript - they might be opinionated, but they do not ship a configuration management system or an ORM integration out of the box. You are in charge of structuring your API as you deem appropriate, bringing all the necessary crates and patterns into the picture;<br></li>
<li><code>rocket</code> is closer to <a href="https://www.djangoproject.com/"><code>Django</code></a> from Python or <a href="https://symfony.com/"><code>Symphony</code></a> from PHP: a stable and solid core with a set of high-quality in-tree components to fulfill your every day needs when building a solid web application. <code>rocket</code> has still a long way to go to match its peers in breadth and scope, but it is definitely off to a good start.</li>
</ul>

<p>Of course this is a snapshot of the landscape as of today, but the situation is continuously shifting according to the maintainers’ intentions - e.g. <code>actix-web</code> has slowly been accumulating more and more supporting functionality (from security to session management) in <a href="https://github.com/actix/actix-extras"><code>actix-extras</code></a>, under the umbrella of the <code>actix</code> GitHub organization.<br>
Furthermore, using a slim web framework does not force you to write everything from scratch as soon as the framework is falling short of your needs: you can leverage the ecosystem built by the community around it to avoid re-inventing the wheel on every single project.</p>

<h2 id="2-community-and-adoption">2. Community and adoption</h2>

<p>Numbers can be misleading, but they are a good conversation starting point. Looking at <a href="https://crates.io/">crates.io</a>, we have:</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th>Total Downloads</th>
<th>Daily Downloads</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>actix-web</code></td>
<td>~1250k</td>
<td>~3000</td>
</tr>

<tr>
<td><code>rocket</code></td>
<td>~525k</td>
<td>~1000</td>
</tr>

<tr>
<td><code>warp</code></td>
<td>~435k</td>
<td>~3000</td>
</tr>

<tr>
<td><code>tide</code></td>
<td>~47k</td>
<td>~300</td>
</tr>
</tbody>
</table>

<p>The number of total downloads is obviously influenced by how long a framework has been around (e.g. <code>actix-web:0.1.0</code> came out at the end of 2017!) while daily downloads are a good gauge for the current level of interest around it.</p>

<p>You should care about adoption and community size for a couple of reasons:</p>

<ul>
<li>consistent production usage over years makes it way less likely that you are going to be the first one to spot a major defect. Others cried so that you could smile (most of the time);<br></li>
<li>it correlates with the number of supporting crates for that framework;<br></li>
<li>it correlates with the amount of tutorials, articles and helping hands you are likely to find if you are struggling.</li>
</ul>

<p>The second point is particularly important for slim frameworks.<br>
You can get a feel of the impact of community size, once again, by looking at the number of results popping up on <a href="https://crates.io/">crates.io</a> when searching a framework name:</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th># results</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>rocket</code></td>
<td>178</td>
</tr>

<tr>
<td><code>actix-web</code></td>
<td>113</td>
</tr>

<tr>
<td><code>warp</code></td>
<td>57</td>
</tr>

<tr>
<td><code>tide</code></td>
<td>20</td>
</tr>
</tbody>
</table>

<p>Will all those crates be relevant? Unlikely.<br>
Will a fair share of them be outdated or unproven? Definitely.</p>

<p>Nonetheless it is a good idea, before starting a project, to have a quick look for functionality you know for a fact you will need. Let’s make a couple of quick examples with features we will be relying on in the email newsletter implementation we are building in <em>Zero To Production</em>:</p>

<ul>
<li>if you need to add Prometheus’ metrics to your API you can get off the ground in a couple of minutes with <a href="https://crates.io/crates/actix-web-prom"><code>actix-web-prom</code></a> or <a href="https://crates.io/crates/rocket_prometheus"><code>rocket-prometheus</code></a>, both with thousands of downloads. If you are using <code>warp</code> or <code>tide</code> you will have to write the integration from scratch;<br></li>
<li>if you want to add distributed tracing, <a href="https://crates.io/crates/actix-web-opentelemetry"><code>actix-web-opentelemetry</code></a> has your back. You will have to re-implement it if you choose any other framework.</li>
</ul>

<p>Most of these features are not too much work to implement, but the effort (especially maintenance) compounds over time. You need to choose your framework with your eyes wide open on the level of commitment it is going to require.</p>

<h2 id="3-sync-vs-async">3. Sync vs Async</h2>

<p>Rust landed its <code>async</code>/<code>await</code> syntax in version <code>1.39</code> - a game changer in terms of ergonomics for asynchronous programming.<br>
It took some time for the whole Rust ecosystem to catch up and adopt it, but it’s fair to say that crates dealing with IO-bound workloads are now generally expected to be async-first (e.g. <code>reqwest</code>).</p>

<p>What about web frameworks?<br>
<code>actix-web</code> adopted <code>async</code>/<code>await</code> with its <code>0.2.x</code> release, same as <code>warp</code>, while <code>tide</code> was using <code>async</code>/<code>await</code> before its stabilisation relying on the <code>nightly</code> Rust compiler.<br>
<code>rocket</code>, instead, still exposes a synchronous interface. <code>async</code>/<code>await</code> support is expected as part of its next <code>0.5</code> release, <a href="https://github.com/SergioBenitez/Rocket/issues/1065">in the making since last summer</a>.</p>

<p>Should you rule out <code>rocket</code> as a viable option because it does not yet support asynchronous programming?<br>
It depends.<br>
If you are implementing an application to handle high volumes of traffic with strict performance requirements it might be better to opt for an async web framework.<br>
If that is not the case, the lack of async support in <code>rocket</code> should not be one of your primary concerns.</p>

<h3 id="3-1-futures-runtime">3.1. Futures runtime</h3>

<p><code>async</code>/<code>await</code> is not all sunshine and roses.<br>
Asynchronous programming in Rust is built on top of the <code>Future</code> trait: a future exposes a <code>poll</code> method which has to be called to allow the future to make progress. You can think of Rust’s futures as <em>lazy</em>: unless polled, there is no guarantee that they will execute to completion.<br>
This is often been described as a <em>pull</em> model compared to the <em>push</em> model adopted by other languages<sup id="fnref:async-announcement"><a href="#fn:async-announcement">1</a></sup>, which has some interesting implications when it comes to performance and task cancellation.</p>

<p>Wait a moment though - if futures are lazy and Rust does not ship a runtime in its standard library, who is in charge to call the <code>poll</code> method?<br>
<strong>BYOR</strong> - <strong>B</strong>ring <strong>Y</strong>our <strong>O</strong>wn <strong>R</strong>untime!<br>
The async runtime is literally a dependency of your project, brought in as a crate.<br>
This provides you with a great deal of flexibility: you could indeed implement your own runtime optimised to cater for the specific requirements of your usecase (see <a href="http://smallcultfollowing.com/babysteps/blog/2019/12/09/async-interview-2-cramertj/#async-interview-2-cramertj">the Fuchsia project</a> or <a href="https://github.com/bastion-rs/bastion"><code>bastion</code></a>’s actor framework) or simply choose the most suitable on a case-by-case basis according to the needs of your application.<br>
That sounds amazing on paper, but reality is a bit less glamorous: interoperability between runtimes is quite poor at the moment; mixing runtimes can be painful, often causing issues that are not straight-forward either to triage, detect or solve.<br>
While most libraries should not depend on runtimes directly, relying instead on the interfaces exposed by the <a href="https://docs.rs/futures/0.3.5/futures/"><code>futures</code></a> crate, this is often not the case due to historical baggage (e.g. <code>tokio</code> was for a long time the only available runtime in the ecosystem), practical needs (e.g. a framework has to be able to spawn tasks) or lack of standardisation (e.g. the ongoing discussion on the <code>AsyncRead</code>/<code>AsyncWrite</code> traits - see <a href="http://smallcultfollowing.com/babysteps/blog/2020/01/20/async-interview-5-steven-fackler/">here</a> and <a href="http://smallcultfollowing.com/babysteps/blog/2020/03/10/async-interview-7-withoutboats/#async-interview-7-withoutboats">here</a>).<br>
Therefore picking an async web framework goes beyond the framework itself: you are choosing an ecosystem of crates, suddenly making it much more cumbersome to consume libraries relying on a different async runtime.</p>

<p>The current state of affairs is far from ideal, but if you are writing async Rust today I’d recommend you to make a <em>deliberate</em> choice when it comes to your async runtime.</p>

<p>The two main general-purpose async runtimes currently available in Rust are <a href="https://tokio.rs/"><code>tokio</code></a> and <a href="https://github.com/async-rs/async-std"><code>async-std</code></a>.<br>
<code>tokio</code> has been around for quite some time and it has seen extensive production usage. It is fairly tunable, although this results in a larger and more complex API surface.<br>
<code>async-std</code> was released almost a year ago, around the time of <code>async</code>/<code>await</code> stabilization. It provides great ergonomics, while leaving less room for configuration knobs.</p>

<p><a href="https://crates.io/">crates.io</a> can once again be used as a gauge for adoption and readiness:</p>

<table>
<thead>
<tr>
<th>Runtime</th>
<th>Total Downloads</th>
<th>Daily Downloads</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>tokio</code></td>
<td>~9600k</td>
<td>~30k</td>
</tr>

<tr>
<td><code>async-std</code></td>
<td>~600k</td>
<td>~4k</td>
</tr>
</tbody>
</table>

<p>How do frameworks map to runtimes?</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th>Runtime</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>actix-web</code></td>
<td><code>tokio</code></td>
</tr>

<tr>
<td><code>rocket</code> (<code>0.5.x</code>)</td>
<td><code>tokio</code></td>
</tr>

<tr>
<td><code>tide</code></td>
<td><code>async-std</code></td>
</tr>

<tr>
<td><code>warp</code></td>
<td><code>tokio</code></td>
</tr>
</tbody>
</table>

<h2 id="4-documentation-tutorials-and-examples">4. Documentation, tutorials and examples</h2>

<p>Having to dive into the source code to understand how something works can be fun (and educational!), but it should be a choice, not a necessity.<br>
In most situations I’d rather rely on the framework being well-documented, including non-trivial examples of relevant usage patterns.<br>
Good documentation, tutorials and fully-featured examples are <strong>mission-critical</strong> if you are working as part of a team, especially if one or more teammates are not experienced Rust developers.</p>

<p>Rust’s tooling treats documentation as a first class concept (just run <code>cargo doc --open</code> to get auto-generated docs for your project!) and it grew to be part of the culture of the Rust community itself. Library authors generally take it seriously and web frameworks are no exception to the general tendency: what you can find on <a href="https://docs.rs/">docs.rs</a> is quite thorough, with contextual examples …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/">https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/</a></em></p>]]>
            </description>
            <link>https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740028</guid>
            <pubDate>Sun, 05 Jul 2020 16:49:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scaling Pandas: Comparing Dask, Ray, Modin, Vaex, and Rapids]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 30 (<a href="https://news.ycombinator.com/item?id=23740012">thread link</a>) | @FHMS
<br/>
July 5, 2020 | https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray | <a href="https://web.archive.org/web/*/https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>Python and its most popular data wrangling library, Pandas, are soaring in popularity. Compared to competitors like Java, Python and Pandas make data exploration and transformation <strong>simple</strong>.</p><p>But both Python and Pandas are known to have issues around <strong>scalability</strong> and <strong>efficiency</strong>.</p><p>Python loses some efficiency right off the bat because it’s an interpreted, dynamically typed language. But more importantly, Python has always focused on simplicity and readability over raw power. Similarly, Pandas focuses on offering a simple, high-level API, largely ignoring performance. In fact, the creator of Pandas wrote “<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">The 10 things I hate about pandas</a>,” which summarizes these issues:</p><figure id="w-node-412b9aecdea3-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62e9007b2509635bd1ba2_image3.png" alt="Ten things Wes McKinney hates about Pandas."></p><figcaption>Performance issues and lack of flexibility are the main things Pandas’ own creator doesn’t like about the library. (<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">source</a>)</figcaption></figure><p>So it’s no surprise that many developers are trying to add more power to Python and Pandas in various ways. Some of the most notable projects are:</p><ul role="list"><li><a href="https://www.datarevenue.com/ml-tools/dask"><strong>Dask</strong></a><strong>:</strong> a low-level scheduler and a high-level partial Pandas replacement, geared toward running code on compute clusters.</li><li><strong>Ray:</strong> a low-level framework for parallelizing Python code across processors or clusters.</li><li><a href="https://www.datarevenue.com/ml-tools/modin"><strong>Modin</strong></a><strong>:</strong> a drop-in replacement for Pandas, powered by either <strong>Dask</strong> or <strong>Ray</strong>.</li><li><a href="https://www.datarevenue.com/ml-tools/vaex"><strong>Vaex</strong></a><strong>:</strong> a partial Pandas replacement that uses lazy evaluation and memory mapping to allow developers to work with large datasets on standard machines.</li><li><a href="https://www.datarevenue.com/ml-tools/rapids"><strong>RAPIDS</strong></a><strong>: </strong>a collection of data-science libraries that run on GPUs and include <a href="https://github.com/rapidsai/cudf">cuDF</a>, a partial replacement for Pandas.</li></ul><p>There are others, too. Below is an overview of the Python data wrangling landscape:</p><figure id="w-node-78c43e6cecae-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62eb85c7038610cea20d0_image2.png" alt="A graph showing how often popular data wrangling libraries are compared in Google searches."></p><figcaption>Dask, Modin, Vaex, Ray, and CuDF are often considered potential alternatives to each other. Source: Created with <a href="https://anvaka.github.io/vs/?query=Dask">this tool</a></figcaption></figure><p>So if you’re working with a lot of data and need faster results, which should you use?</p><h2><strong>Just tell me which one to try</strong></h2><p>Before you can make a decision about which tool to use, it’s good to have some more context about each of their approaches. We’ll compare each of them closely, but you’ll probably want to try them out in the following order:</p><ul role="list"><li><strong>Modin</strong>, with <strong>Ray</strong> as a backend. By installing these, you might see significant benefit by changing just a single line (`import pandas as pd` to `import modin.pandas as pd`). Unlike the other tools, Modin aims to reach full compatibility with Pandas.</li><li><strong>Dask</strong>,<strong> </strong>a larger and hence more complicated project. But Dask also provides <a href="https://docs.dask.org/en/latest/dataframe.html">Dask.dataframe</a>, a higher-level, Pandas-like library that can help you deal with <a href="https://en.wikipedia.org/wiki/External_memory_algorithm">out-of-core</a> datasets.</li><li><strong>Vaex, </strong>which is designed to help you work with large data on a standard laptop. Its Pandas replacement covers some of the Pandas API, but it’s more focused on exploration and visualization.</li><li><strong>RAPIDS, </strong>if you have access to NVIDIA graphics cards<strong>.</strong></li></ul><h2><strong>Quick comparison</strong></h2><p>Each of the libraries we examine has different strengths, weaknesses, and scaling strategies. The following table gives a broad overview of these. Of course, as with many things, most of the scores below are heavily dependent on your exact situation.&nbsp;</p><figure id="w-node-3fc1cb6579be-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62ef85090a97b0c469fa9_image5.png" alt="A table comparing the tools across maturity, popularity, ease of adoption, and other metrics."></p><figcaption>Dask and Ray are more mature, but Modin and Vaex are easier to get started with. Rapids is useful if you have access to GPUs.</figcaption></figure><p>These are subjective grades, and they may vary widely given your specific circumstances. When assigning these grades, we considered:</p><ul role="list"><li><strong>Maturity: </strong>The time since the first commit and the number of commits.</li><li><strong>Popularity: </strong>The number of GitHub stars.</li><li><strong>Ease of Adoption: </strong>The amount of knowledge expected from users, presumed hardware resources, and ease of installation.</li><li><strong>Scaling ability: </strong>The broad dataset size limits for each tool, depending on whether it relies mainly on RAM, hard drive space on a single machine, or can scale up to clusters of machines.&nbsp;</li><li><strong>Use case: </strong>Whether the libraries are designed to speed up Python software in general (“<strong>General</strong>”), are focused on data science and machine learning (“<strong>Data science</strong>”), or are limited to simply replacing Pandas’ ‘DataFrame’ functionality (“<strong>DataFrame</strong>”).</li></ul><h2><strong>CPUs, GPUs, Clusters, or Algorithms?</strong></h2><p>If your dataset is too large to work with efficiently on a single machine, your main options are to run your code across…</p><ul role="list"><li><strong>...multiple threads or processors:</strong> Modern CPUs have several independent cores, and each core can run many threads. Ensuring that your program uses all the potential processing power by parallelizing across cores is often the easiest place to start.</li><li><strong>...GPU cores: </strong>Graphics cards were originally designed to efficiently carry out basic operations on millions of pixels in parallel. However, developers soon saw other uses for this power, and “GP-GPU” (general processing on a graphics processing unit) is now a popular way to speed up code that relies heavily on matrix manipulations.</li><li><strong>...compute clusters: </strong>Once you hit the limits of a single machine, you need a networked cluster of machines, working cooperatively.</li></ul><p>Apart from adding more hardware resources, clever algorithms can also improve efficiency. Tools like Vaex rely heavily on <a href="https://en.wikipedia.org/wiki/Lazy_evaluation"><strong>lazy evaluation</strong></a><strong> </strong>(not doing any computation until it’s certain the results are needed) and <a href="https://en.wikipedia.org/wiki/Memory-mapped_file"><strong>memory mapping</strong></a><strong> </strong>(treating files on hard drives as if they were loaded into RAM).</p><p>None of these strategies is inherently better than the others, and you should choose the one that suits your specific problem.</p><p>Parallel programming (no matter whether you’re using threads, CPU cores, GPUs, or clusters) offers many benefits, but it’s also quite complex, and it makes tasks such as debugging far more difficult.</p><p>Modern libraries can hide some – but not all – of this added complexity. No matter which tools you use, you’ll run the risk of expecting everything to work out neatly (below left), but getting chaos instead (below right).</p><figure id="w-node-7b8872b99c95-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5efef852495a4ce0972910e9_image4_s.jpg" alt="Puppies in a row eating food from different bowls – and then chaos ensues."></p><figcaption>Parallel processing doesn’t always work out as neatly as you expect. (<a href="https://www.reddit.com/r/aww/comments/2oagj8/multithreaded_programming_theory_and_practice/">Source</a>)</figcaption></figure><h2><strong>Dask vs. Ray vs. Modin vs. Vaex vs. RAPIDS</strong></h2><p>While not all of these libraries are direct alternatives to each other, it’s useful to compare them each head-to-head when deciding which one(s) to use for a project.</p><p>Before getting into the details, note that:</p><ul role="list"><li>RAPIDS is a collection of libraries. For this comparison, we consider only the <strong>cuDF</strong> component, which is the RAPIDS equivalent of Pandas.</li><li>Dask is better thought of as two projects: a low-level Python scheduler (similar in some ways to Ray) and a higher-level Dataframe module (similar in many ways to Pandas).</li></ul><h3><strong>Dask vs. Ray</strong></h3><p>Dask (as a lower-level scheduler) and Ray overlap quite a bit in their goal of making it easier to execute Python code in parallel across clusters of machines. Dask focuses more on the data science world, providing higher-level APIs that in turn provide partial replacements for Pandas, NumPy, and scikit-learn, in addition to a low-level scheduling and cluster management framework.</p><p>The creators of Dask and Ray discuss how the libraries compare in <a href="https://github.com/ray-project/ray/issues/642">this GitHub thread</a>, and they conclude that the scheduling strategy is one of the key differentiators. Dask uses a centralized scheduler to share work across multiple cores, while Ray uses distributed bottom-up scheduling.</p><h3><strong>Dask vs. Modin</strong></h3><p>Dask (the higher-level Dataframe) acknowledges the limitations of the Pandas API, and while it partially emulates this for familiarity, it doesn’t aim for full Pandas compatibility. If you have complicated existing Pandas code, it’s unlikely that you can simply switch out Pandas for Dask.Dataframe and have everything work as expected. By contrast, this is exactly the goal Modin is working toward: 100% coverage of Pandas. Modin can run on top of Dask but was originally built to work with Ray, and that integration remains more mature.</p><h3><strong>Dask vs. Vaex</strong></h3><p>Dask (Dataframe) is not fully compatible with Pandas, but it’s pretty close. These close ties mean that Dask also carries some of the baggage inherent to Pandas. Vaex deviates more from Pandas (although for basic operations, like reading data and computing summary statistics, it’s very similar) and therefore is also less constrained by it.</p><p>Ultimately, Dask is more focused on letting you scale your code to compute clusters, while Vaex makes it easier to work with large datasets on a single machine. Vaex also provides features to help you easily visualize and plot large datasets, while Dask focuses more on data processing and wrangling.</p><h3><strong>Dask vs. RAPIDS (cuDF)</strong></h3><p>Dask and RAPIDS play nicely together via an integration <a href="https://rapids.ai/dask.html">provided by</a> RAPIDS. If you have a compute cluster, you should use Dask. If you have an NVIDIA graphics card, you should use RAPIDS. If you have a compute cluster of NVIDIA GPUs, you should use both.</p><h3><strong>Ray vs. Modin or Vaex or RAPIDS</strong></h3><p>It’s not that meaningful to compare Ray to Modin, Vaex, or RAPIDS. Unlike the other libraries, Ray doesn’t offer high-level APIs or a Pandas equivalent. Instead, Ray powers Modin and <a href="https://docs.ray.io/en/latest/tune.html">integrates with RAPIDS</a> in a similar way to Dask.</p><h3><strong>Modin vs. Vaex</strong></h3><p>As with the Dask and Vaex comparison, Modin’s goal is to provide a full Pandas replacement, while Vaex deviates more from Pandas. Modin should be your first port of call if you’re looking for a quick way to speed up existing Pandas code, while Vaex is more likely to be interesting for new projects or specific use cases (especially visualizing large datasets on a single machine).</p><h3><strong>Modin vs. RAPIDS (cuDF)</strong></h3><p>Modin scales Pandas code by using many CPU cores, via Ray or Dask. RAPIDS scales Pandas code by running it on GPUs. If you have GPUs available, give RAPIDS a try. But the easiest win is likely to come from Modin, and you should probably turn to RAPIDS only after you’ve tried Modin first.</p><h3><strong>Vaex vs. RAPIDS (cuDF)</strong></h3><p>Vaex and RAPIDS are similar in that they can both provide performance boosts on a single machine: Vaex by better utilizing your computer’s hard drive and processor cores, and RAPIDS by using your computer’s GPU (if it’s available and compatible). The RAPIDS project as a whole aims to be much broader than Vaex, letting you do machine learning end-to-end without the data leaving your GPU. Vaex is better for prototyping and data exploration, letting you explore large datasets on consumer-grade machines.</p><h2><strong>Final remarks: Premature optimization is the root of all evil</strong></h2><p>It’s fun to play with new, specialized tools. That said, many …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray">https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray</a></em></p>]]>
            </description>
            <link>https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740012</guid>
            <pubDate>Sun, 05 Jul 2020 16:47:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Andrew Wilkinson and Tiny Capital]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 16 (<a href="https://news.ycombinator.com/item?id=23739381">thread link</a>) | @colinkeeley
<br/>
July 5, 2020 | https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual | <a href="https://web.archive.org/web/*/https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-db1d94e86a488296a48d"><div><blockquote><p><em>“Let someone else run the marathon and incentivize them.”&nbsp;</em></p><p><em>-Andrew Wilkinson</em></p></blockquote><p><strong>What is Tiny?</strong></p><p><a href="http://tinycapital.com/">Tiny</a>&nbsp;is a long term holding company for internet businesses started by&nbsp;<a href="https://twitter.com/awilkinson">Andrew Wilkinson</a>&nbsp;and&nbsp;<a href="https://twitter.com/_sparling_?lang=en">Chris Sparling</a>. They take majority, generally whole, stakes in "profitable, simple, and often boring” internet businesses.&nbsp;</p><p><strong>Why are holding companies and micro private equity interesting?&nbsp;</strong></p><p>I suspect this is the most dependable way to become very wealthy. It isn’t as glamorous or as quick (potentially) as founding or investing in the next multi-billion dollar startup. This is a longer-term grind it out approach.&nbsp;</p><p>Starting companies is fun, but anyone who has done it knows it is a lot of work. Buying established businesses with existing cash flow isn’t as sexy so I suspect it is wildly underrated as a way of building wealth.&nbsp;</p><p>The reality is that it is easier to buy and improve businesses than to start them. It is easier to go from 3 to 10 than from 0 to 1. Even for the folks that have done it before.&nbsp;</p><p>There isn’t much info on how holding companies or micro-PEs like Tiny actually operate. I’ve listened to every podcast Andrew has been on and compiled these notes from them.&nbsp;</p><p>Here is what they are doing behind the scenes.</p><p><strong>How Andrew got started? Where the capital comes from?</strong></p><p>In 2006, Andrew founded&nbsp;<a href="http://metalab.co/">MetaLab</a>, a Victoria, Canada-based design agency shortly after high school. After rapid growth, he used the profits to diversify into a variety of businesses, which today form Tiny, a holding company he owns fully with his business partner Chris Sparling.&nbsp;</p><p>Agencies traditionally aren’t very profitable, but MetaLab is able to charge San Francisco agency rates and only pay Victoria, Canada wages.&nbsp;</p><p>Tiny shifted its focus from starting businesses to buying them in 2013 when MetaLab and all their other businesses combined were doing $7M/year in profit. Tiny is fully self-funded today.</p><p><strong>What’s the scale of Tiny now?</strong></p><p>Comfortably not tiny. It sounds like somewhere around $80-95M revenue per year (double-digit millions is what Andrew says) with highly profitable businesses. They have around 350-400 employees across 20ish companies.&nbsp;</p><p><strong>What Tiny looks for in businesses to buy?</strong></p><p>From their site:</p><blockquote><p><em>3-5+ years of operating history</em></p><p><em>Profits. A minimum $500k/year in annual profit, as high as $15MM.</em></p><p><em>A high-quality team in place. This is negotiable if the business is simple to operate and the team wants to leave.</em></p><p><em>We are open to owners sticking around, leaving cold turkey, or transitioning out over time. We'll work with you to transition.</em></p><p><em>Simple internet businesses that have high margins, don't require tons of people or complex technology, and have a competitive advantage that protects them from competitors. For example: A dominant brand, a large and loyal community, a niche vertical, or something similar.</em></p></blockquote><p>Andrew describes these businesses as "New Zealand companies.”</p><p>What is a New Zealand company?</p><ul data-rte-list="default"><li><p>It is in the middle of nowhere, nobody is paying attention to it, but it is quietly growing. It is not at risk of nuclear war.&nbsp;</p></li><li><p>It is self-sufficient and thriving. It’s food &amp; energy independent. A "safe" business isn't beholden to benevolent gatekeepers like Google or Facebook to reach their customer.&nbsp;</p></li></ul><p>Andrew is always worried about staying power.&nbsp;</p><p>An example of one of his New Zealand business is Dribbble:</p><ul data-rte-list="default"><li><p>Top 1,000 site on the internet&nbsp;</p></li><li><p>A huge community of designers</p></li><li><p>Profitable</p></li><li><p>Few competitors. Big companies are not trying to kill it or compete.&nbsp;</p></li><li><p>Not dependent on Facebook or Google for traffic. People type Dribbble.com into the address bar to visit.&nbsp;</p></li></ul><p><strong>Types of businesses Tiny has bought/started?</strong></p><p>I don’t know if this is by design, but it seems like Andrew has progressed from services to tools/products to platforms/communities to digital marketplaces.&nbsp;</p><ul data-rte-list="default"><li><p>Agencies: MetaLab (design agency), Double Up (podcast growth agency), 8020 (no-code agency)</p></li><li><p>SaaS tools:&nbsp;<a href="https://www.getflow.com/">Flow</a>&nbsp;(product management), Castro (podcast player), Supercast (podcast subscriptions)</p></li><li><p>Products: Caramba</p></li><li><p>Communities: Dribbble&nbsp;</p></li><li><p>Media: Designer News, RideHome (podcast network)</p></li><li><p>Job Boards:&nbsp;<a href="https://weworkremotely.com/">We Work Remotely</a></p></li><li><p>Digital goods marketplaces: Creative Market, Pixel Union</p></li></ul><p><strong>How Tiny companies operate?</strong></p><p>Tiny companies have fewer information responsibilities than typical PE-owned companies. There are no formal board meetings for example.&nbsp;</p><p>Once a month companies send Tiny a finance-only update with the P&amp;L, balance sheet, and KPIs. No operational info is included.&nbsp;</p><p>Once a quarter companies send Tiny a SWOT (strengths, weaknesses, opportunities, and threats) analysis.&nbsp;</p><p>Companies contact Tiny ASAP for emergencies, major news, or decisions.&nbsp;</p><p>Some CEOs will go 6 months or more without speaking with Andrew.&nbsp;</p><p><strong>How Tiny launches new businesses?</strong></p><p>Tiny’s primary business is buying majority stakes in businesses, not starting them. For a while Andrew would start a new business in any niche he was interested in. He tries to avoid that now and thinks it’s a lot better to buy something that is already good.</p><p>When Andrew does start a new business now, he delegates almost all aspects of it. He recently said he only spent something like 4 hours on each of the new businesses he has launched.&nbsp;</p><p>Andrew will pay for all the work to be done and the investment will form his stake in the business. He will find a CEO to run the business and pay the new CEOs a month or two of salaries to get things going. Then he’ll help with intros, but otherwise, he’ll be hands-off. All in he said it takes $10-50k to get off the ground with a great operator.</p><p><strong>Why do Founders sell to Tiny?</strong></p><p>Tiny is positioned as the good guys of private equity. The Berkshire Hathway of internet businesses.</p><p>They have become known for doing simple acquisitions. Andrew didn’t like the traditional acquisition process: long due diligence, and renegotiation of terms. Warren Buffet does deals in seven days and those are larger, more complex businesses. Smaller deals should be even quicker.</p><p>A challenge with this model is that it is difficult to acquire tech companies at reasonable prices. Acquiring boring traditional businesses is easier because the valuations are so much lower than tech companies. To successfully use this approach you need discipline around what you’re willing to pay for a business and a reputation for being easy to work with. Andrew gets deals by being a nice guy and offering a good home for businesses to live on. Contrast this with the typical PE approach of dramatically cutting costs (ie firing everyone) and squeezing as much profit out as possible. Some founders are looking more for freedom and an easy process than maximizing their financial outcome. </p><p>These smaller PE opportunities are underserved relative to the typical VC businesses. The lifestyle businesses that VC shuns are Andrew’s ideal companies. He is fishing in a less crowded pond. </p><p>Andrew will occasionally pay 10x for an amazing business, but that is rare.&nbsp;</p><p><strong>What happens to businesses after the sale?&nbsp;</strong></p><p>For the employees, it is business as usual for the most part. The goal is for the employees to not even notice.&nbsp; </p><p>The biggest difference is that Tiny becomes the bank. Cash is kept in the company based on historical working capital needs and any extra goes to the head office for new acquisitions.&nbsp; </p><p>Often Tiny buys product or designer-led startups that have grown organically. They will put standard best-practice marketing and sales processes in place and sometimes raise prices. Each company has its own CEO with a few exceptions like all job boards (5+) are under one CEO.&nbsp; </p><p>Tiny has a preference for remote companies where they can hire more affordably. Andrew estimates the cost of running a business in Canada can be 60-65% the cost of in California. Struggling American companies with inflated cost structures can reduce costs by moving to Canada. Canadian arbitrage includes lower salaries, not needing to pay medical benefits, SRED, and cheaper currency.</p><p><strong>Who runs the business after a sale?&nbsp;</strong></p><p>Often Andrew is buying from bootstrapped founders that have been at it for 5-10 years and want to move on.</p><p>Finding great people to run these companies is one of the hardest aspects of this model.&nbsp;</p><p>Andrew deals with this by paying up and hiring CEOs that have managed similar businesses at larger scales already before instead of trying to find underpriced less-experienced talent.&nbsp;</p><p>Months before closing on a deal Andrew works to identify opportunities for the business and a new leader to come in.&nbsp;</p><p>He finds these new CEOs through his existing CEOs by asking “we’re about to buy a business who’s the smartest person you know in the space."</p><p><strong>What does the operating company and Andrew do day-to-day?</strong></p><blockquote><p><em>“Entrepreneurship is just delegation”&nbsp;</em></p><p><em>-Andrew Wilkinson</em></p></blockquote><p>Andrew spends time looking for new deals and looking at their existing portfolio and thinking "how they could get fucked”.&nbsp;</p><p>Andrew says his strengths are:</p><ul data-rte-list="default"><li><p>Laser focused on problems for a short period of time. Moves fast.&nbsp;</p></li><li><p>Very good at 0 to 1. Burns bright for 15 days.&nbsp;</p></li><li><p>Inch deep and a mile wide</p></li><li><p>Not good at execution or day to day details</p></li></ul><p>Being comfortable with delegation is key to this model. Andrew is the owner, not the CEO. The owner can’t constantly be delegating what can or can’t be done or the CEO grows resentful. Some comfort with decisions being made that you don’t agree with comes with the territory. Large decisions that require more capital than usual are a discussion.&nbsp;</p><p><strong>How connected are businesses in the holding company?</strong></p><p>Tiny companies are not at all connected. They each operate independently.&nbsp;</p><p>CEOs will take calls and give advice on best practices, but nothing beyond small favors. Real work gets paid for. Tiny pays all companies for the work they do for the holding company and all work between companies is paid at the full rate.&nbsp;</p><p>Synergies are appealing, but they generally just make the CEOs resentful so they are avoided entirely.&nbsp;</p><p><strong>How much debt do they use?</strong></p><p>Tiny uses little debt for acquisitions (less than Berkshire Hathaway) and they likes to pay off debt within 6 months. Debt comes from&nbsp;<a href="https://en.wikipedia.org/wiki/Business_Development_Bank_of_Canada"><strong>BDB of Canada</strong></a>, or traditional banks.</p><p><em>If you know of anything I should add to this please reach …</em></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual">https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual</a></em></p>]]>
            </description>
            <link>https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual</link>
            <guid isPermaLink="false">hacker-news-small-sites-23739381</guid>
            <pubDate>Sun, 05 Jul 2020 15:26:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust on the ESP32 (2019)]]>
            </title>
            <description>
<![CDATA[
Score 128 | Comments 42 (<a href="https://news.ycombinator.com/item?id=23737451">thread link</a>) | @lnyan
<br/>
July 5, 2020 | https://mabez.dev/blog/posts/esp32-rust/ | <a href="https://web.archive.org/web/*/https://mabez.dev/blog/posts/esp32-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	  <p>About six months ago, I made a <a href="https://www.reddit.com/r/rust/comments/ar2d3r/espressif_have_finally_released_an_llvm_fork_this/">post on reddit</a> highlighting the launch of Espressif's llvm xtensa fork, not too long after, I had a working <code>rustc</code> toolchain capable of generating xtensa assembly. At this point I had to put this project to the side to finish my final year of university. Funnily enough I didn't stray too far, my final year project used Rust to create a <a href="https://github.com/MWatch">'smartwatch'</a> (I may write about this in the future, if anyone is interested). </p>
<p>Since then I have seen a few posts utilising my fork to run Rust on the <a href="https://www.espressif.com/en/products/hardware/esp32/overview">ESP32</a> (<a href="https://dentrassi.de/2019/06/16/rust-on-the-esp-and-how-to-get-started/">see this great write up</a> by ctron, if you haven't already), most of which are building on top of <a href="https://github.com/espressif/esp-idf">esp-idf</a> which is written in C. In this post I'll be discussing the steps I took to generate valid binaries for the xtensa architecture with <code>rustc</code> and then write some <code>no_std</code> code to build a blinky program for the ESP32 only using Rust!</p>
<h2 id="hacking-the-compiler">Hacking the compiler</h2>
<p>In March of 2019, Espressif released their first run at an <a href="https://github.com/espressif/llvm-xtensa">llvm fork</a> to support the xtensa architecure. Shortly after I got to work bootstrapping Rust to use this newly created fork. Prior to this project, I'd had no experience with the compiler, fortunately I came across the <a href="https://github.com/rust-lang/rust/pull/52787">RISCV PR</a> which gave me a rough idea of what was required. After <em>many</em> build attempts I finally got it working; I was now able to generate xtensa assembly from Rust source code!</p>
<p>The next step was to assemble and link the generated assembly. The llvm fork in it's current state cannot perform object generation, so we must use an external assembler. Luckily Rust allows us to do so by specifying the <code>linker_flavor</code> as <code>gcc</code> and providing a path to the linker with the <code>linker</code> target option, in this case <code>xtensa-esp32-elf-gcc</code>. After that I created a few built-in targets (which you can see <a href="https://github.com/MabezDev/rust-xtensa/blob/ad570c5cb999f62a03156286fdb5d3d1bbd0fb8b/src/librustc_target/spec/xtensa_esp32_none_elf.rs">here</a>); <code>xtensa-esp32-none-elf</code> for the ESP32; <code>xtensa-esp8266-none-elf</code> for the ESP8266; finally the <code>xtensa-unknown-none-elf</code> target for a generic xtensa target.</p>
<h2 id="blinky-code">Blinky code</h2>
<p>Now lets try and get a ESP32 board to blink the onboard LED using just Rust. First off, we need our basic program structure. The <code>xtensa_lx6_rt</code> crate does most of the heavy lifting in this respect, we simply need to define an entry point and the panic handler. Some of this may look vaguely familiar if you have any experience with <code>cortex-m</code> development on Rust, I've tried to mirror the API as best as I can.</p>
<pre><span>#![</span><span>no_std</span><span>]
#![</span><span>no_main</span><span>]


</span><span>use</span><span> xtensa_lx6_rt as _;

</span><span>use </span><span>core::panic::PanicInfo;

</span><span>/// Entry point - called by xtensa_lx6_rt after initialisation
</span><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {
    </span><span>loop </span><span>{}
}

</span><span>/// Simple panic handler
</span><span>#[</span><span>panic_handler</span><span>]
</span><span>fn </span><span>panic</span><span>(</span><span>_info</span><span>: &amp;PanicInfo) -&gt; ! {
    </span><span>loop </span><span>{}
}
</span></pre>
<p>Now lets add some register definitions for the peripherals we want to use. For our blinky program, we will need to control the GPIO peripheral. In the ESP32 (and most modern processors) peripherals are mapped to memory adresses, commonly refered to as memory mapped peripherals. To control a peripheral we simply need to write values to the right addresses in memory, with respect to the reference manual supplied by the chip manufacturer.</p>
<pre><span>/// GPIO output enable reg
</span><span>const </span><span>GPIO_ENABLE_W1TS_REG</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44024</span><span>;

</span><span>/// GPIO output set register
</span><span>const </span><span>GPIO_OUT_W1TS_REG</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44008</span><span>;
</span><span>/// GPIO output clear register
</span><span>const </span><span>GPIO_OUT_W1TC_REG </span><span>: </span><span>u32 </span><span>= </span><span>0x3FF4400C</span><span>;

</span><span>/// The GPIO hooked up to the onboard LED
</span><span>const </span><span>BLINKY_GPIO</span><span>: </span><span>u32 </span><span>= </span><span>2</span><span>;

</span><span>/// GPIO function mode
</span><span>const </span><span>GPIO_FUNCX_OUT_BASE</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44530</span><span>;
</span><span>const </span><span>GPIO_FUNCX_OUT_SEL_CFG</span><span>: </span><span>u32 </span><span>= </span><span>GPIO_FUNCX_OUT_BASE </span><span>+ (</span><span>BLINKY_GPIO </span><span>* </span><span>4</span><span>);
</span></pre>
<p>Using these definitions it should be possible to change the gpio for your board<sup><a href="#gpio_pin">1</a></sup> by changing the <code>BLINKY_GPIO</code>; for my board (NODEMCU ESP-32S) it was GPIO2.</p>
<h3 id="initialisation">Initialisation</h3>
<p>Next lets setup the pin as a GPIO output. For the ESP32, this is a two step process<sup><a href="#gpio_pin">1</a></sup>. Firstly, its simply a case of setting a bit in the GPIO ouput enable register. Secondly the pin has to be configured in GPIO mode. There are not enough pins for all the possible peripherals in the chip, to combat this each pin can have multiple function modes. In the case of the ESP32, each pin has up to 256 different functions, although not all are mapped. To put the pin in GPIO mode, we need to put in mode 256 (0x100), we do this by writing to the function select register. After issuing those two register writes, we should be able to turn on the GPIO by setting the relevant bit inside the GPIO set register<sup><a href="#2">2</a></sup>.</p>
<pre><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {

    </span><span>// configure the pin as an output
    </span><span>unsafe </span><span>{
        core::ptr::write_volatile(</span><span>GPIO_ENABLE_W1TS_REG </span><span>as </span><span>*mut </span><span>_, </span><span>0x1 </span><span>&lt;&lt; </span><span>BLINKY_GPIO</span><span>);
        </span><span>// 0x100 makes this pin a simple gpio pin - see the technical reference for more info
        </span><span>core::ptr::write_volatile(</span><span>GPIO_FUNCX_OUT_SEL_CFG </span><span>as </span><span>*mut </span><span>_, </span><span>0x100</span><span>); 
    }
    </span><span>// turn on the LED
    </span><span>unsafe </span><span>{
        core::ptr::write_volatile(</span><span>GPIO_OUT_W1TS_REG </span><span>as </span><span>*mut </span><span>_, </span><span>0x1 </span><span>&lt;&lt; idx);           
    }
    </span><span>loop </span><span>{}
}
</span></pre><h3 id="delaying">Delaying</h3>
<p>For the next stage of our blinky program, we need a way to delay; a simple approach could use <code>for</code> loop like so.</p>
<pre><span>pub fn </span><span>delay</span><span>(</span><span>clocks</span><span>: </span><span>u32</span><span>) {
    </span><span>let</span><span> dummy_var: </span><span>u32 </span><span>= </span><span>0</span><span>;
    </span><span>for </span><span>_ in </span><span>0</span><span>..clocks {
        </span><span>unsafe </span><span>{ core::ptr::read_volatile(&amp;dummy_var) };
    }
}
</span></pre>
<p>We add the volatile read so that the compiler doesn't optimise our delay away. The problem with this approach is that depending of the optimisation level, the number of clock cycles each iteration of the loop changes. We need a cycle accurate way of delaying, fortunately the ESP32 has an internal clock counting register which can be accessed with the read special register <code>rsr</code> instruction. Now are delay function looks like this.</p>
<pre><span>/// cycle accurate delay using the cycle counter register
</span><span>pub fn </span><span>delay</span><span>(</span><span>clocks</span><span>: </span><span>u32</span><span>) {
    </span><span>// NOTE: does not account for rollover
    // ommitted: the asm to read the ccount
    </span><span>let</span><span> target = </span><span>get_ccount</span><span>() + clocks;
    </span><span>loop </span><span>{
        </span><span>if </span><span>get_ccount</span><span>() &gt; target {
            </span><span>break</span><span>;
        }
    }
}
</span></pre>
<p>Now we have cycle accurate counting we can delay for one second by waiting for the number of cycles the processor will do in one second. The default clock speed on most ESP boards is 40mhz, hence waiting for 40 million cycles equates to a one second delay.</p>
<p>Bringing the snippets together and cleaning up the code into functions, we now have <code>main</code> that looks like this.</p>
<pre><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {
    </span><span>// configure the pin as an output
    </span><span>configure_pin_as_output</span><span>(</span><span>BLINKY_GPIO</span><span>);

    </span><span>loop </span><span>{
        </span><span>set_led</span><span>(</span><span>BLINKY_GPIO</span><span>, </span><span>true</span><span>);
        </span><span>delay</span><span>(</span><span>CORE_HZ</span><span>);
        </span><span>set_led</span><span>(</span><span>BLINKY_GPIO</span><span>, </span><span>false</span><span>);
        </span><span>delay</span><span>(</span><span>CORE_HZ</span><span>);
    }
}
</span></pre>
<p>After flashing to the board, and firing up our JTAG debugger<sup><a href="#1">3</a></sup>, we are greeted with a blinking LED!</p>

<p>The full source can be found in the <a href="https://github.com/MabezDev/xtensa-rust-quickstart">the xtensa quickstart repo</a> if you wish to try it for yourself.</p>
<p>Now I know what most of you are thinking at this point, it's not very Rusty; it contains bundles of unsafe and there are no real abstractions here, and you are right; but it's something to get the ball rolling.</p>
<h2 id="limitations">Limitations</h2>
<p>There are a few small teething issues, but by far the biggest being issue is that the fork struggles with generating debug info; the external assembler does not support <a href="https://sourceware.org/binutils/docs-2.24/as/CFI-directives.html#CFI-directives">CFI directives</a> something that all llvm targets need to support. CFI directives can easily be removed with some preprocessing, but does of course add an extra step. After pushing past that issue, I was still getting relocation linker errors. I opened <a href="https://github.com/espressif/llvm-xtensa/issues/10">an issue</a> to document my findings in the hopes it can be sorted in the next iteration of the llvm fork.</p>
<h2 id="future-work">Future work</h2>
<p>Once the debuginfo issue is sorted, I hope to start developing an ecosystem of HAL's and drivers similar to the <a href="https://github.com/stm32-rs">stm32-rs</a> and <a href="https://github.com/nrf-rs">nrf-rs</a>; I've already started the <a href="https://github.com/esp-rs">esp-rs</a> organization which is where <code>xtensa-lx6-rt</code> currently resides. Espressif has started the upstream process, the first ten patches are now in review, there should be an update coming to their fork moving from the older llvm6 to llvm8 (and hopefully some other additions and fixes too!).</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://github.com/MabezDev/xtensa-rust-quickstart">xtensa-quickstart</a> - A quickstart project for using Rust on xtensa</li>
<li><a href="https://github.com/MabezDev/rust-xtensa">rust-xtensa</a> - The xtensa fork of Rust</li>
<li><a href="https://github.com/MabezDev">github</a> - My github</li>
</ul>
<br>
<hr>
<br>




	</div></div>]]>
            </description>
            <link>https://mabez.dev/blog/posts/esp32-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23737451</guid>
            <pubDate>Sun, 05 Jul 2020 09:00:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Beginner's Guide to Abstraction]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 79 (<a href="https://news.ycombinator.com/item?id=23735991">thread link</a>) | @jesseduffield
<br/>
July 4, 2020 | https://jesseduffield.com/beginners-guide-to-abstraction/ | <a href="https://web.archive.org/web/*/https://jesseduffield.com/beginners-guide-to-abstraction/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-75">
	<!-- .entry-header -->

	
	
	<div>
		<p>In <em>The Pragmatic Programmer</em>, Andrew Hunt and David Thomas introduced the DRY (Don't Repeat Yourself) principle. The rationale being that if you see the same code copy+pasted 10 times you should probably factor that code into its own method/class.</p>
<p>But then Sandi Metz came along and <a href="https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction">said</a>:</p>
<blockquote>
<p>Duplication is far cheaper than the wrong abstraction.</p>
</blockquote>
<p>And so the eternal war began.</p>
<h3>What is abstraction?</h3>
<p>For the purposes of this post I'm referring to the kind of abstraction as described in the <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)#Abstraction_in_object_oriented_programming">Abstraction Principle</a>, which Wikipedia describes like so:</p>
<blockquote>
<p>In software engineering and programming language theory, the abstraction principle (or the principle of abstraction) is a basic dictum that aims to reduce duplication of information in a program (usually with emphasis on code duplication) whenever practical by making use of abstractions provided by the programming language or software libraries</p>
</blockquote>
<p>This post has nothing to say about the conceptual kind of abstraction where from the concrete examples of 'Parrot' and 'Sparrow' you create an abstraction of 'Bird'. This post is about duplicated code, how to respond to it, and how to respond to other people's responses to it.</p>
<p>I define the verb 'abstraction' to be an <em>attempt</em> to reduce complexity by combining repeated commonality into some generalisation. And so, the noun 'abstraction' is the result of that attempt. If you're somebody who believes abstraction is by definition a <em>successful</em> attempt, feel free to substitute the term 'wrong abstraction' with 'failure to abstract' throughout this post.</p>
<p>The process of abstraction typically goes like this:<br>
1) you identify different chunks of code that you think are all essentially doing the same thing<br>
2) you create a method or a class with a narrow interface which can be substituted in for all the chunks of code you found<br>
3) you go and swap out the chunks of code with a call to your method/class</p>
<h3>Abstraction is always a gamble</h3>
<p>In the world of software engineering, when requirements are always changing, every abstraction is a gamble. When you make an abstraction over some concrete things, you're making a bet that the concrete things are more similar than they are different, and that their similarities are not mere coincidences: that there is a common purpose shared by the concrete things which would lead them to evolve in lockstep as requirements evolve. If you win the bet, your codebase will be easier to work in and adding new use cases via your abstraction will be trivially easy. If you lose, you'll see a flash of fear in your colleague's eyes whenever they're assigned a ticket to make yet another extension to the misfigured monster that the once-innocent abstraction has now become</p>
<p>But risk abounds everywhere, and leaving duplicated code unabstracted is its own gamble. You're betting that the chunks of code will evolve in separate directions as requirements change and that their current similarities are more coincidence than a reflection of their common purpose. Win the bet and your colleague gets to sleep soundly at night knowing they won't be facing the abstraction monster at work the next day. Lose, and code that should have evolved in lockstep is now implemented in completely different ways across different files, where a developer fixes a bug identified in one place, only for the same bug to be reported days later in a completely different file.</p>
<p>Your job is to get good at making the right bets.</p>
<h3>The right/wrong abstraction</h3>
<p>You'll know that you've made the <em>right</em> abstraction when a long time passes and you haven't needed to expand the interface (an example of expanding the interface is adding an optional flag argument). You'll also know you've made the right abstraction when another developer doesn't find it that much harder to understand how the code behaves for a given use case than if somebody had written the code to satisfy the use case without the abstraction.</p>
<p>You'll know you've made the <em>wrong</em> abstraction when after a while the interface has been expanded to support various optional flags, each for a different use case, and you need to be a genius to reason about what the code will actually do for a given use case. By the way, if you have a string arg that merely gets fed into a switch statement inside a method and for each new use case you come up with a new accepted value for it, you <em>are</em> expanding the implicit interface, even if that fact isn't captured in your type system.</p>
<p>There is plenty of daylight between the perfect abstraction and the completely wrong abstraction (perhaps the interface needs to be fundamentally changed but afterwards you're back to having a good abstraction), and so the point of this section isn't to prescribe how much you should be abstracting, but to encourage you to think about both perspectives and be able to make a case in a PR review for why you think an abstraction should/should-not exist.</p>
<h3>Do you over or under-abstract?</h3>
<p>Given it is impossible to make the right decision with regards to abstraction every time, you are probably either somebody who over-abstracts or somebody who under-abstracts.</p>
<p>If common feedback on your PR reviews is that you should DRY up your code, you could probably benefit from doing a scan for duplicated code before submitting a PR and considering whether it belongs in its own method/class.</p>
<p>If you commonly get feedback that your methods are hard to understand because they support too many disparate use cases at once, you are probaby over-abstracting and should consider whether you should increase your tolerance for duplication.</p>
<p>Note that it's not always as simple as under-abstracting vs over-abstracting. Sometimes abstraction is appropriate, but you might take the wrong approach. If an abstraction is deemed wrong by the team, that doesn't mean no abstraction is necessarily the best alternative.</p>
<h3>Under-abstraction examples</h3>
<p>The main sign that you could be under-abstracting is that you have a heap of code doing the exact same thing called in a heap of places with no obvious reason why anybody would want the code to diverge.</p>
<h4>Example: Hard-coded formulas</h4>
<h5>Bad:</h5>
<pre><code># sphere has radius of 11
sphere_volume = 4*Math::PI/3*11**3
puts "the volume of the sphere is #{sphere_volume} cm^3"
...

radius = calculate_radius
volume = 4*Math::PI/3*radius**3
sphere.volume = volume</code></pre>
<h5>Good:</h5>
<pre><code>def sphere_volume(radius)
  4*Math::PI/3*radius**3
end

# sphere has radius of 11
sphere_volume = sphere_volume(11)
puts "the volume of the sphere is #{sphere_volume} cm^3"
...

radius = calculate_radius
volume = sphere_volume(radius)
sphere.volume = volume</code></pre>
<p>Why is it a good idea to abstract the formula for a sphere's volume into its own method? Because if mathematicians ever found out they got the formula wrong, you would want to go through all the places in your code that you used the formula and update it to be correct. That is, we know ahead of time that we want the code to be in lockstep. This is as safe a gamble as you can get.</p>
<h3>Over-abstraction examples</h3>
<p>The main sign that you're over-abstracting is that your method accepts a bunch of optional args:</p>
<h4>Example: Bloated method</h4>
<h5>Bad:</h5>
<pre><code>def average(arr, type = Integer, ignore_nulls = false)
  if arr.any?(&amp;:nil?)
    if ignore_nulls
      arr = arr.compact
    else
      return nil
    end
  end

  if type == String
    arr = arr.map(&amp;:to_i)
  end

  arr.sum / arr.size
end

puts average([1,2,3])
=&gt; 2

puts average(['1','2','3'], String)
=&gt; 2

puts average(['1','2','3', nil], String, true)
=&gt; 2

puts average([1, 2, 3, nil], Integer, false)
=&gt; nil</code></pre>
<p>If you want to know how the <code>average</code> method behaves when you're dealing with an array of strings with no <code>nil</code> values, you have to read through the first if condition which has nothing to do with your use case before reaching the code that does. Likewise if you want to know how the <code>average</code> method behaves when the array contains either nils or integers, the second if condition is irrelevant, but you'll still need to read through that to understand how the whole thing works.</p>
<p>If each of the use cases came up dozens or hundreds of times, maybe then it would make sense to retain the abstraction, but when the number of optional arguments is roughly equal to the number of different use cases, chances are you've got the wrong abstraction.</p>
<h5>Good:</h5>
<pre><code>def average(arr)
  arr.sum / arr.size
end

puts average([1,2,3])
=&gt; 2

arr = ['1','2','3'].map(&amp;:to_i)
puts average(arr)
=&gt; 2

arr = ['1','2','3', nil].compact.map(&amp;:to_i)
puts average(arr)
=&gt; 2

arr = [1, 2, 3, nil]
if arr.any?(&amp;:nil?)
  puts nil
else
  puts average(arr)
end
=&gt; nil</code></pre>
<p>In this case we're not removing the abstraction altogether: we're just keeping the part that actually applies to all cases. Now understanding the logic of any one invocation of our <code>average</code> method is trivial.</p>
<p>We now have <code>.map(&amp;:to_i)</code> being duplicated whereas it only appeared once in the <code>Bad</code> alternative, but it's a small cost for a vast improvement.</p>
<p>Note that looking at the <code>Good</code> variant, it's clear that the behaviour is quite different from one use case to the next, but that is not at all clear in the <code>Bad</code> variant because the method calls all look so simple and it was anybody's guess how much code inside <code>average</code> applied to each use case.</p>
<p>This is why abstractions go bad over time: because as you expand the interface more and more, it becomes harder and harder to judge how appropriate the abstraction is to any given use case, and developers end up assuming that all that convoluted code is vaguely relevant to the majority of use cases when in fact it's not.</p>
<h4>Example: Awkward class</h4>
<h5>Bad:</h5>
<pre><code>class Shape
  def initialize(radius: nil, width: nil, type:)
    @radius = radius
    @width = width
    @type = type
  end

  def area
    case @type
    when :square
      @width ** 2
    when :circle
      (@radius ** 2) * Math::PI
    end
  end

  def perimeter
    case @type
    when :square
      @width * 4
    when :circle
      @radius * 2 * Math::PI
    end
  end

  def diameter
    case @type
    when :square
      nil
    when :circle
 …</code></pre></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jesseduffield.com/beginners-guide-to-abstraction/">https://jesseduffield.com/beginners-guide-to-abstraction/</a></em></p>]]>
            </description>
            <link>https://jesseduffield.com/beginners-guide-to-abstraction/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23735991</guid>
            <pubDate>Sun, 05 Jul 2020 01:12:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I grew my Shopify micro-SaaS]]>
            </title>
            <description>
<![CDATA[
Score 136 | Comments 54 (<a href="https://news.ycombinator.com/item?id=23734539">thread link</a>) | @amrrs
<br/>
July 4, 2020 | https://www.preetamnath.com/blog/grow-shopify-micro-saas-to-25k-mrr-in-14-months | <a href="https://web.archive.org/web/*/https://www.preetamnath.com/blog/grow-shopify-micro-saas-to-25k-mrr-in-14-months">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>14 months ago, my co-founder <a href="https://twitter.com/sankalpjonna" target="_blank">Sankalp</a> and I set out to build a business that can sustain our livelihood while allowing us the freedom to live life on our terms. </p><p>When we started, we each had roughly 12 months of savings to survive on, assuming we made $0. Hence our minimum goal was to make $1,500/mo and the case where we would be celebrating with champagne was set to $3,000/mo.</p><p>We launched our app on April 24, 2019. The first paid plan was launched on June 4, 2019. Around the same time, we got <a href="https://www.preetamnath.com/blog/shopify-micro-saas-growth" target="_blank">featured by Shopify</a>. Here's a <a href="https://apps.shopify.com/whatsapp-chat-button" target="_blank">link to the app</a> if you want to check it out.</p><p>14 months later, we grew past $25,000 in MRR while remaining a 2 person team.</p><figure id="w-node-e5ce9f7f0ba4-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004959a0b62d53946fd901_shopify%20earnings%20growth.png" alt=""></p><figcaption><em>Note:&nbsp;Earnings reflect after a 1 month delay</em></figcaption></figure><p>I’m so proud of what we have built here.</p><p>If life is a video game, and there’s 100 levels to it, I feel like we have finally crossed level 1.</p><p>Here's what we've been up to in the 14 months since launch:</p><ul role="list"><li>20,000 active merchants (users) on the app</li><li>1,500+ paying merchants (customers)</li><li>450,000 automated WhatsApp messages sent in June</li><li>2.2 million WhatsApp chat messages initiated in June</li><li>Highest rated 5.0 🌟 app for all things WhatsApp with 500+ reviews</li><li>Users from 50+ countries around the world</li></ul><figure id="w-node-0cd8870c808c-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f007ba5d6c3b3532b0aa784_paying%20customers%20countries.png" alt=""></p></figure><p>We did a lot of things right, but we also messed up here and there. I've tried to capture an honest and accurate account of our journey so far.</p><h2>11 key learnings and mistakes from our journey<br></h2><h3>1- Learning: Always keep moving the needle</h3><p>Running your own business and having 100% freedom and flexibility is a gift or a curse, depending on how you use it. It’s easy to get lost in building, polishing, bringing your ideas to life. But remember, the only things that matter are what move the needle. </p><p>For every decision related to your business, you should always run it through a set of questions that help you filter out whether your idea is aligned to your business goals.</p><ul role="list"><li>Does it help our app get more users</li><li>Does it help convert more users to paying customers</li><li>Does it help users derive more value and ROI from the app</li><li>Does it meaningfully improve user experience</li><li>Does it reduce our support ticket load</li><li>Does it have any unintended 2nd order consequences that can bite us later</li></ul><p>Ultimately, you want to keep making decisions that move the needle for your business - by bringing you more customers and revenue, or making it easier and less time consuming to run the business (if that’s one of your goals, it is for us).<br></p><h3>2- Learning: Help users realise value as soon as possible</h3><p>From our research, we learnt that Shopify merchants are mostly solopreneurs or teams of 2-10. They are always time and resource crunched.</p><p>We wanted to enable our users to start using the app with the least amount of setup time or steps. This reflects in in the user experience of the app. </p><p>For example, when a first time user enters their phone number and hits the “Save” button, at this time the user is looking to test our app and see if it fits their needs. We speed that up by&nbsp;automatically enabling the chat button on first save, along with the best default configuration for all chat button related settings.</p><figure id="w-node-4b9a52f7ced2-2f0d8df6"><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/o3oeGyv-JJs"></iframe></p></figure><p>Another example, when a user wants to start sending automated messages, all they have to do is click on the “Enable” button. All the app’s default configuration is automatically applied and the user can start seeing value immediately.</p><figure id="w-node-4bee860d412d-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004a6e594ffa16384d47ca_automated-message-setup-1.png" alt=""></p></figure><figure id="w-node-8c9743f7a3e0-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004a858d5f7ceaad74de97_automated-message-setup-2.png" alt=""></p></figure><h3>3- Learning: Care about your users, they can feel it</h3><p>Users are on a mission to achieve their goals. They typically need your help when there’s something that stops them from getting there.</p><p>From day 1, we were clear on providing the best possible support. Our users would be thrilled to see bug fixes deployed in 15 minutes, WhatsApp &amp; email replies in 5 minutes, and so on. We were fast, we were responsive.</p><p>This worked in our favour - users showered us with positive reviews which is how we managed 500+ reviews while staying 5.0 star rated.</p><figure id="w-node-f55f45993a1f-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004affbd3802acac60056d_superlemon%20whatsapp%20app%20shopify%20reviews.png" alt=""></p></figure><p>But customer support isn’t only about getting reviews. We’ve had users who prefer using our app instead of competitors because</p><p><strong>A-</strong> We reply quickly</p><p><strong>B-</strong> We proactively provide information beyond the immediate premise of the query</p><figure id="w-node-e499fce41f50-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f0065d67e883834a7802cc8_review-users-care.png" alt=""></p><figcaption><em>Your users can feel when you genuinely care about them</em></figcaption></figure><h3>4- Learning: Be smart about managing and reducing support ticket volume</h3><p>The downside of trying to be a customer support hero is, you only have finite hours in the day, and you don’t want to spend most of it replying to customers. </p><p>We took 3 specific initiatives to reduce support tickets.</p><p><strong>1- Embed FAQs within the product</strong></p><p>We found that users would have FAQs around specific features, and they wouldn’t really go to the help center and reference it. To them, it’s simpler to ask for help.</p><p>To solve this problem, we embedded FAQs inside the product itself. </p><p>Here are a few examples where we included the most commonly asked question beside the feature configuration. </p><figure id="w-node-1def5927520e-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004c97d6c3b3b5f30a4556_faq-inside-product-1.png" alt=""></p></figure><figure id="w-node-11706312cf8a-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004ca08216b9c33aaef853_faq-inside-product-2.png" alt=""></p></figure><figure id="w-node-9239f016ade1-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004ca87bc6f87761475470_faq-inside-product-3.png" alt=""></p></figure><p>The results were always immediately noticeable. Where we would get 10 questions a week about X, would drop to 1-2, thereby freeing us up from answering repetitive support questions.</p><p><strong>2- Fix repetitive bugs</strong></p><p>If a bug appears more than thrice in a span of days, the third time is when we go ahead and permanently fix it. Why thrice? That way we don’t overwhelm ourselves with trying to fix every single bug, but only the ones that appear repeatedly and take up support time.</p><p><strong>3- Remove WhatsApp chat support and go with Email only</strong></p><p>During initial days, we found it useful to let our users contact us on WhatsApp. It lead to tons of conversations which shaped our understanding of the user’s needs and how we can better solve them. </p><p>But having WhatsApp as a support channel had 3 issues:</p><p><strong>1-</strong> With time, we discovered that people find dropping a message on WhatsApp to be very low barrier. Which means they wouldn’t take a moment to look around, be patient enough to find out how the issue that they are about to describe has already been solved inside the app. It’s just easier to WhatsApp us.</p><p><strong>2-</strong> We also discovered that users had an unrealistic expectation of quick replies and quick resolutions on WhatsApp. Granted, we would be quick whenever possible. But if it’s 11pm at night and the user asks if a bug is fixed 10 minutes later, it can get a bit annoying.</p><p><strong>3-</strong> I started dreading picking up my phone, seeing it full of support tickets every morning.</p><p>To solve this, we removed the WhatsApp chat button from our own app, and replaced it with an Email support button instead. All 3 problems listed above got resolved to a great degree.</p><figure id="w-node-09f7b09a3b19-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004d01e45cb8848d6cf0f7_Email%20support%20only.png" alt=""></p></figure><p>We also made numerous smaller tweaks to optimise our support flow. One of the tiny but powerful tweaks was to automatically populate the email subject line with the user’s Shopify store url. That made it so much faster for us to look up the specific user instead of a meaningless back and forth of “what’s your store url”.</p><figure id="w-node-280b459e613d-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f00533cbd380208b160157f_store%20url.jpg" alt=""></p></figure><p>Our efforts have paid off. Unless we’re building new features, we spend only about <strong>15-30 minutes per day</strong> on customer support.<br></p><h3>5- Learning: Say No, often</h3><p>Users ask for all sorts of functionality. It doesn’t mean you should build them. </p><p>Whenever we get feature requests, we add a card in our internal Notion board and judge the card against 3 factors</p><p><strong>1-</strong> how many times do we receive the same request</p><p><strong>2-</strong> does the feature align with the rest of our product</p><p><strong>3-</strong> does the feature add core value to the user</p><p>Many times, we found users requesting features that would save them the hassle of using another app. Now, if that functionality enhances the rest of our app, it’s the right feature to add. But if it doesn’t, then we politely decline.</p><figure id="w-node-2831ff295f9b-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f00517014696c60a040b940_politely%20saying%20no.png" alt=""></p></figure><p>There are 2 types of responses we give to customers regarding feature requests:</p><p><strong>1-</strong> Tell them that this feature isn’t requested by other users that much, and since we are a small team with limited resources, we may not be prioritising this feature in the near future.</p><p><strong>2-</strong> Tell them that this feature is already part of our roadmap, and that we will reach out to them whenever it’s ready. I generally avoid giving timelines, to account for “life happens, plans change”.</p><p>This learning extends beyond just feature requests. </p><p>In the past year, we have received tons of business proposals, partnerships, integration requests, collaborative marketing invitations etc. Saying No allowed us to remain focused on what’s really important.<br></p><h3>6- Mistake: Optimise top of the funnel</h3><p>I was under the impression that during build phase, we shouldn’t focus too much on optimisation. After all, you don’t want to get busy squeezing another 10% when the potential is to 10x. </p><p>In most cases, I think I made the right call. One place I made the wrong call was the user onboarding flow.</p><p>Until March 2020, users would install the app and directly land on the chat settings page, which is the starting point of the user’s journey. We were getting a steady flow of 10-15 paid trials initiated per day. </p><p>Adding a step of friction before this would be counterintuitive, right? It couldn’t possibly increase trials, right?</p><p>Sigh. </p><p>When we added one screen which asked the user upfront whether they want to continue with the Free plan, or start a trial for the Paid plan, daily paid trials DOUBLED overnight, while trial-to-paid conversion rate remained steady at 50%. </p><figure id="w-node-c2d582bcc858-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f004d9fda26a209aa082a72_onboarding%20trial%20doubled.png" alt=""></p></figure><p>The experiment took us a day to build, and after a week we concluded what was clearly the winner.</p><p>I normally advocate for focusing on building vs optimising in the early stages, but in this case I didn’t know that optimisation was waiting to unlock 100% increase in trials.</p><p>My takeaway here is - always keep your eyes open for bottlenecks at the top of funnel. </p><p>If there’s copy on your website, or a specific sign up flow that might be holding you back from getting 2x the users, you need to unblock it early and realise compounded gains over time.<br></p><h3>7- Mistake: Competition will copy, don’t completely ignore them</h3><p>In the past year, we’ve seen competitors copy our app interface, in-app descriptions, button designs, app listing text word-for-word in many cases. Initially we were furious, but instead we kept our head down and focused on customer’s needs. </p><p>Eventually we started ignoring our competitors, which was a mistake. While …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.preetamnath.com/blog/grow-shopify-micro-saas-to-25k-mrr-in-14-months">https://www.preetamnath.com/blog/grow-shopify-micro-saas-to-25k-mrr-in-14-months</a></em></p>]]>
            </description>
            <link>https://www.preetamnath.com/blog/grow-shopify-micro-saas-to-25k-mrr-in-14-months</link>
            <guid isPermaLink="false">hacker-news-small-sites-23734539</guid>
            <pubDate>Sat, 04 Jul 2020 20:00:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[De-escalating social media conflict]]>
            </title>
            <description>
<![CDATA[
Score 548 | Comments 315 (<a href="https://news.ycombinator.com/item?id=23734535">thread link</a>) | @npunt
<br/>
July 4, 2020 | https://nickpunt.com/blog/deescalating-social-media/ | <a href="https://web.archive.org/web/*/https://nickpunt.com/blog/deescalating-social-media/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <p>Social media has a conflict problem.</p><p>Spending even a few minutes on public social media can expose us to dozens of people we know little about, talking about things we know little about. In such a public place, any individual's reputation, perspectives, and history are difficult to ascertain, and therefore their words must be taken at face value. Coupled with an almost complete lack of standards for participation in the community and a high degree of variance in knowledge among participants, and the environment naturally skews toward conflict and tribalism.</p><p>One particular effect of this environment is that small misunderstandings, mistakes, or disagreements can unexpectedly explode due to the public nature of discourse and assumptions of bad faith. Meanwhile, very few tools exist to moderate these effects.</p><!--kg-card-begin: markdown--><p>This is why it's my belief that <mark>as designed today, social media is out of balance</mark>. It is far easier to escalate than it is to de-escalate, and this is a major problem that companies like Twitter and Facebook need to address.</p>
<!--kg-card-end: markdown--><p>This got me thinking about what particular use cases need de-escalation, and whether there's something simple we can do to test the waters and address these types of problems. </p><h2 id="target-use-case-admitting-mistakes">Target Use Case: Admitting Mistakes</h2><p>One obvious issue is that <strong>people are wrong about a lot of things, but struggle to admit their mistakes</strong>. This is why many cultures have created elaborate norms around face-saving. Unfortunately, social media largely lacks these cultural norms, and even makes the problem worse in three ways:</p><ol><li><strong>No social proof</strong> - Admitting mistakes is quite difficult if nobody else is seen (genuinely) admitting mistakes, as we rely heavily on social proof to sense-make around norms. An escalation-oriented culture means there's less genuine "I was wrong" on social media than there should be.</li><li><strong>No respite</strong> - as the visibility of a mistake travels across social media, the poster is subject to a constant deluge of new readers calling them out, with the combined energy of the outraged far exceeding that of the poster. This prevents them from having the necessary emotional cool-down period to respond thoughtfully.</li><li><strong>Digging in</strong> - when feeling personally attacked, its human nature to prefer to dig in rather than apologize. Social media's escalation-oriented culture plays into this human fallibility.</li></ol><p>Take Twitter as an example environment. If we write something that turns out to be wrong and a pile-on begins, we're going to be swimming upstream fighting it. Our options are:</p><ol><li><strong>Ignore</strong> replies and hopefully let it die out</li><li><strong>Delete</strong> your tweet, posting another tweet saying "I was wrong"</li><li><strong>Reply</strong> to your tweet, posting "I was wrong"</li></ol><p>However, none of these are great. If we ignore<strong><em> </em></strong>replies, the simple amplification effects of likes, replies, retweets, and subtweets leave us exposed and the situation can get out of hand. If we delete and post another, people are unlikely to see our follow-up, as corrections are rarely viral. Similarly, even if we reply, only our viral mistake will be seen in the feed of others. </p><p>Let's also not forget the human tendency to not fully walk back statements, and instead offer unsatisfying explanations that can accidentally further enflame situations. Blaming, justifying, minimizing, or excusing are all self-sabotaging ways that people qualify their apologies to preserve their self-esteem or resolve their own cognitive dissonance, and social media is right there ready to give us this kind of rope to hang ourselves with when we're under assault.</p><!--kg-card-begin: markdown--><p>In all these cases, a person's reputation in the minds of others is damaged because <mark>social media engagement engines favor the drama of the mistake over the correction and reconciliation</mark>, which leads few people to see or remember anything other than the mistake.</p>
<!--kg-card-end: markdown--><p>This seems like a perfect use case to design for, so I came up with a little design change I call the Twitter Mea Culpa.</p><h2 id="design-exploration-twitter-mea-culpa">Design Exploration: Twitter Mea Culpa</h2><p>Twitter Mea Culpa is a way for a poster to flag their tweet as a mistake and de-escalate a situation, using the same action menu that deleting a post uses, and the same visual design as flagged tweets:</p><figure><img src="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Blue-w-Menu-NoLabel-1.png" alt="" srcset="https://nickpunt.com/content/images/size/w600/2020/07/TwitterMeaCulpa-Blue-w-Menu-NoLabel-1.png 600w, https://nickpunt.com/content/images/size/w1000/2020/07/TwitterMeaCulpa-Blue-w-Menu-NoLabel-1.png 1000w, https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Blue-w-Menu-NoLabel-1.png 1260w" sizes="(min-width: 720px) 720px"><figcaption>Twitter Mea Culpa: flagging a post as a mistake</figcaption></figure><h3 id="clear-language">Clear Language</h3><!--kg-card-begin: html--><figure><a href="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Language-1.png"><img src="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Language-1.png" alt="" srcset="https://nickpunt.com/content/images/size/w600/2020/07/TwitterMeaCulpa-Language-1.png 600w, https://nickpunt.com/content/images/size/w1000/2020/07/TwitterMeaCulpa-Language-1.png 1000w, https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Language-1.png 1198w" sizes="(min-width: 720px) 720px"></a><figcaption>Language:<em> @user has indicated they made a mistake in this tweet</em></figcaption></figure><!--kg-card-end: html--><p>I picked this language carefully for several reasons:</p><ol><li>Create separation between the individual and what they say (the specific post), which reduces ego threat.</li><li>Use an active voice to indicate responsibility (<em>I made a mistake</em> not <em>Mistakes were made</em>).</li><li>Indicate the poster was the one who chose to flag the post, not Twitter or any other user, to reduce the impression to other users that the poster was censored.</li><li>State the mistake unequivocally and leaving no room for ambiguity about whether it is actually a mistake, to reduce inauthentic use and self-sabotage.</li><li>Avoid direct use of 'sorry' or 'I apologize' language, because for some people these words are extremely <a href="https://www.psychologytoday.com/us/blog/the-squeaky-wheel/201305/5-reasons-why-some-people-will-never-say-sorry">emotionally challenging</a> to use.</li></ol><h3 id="disable-amplification">Disable Amplification</h3><!--kg-card-begin: html--><figure><a href="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Amplification-1.png"><img src="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Amplification-1.png" alt="" srcset="https://nickpunt.com/content/images/size/w600/2020/07/TwitterMeaCulpa-Amplification-1.png 600w, https://nickpunt.com/content/images/size/w1000/2020/07/TwitterMeaCulpa-Amplification-1.png 1000w, https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-Amplification-1.png 1262w" sizes="(min-width: 720px) 720px"></a></figure><!--kg-card-end: html--><p>Much like rules violations, once a user admits a mistake, the Mea Culpa tweet is no longer amplified. This means:</p><ul><li>The tweet is no longer made visible on everyone's home feed</li><li>Users cannot reply, like, or retweet without comment</li><li>Users may retweet <em>with</em> comment, to allow users to amplify the correction (e.g. <em>I thought this wasn't the case, looks like they realized it</em>)</li></ul><!--kg-card-begin: markdown--><p>By admitting a mistake, the poster stops the runaway train of replies and amplifications of their mistake, and the reputation damage that follows. In other words, <mark>Mea Culpas are intentionally designed to favor respectful debate and ability to cool off over maximal information exchange</mark>.</p>
<!--kg-card-end: markdown--><figure><img src="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-ExampleViolation.png" alt=""><figcaption>How Twitter handles rules violations today; notice the lack of like/reply/retweet counts</figcaption></figure><p>This amplification disabling is how rules violations work today. However admitting a mistake is much more innocuous, so I would recommend that existing replies, retweets, and likes remain accessible, which is not the case with violations. After all, sometimes mistakes can create interesting conversation.</p><h3 id="surface-corrections">Surface Corrections</h3><p>One possible exception to the disable amplification rule is that a Mea Culpa tweet could show up again <em>within the home feeds of users who originally reacted to it</em>. This would basically be like issuing a correction, which would be intended to help limit accidental falsehoods from spreading. </p><figure><img src="https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-FeedCorrection-1.png" alt="" srcset="https://nickpunt.com/content/images/size/w600/2020/07/TwitterMeaCulpa-FeedCorrection-1.png 600w, https://nickpunt.com/content/images/2020/07/TwitterMeaCulpa-FeedCorrection-1.png 707w"><figcaption>Mistake correction shown in Home Feed, shown because user liked the tweet</figcaption></figure><p>For instance, if someone tweeted an unsubstantiated rumor and we liked or replied to it, when we go back to our home feed a day or two later we'd see their tweet with the notice that they'd made a mistake.</p><p>This seems like the right thing to do, and a way to help social media stay more focused on truth.</p><h3 id="culture-of-use">Culture of Use</h3><p>Twitter Mea Culpa is meant to solve for a few specific use cases that involve the poster's authentic desire for de-escalation at some level:</p><ol><li><strong>I'm actually wrong </strong>- when the poster knows they are wrong and wants to stop hearing from people and just correct their mistake.</li><li><strong>I'm right if you look at it this way</strong> - when the poster is struggling to understand in what ways or from what perspectives they're wrong, but can tell they are a little bit wrong, and are considering explaining themselves.</li><li><strong>Fine, I give up </strong>- when the poster is struggling to apologize or admit they're wrong, as is often the case when people feel vulnerable or when their ego is bruised, and are starting to think it might be better to tap out than to continue.</li></ol><p>Today, the <em><strong>I'm actually wrong</strong></em><strong> </strong>case is an obvious flaw in the system, where posters have to constantly reply to people saying <em>"yeah, I know, I know, I was wrong, look at my other tweet"</em>. In this case, their reputation is less at risk and they're already at a point of emotional acceptance, so it's simply a matter of unnecessary UX friction.</p><p>The much more interesting and subtle cases are <strong><em>I'm right if you look at it this way</em></strong> and <strong><em>Fine, I give up</em></strong>. Posters in these situations may not yet have the emotional distance to gain proper perspective, still feeling a connection between what they said and who they are (ego threat). As mentioned before, the lack of respite in social media can keep people in this emotional state for a lot longer than they might otherwise be, as each new notification they receive keeps them in a state of alert.</p><p>In these situations, there's an emotional temptation to respond when it's often best not to, given the escalating nature of the medium. The temptation comes from a poster having only one tool at their disposal to repair their reputation: replying to the tweets contributing to their threatened feelings. </p><p>With Mea Culpa, users gain a second tool specifically designed to de-escalate, that only requires their acknowledgement that a) they may be at least partly wrong, and b) they do not wish to continue. When in use, it can cut off potentially escalating conversations early, giving everyone else who sees the tweet the signal to step back because the poster may not have meant it and (at least partially) admits their mistake.</p><p>There is no guarantee that most or all conversations would de-escalate after a Mea Culpa, but in imagining the possible uses for it, I think it would help considerably.</p><h3 id="correcting-for-inauthenticity">Correcting for Inauthenticity</h3><p>Unfortunately, there are also many bad faith actors on social media, and there's potential for any feature to be used in a way that means something other than what was intended, creating a culture of alternate use and meaning. A few anti-patterns may emerge here:</p><ol><li><strong>Growth hacking</strong> - a user posts something intentionally fabulist they want others to see, get a lot of engagement, and then post a mistake tag to try to have people see it again.</li><li><strong>Ironic opposite</strong> - a user flags everything they post as a mistake, either in protest or just as an ironic statement.</li><li><strong>Just kidding </strong>- a user posts something boundary-pushing to float it as an idea, then walks it back with a mistake tag.</li><li><strong>I had to do it </strong>- a user posts something boundary-pushing that they believe, then walks it back with a mistake tag as a …</li></ol></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nickpunt.com/blog/deescalating-social-media/">https://nickpunt.com/blog/deescalating-social-media/</a></em></p>]]>
            </description>
            <link>https://nickpunt.com/blog/deescalating-social-media/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23734535</guid>
            <pubDate>Sat, 04 Jul 2020 19:59:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Iran dumps Flight 752 investigator, airspace open to conceal 'imminent' attack]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 7 (<a href="https://news.ycombinator.com/item?id=23733910">thread link</a>) | @refurb
<br/>
July 4, 2020 | https://www.cbc.ca/news/politics/audio-recording-iran-lead-investigator-flight-ps752-1.5636450 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/politics/audio-recording-iran-lead-investigator-flight-ps752-1.5636450">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>A newly released audio recording suggests Iran's highest authorities allowed commercial airliners to fly in and out of Tehran during the period of intense military activity when Flight 752 was shot down — because closing the airspace would have given away the regime's plan to strike U.S. military bases in Iraq.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5636477.1593791704!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/plane-crash.jpg"></p></div><figcaption>The head of Iran's investigation into Flight PS752 has been replaced following the release of a recording of a conversation he had with a victim's relation in Canada.<!-- --> <!-- -->(Reuters)</figcaption></figure><p><span><p>A newly released audio recording suggests Iran's highest authorities allowed commercial airliners to fly in and out of Tehran during the period of intense military activity when Flight 752 was shot down — because closing the airspace would have given away the regime's plan to strike U.S. military bases in Iraq.</p>  <p>CBC News obtained a recording of a&nbsp;91-minute conversation that took place March 7&nbsp;between a victim's family member in Canada and Hassan Rezaeifar, who was appointed the head of Iran's investigation into the downing of the Kyiv-bound Ukraine International Airlines aircraft. The crash of Flight 752 killed 176 people, including 57 Canadians.</p>  <p>The recording, which reveals a number of damning details about the downing of the plane and Iran's response, is also in the custody of Canadian authorities.</p>  <p>Less than 24 hours after CBC News emailed Rezaeifar a copy of the recording and requested a response Thursday, news broke that he had been removed from his role overseeing Iran's investigation into the downing of Flight 752. Families in the United Kingdom — which has&nbsp;an embassy in Iran — were notified this morning that a new investigator is now in charge.</p>  <h2>Airspace kept open to avoid signalling&nbsp;attack:&nbsp;Rezaeifar</h2>  <p>In the recording, Rezaeifar said closing the airspace over Tehran could have exposed Iran's pending ballistic missile attack on U.S. air bases in Iraq in advance. That attack was retaliation for the United States' killing of Iran's top military leader,&nbsp;Gen.&nbsp;Qasem Soleimani.</p>  <p>"Some say we should have cleared the airspace," Rezaeifar said in Farsi on the recording. "The National Security Council is in charge.</p>  <p>"But let's say we had cleared the airspace. Wouldn't [it] give away our imminent attack?"</p>  <p>Flight 752 was shot down just four hours after the strike on the U.S. base. Rezaeifar added that&nbsp;closing the airspace could have meant cancelling flights. Iran earns hundreds of thousands of dollars <a href="https://www.cbc.ca/news/politics/iran-airspace-canadians-downed-flight-1.5481818" target="_blank">daily in fees for allowing flights in its airspace.</a></p>  <p>"Ok, let's assume we had delayed the Ukrainian flight for ten hours. Wouldn't it have cancelled all other flights after?" said Rezaeifar on the call.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5636474.1593790216!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/flight-752-crash.jpg 300w,https://i.cbc.ca/1.5636474.1593790216!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/flight-752-crash.jpg 460w,https://i.cbc.ca/1.5636474.1593790216!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/flight-752-crash.jpg 620w,https://i.cbc.ca/1.5636474.1593790216!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/flight-752-crash.jpg 780w,https://i.cbc.ca/1.5636474.1593790216!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/flight-752-crash.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5636474.1593790216!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/flight-752-crash.jpg"></p></div><figcaption>Investigators pick up debris at the crash site of the Ukraine International Airlines plane shot down after takeoff from Iran's Imam Khomeini Airport on Jan. 8, 2020. <!-- --> <!-- -->(Nazanin Tabatabaee/WANA via Reuters)</figcaption></figure></span></p>  <p>Thomas Juneau, an associate professor of international affairs at the University of Ottawa and former analyst of&nbsp;Middle East affairs, said Iran has been insisting the investigation will be independent — and the audio recording proves&nbsp;it's not.&nbsp;</p>  <p>"Having the lead investigator saying those things on that phone call really damages that fiction," said Juneau. "By removing him, they're trying to protect that facade."</p>  <h2>Passengers used as human shields, says expert</h2>  <p>Payam Akhavan, a Canadian-Iranian international law professor at McGill University and former UN prosecutor at The Hague, also reviewed CBC's copy of the recording. Akhavan argues the audio is a new piece of evidence showing the highest levels of Iran's government chose to keep planes full of people in the sky on a day of intense military activity.&nbsp;</p>  <p>"The senior leadership of the government willingly and knowingly disregarded these risks," said Akhavan. "This is not just a question of human error or mistake. It's a question of criminal&nbsp;recklessness.</p>  <p>"To knowingly put civilian aircraft in harm's way, to use civilian airliners in effect as human shields, clearly implicates criminal responsibility."</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5421757.1593792034!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/ps752-victims.jpg 300w,https://i.cbc.ca/1.5421757.1593792034!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/ps752-victims.jpg 460w,https://i.cbc.ca/1.5421757.1593792034!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/ps752-victims.jpg 620w,https://i.cbc.ca/1.5421757.1593792034!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/ps752-victims.jpg 780w,https://i.cbc.ca/1.5421757.1593792034!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/ps752-victims.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5421757.1593792034!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/ps752-victims.jpg"></p></div><figcaption>Dozens of Canadians, as well as students and academics studying in Canada, were killed in the downing of Flight 752.<!-- --> <!-- -->(CBC)</figcaption></figure></span></p>  <h2>Crash investigator in immediate contact with military</h2>  <p>Akhavan also said the audio implicates&nbsp;the investigation team in a cover-up.</p>  <p>In the recording,&nbsp;Rezaeifar —&nbsp;who was the head of the accident investigation board at the Iran Civil Aviation Organization at the time — says he&nbsp;picked up the phone five minutes after the plane crashed and was in immediate contact with Iran's military.</p>  <p>Rezaeifar said Amir-Ali Hajizadeh, the commander of the Aerospace Force of the Islamic Revolutionary Guard Corps (IRGC), admitted the military was ordered to shoot missiles due to national security concerns.</p>  <p>"I was informed at 6:30 a.m. and I called the IRGC at 6:35 a.m. and asked, 'Did you have a missile attack?'" Rezaeifar says in the recording. "Mr. Hajizadeh explained and said&nbsp;yes, and we had orders. He said there are some national security considerations in the country."</p>  <p>On January 11, Iran's military admitted it unintentionally shot down the plane and blamed human error, saying the military&nbsp;mistook the jetliner for a hostile target. That acknowledgement&nbsp;came after three days of denial and after satellite evidence showed&nbsp;that missiles had hit the aircraft.</p>  <p>Rezaeifar did not respond to CBC's request for a comment. His name is used throughout the audio recording&nbsp;and CBC News has copies of messages sent from his Instagram account setting up the phone call.</p>  <h2>Victim's family member 'intimidated'</h2>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5521619.1585951192!/fileImage/httpImage/image.jpeg_gen/derivatives/original_300/javad-soleimani.jpeg 300w,https://i.cbc.ca/1.5521619.1585951192!/fileImage/httpImage/image.jpeg_gen/derivatives/original_460/javad-soleimani.jpeg 460w,https://i.cbc.ca/1.5521619.1585951192!/fileImage/httpImage/image.jpeg_gen/derivatives/original_620/javad-soleimani.jpeg 620w,https://i.cbc.ca/1.5521619.1585951192!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/javad-soleimani.jpeg 780w,https://i.cbc.ca/1.5521619.1585951192!/fileImage/httpImage/image.jpeg_gen/derivatives/original_1180/javad-soleimani.jpeg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5521619.1585951192!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/javad-soleimani.jpeg"></p></div><figcaption>Javad Soleimani's wife Elnaz Nabiy died in the destruction of Flight PS752.<!-- --> <!-- -->(Supplied)</figcaption></figure></span></p>  <p>Javad Soleimani (no relation to Gen.&nbsp;Qasem Soleimani) was&nbsp;the Edmonton PhD student on the other end of the call with Rezaeifar. He said&nbsp;Rezaeifar&nbsp;pressured him&nbsp;to remove an&nbsp;Instagram&nbsp;post critical of the Iranian regime. Soleimani has been an outspoken critic of Tehran since his wife Elnaz Nabiy died in the crash.</p>  <p>In that Instagram post, Soleimani wrote Iranians&nbsp;won't forget about the crimes the regime has committed against its own people.</p>  <p>"Please delete it from your Instagram," Rezaeifar tells&nbsp;Soleimani on the call. "Do you agree that out of 83 million people of Iran, only 10 or 12 people have hurt you? Why should those other 82 million people be insulted by this post?"</p>  <p>He then asks Soleimani if he thinks the Canadian government is more "benevolent" toward him.&nbsp;</p>  <p>"Are you certain that the whole Canadian government is good and uncorrupt?" asked Rezaeifar.</p>  <p>Two days later, Soleimani&nbsp;said,&nbsp;Iran's Ministry of Intelligence contacted his family members in Iran to exert more pressure on them about his behaviour on social media.&nbsp;</p>  <ul>   <li> <p><a href="https://www.cbc.ca/news/politics/iran-flight-752-ukraine-international-airlines-crash-1.5521377" target="_blank"><strong>Families of Flight 752 victims report threats, acts of intimidation — and blame Tehran</strong></a></p> </li>  </ul>  <p>"It's ridiculous," Soleimani told CBC News. "They just wanted to somehow threaten me to stop criticizing the regime on social media because I had many followers on Instagram.</p>  <p>"They tried to force me to be silent ... but honestly, I have nothing to lose. And I told him,&nbsp;I told him I have nothing to lose,&nbsp;so you cannot stop me by just threatening me by conversation over the phone."</p>  <h2>'Inappropriate' for investigator to pressure&nbsp;family member</h2>  <p>Juneau said it's "totally inappropriate" and "absurd" for the lead crash&nbsp;investigator&nbsp;to put pressure on&nbsp;a victim's family member in Canada. He also said it's not surprising.</p>  <p>"I did not expect the investigation to be independent and few serious analysts did," Juneau said. "This basically confirms it.</p>  <p>"It's not very smart. It's just not a good move."</p>  <p>He said the practice&nbsp;of the Islamic Republic is to exert psychological&nbsp;pressure,&nbsp;and often physical&nbsp;pressure, on anyone opposed to the regime,&nbsp;at home and abroad.</p>  <p>Juneau said he wants to know the extent of Rezaeifar's relationship with the IRGC and Iran's&nbsp;Ministry of Intelligence. He cautions the details of what Rezaeifar said on the recording might not be accurate, and might have been meant to exert pressure on Soleimani.</p>  <p>"Is it true?" said Juneau. "Is he boasting? Is he exaggerating some things to increase the level of intimidation towards family members? These are all questions that we don't know the answer to."</p>  <p>Syrine Khoury, press secretary to&nbsp;Foreign&nbsp;Affairs Minister&nbsp;François-Philippe Champagne, sent a written statement to CBC&nbsp;saying "interference with Canadian citizens is totally unacceptable, very troubling and won't be tolerated.</p>  <p>"The government of Canada denounces any and all attempts to coerce or pressure Canadians, especially those suffering the loss of a loved one," she added. "The government of Canada encourages anyone who feels threatened, unsafe or vulnerable to contact local law enforcement authorities."</p>  <p>Ralph Goodale, Canada's special adviser to the Trudeau government on the&nbsp;Flight 752 file, said the phone call&nbsp;"constitutes outrageous behaviour."</p>  <p>"It's wrong on every count of procedure, propriety, appropriateness. It's simply completely wrong," he said.&nbsp;</p>  <p>He added the International Coordination and Response Group formed by&nbsp;Canada, Ukraine, Sweden, Afghanistan and the United Kingdom to support victims' families will be investigating&nbsp;Rezaeifar's&nbsp;"interesting and provocative" comments about keeping the airspace open to see if there's truth to the remarks.&nbsp;</p>  <h2>Audio casts doubt on past reports</h2>  <p>Hamed Esmaeilion, the interim spokesperson for the association representing the families of the Canadian victims, said the recording raises serious concerns about the two reports Iran's Civil Aviation Organization has already published about Flight 752. Esmaeilion lost his wife, Parisa Eghbalian, and their nine-year-old daughter Reera in the destruction&nbsp;of Flight 752.</p>  <p>"I don't see any different between Rezaeifar and the new investigator," he said. "CAO is not independent. The whole organization is closely working with the IRGC."</p>  <p>Iran is expected to publish another report on the crash&nbsp;before heading to France on July 20 to download and analyze the plane's flight data recorders, according …</p></span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cbc.ca/news/politics/audio-recording-iran-lead-investigator-flight-ps752-1.5636450">https://www.cbc.ca/news/politics/audio-recording-iran-lead-investigator-flight-ps752-1.5636450</a></em></p>]]>
            </description>
            <link>https://www.cbc.ca/news/politics/audio-recording-iran-lead-investigator-flight-ps752-1.5636450</link>
            <guid isPermaLink="false">hacker-news-small-sites-23733910</guid>
            <pubDate>Sat, 04 Jul 2020 18:10:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Uber wants to cut costs by shifting its engineering to India]]>
            </title>
            <description>
<![CDATA[
Score 62 | Comments 103 (<a href="https://news.ycombinator.com/item?id=23731891">thread link</a>) | @SQL2219
<br/>
July 4, 2020 | https://www.businessinsider.in/business/news/uber-is-struggling-due-to-covid19-now-it-wants-to-cut-costs-by-shifting-its-engineering-to-india/articleshow/76768028.cms | <a href="https://web.archive.org/web/*/https://www.businessinsider.in/business/news/uber-is-struggling-due-to-covid19-now-it-wants-to-cut-costs-by-shifting-its-engineering-to-india/articleshow/76768028.cms">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="datatxt76768028read"><div data-den="denmark"><div><ul>
 <li><keyword keytype="person" smid="0" usetype="2" keywordseo="Uber-CEO" actualkeyword="Uber CEO">Uber CEO</keyword> Dara Khosrowshahi is eager to shift engineering innovation to India as it will be cheaper for the company, said a report by The Information.</li>
 <li>Globally, Uber lost $2.9 billion in the first three months of the year because of the coronavirus pandemic. </li>
 <li>Khosrowshahi had earlier said that the impact of COVID-19 would be drastic for the company. </li>
</ul><p>Ride hailing giant Uber has been severely impacted by the coronavirus pandemic across the world. Globally, Uber lost 
<a target="_blank" href="https://www.cbsnews.com/news/uber-earnings-q1-161-million-loss-ridership-falls-3-percent/" rel="nofollow">$2.9 billion</a> in the first three months of the year because of the coronavirus pandemic. 
</p><p>
And now, in a step to cut costs by shifting its engineering jobs to India, said a report by 
<a target="_blank" href="https://www.theinformation.com/articles/uber-ceo-wants-to-shift-more-engineering-jobs-to-india-sparking-internal-debate?shared=53eda2af81f50ffb" rel="nofollow">The Information.</a> According to the report, Uber CEO Dara Khosrowshahi is eager to shift engineering innovation to India as it will be cheaper for the company.
</p><keyword keytype="person" smid="0" usetype="2" keywordseo="Uber-India" actualkeyword="Uber India">Uber India</keyword><p> didn’t comment on the same. 
</p><p>
<span>Advertisement</span></p><hr><p>Khosrowshahi had earlier said that the impact of COVID-19 would be drastic for the company. 
</p><p>
    "I won't sugarcoat it. COVID-19 has had a dramatic impact on rides, with the business down globally around 80% in April,” he 
<a target="_blank" href="https://www.cbsnews.com/news/uber-earnings-q1-161-million-loss-ridership-falls-3-percent/" rel="nofollow">said</a> during the conference call for its earnings. 
</p><p>
Earlier, as a part of its massive global layoff of 6,700 employees, Uber India had laid off 600 employees in India. “The impact of Covid-19 and the unpredictable nature of the recovery has left Uber IndiaSA with no choice but to reduce the size of its workforce. Around 600 full time positions across driver and rider support, as well as other functions, are being impacted. These reductions are part of previously announced global job cuts this month,” Pradeep Parameswaran, President, Uber India and South Asia had said during the layoffs in May 2020. 
</p><p><span>Advertisement</span></p><hr><p>However, Uber’s wish to shift its engineering to India is not something new. During his visit to India in 2019, Khosrowshahi had said that India is going to be a big source of innovation for them. “One example is the Uber Lite app, was designed and built in India and moved to over 30 countries around the world,” he had said.
</p><p>
Manik Gupta who was then the Chief Product Officer of Uber had said that Uber is proud of the engineering and product talent based out of Bengaluru and Hyderabad. “We have a goal to double the headcount in these offices and have 1,000 people to build innovative products out of India,” he had said.
</p><p>
    SEE ALSO:
<br>
<a href="https://www.businessinsider.in/business/telecom/news/vodafone-idea-news-how-much-cash-does-vodafone-idea-need-to-survive/articleshow/76766763.cms">Only cash can save Vodafone Idea from a slow death⁠— and it needs a lot of it</a>
<br data-name="47" data-nm1="br"><span>Advertisement</span></p><hr>

<p><a href="https://www.businessinsider.in/business/news/reliance-jio-gets-investment-from-intel-capital/articleshow/76761995.cms">Reliance Jio gets ₹1894.5 crore from Intel Capital – the 12th investment in 11 weeks</a></p></div></div></div></div></div>]]>
            </description>
            <link>https://www.businessinsider.in/business/news/uber-is-struggling-due-to-covid19-now-it-wants-to-cut-costs-by-shifting-its-engineering-to-india/articleshow/76768028.cms</link>
            <guid isPermaLink="false">hacker-news-small-sites-23731891</guid>
            <pubDate>Sat, 04 Jul 2020 12:48:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Targeted MitM attacks using information leakage in SSH clients [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 83 (<a href="https://news.ycombinator.com/item?id=23731310">thread link</a>) | @based2
<br/>
July 4, 2020 | https://www.fzi.de/fileadmin/user_upload/2020-06-26-FSA-2020-2.pdf | <a href="https://web.archive.org/web/*/https://www.fzi.de/fileadmin/user_upload/2020-06-26-FSA-2020-2.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.fzi.de/fileadmin/user_upload/2020-06-26-FSA-2020-2.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-23731310</guid>
            <pubDate>Sat, 04 Jul 2020 10:01:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learn to Create and Flex Flexagons]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 14 (<a href="https://news.ycombinator.com/item?id=23730865">thread link</a>) | @bhy
<br/>
July 4, 2020 | http://loki3.com/flex/explore/ | <a href="https://web.archive.org/web/*/http://loki3.com/flex/explore/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            
            <h2>Learn to create and flex flexagons</h2>
            
          </div>
          <hr>
          <div>
            <div><p><img src="http://loki3.com/flex/explore/static/pinch-flex.jpg"></p><canvas width="300" height="100"></canvas>
            </div>
          </div>
          <a href="http://loki3.com/flex/explore/pinch-flex.html"><p> Chapter 1: Flexagon Intro </p><p> Exploring the hidden sides of hexaflexagons </p></a>
          <ul>
            <li>Introduces the <em>hexaflexagon</em>, with
              <!-- -->6
              <!-- --> equilateral triangles per side, and the <em>pinch flex</em>, a way of folding a flexagon to
              reveal previously hidden sides</li>
            <li>Let’s you explore how to visit every side of a flexagon</li>
            <li>Describes <em>flex notation</em>, which can be used to precisely describe a sequence of flexes</li>
            <li>Introduces the <em>Tuckerman Traverse</em> <!-- -->as a technique for visiting every side of a flexagon
              using the pinch flex</li>
            <li>Provides both a flexagon simulation and the unfolded strip for all hexaflexagons with
              <!-- -->3
              <!-- -->,
              <!-- -->4
              <!-- -->,
              <!-- -->5
              <!-- -->,
              <!-- -->6
              <!-- -->,
              <!-- -->7
              <!-- -->, and
              <!-- -->8
              <!-- --> sides so you can make your own</li>
            <li>Shows flexagons made up of different shapes of triangles, like using angles of
              <!-- -->3
              <!-- -->0
              <!-- -->-
              <!-- -->6
              <!-- -->0
              <!-- -->-
              <!-- -->9
              <!-- -->0
              <!-- --> or
              <!-- -->5
              <!-- -->4
              <!-- -->-
              <!-- -->5
              <!-- -->4
              <!-- -->-
              <!-- -->7
              <!-- -->2
              <!-- -->, and different numbers of triangles per side, like
              <!-- -->4
              <!-- -->,
              <!-- -->8
              <!-- -->,
              <!-- -->1
              <!-- -->0
              <!-- -->, or
              <!-- -->1
              <!-- -->2</li>
          </ul>
          <hr>
          <div>
            <div>
              <div><p><img src="http://loki3.com/flex/explore/static/F/F1.jpg" width="320" height="240"></p>
              </div>
            </div>
          </div>
          <a href="http://loki3.com/flex/explore/flexing.html"><p> Chapter 2: Flexing Flexagons </p><p> A short tour of different ways to flex a flexagon </p></a>
          <ul>
            <li>Describes a naming convention for a wide variety of triangle flexagons, like <em>pentaflexagon</em>
              <!-- -->and <em>silver octaflexagon</em></li>
            <li>Defines a <em>minimal flexagon</em> <!-- -->as the simplest flexagon that supports a given flex</li>
            <li>Demonstrates a sampling of interesting flexes on triangle flexagons: the pyramid shuffle, flip flex,
              tuck flex, v-flex, and silver tetra flex</li>
            <li>Shows that once you generalize the types of flexes you use, the concept of “sides” of a flexagon is no
              longer sufficient for understanding all the states you can explore</li>
          </ul>
          <hr>
          
          <a href="http://loki3.com/flex/explore/generators.html"><p> Chapter 3: Generating Sequences </p><p> Creating flexagons with flex sequences </p></a>
          <ul>
            <li>Summarizes the <em>flex notation</em> <!-- -->that was previously introduced for describing sequences of
              flexes</li>
            <li>Defines a <em>generating sequence</em> <!-- -->as a sequence of flexes used to create the structure
              necessary for performing them on a given flexagon</li>
            <li>Shows how different flexes move you around a pinch flex diagram</li>
            <li>Gives you an interactive tool for typing in generating sequences to see what pinch flex diagram is
              generated</li>
            <li>Allows you to type in generating sequences consisting of any of the flexes introduced so far on a wide
              variety of triangle flexagons, giving you the unfolded strip that can be used to construct a working
              flexagon</li>
          </ul>
          <hr>
          <div>
            <div>
              <div><p><img src="http://loki3.com/flex/explore/static/P4-4-4/1.jpg" width="320" height="240"></p>
              </div>
            </div>
          </div>
          <a href="http://loki3.com/flex/explore/pinch-variations.html"><p> Chapter 4: Pinch Flex Variations </p><p> Relatives of the pinch flex on different flexagons </p></a>
          <ul>
            <li>Describes a class of flexes that are related to the standard pinch flex</li>
            <li>Shows the minimal flexagon for a variety of flexes (e.g. P
              <!-- -->3
              <!-- -->3
              <!-- -->4
              <!-- --> and P
              <!-- -->3
              <!-- -->3
              <!-- -->3
              <!-- -->3
              <!-- -->) on several flexagons (e.g. the enneaflexagon and decaflexagon)</li>
            <li>Demonstrates generating sequences using these flexes to create interesting flexagons</li>
          </ul>
          <hr>
          <div>
            <p><img src="http://loki3.com/flex/explore/static/slot-example.jpg"></p>
            </div>
            <a href="http://loki3.com/flex/explore/slot-flexes.html"><p> Chapter 5: Slot Flexes </p><p> Flexes that involve sliding leaves through slots </p></a>
            <ul>
              <li>Describes a class of flexes called <em>slot flexes</em></li>
              <li>Demonstrates the related flexes called the <em>slot half</em> <!-- -->and <em>slot pocket</em></li>
              <li>Shows four <em>slot tuck flex</em> <!-- -->variants</li>
              <li>Finishes with the <em>slot triple pocket flex</em> <!-- -->on a pentaflexagon</li>
            </ul>
            <hr>
            <div>
              <p><em>pre</em>:
                <code>[[[1,2],3], 4, 5, [6,7], 8, 9]</code><br><em>post</em>:
                <code>[2, 4, 5, [6,7], 8, [-1,[9,-3]]]</code></p>
            </div>
            <a href="http://loki3.com/flex/explore/new-flexes.html"><p> Chapter 6: New flexes </p><p> How to define flexes with pat notation </p></a>
            <ul>
              <li>Describes using <em>pat notation</em> <!-- -->for the internal structure of a flexagon</li>
              <li>Shows how to define what a flex does by using pat notation to enumerate a flexagon’s structure before
                and after a flex</li>
              <li>Demonstrates using a flex definition to predict where you can do a flex and to predict exactly how a
                flexagon will change if you do the flex</li>
              <li>Provides definitions of the flexes discussed so far for hexaflexagons</li>
              <li>Lists additional details you need to fully understand when a flex can be performed</li>
              <li>Walks through an example of how to define a new flex and how to add it to the flexagon simulator</li>
            </ul>
            <hr>
            <div>
              <div>
                <ul>
                  <li><code>I ~= (Sh&gt;&gt;T'&gt;^T&lt;&lt;^) 2</code></li>
                  <li><code>I ~= (Ltb&gt;P&gt;P&gt;) 12</code></li>
                  <li><code>P ~= T&gt;&gt;&gt;V&gt;&gt;&gt;</code> so
                    <code>T' ~= &gt;&gt;&gt;V&gt;&gt;&gt;^P^</code></li>
                </ul>
              </div>
            </div>
            <a href="http://loki3.com/flex/explore/flex-sequences.html"><p> Chapter 7: Flex Sequences </p><p> Cycles, traversals, primes, and equalities </p></a>
            <ul>
              <li>Tools for finding cycles, flex sequences that end where they begin</li>
              <li>Examples of traversals, cycles that visit every state of a flexagon</li>
              <li><em>Prime flexes</em> <!-- -->that can’t be replaced by other flexes</li>
              <li>Sequences of flexes that are equal or almost equal to each other</li>
            </ul>
            <hr>
            <a href="http://loki3.com/flex/explore/playground.html"><p> Appendix: Flexagon Playground </p><p> General tools for exploring flexagons </p></a>
            <ul>
              <li>Tools for creating new flexagons from generating sequences or scripts and exploring them using a
                variety of flexes</li>
              <li>Get the unfolded strip for any of your creations so you can try out the physical version</li>
              <li>An explorer that can find all the states accessible with a specified set of flexes from any flexagon
              </li>
            </ul>
            <hr>
            <a href="http://loki3.com/flex/explore/flex-compendium.html"><p> Appendix: Flex Compendium </p><p> Details about flexes on triangle flexagons </p></a>
            <ul>
              <li>Reference guide to a wide variety of flexes on triangle flexagons</li>
              <li>Shows sample slides and sample strips to fold on various triangle flexagons</li>
              <li>Shows flex definitions in pat notation, etc.</li>
            </ul>
            <hr>
            <a href="http://loki3.com/flex/explore/references.html"><p> Flexagon References </p><p> Sources for additional flexagon information </p></a>
            <ul>
              <li>Bibliography for papers and publications</li>
              <li>Additional references</li>
            </ul>
            <hr>
            <p>For additional information, see <a href="http://loki3.com/flex/">loki3.com</a>.</p>
          </div><div>
              <p>Copyright © 2018-2019 Scott Sherman</p>
            </div></div>]]>
            </description>
            <link>http://loki3.com/flex/explore/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23730865</guid>
            <pubDate>Sat, 04 Jul 2020 07:57:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A look at the Gemini protocol: a brutally simple alternative to the web]]>
            </title>
            <description>
<![CDATA[
Score 409 | Comments 331 (<a href="https://news.ycombinator.com/item?id=23730408">thread link</a>) | @flatlanderwoman
<br/>
July 3, 2020 | https://toffelblog.xyz/blog/gemini-overview/ | <a href="https://web.archive.org/web/*/https://toffelblog.xyz/blog/gemini-overview/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<article>

<p>Published on <b>2020-06-18</b></p>
<p>I have really come to hate the World Wide Web. It is bloated at every level! Websites themselves are doubling in size at an alarming rate. The web standards are expanding at an alarming rate. Trying to build a web browser that works with the modern web from scratch, would take the manpower equivalent to the Snowy Mountains Scheme or the Manhattan Project (no hyperbole).</p>
<p>The state of the web has lead me to only one conclusion: It is broken beyond repair.</p>
<p>I don't think the web can replaced, but we can still look for simpler (open standards) alternatives to move our works to. Gemini is one of these alternatives that I have taken interest in recently. A fairly recent protocol, created in 2019, Gemini defines a markdown-inspired document format, and it's own protocol to serve these documents, or other files.</p>
<p>If you a looking for a 1:1 clone for the early web, you will probably be disappointed. Gemini takes way more design hints from <a href="https://en.wikipedia.org/wiki/Gopher_(protocol)">Gopher</a> then it ever will from the web. Gopher (widely considered a predecessor to the web) is built on the idea of text only documents. Gemini expands on this idea by offering basic formatting, and fixing drawbacks unaccounted for in the original Gopher protocol.</p>
<p>Gemini shys away from many standard features of the web. Although there may be plenty of benefits that one accustomed to web wouldn't be able to identify. For example the lack of styling sheets may mean that Gemini sites look plain, however this allows your readers to decide on a colour scheme which suits them best. A night reader would prefer a dark theme, someone with a vision impairment will prefer high contrast, the list goes on.</p>
<p>Gemini offers no in-line image support no client-side scripting (such as Javascript). But server-side scripting (CGI) can work, so you could expect Gemini to be a valid interface for some online services.</p>
<p>The Gemini transport protocol is unsuitable for the transfer of large files, since it misses many features that protocols such as FTP or HTTP use to recover from network instability.</p>
<p>The internet protocols of old aren't encrypted by default, since security wasn't seen as being important back in the late-80's/early-90's. Thankfully Gemini, being a recent protocol, mandates the use of TLS. There is no unencrypted version of Gemini available.</p>
<p>I could totally see Gemini being used as an alternative particularly for the non-commercial individuals who use text as a primary medium. Blogs, poems, recipes, tutorials are perfect for the Gemini format. There are always ways around the lack of media functionality, since although Gemini lacks in-line images, you can still use in-line links to images. As time goes on I could imagine some Gemini browsers providing a fancy image viewer for links to images.</p>
<p>Now, what does Gemini currently have to offer? The best way to find out is to head over to the official site: <code>gemini.circumlunar.space</code> in your Gemini browser. Here you can find two search engines dedicated to finding pages based on what you search for. And two aggregators which acts as a news feed of Gemini posts.</p>
<p>Personally I enjoy scrolling through the aggregators and seeing what people are blogging about, I find interesting stuff frequently.</p>
<p>From a server management perspective, Gemini is just a diet-Web. You buy a domain name (using the web), obtain or rent a server, install a Gemini server, and start serving your site. In order to post to Gemini you either need to self-host, or take advantage of Gemini hosting which is currently on offer by circumlunar.space. No fancy P2P or Blockchain solution here, just a good old client/server model.</p>
<p>I will no doubt keep an eye on Gemini, and serving my blog posts to Gemini should be easy since they are already in markdown. No promises yet though.</p>
<p>I will leave you with the following quote and some interesting links I found.</p>
<blockquote>
<p>"When I picture it in my head I think of the early web as more of a library. Over time it has transitioned into a shopping mall." - chris_f (Hacker News comments)</p>
</blockquote>

<ul>
<li><a href="https://gemini.circumlunar.space/">(www) Project Gemini</a> - The official website for the Gemini project. FAQ is recommended reading. (<a href="https://news.ycombinator.com/item?id=23042424">hn thread</a>)</li>
<li><a href="gemini://gemini.circumlunar.space/">(gmi) Project Gemini</a> - The defacto Gemini main page.</li>
<li><a href="gemini://gemini.circumlunar.space/software/">(gmi) Gemini software</a> - The official page on Gemini lists out some clients and servers.</li>
<li><a href="gemini://gempaper.strangled.net/mirrorlist/">(gmi) List of services with a gemini mirror</a> - Includes Wikipedia, YouTube and Lobste.rs.</li>
</ul>

<ul>
<li><a href="https://portal.mozz.us/gemini/gemini.circumlunar.space/">portal.mozz.us</a> - A Gemini gateway for Web users.</li>
<li><a href="https://sr.ht/~julienxx/Castor/">Castor</a> - A browser for Gemini, Gopher and Finger. Written in Rust and GTK. Still early days, but probably the most normie friendly. (<a href="https://news.ycombinator.com/item?id=23161922">hn thread</a>)</li>
<li><a href="https://framagit.org/waweic/gemini-client">Android</a> - Barebones Android client.</li>
<li><a href="https://www.marmaladefoo.com/pages/geminaut">GemiNaut</a> - GUI client for Windows users.</li>
<li><a href="https://github.com/makeworld-the-better-one/md2gemini">md2gemini</a> - Python library that converts markdown to gemini.</li>
</ul>

<p><a href="https://news.ycombinator.com/item?id=23730408">Comments on Hacker News</a></p>
<p><a href="https://www.reddit.com/r/coding/comments/hl6qfv/a_look_at_the_gemini_protocol_a_brutally_simple/">Comments on reddit r/coding</a></p>

</article>

    </div></div>]]>
            </description>
            <link>https://toffelblog.xyz/blog/gemini-overview/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23730408</guid>
            <pubDate>Sat, 04 Jul 2020 05:44:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Closos: Specification of a Lisp operating system (2013) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 21 (<a href="https://news.ycombinator.com/item?id=23730107">thread link</a>) | @ska80
<br/>
July 3, 2020 | http://metamodular.com/closos.pdf | <a href="https://web.archive.org/web/*/http://metamodular.com/closos.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>û¨&lt;“
endstream
endobj
20 0 obj
&lt;&lt;
/Length 662       
/Filter /FlateDecode
&gt;&gt;
stream
xÚåVMSÛ0½çWø(l´ú´Ž|=@§¸'Úƒ“ˆÄƒ±SÛaÊ¿ïÊ²ÓJÓ2Ó“eKZï{zoW,ZD,:�d“½Ð°Ä2Qv–¤ZGdb…ˆ²ytEŠ;ú5û°wÂíöJÁÒDp��úE‡çÙñyvé—NØðƒ½Û{âq£Ò°UÑØrFNófš/�E
dV—¥›uE]Ñ˜ƒ5)á&amp;d±›/è„ˆb‰’vˆ—���&amp;gå†tM�A™¯‡€B’ÐX1öÞÛdGWÈ‚Ð„S fw†=$ˆ}ôü¸&amp;î–�ËçÈŒ²Øœƒàã9Ô
�¥¶o¶"&lt;ý%f5BäÓ²žæe@·¶¡j-ßËy?þÈs£E¸L„Õ�,w¸t3ŠÐoVH�º¨(²ÒÕ
#Yj‰€çºNo\—ÑTŒÒZVÅ·õÀo�Ó¤jWEã¥Mý’û0&gt;þä_..114û¿gþšç@Å“¦ÒS=�êñçhÎÖFúz„¤—õ"n»KÔzÃÏ&amp;d9±½o;w‹[¹…×Ñ‘P/Ð‘	&nbsp;MÐÑ‘»+fCåž7Ø/0a×ød%€"òÙ"2ÿwé–¨Žß«ÌüTYÑ+ì†ò”¸®
*yÿÔÀK$+ÿDÖXô�Šö&amp;0ÔËÓSæå©Ì›�÷
†–ü¡¡U�X¥øŽ¡…ÁoCc8CÂ€+²¿
-ÁUst¶w2×À‰TÁÉ»¥�í6—}l&nbsp;Ü’Ïíx—ËÛ&gt;‚VÊùìkÜ¾¯F§äSñ'Võ}ÉÆ|8ËºŸ˜­o]åÏ´Ÿ¯}÷Ëü�¦ßc5#ÍZš—ÙƒbZõ¢ÉWK_Xï=•Iëc=dÿ¬š»ï¾MX©‰b;¿ŸÇÙäi€{
endstream
endobj
25 0 obj
&lt;&lt;
/Length 1006      
/Filter /FlateDecode
&gt;&gt;
stream
xÚ�VI¯ä4¾¿_‘£#ÑÁKœEœ1h0HÓ$†ƒ_âî¥ãVìL3ÿž*—“×óÔ#qr-®ª¯WÂ³sÆ³Ÿx:¿;&gt;}ýFÕ™äEUI�OYÛR¨Ls^´ZdÇ&gt;û‹}?˜k°K~Pºd"ÿûøXñL–EÝÔ­xvÐM¡DMoç\¶,,.?ÀÙ¯]ÝœìÚL”…*+™ìªª�M²…ÈBpÎþL€xUÅ5ûyôW¢žœÙÅ„q&gt;“ÌòÁ^È`ôIT™€$x»!TeÁyK‘¾Íe#’W¤ÞÁ)ë¯({Ÿ¼"ý�kŽ·ß½'þä$8ó@f.l	pI�@Äsv�ˆVŸ(²Á.Ä¸ÏÃpD*T¡Ë„tK­T’…X¤&nbsp;#hÙ0;û¤st&gt;“˜˜Û2†`gbÆt¦¤�Â¤ÌöV$ÎÃë`·&lt;ºuêD0tœ]º˜Mÿ(�0ÄºÊ²�@â¼�Nù¡ŽHD®„foòF1¬.^�Jq]JØù$q§í
©ž@tëR"|&lt;�ª·öÅÐÉÄLÞ%*�r©®ÿ£#ºÚŠ¤k¶zë7™%"”#á§ð&gt;æØqºB@?Ž‹›#èšR˜äa‘“›IA÷åòd:K/ckœ€ )¶Õât&nbsp;2¢V•ÆD–ôôæžDIW¥Ô¡¬ª._ÔÉo@·˜9Õ¦t»ˆÚ?ðùÅâ›JÊ‚ërß*ÕÊÔpÕŠ‡
/ù–Ý&gt;c½wÎ»K¢ÆÞšäÆ%Ç7Z: 1$Ø&amp;éËcÛ9ŠhÆ9åÿ¨h¤%‡²ûCB¦?p¥ì²O
O‹»eÿ}ÚwÀ½Ò(£ÊylŽnR�AŒ°°"tgíä€œŠYî†�pßç|ÿð•¬[¬	¶/¾°ÖË¦¨J½­u™Öúo‹{ž$ÍÂmÄÕƒûû.»}›§5®t&amp;|‹´|YãPîÝ3}2¤fÇ8¤µÆÖtöˆÁ-‡•¼•üÞ›(³hÁkôv]rxO.‡òuHYÿðSrØÌà3TÈí#ù‹‹^5ŒV:¶6êdð™ª=Å¸aIòº‰h»¥³X"z»¤)îI@s�Ôï¿¾ý“¨Û0nÍ�,ÒÆ?jëþUPœ–±&lt;½qˆ¶æw±Új
ÂÎ]®+,Ol\&gt;huYBƒ7¶ðQ¼í¢¹ÇP2˜žî:h'.Ÿîá¦éñ3M8s0°}¿`S"ã¯°Ø¾!:ìŸœÓ;
Ä%¶µñ#ç
lÁg¤&amp;KbwX=JpA&amp;«ûý…%)+vŒŸ§ÁMi¹ìý¯*�,dÙÀöªU¥IýìÊÇ§ÿ-¢|
endstream
endobj
31 0 obj
&lt;&lt;
/Length 2378      
/Filter /FlateDecode
&gt;&gt;
stream
xÚ�É’Û¸õî¯Ð‘ªiÑ$¸çæ8žÄ©ÏÔ”\•ªLh’XÃEC�îî|}ÞŠTÓv’Þ{Á·/vç]°ûë›?ß¼ý1LwaàAîŽ§]øyšîÒ0ö‹(Ú«Ý?=µÿ×ñïoTÅò&nbsp;
R_…p
yÿ·w¿?üº?D‘òBÈ²Èûøéøë^eÞÏùüþøñçOxÍ›@þ}ÿãƒ\xˆ?Kr¾v¼Ôvˆ“À»ýccÚÀ²è†Éå`ôØr¨?ñúùÓÇÞ´?„ž5c·Ïú®4×qý•f1£x%f^øq‘;9¯Ã&gt;	½~�w d¬å¯ÖÂDÊÏ²Â}„ê(2ï3Ñèálð#U)_eÙîF~|Z_¯M]ê±î;Pg{O{•{Ú
2ÔãhdÇö¼Ž=2TË
ÚÚŽ(8¢("}`¾àefÐ�ZÝ4f`t|tkAÉQ¦&lt;£K&lt;|!^ƒ5“t¡J¼§K-‡tÇ@íÖÑ2Ðã©'¡êªPs„Ø«.��º	¼ãÅX#ä™3uãL~ŠNÕOMÅ›eß¶H™KNf´CU¢ÕƒÈ{ÄS/_4"_ö¡W£‹tg&amp;÷�œ]Y!Xñ²7šçQ&nbsp;ž7ILú~¯ÓÈD;‚sÂZFOý Ww=8â°¥Öµí‹„~€+ÜT�ŽRˆ®#Å[3^`ä¨˜@V�•5’{S·ºŒô¦åãYkPö†¼¤ˆ· 
ó¥/_ë+ùý–×«rFêNêN¸Òâì1E˜¬¶æ�É@,¢%Cñª�mÏàB#–)·p¤D'©äèfü‰ŸÇ¯ØoênS„(ñ‹T-7Žrï�u?&nbsp;e$7ºWÂ£yÖíµq2€ZO† ±Å¡Ÿ‡áZpŽMq’*	Ðµ¸†iÁ¡\qŒ†œÂ@IË$
‘ÈÃhoèi¯šÊ=›n–ï™yòˆµ‘©ú»R9µ�^·ô· 	(F¢DÜ
Væ	€–åèÎl@aæ’uÊHRÉÕ´1�ÊØúÜ™
~ˆå$	gÇ‡Íù»Ì?»èj#òQà,w©‘B˜*Õ
Ö
`£†ÚbùJ(^Eþ‡KÿóÀÂÙAÆÇfË‹ÂÔ/n‡P´e|›§³é 1ß¸'È“ìEa—ªøµ9¾Î,8s^,\¿ü¯y�£y�Ã¯ˆp×¼"aìŒ�è·ùR�(Ô-&amp;ÍÝÿÆ&amp;ünXË~¬”TU(¿ÈÔZMì&nbsp;�;Gt±	ð~Qv.LÇ �›æÄßñ‡¬ð3°Šó‡¡?�6Scî‡yºÆ~ƒÄ„UùA"ýÖ‘ÙÅÐo¡šlo"ÈCRæaC‚&nbsp;[Ð´åD&nbsp;Pý€O‡©cB/kUW¼Åˆ)Xs¨ˆ˜xKwR0iMÛ/‡­¤ÐêNŸ
¥
ø;Ú$ÀÒ#*ð§Ÿ&gt;K #…äC@?ZNþˆ°5ù‘À7·FK~"””ò:ú¹Õû•R!g”úIò8‚{îÐ)xK˜Æ©y$gƒC˜lpëz!²Ô6”p
«nÌ|\É°]Å!´^.xº˜¯•î&nbsp;ÃÍbT¹Í`Üøî¬ o�JZ`Çq‘±[ ™|Ä4
¨?Ï!�îó“=±}3ËM}umçÿK¿�„®„ß›ÝHëj‹ùA%Þ2PÖu8Ïý$�Çû5³*ñ*TfÿÆë'èä±™ïƒæ"€ñ"I©xýÉÜ/ãöË¸r !T�’]£ÔÏâ»Në
dâÀÝÂ­“í¡ÍÁý¿µ·°m¯xX£§Ùy‡{c Tµå”Ô[Ý<puàî)•Î0à†ß]tâgØ"�ûá÷‡mŸrÕš›(ðÌéqv\@+uk6*ñÊÛ�àÊî�àxÿÙùÀ½¨d‡èj ]="" ¯?×snÁŸ¹ž¡nšÉr‹Ê-Åëxæ^¶ëªm="" ª�s·àaxtd¢ÄöÉç="" ,éÊ9o˜áË="" Ýx="">Ã©€r†[÷*BÊÍ—‚dN¯Ã0]a´ÚTwÛß²(XP�¡Nä-=OŠL‰*$H¢a44·:¾J;òM§è#Š=wÜA$ø»ÆV‚lp9‹¥@1M¯©^ ,"öŒ	£7ËÊ,i‰ÃÌûe6‡\7GœÜGß7”bŸ�&nbsp;�üZwç	2ü&amp;“vZö‰ÎßJñ�ø?e/1	°ëF|ò60/.©OŒƒþ^f—Gß¼]ÁeFœ&gt;å0„s�Ü;0bžM9ñ��¾Û}GŠgê¡yaŠ;üŽ²Õxß}½l&gt;[îP
®ùË
@›‡Ë8�‹ùm$vã/…uº�Å%µØüD‘¹±ù '‚Â�$?á)“Úƒ�®{5Á‚˜nNB~{ÈÜÛÉÚê
B3ñDÓèº{$Š¢»ž0J¡Æl]d®™
³o¾,1è
Š«~øîk51†¹gF*=j†`šŸ¤œ[Ü…mC™¬aÆ~Lö‚Qw»PŸ®ëf°ï}-ÙY4Bœ‘Í8gÆÈY'ÉŸpØï€LF�SJãqÊÏ-Ãd*Xùåc$žf”aNÄ,F"ÄØú¹X¤B¶ñ•¬«¶Üœº½&lt;]°�Ø-ˆ@/×¼ü›Þâr	`®z°Ž(»wGx…á¢–næÖèl„fã¸ßb�l±É¹´?Y™]Š{Qázì1‚ÔûD
"î»æ…ÏR{DÆsÐÔ¡pÙ[Y™C9ÄTZÖ·CÔM¦Ì@O[ø~tâ•;¶&nbsp;àu0ã@6G#…÷ˆoL¸Aï}Xêä{(r=CÒÀ�IK&amp;œ†¾•C¼ØþD¯\³ÙÛ›î¢˜�YQÊ“œP;C m~óPÞ—Ú&lt;=ð»œøn©'~ÓSâk˜g�¹sï'0U¶Z®Z¸?`·l‹¯B’ëJÌV±Ùœù©*@Ö¡ÊE9¸šKDšQkÀ`õHœo™@–ˆ¯&nbsp;.iß%Eÿ›	ë Éª�vú¿2Ö»NXF’¼¬u³Õb§èa˜{z©®xÐx5Žþ&lt;³Þ•…\ÊBäÞš(Sß×š¤î3„è·v«S•Å^‰k@ïn–¡í�~‘¨•·/N ÿÑë2&lt;³&amp;ÙÜù2¦å\S?z¨�&nbsp;Ò($ýÝõÝŸ°×ÀA;Ë­Žoþù2¥&gt;
endstream
endobj
36 0 obj
&lt;&lt;
/Length 2370      
/Filter /FlateDecode
&gt;&gt;
stream
xÚuXYsã6~÷¯Ð#U5æx=n²“Œ·’Lv­ªl*ÉDBË&lt;öxýöJ²9/Ðh�&gt;¿f´9n¢Í�wßíî&gt;þ…ET¨Íî°)Š0Vz“*ZovÕæ�@…q¸½Ï2üº½WÁ¶q|ùî§O??nïµŽƒß¶*Ø}æÉ§ÿ&gt;&lt;î~ù‘g�¿?î�ï¯Ý¿&gt;þ&nbsp;Òë›tjBSd ]¢‘ç.É6°;ã8�AfIÎL¶Ã»_·÷F%A?TnÁÒîé[7Õ­qÉFIdGf«;þŸNŽî«mÏ�cÎþpY%!�º2ÖE˜Å^Æiè‡Õ·Äa¦”g+QÈ“­;C½½¼«ð€Í}œÅ¡Ñjs¯t˜˜‚wnœ›	8Í’Á¿å¿²oÏód§ºï˜P¹óNÁG¦Ô²2Îû‰Þã—-°XÒšpùý,P�8Œƒ=œ˜nB~Úé\GâF·rÒ^­2’Š¤è;à„ÿiÄ…”¤G†§W
XÌè,x S‚FéÊj.ëîÈœ–OèÜËÍyÐƒ›uè�Ž÷òrkÅ'pÓàþžëÁ]Ëœ°È=H2°S®­U0õÌ°gÅò¤íiVÕxóŸ‘6 …«pzK†`LpÔ4L_ökÍ³?‚
›±çÑ‰¤}ÞòùHÁu…«zÞ&amp;‰8ùd�~÷têçã	=='½ñl‡©.çÆP¡5ý*K&nbsp;T8ÿ¹®üÁì³ÚÜøl…q¾øöøÚÝó{üÂ�ÅËÓ°Mh¬þûàJ¤q¨ã%&nbsp;['&amp;ïê±UÆ©×ë„Ò½à"º¬$(òq°íø§hÓ'vXª'þg�
¯<sv” ù0¨[1(¿dºvƒóÅáÜð±ì»qnÙ="" øoú†="" ã4Ìå4ct†Þ£â8ŒÉ‹(™�¸ùßv2çcì›ø@ì÷£À¾i="" 1¬="" ÚÌnËúoûq(“û*#vËÝ?Ée½lèÅÓ{¹k|'‡Êüvè�="" ¥+Ý“="" 89%ÍÉ¦^k¥g^¨g¿âfÑ1’¬°œŽÄšbô(‹r™„ì`&€ŽspÖÒor¿&;¿Ø#0 ëc‡¾Žõ±íhxa‡Á="" É8e?Áÿ%¸qâŸ“Ës€ä�akš(æîÐÓl�²“cbçp55yòëš¨ÕìÄÄ½˜ó$„¦nkº="" '¶ªÒ1™ílka¢ô="" $÷µf…"…ë€ffô~³¯Š,1Çf¹8¨¹8(¦¡÷©ñ�4*a%e¤jv²tc_2o³€*b­Ì%@öâˆÃ“ã�”Éƒ~?a5="" ŸÅ©¤aaè‹ýam="" •3�—f‡‰©œ‡zz]Ç¡ÎrÏmá“Ÿ)±+="" r@e�¤^¨°�cÞÕžÏm]jüÆy†¾Å‘ybžÕ="" ¶i^™="" Œ,¶,!#3ßeu)="" ‡w’idlÜbeÆbÁ™¬œuàÖ²p][ñÀ+aÁ’ÂpéÐy="" eÒ6Þus�*Ñ]:8f°~0:Éº¶h ¬miëÈòÔüÆœ´s.‘ëäÏ!û¼½Ò="" zæŠ…šÎõuˆàagÒh‰5xäõªcsñxðóbrgnpú@ñ4ŠÆ·Œ–i7®j�­§="" m’|…n·Ý·c¿ßwòÕòk¿�-[="" h£="" Ý„¹Ê="" °�…Š¨Åi šx“…ú!i¶Ôaá‘³z�<t—›$daƒ\”&ìkyy…Š”k�vœvqy°dÇ}#Á¤rh±¾i:¤Ò4¾˜—&="" ×¿8ua1="" àúžt39á¢="" €ÿÊn3�*‚z•œxtrfp$€¼öv'û"·_kí±«§¹rk¶å¼l®Ý³8ÿ‘µç¤È="" 6Ó&'w¬é¦ëª¿l<ØzÀ| m� ;j*g*ßî(­@‡son;äp–eqkysoÑšn‚Ý‰ð—Š�n1xãÙž|›Ž�cùˆ="" ‚ÿèþ†›†�="" ÐhË~|ä!Ëkgúˆ#†Á•="fÞ•«ëÛa" „úÚ;®1j�}="" ö2¿¡poäj�™="" oœg!_l¡s©›š(økËz²n®ãfci›·yneh¸ä'd‘Î{w¤¾¨a#ôo8¤+ß¤h^atÃcfßÿôåñË#�'ûd‰yä)å›t’æ›Û�Ó©fï]ú:¯j="ÞCîˆ¸ï!RñÜ7Š“ÇÎð”KÍÈM“ï¤D½Ê«×/³?3}±¢“Un" Ù¹tÖŠd9\]Ô="" lh6="" ªv’Ä˜Üà!&="" Ót]="">bÄð­Âçbe(·pÆ	�¹¤òeŸ‰¸Ç¢×ò&amp;¦ñ"„nŒ�þÄ íÈÒ½ä@óÀUºOé,¥¾J^Ã9y¦Eî‘Ë	„F¤cé·&lt;Õå6�ØAÉ“+ÉÁ
ä¶5lea”.Ž¼Î&nbsp;×
"’õƒË'€Þ¢$ÌRý¦Rƒ“Öxui—–œÛ2öžv¶uÍëêuf:]{"¤¶�¼rå5	è'/üfo¯ˆUK—+jZS�sm.šáGkè ¡©¿©Ú;�{ß\T„*1ßzÊ*454ÚË¨ÅBœÈ2*&amp;K®:µÌ—A"w²eo©=	KïY¨'c$˜l	\#b�øt†tãÍç·O»»¿ïð{V´Qþ“&nbsp;ŠsÒÙ¦lïþø+ÚT°Ù"Ôà¡/ÄÙn”’¢9›ÍãÝ¿ùû¢É6	 €à¨eÐÔ€À,YœÜ`£�ÀvCªe"2É£â¢uªjÏ5tcäö·ý“os#ì›ñ‰�&amp;x°…j	:}Áy¾¤V˜�
Î¥ÀGi?5Érël÷®n7ÍõU¯’Ë9‚àœ™À'7z¾]ç�Œ‰Ò0É1œÀyãåFLÜoJJ5òEÍøŠe¡•Èx‚nÒ¤€—Áç-ÊÏáÏâÉH$9R&gt;žOJ±y–[‘Ë?½¹—vÔB}8é�¿pâ–Þï^¿†öW/üÇÏÿÄÂ_óôž”ùPH%§¬¥Ÿt
œ
4ö3šœI{‚õ¤q˜±Æayá7£—#MJ°-è¬&nbsp;êš˜Å34õ©Š`I$_�^w%~\�K¨á¦ƒÝÚ3š"¼‰¦·ÿ]ÿl€W
endstream
endobj
43 0 obj
&lt;&lt;
/Length 2250      
/Filter /FlateDecode
&gt;&gt;
stream
xÚ�Ù’ã¶ñ}¾B�TÕ
C&lt;ý¶&gt;6ÞT²v¼rÅUŽ+…•0#Öò�	pfå¯w7ºAQc®]É4�¾»Átó¸I7»ûrwÿF™Š:­åfÿ°)SQÅ¦�™¨µÞì�›Ÿ“lûËþï÷oT½DTi!”2å«o_¿ÿæ‡íNk•H±Ý•¥NÞ¾Ûÿ°UeòÝ×?~µûÝ;$s—òÝ//Þ1Á�NE™WDö0ô®9ÚÑ·;U—‰¡ái›ç‰qžVMw‡­ª—Ùn+Ûã”÷‡žÆÿ¤:7~žg­¥¹»8o;Lg:M¾ðè3Ò³Oá;¾±Ò.dü¡ã“fò§at¤žLmJQ—i�F¹�õF‹:¯IEX/„f¼�,Ež²ÐöÓ¹5M·ªªNº!ÈY‚8§íN&amp;MÿèhmFd£¬’~ð&lt;1~MÛ^hyjìhÆ
qjðìTcZTåÉk¢B;v!Ó…a|4}ó›ñ
(øËí¤yÆB�ê‘Ëdp˜ÊP%=Ž0çTò¡AZ¸3Œ`Hš~‡¬GÎžƒÑlO;½é,Z¢ÖUòoà)ˆlý‰Nèˆv…ÃÔ¡¥+´4‚føoÁeÈ,r©pæ?'öÿÑ¸óý÷ãð8šÎÝ«.”úÞùéáaÍZRçBÕYôùWk”¢ÔuÄ˜I‡‹�Oÿš¾¾Ò‡Ô&nbsp;7¥nçO`~ZÚÖÙ`4”Ðª¸µLÛ|Î»Æ£¬EU«x�BžfN�ÇÀìÿÀ$†<j^k™ln"ÿc`ðk„Ã¸do½¡�$�Ì! ªdq"Í9ëìo!ÊtÑý¡…°‹çÃ@ÕàÈÄ�Î–ÜÕ…È®â(_“¢•."^ãˆ¸?q~�y<]w†="">Ì
Îþ:Ùþ`	mx è”êâ­#pdUÁÇü8 Hm”ò,¤ÙÓíOáÖB&amp;­	Áû‘Vág´D!=ð'þøîíO4;7¤tÞè� ÛÒšræx¡¥½¶ÅÜSæ1Nfr|¼¡p³cg��ñv-)@6=£;€^zE�Ž;Mlžšã„Ù×nxðsâ
™
¡‡!$ÌáœÒ™ðÁô×Ê„Ñ�¹–�#�ü€cŠfÃeÌûš­3L~)çJ™¦qÁs!âTV°r³2­›Z�iTfÉ—“§íà2°kZ7Ð¬³¦wñ¬ám$‰#§CÜ|F‚¡¸­ýD‹£ñ†ögƒ»5u	•Žº¦…óC(šZeáF„þq2+6ÝÂÒ¡”&gt;TNŒg(Tk°*\ï´P-“[»×� 2ÈÓÑå:8L9ãæn$=&lt;¬‰õïÃP=çªN^÷H}ôdûX™’&amp;n¢â†F#ˆ!ÔE¤ ´·öèh'0&nbsp;+ó°è¨ªCá
i¾(T~<o(™Á§…j¹[µÔÌ˜9vhkr¼2�&˜f«…Ãâlfn‡¹ rò­b‘&eâñ8Óö»˜km7Œœˆo³äÌ|#%´&¹âš˜a×'9ïj¡„"ÜàëÆù¦?„ò¿Ó”4ä­Æd¤) aç<6�="" Â‚Ø†‰³ÐÂgxgl­$d`bÕ¢”%÷“Ó8Î¥]ÕylÒp‘�ç©á�Ör#˜qÁähÝg?œ="" ƒj‚Œåà”¤ýÎpq="" †€ìr="">t#µ…Vu5Çåj¡’<z2w7*�%«¤Ò :zª’whÀ="" è="" �jo="" 9wÉ8™="" @%w‰â £zµÊ÷äÉ;�õÂrÂ^„;:àà¯•="„ˆ¶W/³�¡usp_¼hö¡È-Ëo!t‘Aû«D" �kyþé�¦×Þ**2d�ó°[m="" r1}zèh×­evÎÝ="" æøªxt¢="ÝÁEéúRç»Ã±Ö‡@…ù€ªÂD�§�»&nbsp;f^sWëkzî" t©ojÒÕ™@Õ="" â›º="" •´ðvªø+q§ ãÊŠÿkóïŒÁ�ùºî%¼ìê2?os`~_±½_3av‹"«&(àyö="">4ÈP5DésÓ¶#Ç§h5gny0‚†%#vš…(du	Ü•^x¼UëlMƒ`Mo¡ÐJK/¦æ-++N¡2QþDàa˜Îø4	jq¢5+ëÛ¢=lû‡J2-æ§Ó@€`…Æò™Ãh��³ý¬!2C|B`Z\M˜­àI8uD#¤VÜŸ9…–�AÄ`|©â€eÅvàù¹®Íç
#5|¥¡€_3þ;²EŒ¤ÙòÏù\­D–æÑ‰Ð;V<m¥bÊùÆm÷6ˆ.Ì©†Ãuìr0»mx´aøÀsx0Ž÷pa8Þúyi~fgßÇu"gíða Ðß‹g·_3qårñ¦y{÷(müg%Ö2jŒ}ÈhŸšar1éÏØu›¶b|âñ#œe‘ä–y="" bfƒÊÕ‹—v?kÿúzct\ðv’¥Õ‘i¶û.f‘,c+æh="" �ÌfÑßdÉþ="" �y)£‘3="" $tü®àwuqÍ³�v\}á,k¸¼f@˜‡tŽ#w_ zm}€="">ÌÄf*ç&amp;4—±VËS¡�É›m¥ÂÓáö“Á‡Að‘âŠþÏ©ýLçrGšq×„óJú‘€†÷˜µ°·øSún‡ÈN³ð`ød�;×üf	ºLqƒo\\ÄÏSþè8ûÔPöÇç¼ +@ç®�£q5c!~x¼áÖ#ê(ËRjn2Y/èBêm#ÉÐçàöÃö!ki'^KéøB@l¯—…ò›ýÝ¯wNéFÆ_§Ðiä›Cw÷ó/éæ ©ÐuµyhÝFf¥¨ü÷ÐnÞßý‹~Âfå”²,0H+-ô¦ªD•U7ÿ3€âÑ,TI!s(=¢JëX–ãÏžj�†�V5ÿè8yþâþþùùY¸S3~¼ˆ*®»w?ô~h‡ÇË‡'Vuˆ“ïÚ©_Ž&nbsp;…ßâÊ‚
endstream
endobj
47 0 obj
&lt;&lt;
/Length 2340      
/Filter /FlateDecode
&gt;&gt;
stream
xÚ…XÉ’äÆ
½÷WðÈŽ˜¢¸³x´&amp;fävØš±»’CÒ!›du1šK‰INOûë�€K�hûPÅL $–dúÎ³ã;?Ü}ºûîc˜;�ïå~8§³“ç^DNÄ^EÎ©t~q/ôîY¹Ÿï�û�ûðè~úþ¯þöxˆ¢Ðýéá&gt;pO–Î‡ŸO?þ ½Ç=ž0î·Ó_¾û¤Û�¢4öâ&lt;#Ax“cî|•Ì9ÌìCä{Yr”AmÕöÃ-�ù®I´ûtfÜNs×\¯´©3Ö}gIú$ÉÜ?YÚ—&amp;Vq»g¡Ù7;V­(«¡þ‚µ«Rç¡o¥õÏ~ž7äE�›d&gt;‘—Ä¹ÈY˜¶‚÷µ.!JE²h\‡Ê”ï„7^*[‰žÆªÀ~—®þ}ª¬Ð^Y‚AG4fx®š7™yî‡ç~«ÎÍF‰^ž$!4KÂD¹wŒƒÕ|1í„‰ûñ&gt;�Ý©ip„Ä5e9TÖìÕ•’HÐÈ‹ßŽåhý0÷’0”õªÇé"9Šb³U±&nbsp;-ŠåÎ$Ç”žQ*túNš×¡ŒÏÜ×¡Æ!…Uë·é±Äë¡™õú–¯åšîy2Ï•Ý3ËfûP·§©ïåc&nbsp;i4Ö]©c{ùŽ3Î­êMZö?Ù^× ÕY]v\f*óÌzGëW?ñ¿ÔÃ8™†š�Î-ÙS†e±
ë?�ÌË3_'ñ‚Ü‰ÈÞz®hÏJw2/ñ5€ª¯Eu¥ƒDI¨GŠ’h£ôŒ°ÕÒéÌ8
¦aÿ£!Ü¤“v×ëzóáAäCƒXô–†&gt;¡+Ôþ|³•êbk²DäUÕ �ÉÿË^‚ÏJWD&nbsp;·Ët3.PçzáŽ%\h„²
M6–��¹EmFŽ|!;7†Ö£7†žŸ(8~^&lt;6Nq"4'B§©_8–¥'Ç@‹ÂX—ÚŽý b¡O a·ŽAª&nbsp;x0Š¾½Nc5èvSîùyGçˆ‚TOwhMGÑ’ÉÔ
Â�ºz9{9Hd6Æ› úd"&nbsp;Ë‚3qF]LÐ‰k˜bm!š™7;uIwDçÔˆC
š8Úñ°á?ø.ÖåŽø
)-†Ò¬Õ95ûí1r�1©/‡¨dŽkJÚ	Sê… ‹8[©çÞ^´&gt;+èoðÅ'9"šSJ¢GEÙ‹oÎÖ[MG,˜n×?âœŒ3qì‹ïLÒ E»Æ7Êa't)ŠÈûI¹¶Ÿ†yôÍ4JS´;ãT=ª€A{jæIÅqš1VceNÃÀŠá°Üc]L”Ö�xÞ£,‡Z°îSÝqî¢*‡†ù¨…ZÍßù‚ûÂ^¼÷žPO—™&nbsp;£æ9{©Bk„¢þÕ�âb‘%s›j ¢µ+Ìd©kkB3™ÀY´I:š eÕŠ�ãnLÌRF¶½eUxHÝˆ‡Pw4/³_Ñ0SîV_î�B3¬Ž�’:‡Kä�ÉÑ:UU¢¦ar¯Ã®cÝÖÿÖINhÌª`qÃcºŒ— ¥FY�CÝ.±¿Ý²¤HÓ,:ÿ_vN-¸Ü´øÚ1t9¼®åÂÄ‡?`¹ùÔŒàä}£k]B”Þ¼��
IG�÷²Ì¶îä€ô%ð±ã0£î5&lt;ÁNõ8˜aJ3Â?�4´+µçßNcÔÀd÷¹ÝŒÐš\ïÉè«FÉ‰“¾õLÐ•Ÿ&nbsp;ö|Û	8Ž1&nbsp;P7ÃêÐ7Í’ê@1âÇ¤÷]|9]0æ9ã]˜sP¢»=h›�.§hb/æÐù™2�2
)ÀˆÈê§ïUê¡¥Ä[ü+ÏeKä…Vœªå‚ö4QpEìuÔ;ó"PÉnÞé×‚a&nbsp;i‰—+Í4E&amp;.d’œãßê+™©ã‚z\¶iå0mpšXÛÂ*—4‘äRõäš%ž¤sì¢V»äWÊUý
5ƒ$¨�3À?¶Q¯‘iGÃ¼ìÔ‘YJw„›:2Þ«#3/q
[‡¡|}½ÔK´Ò&gt;3®4ª)&nbsp;˜Ó}KIA53Ý‚T%<r'6-Êdd~¹�ÓvcÃ 9c="" ÉÒ[µlü€vë¡mÕäjháf…ÔøÛ¨0& Ò·“öfö?(›–™†aÍ�z="" ¤ê+Á—^†¨{¦Ècbx·œ\ƒÁ¥¥ÊÍ‘éêdjéÀƒÏÒ‰jnðÌ<£ðÚm™„¬Çíh«‹÷2Þö|a%j¡Õmu»{&jébvÕív?œc·buœ¼‚y¡lk9�®k(j="˜Í&nbsp;c¥cžlßPMÄ×Œ,Ôº">C)d–°ÊŸÁß–û¸ã§¡û&nbsp;›.7j—ý,‰´s½Éfâëxª&amp;¥ûW
)
ø'obÂ×…Ø½,�){º^ûn–°ò�ÊÝšÁb¾Ó¤K0[×Õ™ð&lt;®Rqµ™µ÷Ü0Ûð�ÔªR¼QÚgc©Ø*àÉg-ÎS�4J]@ë®õ8±h$¢Ãã ¢ñ9ÞxÅ;l¡g,«¦âØuP#÷8JÑÈ}Ò’éì™3!Q©Ž§Šª^ou¨/Ïpx-Ê"Õz$•ú„Ž²¥Ôæw—\�It5¼tÆÙk‚™hìÞ¥TŠpÔüÌ¤E9‰òèš2äþÉÄ­@ËýUÓø¿$[¾7Äa�î¦î5eùé¬|Í¤®‘Q`
TjKº{6£.¤oN&lt;ÛÐEX¨sù.±:èFx¹xSJq¸y¨ûpºûý.&nbsp;¦ïóãad^�¢½ûå7ß)‰GgóKœWÙ:ALy(M©Ý8�w——È8sÊN„ã”v‚ ðÒ˜2¥¢ŒFn:âÈ9bzŒ�±$HOG_]êü†.t„q¸òŽ\|µìc10£9ŸíE¹Çó£®—¨)òÍ!nŠo–WªZéSècD‡‡'^�§ÅtãMçm•J4È&nbsp;/w7g&gt;ÈY2üoòÿ&gt;ïçÕxxxãQkÍq/•DþMþÛ¼ZÄ¸òbAn`ã‹~~¸Ð©*r÷©æ(š”×‰i°@â,�ÜOØŒuÂûIÒR/9Fôõ×ÚbŠj4Nô=jîD›ªVÒee®jG+£ckéå!ç‚Š˜êé2ÄèŒW–´i¼×þöK®þ‡ú 
endstream
endobj
50 0 obj
&lt;&lt;
/Length 2216      
/Filter /FlateDecode
&gt;&gt;
stream
xÚ•XÉ’ã6½×WðHE”X\Eræä¥ÛÓsðÒ–O¶£"¡¢HBÉ–Ë_ïÜ@QÕòDÌ‰ÈD"äò2Á8xâà»‡¯÷Oï“]�ÄQ×I°?eU»]°Kò¨Î²`ß¿†»Íïûÿ&gt;½Oëµ`ï¢45$òÍ¾úqÿîãf›ei˜D›mYfá‡ï÷7iþðí/ßì?üð=ªyˆeï·oEá6‹£²¨Xm£ÇÑ:
zÓ"Tmë€�Ä.ÏªÑ°QžÕáGÝèwšº×M•…�,rÜl“Ðn’Ð1m†qRC£eZ±ÖI7¸ô4˜?fÙ¨Q]§[¾u–¯O™Åe”�]äÚx¢M‘„|ª¼GÜòŒ,ÕÐ*Á;ñ²m’EE^óâN½Úy‚ue’5´¶7©ÉØ�7ÏÓ&nbsp;Œê2¦½“,“WAÕ…¨(XìÖ’UTì*Øó*vRx¼"
áþn‚›‘„‡Í6­Bm†gžœGžIÃÉ²ÄÙéÏh4nN2yÒ&lt;8;®éEuvV3^dÓÁeã:Êàl7·&gt;&nbsp;ªWX³+Â#ÉwSâÞ-yVX
Fx¹ÈÓpî[�«±ÃhÆidÊùûl&gt;óípÑi¥{·úVçñÔ1·àÓöÊèÙoûìTÏxŠ´¾¬�98åŒ=_M&lt;2ò+Ë\k~‹³L»«u�ùÙ¸iV�lÃñ4ÄHžÖçôf�LOn)½JbòÿÈ\ý§nfˆ4jR�Qé€0qAÝ

»äÐ.£$�É^u¦1ve-gÖ„›¹—Üã©F
¼d°¢ØiÈÊ;G¶(XVr`\ó»,9�éô4;/)¼í¬Ñ5IøÌòt</r'6-êdd~¹�óvcã></m¥bêùæm÷6ˆ.ì©†ãuìr0»mx´aøàsx0ž÷pa8þúyi~fgßçu"gíða></z2w7*�%«¤ò></o(™á§…j¹[µôì˜9vhkr¼2�&˜f«…ãâlfn‡¹></j^k™ln"ÿc`ðk„ã¸do½¡�$�ì!></sv”></puàî)•î0à†ß]tâgø"�ûá÷‡mÿrõš›(ðìéqv\@+uk6*ñêû�àêî�àxÿùùà½¨d‡èj></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://metamodular.com/closos.pdf">http://metamodular.com/closos.pdf</a></em></p>]]>
            </description>
            <link>http://metamodular.com/closos.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-23730107</guid>
            <pubDate>Sat, 04 Jul 2020 04:24:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lisp Badge: A single-board computer that you can program in uLisp]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 24 (<a href="https://news.ycombinator.com/item?id=23729970">thread link</a>) | @lnyan
<br/>
July 3, 2020 | http://www.ulisp.com/show?2L0C | <a href="https://web.archive.org/web/*/http://www.ulisp.com/show?2L0C">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<p>The Lisp Badge is a self-contained computer with its own display and keyboard, based on an ATmega1284, that you can program in uLisp:</p>
<p><img src="http://www.ulisp.com/pictures/3j/lispbadgehand.jpg" alt="LispBadgeHand.jpg" width="720" height="462"></p>
<p>You can use it to run programs that interface to components such as LEDs and push-buttons via the I/O pins, read the analogue inputs, and operate external devices via the I2C and SPI interfaces. It has a greyscale OLED display that gives 8 lines of 42 characters, and an integrated 45-key keyboard optimised for Lisp.</p>
<p>For details of how to build one see&nbsp;<a href="http://www.technoblogy.com/show?2AEE" target="_blank">Lisp Badge</a>&nbsp;on Technoblogy.</p>
<h3><span>Specification</span></h3>
<p><strong>Size:</strong> 107mm x 61mm (4.2" x 2.4").</p>
<p><strong>Display</strong>: 42 characters x 8 lines.</p>
<p><strong>Keyboard:</strong>&nbsp;Integrated 45-key keyboard providing upper and lower-case characters, digits, and the symbols required by uLisp.</p>
<p><strong>Memory available</strong>: 2816 Lisp cells (11264 bytes).</p>
<p><strong>EEPROM</strong>: 1024 Lisp cells (4096 bytes), allows you to save the Lisp workspace using <strong>save-image</strong>.</p>
<p><strong>Processor:</strong> ATmega1284P</p>
<p><strong>Clock speed:</strong> 16 MHz.</p>
<p><strong>Current consumption:</strong> Approx. 20 mA.</p>
<h4><strong>Language</strong></h4>
<p>uLisp, a subset of Common Lisp, with 122 Lisp functions and special forms. For a full definition see <a href="http://www.ulisp.com/show?3L" target="_blank">uLisp Language Reference</a>.</p>
<p>The language includes two extensions, <strong>plot</strong> and <strong>plot3d</strong>, for plotting graphs and 3d functions.&nbsp;</p>
<p><strong>Types supported</strong>: list, symbol, integer, character, string, and stream.</p>
<p>An integer is a sequence of digits, optionally prefixed with "+" or "-". Integers can be between -32768 and 32767. You can enter numbers in hexadecimal, octal, or binary with the notations #x2A, #o52, or #b101010, all of which represent 42.</p>
<p>User-defined symbol names can have arbitrary names. Any sequence that isn't an integer can be used as a symbol; so, for example, 12a is a valid symbol.</p>
<p>There is one namespace for functions and variables; in other words, you cannot use the same name for a function and a variable.</p>
<p>Includes a mark and sweep garbage collector. Garbage collection takes 5 msec.</p>
<h4><strong>Interfaces</strong></h4>
<p>These interfaces are brought to headers at the edge of the Lisp Badge board. The numbers in brackets refer to Arduino pin numbers:</p>
<ul>
<li>Four analogue input pins using <strong>analogread</strong>: A0 to A3 (24 to 27) plus VCC and GND.</li>
<li>Two analogue outputs using <strong>analogwrite</strong>:&nbsp;MISO (6), and SCK (7).</li>
<li>Digital input and output using <strong>pinmode</strong>, <strong>digitalread</strong>, and <strong>digitalwrite</strong>:&nbsp;MOSI (5), MISO (6), SCK (7),&nbsp;RX0 (8), TX0 (9), SCL (16), SDA (17), and A0 to A3 (24 to 27)</li>
<li>I2C interface using <strong>with-i2c</strong> and <strong>restart-i2c</strong>: SCL (16) and SDA (17).</li>
<li>SPI interface using <strong>with-spi</strong>: MOSI (5), MISO (6), and SCK (7).</li>
<li>Serial interface (FTDI) using&nbsp;<strong>with-serial</strong>: RX0 (8) and TX0 (9).</li>
</ul>
<p>The shift key can be used as a digital input: SHIFT (23).</p>
<p>SCK (7) is connected to an LED on the front panel. This is an analogue output pin, so you can vary the brightness of the LED.</p>
<h3>Plotting extensions</h3>
<p>The Lisp Badge contains two plotting extensions, plot and plot3d, designed to allow plotting to the greyscale graphics display.</p>
<p><span>After generating a plot both functions wait for the ESC key to be pressed before displaying the uLisp prompt.</span></p>
<h3 id="plot">plot&nbsp;<span>function</span></h3>
<p><strong>Syntax:</strong>&nbsp;<code>(plot [<em>x-intercept y-intercept</em>] [<em>function</em>]...)</code></p>
<p>Plots up to four functions on the same graph, optionally with axes.</p>
<p>Each function should be a function of one parameter, the x coordinate, and it will be called with each value of x from 0 to 255. The function should return the y value, from 0 to 63.</p>
<p>If&nbsp;<em>x-intercept</em>&nbsp;and&nbsp;<em>y-intercept</em>&nbsp;are specified,&nbsp;<strong>plot3d</strong>&nbsp;draws axes through those intercepts.</p>
<p>For example, defining:</p>
<pre>(defun sine ()
  (let ((x 0) (y 2045))
    (lambda (n) 
      n
      (incf x (/ (* y 16) 163))
      (decf y (/ (* x 16) 163))
    (+ 32 (ash x -6)))))</pre>
<p>the following command:</p>
<pre>(plot 0 32 (sine))</pre>
<p>will plot:</p>
<p><img src="http://www.ulisp.com/pictures/3j/lispsine.jpg" alt="LispSine.jpg" width="600" height="211"></p>
<h4 id="plot3d">Plotting multiple functions</h4>
<p>The following example plots the voltages on the analogue inputs 0 to 3 once a second on a single plot. First define:</p>
<pre>(defun adc (n) (lambda (x) (delay 250) (/ (analogread n) 8)))</pre>
<p>Then give the command:</p>
<pre>(plot 0 0 (adc 0) (adc 1) (adc 2) (adc 3))</pre>
<h3>plot3d&nbsp;<span>function</span></h3>
<p><strong>Syntax:</strong>&nbsp;<code>(plot3d&nbsp;[<em>x-intercept y-intercept</em>] [<em>function</em>])</code></p>
<p>The function should be a function of two parameters, the x and y coordinates, and it will be called with each value of x from 0 to 255 and y from 0 to 63. The function should return the greyscale value to be plotted, from 0 to 15.</p>
<p>If&nbsp;<em>x-intercept</em>&nbsp;and&nbsp;<em>y-intercept</em>&nbsp;are specified,&nbsp;<strong>plot3d</strong>&nbsp;draws axes through those intercepts.</p>
<p>For example, defining:</p>
<pre>(defun p (x y) 
  (let ((a (/ (- x 128) 2))
        (b (- y 32))) 
    (min (abs (- (logand (/ (+ (* a a) (* b b) (* a b)) 16) 31) 15)) 15)))</pre>
<p>the following command:</p>
<pre>(plot3d 128 32 p)</pre>
<p>will plot:</p>
<p><img src="http://www.ulisp.com/pictures/3j/lispellipses.jpg" alt="LispEllipses.jpg" width="600" height="211"></p>
</div></div>]]>
            </description>
            <link>http://www.ulisp.com/show?2L0C</link>
            <guid isPermaLink="false">hacker-news-small-sites-23729970</guid>
            <pubDate>Sat, 04 Jul 2020 03:54:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Where Am I? NYTimes or Google?]]>
            </title>
            <description>
<![CDATA[
Score 1114 | Comments 350 (<a href="https://news.ycombinator.com/item?id=23729160">thread link</a>) | @rwoll
<br/>
July 3, 2020 | https://theinternetbytes.com/2020/07/03/where-am-i/ | <a href="https://web.archive.org/web/*/https://theinternetbytes.com/2020/07/03/where-am-i/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            




            
<article>
    <header>
        
        <p>
            JULY 3, 2020
        </p>
    </header>
    <section>
        <p>Take a look at the below screenshot from Safari for iOS. What website am I on?</p>

<p><img src="https://theinternetbytes.com/images/amp-nytimes-example.png" alt="AMP Example with NYTimes"></p>

<p>Based on the contents of the page, I think I’m on The New York Times’ site.
However, based on the domain visible in the address bar,
I think I’m on <code>google.com</code>. If I click in the address bar
I see <code>https://www.google.com/amp/s/www.nytimes.com/2020/05/22/technology/google-antitrust.amp.html</code>.
So, where am I? What am I looking at? What’s up with the mix of Google and
New York Times’ branding? Do I trust the contents of the page or the address
bar? How does the content of the page and the address bar relate in this context
and elsewhere on the web?</p>

<p>Comments on a draft of this post can be viewed here: <a href="https://news.ycombinator.com/item?id=23729160">https://news.ycombinator.com/item?id=23729160</a>.</p>

    </section>
</article>

        </div></div>]]>
            </description>
            <link>https://theinternetbytes.com/2020/07/03/where-am-i/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23729160</guid>
            <pubDate>Sat, 04 Jul 2020 01:14:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scaling Linux Services: Before accepting connections]]>
            </title>
            <description>
<![CDATA[
Score 169 | Comments 28 (<a href="https://news.ycombinator.com/item?id=23729072">thread link</a>) | @theojulienne
<br/>
July 3, 2020 | https://theojulienne.io/2020/07/03/scaling-linux-services-before-accepting-connections.html | <a href="https://web.archive.org/web/*/https://theojulienne.io/2020/07/03/scaling-linux-services-before-accepting-connections.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <p>When writing services that accept TCP connections, we tend to think of our work as starting from the point where our service accepts a new client connection and finishing when we complete the request and close the socket. For services at scale, operations can happen at such a high rate that some of the default resource limits of the Linux kernel can break this abstraction and start causing impact to incoming connections outside of that connection lifecycle. This post focuses on some standard resource limitations that exist before the client socket is handed to the application - all of which came up during the course of investigating errors on production systems as part of my role at GitHub (in some cases, multiple times across different applications).</p>

<p>In its most basic form (ignoring non-blocking variants), listening for TCP connections requires a call to <code>listen()</code> to actually start allowing incoming connections, followed by repeated calls to <code>accept()</code> to take the next pending connection and return a file descriptor that is for that particular client. In C this pattern looks something like:</p>
<div><div><pre><code><span>int</span> <span>server_fd</span> <span>=</span> <span>socket</span><span>(</span><span>AF_INET</span><span>,</span> <span>SOCK_STREAM</span><span>,</span> <span>0</span><span>);</span>
<span>bind</span><span>(</span><span>server_fd</span><span>,</span> <span>/* ... */</span><span>);</span>

<span>// start listening for connections</span>
<span>listen</span><span>(</span><span>server_fd</span><span>,</span> <span>512</span><span>);</span>

<span>while</span> <span>(</span><span>running</span><span>)</span> <span>{</span>
    <span>// block until a connection arrives and then accept it</span>
    <span>int</span> <span>client_fd</span> <span>=</span> <span>accept</span><span>(</span><span>server_fd</span><span>,</span> <span>NULL</span><span>,</span> <span>NULL</span><span>);</span>

    <span>// ... handle client_fd ...</span>
<span>}</span>

<span>close</span><span>(</span><span>server_fd</span><span>);</span>
</code></pre></div></div>

<p>This is often hidden behind further layers of abstraction, and we tend to hide away all the implementation details of accepting connections and view it as a stream of new connections that we pick up and then process in parallel. However, when building a system that runs at a certain scale, this abstraction tends to break down because there are resource limitations introduced in the period where connections are being established but are not yet returned by <code>accept()</code>. Before that point, those client connections are considered as being part of the server/listen socket and not as independent resources exposed to the application.</p>

<p>The examples mentioned in this blog post will be reproducible in the lab from <i></i> <a href="https://github.com/theojulienne/blog-lab-scaling-accept">theojulienne/blog-lab-scaling-accept</a> - clone this repository bring up the lab, then poke around at these examples in a real system:</p>
<div><div><pre><code>$ git clone https://github.com/theojulienne/blog-lab-scaling-accept.git
$ cd blog-lab-scaling-accept
$ vagrant up
$ vagrant ssh
</code></pre></div></div>

<h2 id="from-syn-to-accept">From SYN to accept()</h2>

<p>The Linux kernel maintains 2 queues of connections that maintain the backlog of connections that are not yet <code>accept()</code>ed by the application:</p>
<p><img alt="LISTEN backlogs" src="https://theojulienne.io/images/scaling-linux-services-before-accepting-connections/pre-accept-backlogs.png">
</p>

<p>When a SYN packet is received to initiate a new connection to a listen socket, a SYN-ACK is sent to the client and the half-completed connection state is stored in the “SYN backlog” or “Request Socket Queue”. This represents connections that have not yet been fully validated as having two-way communication between the hosts, because we haven’t yet validated that the remote end has received a packet from us (the SYN could have come from another host spoofing the source IP).</p>

<p>Once the client responds to the server’s SYN-ACK with an ACK, the connection has then completed the full <a href="https://en.wikipedia.org/wiki/Handshaking#TCP_three-way_handshake">TCP 3-way handshake</a>, and the server knows that two-way communication has been established. At this point, the client connection is ready to be provided to the application at a future call to <code>accept()</code>, and is added to the appropriately named “accept queue”.</p>

<h2 id="syn-backlog">SYN backlog</h2>

<p>Connections in the SYN backlog remain there for a period of time relative to the Round Trip Time between the server and the client. If there are N slots in the backlog then you can have at most N connections in this backlog per average RTT, after which the backlog overflows.</p>

<p>This won’t actually cause the connections to fail by default on Linux, but cause <a href="https://en.wikipedia.org/wiki/SYN_cookies">SYN cookies</a> to be sent instead. This is because when only a SYN packet has been received, the server hasn’t yet validated that the client is who they say they are, so they could be spoofing packets as coming from a different IP. This is a common Denial of Service attack called a <a href="https://en.wikipedia.org/wiki/SYN_flood">SYN Flood</a>. Because this is so common, the Linux kernel has built in mitigation to SYN floods by sending a SYN cookie when there is no room in the SYN backlog for the new connection.</p>

<p>This means that even if the SYN backlog overflows under normal circumstances with no DoS attack, the kernel allows the connection to move further along in the handshake, and won’t store any resources for the connection until an ACK completes the handshake. Sending SYN cookies during normal circumstances does however indicate that the rate of connections is probably too high for the default limits - SYN cookies are really only meant for mitigating SYN floods.</p>

<p>Whether SYN cookies are enabled is controlled via the sysctl <code>net.ipv4.tcp_syncookies</code>, by default this is set to <code>1</code> which indicates that SYN cookies should be sent when the SYN backlog overflows, but can also be set to <code>0</code> to disable entirely or <code>2</code> to force SYN cookies to be sent 100% of the time.</p>

<p>When SYN cookies are enabled as needed (the default), they are triggered when the number of pending connections in the SYN backlog is more than the configured accept queue backlog size for the socket - the logic for this is <a href="https://github.com/torvalds/linux/blob/cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2/net/ipv4/tcp_input.c#L6612-L6613">here</a>. When SYN cookies are fully disabled, <code>net.ipv4.tcp_max_syn_backlog</code> configures the number of connections allowed in the SYN backlog separately - the logic for this case is <a href="https://github.com/torvalds/linux/blob/cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2/net/ipv4/tcp_input.c#L6670-L6672">here</a>.</p>

<p>In the default configuration where the backlog overflows and SYN cookies are sent, the kernel increments the <code>TCPReqQFullDoCookies</code> counter and logs this line to the kernel log, which is often confused for an indicator of a real SYN flood even when it’s just due to legitimate connections coming in too fast:</p>
<div><div><pre><code>TCP: request_sock_TCP: Possible SYN flooding on port 8080. Sending cookies.  Check SNMP counters.
</code></pre></div></div>

<p>If SYN cookies are explicitly disabled, the kernel increments the <code>TCPReqQFullDrop</code> counter and the following would be logged instead:</p>
<div><div><pre><code>TCP: request_sock_TCP: Possible SYN flooding on port 8080. Dropping request.  Check SNMP counters.
</code></pre></div></div>

<p>If you see this message for internal service-to-service connections, chances are you’re dealing with a scaling problem or a <a href="https://en.wikipedia.org/wiki/Thundering_herd_problem">thundering herd problem</a>, and not an actual SYN flood or intentional bad actor.</p>

<p>Those “SNMP Counters” that the kernel mentions are available in <code>nstat</code> in the network namespace:</p>
<div><div><pre><code>$ nstat -a | grep TCPReqQFull
TcpExtTCPReqQFullDoCookies      706                0.0
</code></pre></div></div>

<h3 id="syn-backlog-in-the-lab">SYN backlog in the lab</h3>

<p>To see how the SYN backlog behaves in the lab, we can simulate latency on local connections with the following:</p>
<div><div><pre><code>vagrant@blog-lab-scaling-accept:~$ sudo /vagrant/reset-lab.sh
vagrant@blog-lab-scaling-accept:~$ sudo tc qdisc add dev lo root netem delay 200ms
vagrant@blog-lab-scaling-accept:~$ ping 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=401 ms
64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=400 ms
</code></pre></div></div>

<p>This means we can now simulate overflowing the SYN backlog. To make it a bit easier to reproduce, reduce the size of the SYN backlog from the default of <code>128</code> by reducing <code>net.core.somaxconn</code> (because SYN cookies are enabled):</p>
<div><div><pre><code>vagrant@blog-lab-scaling-accept:~$ sudo sysctl net.core.somaxconn
net.core.somaxconn = 128
vagrant@blog-lab-scaling-accept:~$ sudo sysctl -w net.core.somaxconn=10
net.core.somaxconn = 10
vagrant@blog-lab-scaling-accept:~$ sudo systemctl restart nginx
vagrant@blog-lab-scaling-accept:~$ 
</code></pre></div></div>

<p>At this point, if more than 10 connections arrive within 400ms (before the SYN-ACK and ACK handshake complete), then 10 connections will be in the SYN backlog, which is the maximum configured. This will trigger SYN cookies to be sent, triggering the message and counters above. Let’s test out if this works, run a simulation that opens N concurrent connections:</p>
<div><div><pre><code>vagrant@blog-lab-scaling-accept:~$ python /vagrant/test_send_concurrent_connections.py 127.0.0.1:80 20
Waiting, Ctrl+C to exit.
</code></pre></div></div>

<p>And verify that SYN cookies were sent as expected:</p>
<div><div><pre><code>vagrant@blog-lab-scaling-accept:~$ sudo dmesg -c
[ 2571.784749] TCP: request_sock_TCP: Possible SYN flooding on port 80. Sending cookies.  Check SNMP counters.
vagrant@blog-lab-scaling-accept:~$ sudo nstat | grep ReqQ
TcpExtTCPReqQFullDoCookies      10                 0.0
vagrant@blog-lab-scaling-accept:~$
</code></pre></div></div>

<p>Try disabling the simulated latency with <code>sudo tc qdisc del dev lo root</code> and re-run the same test, now connections move through fast enough that SYN cookies are not sent.</p>

<p>This also demonstrates the way the Round Trip Time effects the amount of connections that can burst into a LISTEN socket - <code>test_send_concurrent_connections.py</code> does actually attempt to initiate complete connections and <code>nginx</code> on the other end is readily calling <code>accept()</code> as fast as it can, but because 20 connections are initiated at once, 20 SYN packets arrive before any handshake can continue, and the SYN backlog overflows. You can imagine that in the real world, with some of these values often defaulting to <code>128</code>, and users often being far away from servers (on the other side of the world), it’s pretty easy to accidentally trigger this scenario without a true SYN flood.</p>

<h2 id="accept-queue">Accept queue</h2>

<p>Once an ACK packet is received and validated, a new client connection is ready for the application to process. This connection is moved into the accept queue, waiting for the application to call <code>accept()</code> and receive it.</p>

<p>Unlike the SYN backlog, the accept queue has no backup plan for when it overflows. Back in the original call to <code>listen()</code>, a <code>backlog</code> argument was provided, which indicates how many connections can have completed the 3-way handshake and be waiting in the kernel for the application to accept.</p>

<p>This is the first common issue: If the <code>backlog</code> number provided to <code>listen()</code> is not large enough to contain any number of connections that could reasonably complete their handshake between 2 calls to <code>accept()</code>, then a connection will be dropped on the floor, and the application generally won’t even notice - the next call to …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://theojulienne.io/2020/07/03/scaling-linux-services-before-accepting-connections.html">https://theojulienne.io/2020/07/03/scaling-linux-services-before-accepting-connections.html</a></em></p>]]>
            </description>
            <link>https://theojulienne.io/2020/07/03/scaling-linux-services-before-accepting-connections.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23729072</guid>
            <pubDate>Sat, 04 Jul 2020 00:58:29 GMT</pubDate>
        </item>
    </channel>
</rss>
