<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 08 Jul 2020 12:19:59 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 08 Jul 2020 12:19:59 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[CoreBGP – Plugging in to BGP]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 8 (<a href="https://news.ycombinator.com/item?id=23744167">thread link</a>) | @jordanwhited
<br/>
July 5, 2020 | https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/ | <a href="https://web.archive.org/web/*/https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<hr>
<p><img src="https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/cover.png" alt="cover"></p>
<hr>

<p><a href="https://tools.ietf.org/html/rfc4271" target="_blank">BGP</a> is one of many protocols that powers the Internet. Chances are you have heard of it, even if you don’t work in or around the computer networking space. If you aren’t familiar, I’ll try to provide some quick background:</p>
<ul>
<li>BGP is a <a href="https://en.wikipedia.org/wiki/Distance-vector_routing_protocol" target="_blank">distance-vector routing protocol</a> used to disseminate routing information.</li>
<li>A BGP speaker implements a <a href="https://en.wikipedia.org/wiki/Finite-state_machine" target="_blank">finite state machine</a> with 6 states:
<ul>
<li>Idle</li>
<li>Active</li>
<li>Connect</li>
<li>OpenSent</li>
<li>OpenConfirm</li>
<li>Established</li>
</ul>
</li>
<li>Inputs to the BGP FSM include messages, timer events, and administrative events.</li>
<li>Routing information is exchanged via UPDATE messages in the Established state.</li>
<li>BGP is extensible; speakers communicate their capabilities via OPEN messages.</li>
</ul>
<p>Expanding on that last bullet point, it’s difficult to summarize exactly how/where BGP is used due to its flexibility and extensibility. Various <a href="https://ietf.org/about/" target="_blank">IETF</a> Working Groups continue to publish BGP-related RFCs for a protocol that took shape in the early 90s. As the BGP landscape and application widens, we need software that enables us to keep up.</p>
<p>In this post I’ll provide some of my personal experience and history working with BGP, and introduce a new BGP library, <a href="https://github.com/jwhited/corebgp" target="_blank">CoreBGP</a>, which can be used to build the next generation of BGP-enabled applications.</p>

<p>In October of 2010 I attended my first <a href="https://www.nanog.org/" target="_blank">NANOG</a> meeting in Atlanta, GA after accidentally falling into the position of Network Operations Engineer at work. I worked for a modest-sized hosting provider at the time, and was intrigued with BGP. Upon arriving in Atlanta, I vaguely remember some confusion after telling a cab driver that the hotel I needed to be dropped at was on Peachtree St. I later learned that there are 71 streets in Atlanta with a variant of “Peachtree” in their name, according to <a href="https://en.wikipedia.org/wiki/Peachtree_Street#Nomenclature" target="_blank">Wikpedia</a>.</p>
<p>I got where I needed to go, eventually, and the first talk I attended was <a href="https://archive.nanog.org/meetings/nanog50/presentations/Sunday/NANOG50.Talk33.NANOG50-BGP-Techniques.pdf" target="_blank">BGP techniques for Internet Service Providers</a> by <a href="http://www.bgp4all.com.au/" target="_blank">Philip Smith</a>. Philip started with the basics before getting into the techniques used at ISPs. So many light bulbs went off for me during this talk. I have yet to see any other BGP presentation cover such a breadth of information but still do it in a way that is beginner-friendly, useful as a refresher for any expert, and just downright interesting.</p>
<p>Fast-forward 10 years and I’ve gained a fair share of experience operating networks that use BGP. In more recent years I’ve shifted to software engineering where I’ve had the opportunity to implement various BGP-enabled applications for network observability, data analytics, and SDN purposes.</p>
<p>Each time I started a new BGP-enabled app, I had to answer the following question – which existing BGP implementation should be its foundation?</p>

<p>Of the handful of open source BGP implementations out there, I’ve had hands-on experience with projects making use of:</p>
<ul>
<li><a href="https://bird.network.cz/" target="_blank">BIRD</a></li>
<li><a href="https://osrg.github.io/gobgp/" target="_blank">GoBGP</a></li>
<li><a href="https://www.opendaylight.org/what-we-do/odl-platform-overview" target="_blank">OpenDaylight</a></li>
<li><a href="https://www.quagga.net/" target="_blank">Quagga</a></li>
</ul>
<p>BIRD shines where a <a href="https://bird.network.cz/?get_doc&amp;v=20&amp;f=bird-5.html" target="_blank">rich policy language</a> is needed. GoBGP has a <a href="https://github.com/osrg/gobgp/tree/master/api" target="_blank">feature-rich gRPC API</a>, and can be embedded as a library. OpenDaylight’s BGP implementation is part of a larger SDN controller solution and has extensive support for <a href="https://docs.opendaylight.org/en/stable-oxygen/user-guide/bgpcep-guide/bgp/bgp-user-guide-linkstate-family.html" target="_blank">BGP-LS</a>. Quagga can reliably produce <a href="https://tools.ietf.org/html/rfc6396" target="_blank">MRT</a> dumps and has been around a long time, though I believe <a href="https://frrouting.org/" target="_blank">FRRouting</a> is now considered its successor.</p>
<p>These are all mature, established implementations. Some of them are in production at large ISPs, <a href="https://www.digitalocean.com/blog/scaling-droplet-public-networking/" target="_blank">Cloud Providers</a>, and <a href="https://joinup.ec.europa.eu/collection/open-source-observatory-osor/document/bird-manages-routing-worlds-largest-internet-exchanges-bird" target="_blank">Internet Exchange Points</a>. They are purpose-built and make various tradeoffs to suit their use cases (programming language, threading model, data structures, API, etc…).</p>
<p>But what if we are building something that doesn’t line up with the primary use cases of these widely used implementations? We may be locked in to decisions that are ultimately burdensome if we choose to build around them. Swapping in our own data structures for routing tables, or adding a new NLRI is non-trivial. Even if an implementation is intended to be embedded as library, it can still back us into a corner with resource consumption. There’s clearly a need to plug in or hook into specific parts of the BGP FSM, without inheriting decisions that went into a full-blown BGP daemon.</p>
<p>At the 27th IEEE International Conference On Network Protocols (ICNP), a group from the Université catholique de Louvain presented a paper on <code>The Case for Pluginized Routing Protocols</code>:</p>
<blockquote>
<p>Abstract—Routing protocols such as BGP and OSPF are key components of Internet Service Provider (ISP) networks. These protocols and the operator’s requirements evolve over time, but it often takes many years for network operators to convince their different router vendors and the IETF to extend routing protocols. Some network operators, notably in enterprise and datacenters have adopted Software Defined Networking (SDN) with its centralised control to be more agile. We propose a new approach to implement routing protocols that enables network operators to innovate while still using distributed routing protocols and thus keeping all their benefits compared to centralised routing approaches. We extend a routing protocol with a virtual machine that is capable of executing plugins. These plugins extend the protocol or modify its underlying algorithms through a simple API to meet the specific requirements of operators. We modify the OSPF and BGP implementations provided by FRRouting and demonstrate the applicability of our approach with several use cases.</p>
<p>— <!-- raw HTML omitted -->The Case for Pluginized Routing Protocols<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><!-- raw HTML omitted --></p>
</blockquote>
<p>In their paper they present a method for plugging into a previously mentioned open-source BGP implementation, FRRouting. Plugins exist at a function level, either prior to invocation (PRE), as a replacement (REPLACE), or just before returning (POST). Much of their BGP plugin focus is around the reception of messages, and decisions made shortly after:</p>
<blockquote>
<p>The BGP daemon is also extended similarly. We add insertion points on functions receiving BGP messages from neighbours, on filters and inside the decision process. We also expose specific functions to the plugins that are executed by the uBPF VM.</p>
<p>— <!-- raw HTML omitted -->The Case for Pluginized Routing Protocols<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><!-- raw HTML omitted --></p>
</blockquote>
<p>They take a clever approach with plugin sandboxing by leveraging a user space eBPF VM (<a href="https://github.com/iovisor/ubpf" target="_blank">uBPF</a>) linked to the FRRouting protocol implementation. Each plugin compiles to eBPF bytecode and runs inside of said VM. Plugins can be loaded and unloaded without impacting the primary protocol implementation. Using an eBPF VM also allowed them to utilise all the pre-existing Linux Kernel tooling.</p>
<p>I found this approach inspiring, but still not quite a match for my use cases:</p>
<ul>
<li>Plugins appear to be built around “incoming” events, or messages. What if I want to inject an UPDATE message to a peer irrespective of what FRRouting wants to send?</li>
<li>FRRouting was not built with this plugin model in mind. Changes/Updates to FRRouting will result in a maintenance headache for the VM hook points.</li>
<li>eBPF bytecode is typically compiled from C. Writing C can be time-consuming in comparison to more modern languages.</li>
<li>I need to be an FRRouting expert to do anything non-trivial.</li>
</ul>
<p>This experience and research led me to create CoreBGP, a BGP library that I could re-use across my BGP-enabled applications.</p>

<p>CoreBGP is a BGP library written in Go that implements the BGP FSM with an event-driven, pluggable model. It exposes an API that empowers the user to:</p>
<ul>
<li>send and validate OPEN message capabilities</li>
<li>handle “important” state transitions</li>
<li>handle incoming UPDATE messages</li>
<li>send outgoing UPDATE messages</li>
</ul>
<p>CoreBGP does not decode UPDATE messages (besides header validation), manage a routing table, or send its own UPDATE messages. These responsibilities are all passed down to the user. Therefore, the intended user is someone who wants that responsibility.</p>
<p>The primary building block of CoreBGP is a Plugin, defined by the following interface:</p>
<div><pre><code data-lang="go"><span>// Plugin is a BGP peer plugin.
</span><span></span><span>type</span> Plugin <span>interface</span> {
	<span>// GetCapabilities is fired when a peer's FSM is in the Connect state prior
</span><span></span>	<span>// to sending an Open message. The returned capabilities are included in the
</span><span></span>	<span>// Open message sent to the peer.
</span><span></span>	<span>GetCapabilities</span>(peer <span>*</span>PeerConfig) []<span>*</span>Capability

	<span>// OnOpenMessage is fired when an Open message is received from a peer
</span><span></span>	<span>// during the OpenSent state. Returning a non-nil Notification will cause it
</span><span></span>	<span>// to be sent to the peer and the FSM will transition to the Idle state.
</span><span></span>	<span>//
</span><span></span>	<span>// Per RFC5492 a BGP speaker should only send a Notification if a required
</span><span></span>	<span>// capability is missing; unknown or unsupported capabilities should be
</span><span></span>	<span>// ignored.
</span><span></span>	<span>OnOpenMessage</span>(peer <span>*</span>PeerConfig, capabilities []<span>*</span>Capability) <span>*</span>Notification

	<span>// OnEstablished is fired when a peer's FSM transitions to the Established
</span><span></span>	<span>// state. The returned UpdateMessageHandler will be fired when an Update
</span><span></span>	<span>// message is received from the peer.
</span><span></span>	<span>//
</span><span></span>	<span>// The provided writer can be used to send Update messages to the peer for
</span><span></span>	<span>// the lifetime of the FSM's current, established state. It should be
</span><span></span>	<span>// discarded once OnClose() fires.
</span><span></span>	<span>OnEstablished</span>(peer <span>*</span>PeerConfig, writer UpdateMessageWriter) UpdateMessageHandler

	<span>// OnClose is fired when a peer's FSM transitions out of the Established
</span><span></span>	<span>// state.
</span><span></span>	<span>OnClose</span>(peer <span>*</span>PeerConfig)
}
</code></pre></div><p>Here’s an example Plugin that logs when a peer enters/leaves an established state and when an UPDATE message is received:</p>
<div><pre><code data-lang="go"><span>type</span> plugin <span>struct</span>{}

<span>func</span> (p <span>*</span>plugin) <span>GetCapabilities</span>(c <span>*</span>corebgp.PeerConfig) []<span>*</span>corebgp.Capability {
	caps <span>:=</span> <span>make</span>([]<span>*</span>corebgp.Capability, <span>0</span>)
	<span>return</span> caps
}

<span>func</span> (p <span>*</span>plugin) <span>OnOpenMessage</span>(peer <span>*</span>corebgp.PeerConfig, capabilities []<span>*</span>corebgp.Capability) <span>*</span>corebgp.Notification {
	<span>return</span> <span>nil</span>
}

<span>func</span> (p <span>*</span>plugin) <span>OnEstablished</span>(peer <span>*</span>corebgp.PeerConfig, writer corebgp.UpdateMessageWriter) corebgp.UpdateMessageHandler {
	log.<span>Println</span>(<span>"peer established"</span>)
	<span>// send End-of-Rib
</span><span></span>	writer.<span>WriteUpdate</span>([]<span>byte</span>{<span>0</span>, <span>0</span>, <span>0</span>, <span>0</span>})
	<span>return</span> p.handleUpdate
}

<span>func</span> (p <span>*</span>plugin) <span>OnClose</span>(peer <span>*</span>corebgp.PeerConfig) {
	log.<span>Println</span>(<span>"peer closed"</span>)
}

<span>func</span> (p <span>*</span>plugin) <span>handleUpdate</span>(peer <span>*</span>corebgp.PeerConfig, u []<span>byte</span>) <span>*</span>corebgp.Notification {
	log.<span>Printf</span>(<span>"got update message of len: %d"</span>, <span>len</span>(u))
	<span>return</span> <span>nil</span>
}
</code></pre></div><p>Plugins are attached to peers when they are added to the Server, which manages their lifetime:</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/">https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/</a></em></p>]]>
            </description>
            <link>https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23744167</guid>
            <pubDate>Mon, 06 Jul 2020 02:39:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust for JavaScript Developers – Functions and Control Flow]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 39 (<a href="https://news.ycombinator.com/item?id=23743363">thread link</a>) | @rkwz
<br/>
July 5, 2020 | http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/ | <a href="https://web.archive.org/web/*/http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>This is the third part in a series about introducing the Rust language to JavaScript developers. Here are the past chapters:</p>
<ol>
<li><a href="http://www.sheshbabu.com/posts/rust-for-javascript-developers-tooling-ecosystem-overview/">Tooling Ecosystem Overview</a></li>
<li><a href="http://www.sheshbabu.com/posts/rust-for-javascript-developers-variables-and-data-types/">Variables and Data Types</a></li>
</ol>
<h2 id="Functions"><a href="#Functions" title="Functions"></a>Functions</h2><p>Rust’s function syntax is pretty much similar to the one in JavaScript.</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> income <span>=</span> <span>100</span><span>;</span>
  <span>let</span> tax <span>=</span> <span>calculate_tax</span><span>(</span>income<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> tax<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>calculate_tax</span><span>(</span>income<span>:</span> i32<span>)</span> <span>-&gt;</span> i32 <span>{</span>
  <span>return</span> income <span>*</span> <span>90</span> <span>/</span> <span>100</span><span>;</span>
<span>}</span></code></pre>
<p>The only difference you might see above is the type annotations for arguments and return values.</p>
<p>The <code>return</code> keyword can be skipped and it’s very common to see code without an explicit return. If you’re returning implicitly, make sure to remove the semicolon from that line. The above function can be refactored as:</p>
<pre><code>fn main() {
  let income = 100;
  let tax = calculate_tax(income);
  println!("{}", tax);
}

fn calculate_tax(income: i32) -&gt; i32 {
<span>- return income * 90 / 100;</span>
<span>+ income * 90 / 100</span>
}</code></pre>
<h2 id="Arrow-Functions"><a href="#Arrow-Functions" title="Arrow Functions"></a>Arrow Functions</h2><p>Arrow functions are a popular feature in modern JavaScript - they allow us to write functional code in a concise way.</p>
<p>Rust has something similar and they are called “Closures”. The name might be a bit confusing and would require getting used to because in JavaScript, closures can be created using both normal and arrow functions.</p>
<p>Rust’s closure syntax is very similar to JavaScript’s arrow functions:</p>
<p><strong>Without arguments:</strong></p>
<pre><code>
<span>let</span> greet <span>=</span> <span>(</span><span>)</span> <span>=</span><span>&gt;</span> console<span>.</span><span>log</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>

<span>greet</span><span>(</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> greet <span>=</span> <span>||</span> <span>println!</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>

<span>greet</span><span>(</span><span>)</span><span>;</span> </code></pre>
<p><strong>With arguments:</strong></p>
<pre><code>
<span>let</span> greet <span>=</span> <span>(</span>msg<span>)</span> <span>=</span><span>&gt;</span> console<span>.</span><span>log</span><span>(</span>msg<span>)</span><span>;</span>

<span>greet</span><span>(</span><span>"good morning!"</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> greet <span>=</span> <span>|</span>msg<span>:</span> <span>&amp;</span>str<span>|</span> <span>println!</span><span>(</span><span>"{}"</span><span>,</span> msg<span>)</span><span>;</span>

<span>greet</span><span>(</span><span>"good morning!"</span><span>)</span><span>;</span> </code></pre>
<p><strong>Returning values:</strong></p>
<pre><code>
<span>let</span> add <span>=</span> <span>(</span>a<span>,</span> b<span>)</span> <span>=</span><span>&gt;</span> a <span>+</span> b<span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> add <span>=</span> <span><span>|</span>a<span>:</span> i32<span>,</span> b<span>:</span> i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span> a <span>+</span> b <span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<p><strong>Multiline:</strong></p>
<pre><code>
<span>let</span> add <span>=</span> <span>(</span>a<span>,</span> b<span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
  <span>let</span> sum <span>=</span> a <span>+</span> b<span>;</span>
  <span>return</span> sum<span>;</span>
<span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> add <span>=</span> <span><span>|</span>a<span>:</span> i32<span>,</span> b<span>:</span> i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span>
  <span>let</span> sum <span>=</span> a <span>+</span> b<span>;</span>
  <span>return</span> sum<span>;</span>
<span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<p>Here’s a cheatsheet:<br><img src="http://www.sheshbabu.com/images/2020-rust-for-javascript-developers-3/image-2.png" alt=""></p>
<p>Closures don’t need the type annotations most of the time, but I’ve added them here for clarity.</p>
<h2 id="If-Else"><a href="#If-Else" title="If Else"></a>If Else</h2><pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> income <span>=</span> <span>100</span><span>;</span>
  <span>let</span> tax <span>=</span> <span>calculate_tax</span><span>(</span>income<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> tax<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>calculate_tax</span><span>(</span>income<span>:</span> i32<span>)</span> <span>-&gt;</span> i32 <span>{</span>
  <span>if</span> income <span>&lt;</span> <span>10</span> <span>{</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> income <span>&gt;=</span> <span>10</span> <span>&amp;&amp;</span> income <span>&lt;</span> <span>50</span> <span>{</span>
    <span>return</span> <span>20</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>return</span> <span>50</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<h2 id="Loops"><a href="#Loops" title="Loops"></a>Loops</h2><p>While loops:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> <span>mut</span> count <span>=</span> <span>0</span><span>;</span>

  <span>while</span> count <span>&lt;</span> <span>10</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> count<span>)</span><span>;</span>
    count <span>+=</span> <span>1</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Normal <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for" target="_blank" rel="noopener">for loops</a> don’t exist in Rust, we need to use <code>while</code> or <code>for..in</code> loops. <code>for..in</code> loops are similar to the <code>for..of</code> loops in JavaScript and they loop over an iterator.</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>for</span> n <span>in</span> numbers<span>.</span><span>iter</span><span>(</span><span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> n<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Notice that we’re not iterating directly over the array but instead using the <code>iter</code> method of the array.</p>
<p>We can also loop over <a href="https://doc.rust-lang.org/reference/expressions/range-expr.html" target="_blank" rel="noopener">ranges</a>:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>for</span> n <span>in</span> <span>1</span><span>..</span><span>5</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> n<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<h2 id="Iterators"><a href="#Iterators" title="Iterators"></a>Iterators</h2><p>In JavaScript, we can use array methods like map/filter/reduce/etc instead of <code>for</code> loops to perform calculations or transformations on an array.</p>
<p>For example, here we take an array of numbers, double them and filter out the elements that are less than 10:</p>
<pre><code><span>function</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>let</span> double <span>=</span> <span>(</span>n<span>)</span> <span>=</span><span>&gt;</span> n <span>*</span> <span>2</span><span>;</span>
  <span>let</span> less_than_ten <span>=</span> <span>(</span>n<span>)</span> <span>=</span><span>&gt;</span> n <span>&lt;</span> <span>10</span><span>;</span>

  <span>let</span> result <span>=</span> numbers<span>.</span><span>map</span><span>(</span>double<span>)</span><span>.</span><span>filter</span><span>(</span>less_than_ten<span>)</span><span>;</span>

  console<span>.</span><span>log</span><span>(</span>result<span>)</span><span>;</span> 
<span>}</span></code></pre>
<p>In Rust, we can’t directly use map/filter/etc over vectors, we need to follow these steps:</p>
<ol>
<li>Convert the vector into an iterator using <code>iter</code>, <code>into_iter</code> or <code>iter_mut</code> methods</li>
<li>Chain <code>adapters</code> such as map/filter/etc on the iterator</li>
<li>Finally convert the iterator back to a vector using <code>consumers</code> such as <code>collect</code>, <code>find</code>, <code>sum</code> etc</li>
</ol>
<p>Here’s the equivalent Rust code:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>vec!</span><span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>let</span> double <span>=</span> <span><span>|</span>n<span>:</span> <span>&amp;</span>i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span> n <span>*</span> <span>2</span> <span>}</span><span>;</span>
  <span>let</span> less_than_10 <span>=</span> <span><span>|</span>n<span>:</span> <span>&amp;</span>i32<span>|</span></span> <span>-&gt;</span> bool <span>{</span> <span>*</span>n <span>&lt;</span> <span>10</span> <span>}</span><span>;</span>

  <span>let</span> result<span>:</span> Vec<span>&lt;</span>i32<span>&gt;</span> <span>=</span> numbers<span>.</span><span>iter</span><span>(</span><span>)</span><span>.</span><span>map</span><span>(</span>double<span>)</span><span>.</span><span>filter</span><span>(</span>less_than_10<span>)</span><span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span>

  <span>println!</span><span>(</span><span>"{:?}"</span><span>,</span> result<span>)</span><span>;</span> 
<span>}</span></code></pre>
<p>You should be able to understand most of the code above but you might notice few things off here:</p>
<ul>
<li>The usage of <code>&amp;</code> and <code>*</code> in the closure</li>
<li>The <code>Vec&lt;i32&gt;</code> type annotation for the <code>result</code> variable</li>
</ul>
<p>The <code>&amp;</code> is the reference operator and the <code>*</code> is the dereference operator. The <code>iter</code> method instead of copying the elements in the vector, it passes them as references to the next adapter in the chain. This is why we use <code>&amp;i32</code> in the map’s closure (double). This closure returns <code>i32</code> but <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter" target="_blank" rel="noopener">filter</a> calls its closure (less_than_10) with reference so that’s why we need to use <code>&amp;i32</code> again. To dereference the argument, we use the <code>*</code> operator. We’ll cover this in more detail in future chapters.</p>
<p>Regarding <code>Vec&lt;i32&gt;</code>, so far we haven’t added type annotations to variables as Rust can infer the types automatically, but for <code>collect</code>, we need to be explicitly tell Rust that we expect a <code>Vec&lt;i32&gt;</code> output.</p>
<p>Aside from map and filter, there are ton of other <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html" target="_blank" rel="noopener">useful adapters</a> that we can use in iterators.</p>
<p>Thanks for reading! Feel free to follow me in <a href="https://twitter.com/sheshbabu" target="_blank" rel="noopener">Twitter</a> for updates :)</p>
</div></div>]]>
            </description>
            <link>http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23743363</guid>
            <pubDate>Mon, 06 Jul 2020 00:07:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I’m Writing a Book on Cryptography]]>
            </title>
            <description>
<![CDATA[
Score 171 | Comments 23 (<a href="https://news.ycombinator.com/item?id=23743218">thread link</a>) | @gedigi
<br/>
July 5, 2020 | https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/ | <a href="https://web.archive.org/web/*/https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<p>I’ve now been writing a book on <strong>applied cryptography</strong> for a year and a half.
I’m nearing the end of my journey, as I have one last ambitious chapter left to write: next-generation cryptography (a chapter that I’ll use to talk about cryptography that will become more and more practical: post-quantum cryptography, homomorphic encryption, multi-party computation, and zk-SNARKs).</p>
<p>I’ve been asked multiple times <strong>why write a new book about cryptography?</strong> and <strong>why should I read your book?</strong>.
To answer this, you have to understand when it all started…</p>
<h2>Diagrams are everything</h2>
<p>Today if you want to learn about almost anything, you just google it.
Yet, for cryptography, and depending on what you're looking for, resources can be quite lacking.</p>
<p>It all started a long time ago.
For a class, I had to implement a <a href="https://www.paulkocher.com/doc/DifferentialPowerAnalysis.pdf">differential power analysis attack</a>, a breakthrough in cryptanalysis as it was the first side-channel attack to be published.
A differential power analysis uses the power consumption of a device during an encryption to leak its private key.
At the time, I realized that great papers could convey great ideas with very little emphasis on understanding.
I remember banging my head against the wall trying to figure out what the author of the white paper was trying to say.
Worse, I couldn’t find a good resource that explained the paper.
So I banged my head a bit more, and finally I got it.
And then I thought I would help others.
So I drew some diagrams, animated them, and recorded myself going over them.
That was <a href="https://www.youtube.com/watch?v=gbqNCgVcXsM">my first screencast</a>.</p>
<p>This first step in education was enough to make me want to do more.
I started making more of these videos, and started writing more articles about cryptography on this blog (today totaling more than 500 articles).</p>
<p><img alt="we want to know" src="https://www.cryptologie.net/upload/we_want_to_know.png"></p>
<p>I realized early that diagrams were extremely helpful to understand complicated concepts, and that strangely most resources in the field shied away from them.</p>
<p>For example, anyone in cryptography who thinks about AES-CBC would immediately think about the following wikipedia diagram:</p>
<p><img alt="aes cbc" src="https://www.cryptologie.net/upload/600px-CBC_encryption.svg_.png"></p>
<p>So here I was, trying to explain everything I learned, and thinking hard about what sorts of simple diagrams could easily convey these complex ideas.
That’s when I started thinking about a book, years and years before <a href="https://manning.com/">Manning Publications</a> would reach out to me with a book deal.</p>
<h2>The applied cryptographer curriculum</h2>
<p> I hadn’t started cryptography due to a long-life passion.
I had finished a bachelor in theoretical mathematics and didn’t know what was next for me.
I had also been programming my whole life, and I wanted to reconcile the two.
Naturally, I got curious about cryptography, which seemed to have the best of both world, and started reading the different books at my disposal.
I quickly discovered my life's calling.</p>
<p>Some things were annoying me though. In particular, the long introductions that would start with history.
I was only interested in the technicalities, and always had been.
I swore to myself, if I ever wrote a book about cryptography, I would not write a single line on Vigenère ciphers, Caesar ciphers, and others.</p>
<p>And so after applying to the masters of Cryptography at the university of Bordeaux, and obtaining a degree in the subject, I thought I was ready for the world.
Little did I know.
What I thought was a very applied degree actually lacked a lot on the real world protocols I was about to attack.
I had spent a lot of time learning about the mathematics of elliptic curves, but nothing about how they were used in cryptographic algorithms.
I had learned about LFSRs, and ElGamal, and DES, and a series of other cryptographic primitives that I would never see again.</p>
<p>When I started working in the industry at Matasano, which then became NCC Group, my first gig was to audit <a href="https://www.openssl.org/">OpenSSL</a> (the most popular TLS implementation).
Oh boy, did it hurt my brain.
I remember coming back home every day with a strong headache.
What a clusterfuck of a library.
I had no idea at the time that I would years later become a co-author of TLS 1.3.</p>
<p><img alt="sign" src="https://www.cryptologie.net/upload/7._Note_that_digital_signatures_are_specified_with_a_hash_function,_allowing_you_to_.png"></p>
<p>But at that point I was already thinking: this is what I should have learned in school.
The knowledge I’m getting now is what would have been useful to prepare me for the real world.
After all, I was now a security practitioner specialized in cryptography.
I was reviewing real-world cryptographic applications.
I was doing the job that one would wish they had after finishing a cryptography degree.
I implemented, verified, used, and advised on what cryptographic algorithms to use.</p>
<p>This is the reason I’m the first reader of the book I’m writing.
This is what I would have written to my past self in order to prepare me for the real world.</p>
<h2>The use of cryptography is where most of the bugs are</h2>
<p>My consulting job led me to audit many real world cryptographic applications like the <a href="https://www.nccgroup.com/us/about-us/newsroom-and-events/blog/2015/may/openssl-audit/">OpenSSL</a>, the <a href="https://www.nccgroup.trust/globalassets/our-research/us/public-reports/2018/final_public_report_ncc_group_google_encryptedbackup_2018-10-10_v1.0.pdf">encrypted backup system of Google</a>, the <a href="https://blog.cloudflare.com/ncc-groups-cryptography-services-audit-of-tls-1-3/">TLS 1.3 implementation of Cloudflare</a>, the <a href="https://letsencrypt.org/2015/04/14/ncc-group-audit.html">certificate authority protocol of Let’s Encrypt</a>, the <a href="https://www.nccgroup.com/us/our-research/zcash-overwinter-consensus-and-sapling-cryptography-review/">sapling protocol of Zcash</a>, the <a href="https://blog.nucypher.com/security-audits--round-1--3/">threshold proxy re-encryption scheme of NuCypher</a> and dozens and dozens of other real-world cryptographic applications that I unfortunately cannot mention publicly.</p>
<p>Early in my job, I was tasked to audit the custom protocol a big corporation (that I can’t name) had written to encrypt their communications.
It turns out that, they were signing everything but the ephemeral keys, which completely broke the whole protocol (as one could have easily replaced the ephemeral keys).
A rookie mistake from anyone with some experience with secure transport protocols, but something that was missed by people who thought they were experienced enough to roll their own crypto.
I remember explaining the vulnerability at the end of the engagement, and a room full of engineers turning silent for a good 30 seconds.</p>
<p>This story repeated itself many times during my career.
There was this time where while auditing a cryptocurrency for another client, I found a way to forge transactions from already existing ones (due to some ambiguity of what was being signed).
Looking at TLS implementations for another client, I found some subtle ways to break an RSA implementation, which in turned transformed into a white paper (with one of the inventor of RSA) leading to a number of <a href="https://eprint.iacr.org/2018/1173">Common Vulnerabilities and Exposures (CVEs) reported to a dozen of open source projects</a>.
More recently, reading about Matrix as part of writing my book, I realized that their authentication protocol was completely broken, <a href="https://matrix.org/security-disclosure-policy/">leading to a complete break of their end-to-end encryption</a>.</p>
<p><img alt="comic" src="https://www.cryptologie.net/upload/HEY_MERE_S_AN.png"></p>
<p>There’s so many details that can unfortunately collapse under you, when making use of cryptography.
At that point, I knew I had to write something about it.
This is why my book contains many of these anecdotes.</p>
<p>As part of the job, I would review cryptography libraries and applications in a multitude of programming languages.
I discovered bugs (for example <a href="https://cryptologie.net/article/347/my-first-cve-o/?utm_content=buffer5c408&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">CVE-2016-3959</a> in Golang’s standard library), I researched ways that libraries could fool you into misusing them (for example see my paper <a href="https://eprint.iacr.org/2016/644">How to Backdoor Diffie-Hellman</a>), and I advised on what libraries to use.
Developers never knew what library to use, and I always found the answer to be tricky.</p>
<p>I went on to invent the <a href="https://discocrypto.com/">disco protocol</a>, and wrote a fully-featured cryptographic library in less than 1,000 lines of code in several languages.
Disco only relied on two cryptographic primitives: the permutation of SHA-3 and curve25519.
Yes, from only these two things in 1,000 lines of code a developer could do any type of authenticated key exchange, signatures, encryption, MACs, hashing, key derivation, etc.
This gave me a unique perspective as to what a good cryptographic library was supposed to be.</p>
<p>I wanted my book to contain these kind of practical insights.
So naturally, the different chapters contain examples on how to do crypto in different programming languages, using well-respected cryptographic libraries.</p>
<h2>A need for a new book?</h2>
<p>As I was giving <a href="https://www.blackhat.com/us-17/training/beyond-the-beast-a-broad-survey-of-crypto-vulnerabilities.html">one of my annual cryptography training at Black Hat</a>, one student came to me and asked if I could recommend a good book or online course on cryptography.
I remember advising the student to read <a href="http://toc.cryptobook.us/">the book from Boneh &amp; Shoup</a> and <a href="https://crypto.stanford.edu/~dabo/courses/OnlineCrypto/">Cryptography I from Boneh on Coursera</a>.</p>
<p>The student told me “<em>Ah, I tried, it’s too theoretical!</em>”.
This answer stayed with me.
I disagreed at first, but slowly realized that they were right.
Most of these resources were pretty heavy in math, and most developers interacting with cryptography don’t want to deal with math.
 What else was there for them?
The other two somewhat respected resources at the time were Applied Cryptography and Cryptography Engineering (both from Schneier).
But these books were starting to be quite outdated.
Applied Cryptography spent 4 chapters on block ciphers, with a whole chapter on cipher modes of operation but none on authenticated encryption.
Cryptography Engineering had a single mention of elliptic curve cryptography (in a footnote).</p>
<p>On the other hand, many of my videos or blog posts were becoming good primary references for some cryptographic concepts.</p>
<p><strong>I knew I could do something special</strong>.</p>
<p>Gradually, many of my students started becoming interested in cryptocurrencies, asking more and more questions on the subject.
At the same time, I started to audit more and more cryptocurrency applications.
I finally moved to a job at Facebook to work on <a href="https://libra.org/">Libra</a>.
Cryptocurrency was now one of the hottest field to work on, mixing a multitude of extremely interesting cryptographic primitives that so far had seen no real-world use case (zero knowledge proofs, aggregated signatures, threshold cryptography, multi-party computations, consensus protocols, cryptographic accumulators, verifiable random functions, verifiable delay functions, ... the list goes on)</p>
<p><strong>I was now in a unique position</strong>.</p>
<p>I knew I could write something that would tell students, developers, consultants, security engineers, and others, what modern applied cryptography was all about.</p>
<p><img alt="book" src="https://www.cryptologie.net/upload/needs_to_send_a_let.png"></p>
<p>This was going to be a book with very little …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/">https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/</a></em></p>]]>
            </description>
            <link>https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23743218</guid>
            <pubDate>Sun, 05 Jul 2020 23:45:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Modern Object Pascal Introduction for Programmers]]>
            </title>
            <description>
<![CDATA[
Score 96 | Comments 36 (<a href="https://news.ycombinator.com/item?id=23742999">thread link</a>) | @eatonphil
<br/>
July 5, 2020 | http://newpascal.org/assets/modern_pascal_introduction.html | <a href="https://web.archive.org/web/*/http://newpascal.org/assets/modern_pascal_introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
<div>
<h2 id="_why">1. Why</h2>
<div>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
This is a modified version of the original document from Michalis, because we (authors of <a href="http://newpascal.org/">http://newpascal.org/</a> and <a href="http://synopse.info/">http://synopse.info/</a> ) prefer the "mode delphi" without the "generic" / "specialize" keywords.
</td>
</tr>
</tbody></table>
</div>
<p>There are many books and resources about Pascal out there, but too many of them talk about the old Pascal, without classes, units or generics.</p>
<p>So I wrote this quick introduction to what I call <strong>modern Object Pascal</strong>. Most of the programmers using it don’t really call it <em>"modern Object Pascal"</em>, we just call it  <em>"our Pascal"</em>. But when introducing the language, I feel it’s important to emphasize that it’s a modern, object-oriented language. It evolved a <strong>lot</strong> since the old (Turbo) Pascal that many people learned in schools long time ago. Feature-wise, it’s quite similar to C++ or Java or C#.</p>
<div>
<ul>
<li>
<p>It has all the modern features you expect — classes, units, interfaces, generics…​</p>
</li>
<li>
<p>It’s compiled to a fast, native code,</p>
</li>
<li>
<p>It’s very type safe,</p>
</li>
<li>
<p>High-level but can also be low-level if you need it to be.</p>
</li>
</ul>
</div>
<p>It also has excellent, portable and open-source compiler called the <em>Free Pascal Compiler</em>, <a href="http://freepascal.org/">http://freepascal.org/</a> . And an accompanying IDE (editor, debugger, a library of visual components, form designer) called <em>Lazarus</em> <a href="http://lazarus.freepascal.org/">http://lazarus.freepascal.org/</a> . Myself, I’m the creator of <em>Castle Game Engine</em>, <a href="https://castle-engine.io/">https://castle-engine.io/</a> , which is a cool portable 3D and 2D game engine using this language to create games on many platforms (Windows, Linux, MacOSX, Android, iOS, web plugin).</p>
<p>This introduction is mostly directed at programmers who already have experience in other languages. We will not cover here the meanings of some universal concepts, like <em>"what is a class"</em>, we’ll only show how to do them in Pascal.</p>
</div>
</div>
<div>
<h2 id="_basics">2. Basics</h2>
<div>
<div>
<h3 id="__hello_world_program">2.1. "Hello world" program</h3>
<div>
<div>
<pre><code data-lang="pascal"><span>{$mode delphi}</span> 

<span>program</span> MyProgram; 
<span>begin</span>
  Writeln(<span><span>'</span><span>Hello world!</span><span>'</span></span>);
<span>end</span>.</code></pre>
</div>
</div>
<p>This is a complete program that you can <em>compile</em> and <em>run</em>.</p>
<div>
<ul>
<li>
<p>If you use the command-line FPC, just create a new file <code>myprogram.lpr</code> and execute <code>fpc myprogram.lpr</code>.</p>
</li>
<li>
<p>If you use <em>Lazarus</em>, create a new project (menu <em>Project</em> → <em>New Project</em> → <em>Simple Program</em>). Save it as <code>myprogram</code> and paste this source code as the main file. Compile using the menu item <em>Run → Compile</em>.</p>
</li>
<li>
<p>This is a command-line program, so in either case — just run the compiled executable from the command-line.</p>
</li>
</ul>
</div>
<p>The rest of this article talks about the Object Pascal language, so don’t expect to see anything more fancy than the command-line stuff. If you want to see something cool, just create a new GUI project in <em>Lazarus</em> (<em>Project</em> → <em>New Project</em> → <em>Application</em>).
Voila — a working GUI application, cross-platform, with native look everywhere, using a comfortable visual component library. The <em>Lazarus</em> and <em>Free Pascal Compiler</em> come with lots of ready units for networking, GUI, database, file formats (XML, json, images…​), threading and everything else you may need. I already mentioned my cool <em>Castle Game Engine</em> earlier:)</p>
</div>
<div>
<h3 id="_functions_procedures_primitive_types">2.2. Functions, procedures, primitive types</h3>
<div>
<div>
<pre><code data-lang="pascal"><span>{$mode delphi}</span>

<span>program</span> MyProgram;

<span>procedure</span> MyProcedure(<span>const</span> A: Integer);
<span>begin</span>
  Writeln(<span><span>'</span><span>A + 10 is: </span><span>'</span></span>, A + <span>10</span>);
<span>end</span>;

<span>function</span> MyFunction(<span>const</span> S: <span>string</span>): <span>string</span>;
<span>begin</span>
  Result := S + <span><span>'</span><span>strings are automatically managed</span><span>'</span></span>;
<span>end</span>;

<span>var</span>
  X: Single;
<span>begin</span>
  Writeln(MyFunction(<span><span>'</span><span>Note: </span><span>'</span></span>));
  MyProcedure(<span>5</span>);

  
  X := <span>15</span> / <span>5</span>;
  Writeln(<span><span>'</span><span>X is now: </span><span>'</span></span>, X); 
  Writeln(<span><span>'</span><span>X is now: </span><span>'</span></span>, X:<span>1</span>:<span>2</span>); 
<span>end</span>.</code></pre>
</div>
</div>
<p>To return a value from a function, assign something to the magic <code>Result</code> variable. You can read and set the <code>Result</code> freely, just like a local variable.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> MyFunction(<span>const</span> S: <span>string</span>): <span>string</span>;
<span>begin</span>
  Result := S + <span><span>'</span><span>something</span><span>'</span></span>;
  Result := Result + <span><span>'</span><span> something more!</span><span>'</span></span>;
  Result := Result + <span><span>'</span><span> and more!</span><span>'</span></span>;
<span>end</span>;</code></pre>
</div>
</div>
<p>You can also treat the function name (like <code>MyFunction</code> in example above) as the variable, to which you can assign. But I would discourage it in new code, as it looks "fishy" when used on the right side of the assignment expression. Just use <code>Result</code> always when you want to read or set the function result.</p>
<p>If you want to call the function itself recursively, you can of course do it. If you’re calling a parameter-less function recursively, be sure to specify the parenthesis (even though in Pascal you can usually omit the parentheses for a parameter-less function), this makes a recursive call to a parameter-less function different from accessing this function’s current result. Like this:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> ReadIntegersUntilZero: <span>string</span>;
<span>var</span>
  I: Integer;
<span>begin</span>
  Readln(I);
  Result := IntToStr(I);
  <span>if</span> I &lt;&gt; <span>0</span> <span>then</span>
    Result := Result + <span><span>'</span><span> </span><span>'</span></span> + ReadIntegersUntilZero();
<span>end</span>;</code></pre>
</div>
</div>
<p>You can call <code>Exit</code> to end the execution of the procedure or function before it reaches the final <code>end;</code>. If you call parameter-less <code>Exit</code> in a function, it will return the last thing you set as <code>Result</code>. You can also use <code>Exit(X)</code> construct, to set the function result and exit <strong>now</strong> — this is just like <code>return X</code> construct in C-like languages.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> AddName(<span>const</span> ExistingNames, NewName: <span>string</span>): <span>string</span>;
<span>begin</span>
  <span>if</span> ExistingNames = <span><span>'</span><span>'</span></span> <span>then</span>
    Exit(NewName);
  Result := ExistingNames + <span><span>'</span><span>, </span><span>'</span></span> + NewName;
<span>end</span>;</code></pre>
</div>
</div>
</div>
<div>
<h3 id="_testing_if">2.3. Testing (if)</h3>
<p>Use <code>if .. then</code> or <code>if .. then .. else</code> to run some code when some condition is satisfied. Unlike in the C-like languages, in Pascal you don’t have to wrap the condition in parenthesis.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A: Integer;
  B: boolean;
<span>begin</span>
  <span>if</span> A &gt; <span>0</span> <span>then</span>
    DoSomething;

  <span>if</span> A &gt; <span>0</span> <span>then</span>
  <span>begin</span>
    DoSomething;
    AndDoSomethingMore;
  <span>end</span>;

  <span>if</span> A &gt; <span>10</span> <span>then</span>
    DoSomething
  <span>else</span>
    DoSomethingElse;

  
  B := A &gt; <span>10</span>;
  <span>if</span> B <span>then</span>
    DoSomething
  <span>else</span>
    DoSomethingElse;
<span>end</span>;</code></pre>
</div>
</div>
<p>The <code>else</code> is paired with the last <code>if</code>. So this works as you expect:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> A &lt;&gt; <span>0</span> <span>then</span>
  <span>if</span> B &lt;&gt; <span>0</span> <span>then</span>
    AIsNonzeroAndBToo
  <span>else</span>
    AIsNonzeroButBIsZero;</code></pre>
</div>
</div>
<p>While the example with nested <code>if</code> above is correct, it is often better to place the nested <code>if</code> inside a <code>begin</code> …​ <code>end</code> block in such cases. This makes the code more obvious to the reader, and it will remain obvious even if you mess up the indentation. The improved version of the example is below. When you add or remove some <code>else</code> clause in the code below, it’s obvious to which condition it will apply (to the <code>A</code> test or the <code>B</code> test), so it’s less error-prone.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> A &lt;&gt; <span>0</span> <span>then</span>
<span>begin</span>
  <span>if</span> B &lt;&gt; <span>0</span> <span>then</span>
    AIsNonzeroAndBToo
  <span>else</span>
    AIsNonzeroButBIsZero;
<span>end</span>;</code></pre>
</div>
</div>
</div>
<div>
<h3 id="_logical_relational_and_bit_wise_operators">2.4. Logical, relational and bit-wise operators</h3>
<p>The <em>logical operators</em> are called <code>and</code>, <code>or</code>, <code>not</code>, <code>xor</code>. Their meaning is probably obvious (search for <em>"exclusive or"</em> if you’re unsure what <em>xor</em> does:). They take <em>boolean arguments</em>, and return a <em>boolean</em>. They can also act as <em>bit-wise operators</em> when both arguments are integer values, in which case they return an integer.</p>
<p>The <em>relational (comparison)</em> operators are <code>=</code>, <code>&lt;&gt;</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>. If you’re accustomed to C-like languages, note that in Pascal you compare two values (check are they equal) using a single equality character <code>A = B</code> (unlike in C where you use <code>A == B</code>). The special <em>assignment</em> operator in Pascal is <code>:=</code>.</p>
<p>The <em>logical (or bit-wise) operators have a higher precedence than relational operators</em>. So you may need to use parenthesis around some expressions.</p>
<p>For example this is a compilation error:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A, B: Integer;
<span>begin</span>
  <span>if</span> A = <span>0</span> <span>and</span> B &lt;&gt; <span>0</span> <span>then</span> ... </code></pre>
</div>
</div>
<p>The above fails to compile, because the compiler sees the bit-wise <code>and</code> inside: <code>(0 and B)</code>.</p>
<p>This is correct:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A, B: Integer;
<span>begin</span>
  <span>if</span> (A = <span>0</span>) <span>and</span> (B &lt;&gt; <span>0</span>) <span>then</span> ...</code></pre>
</div>
</div>
<p>The <em>short-circuit evaluation</em> is used. Consider this expression:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> MyFunction(X) <span>and</span> MyOtherFunction(Y) <span>then</span>...</code></pre>
</div>
</div>
<div>
<ul>
<li>
<p>It’s guaranteed that <code>MyFunction(X)</code> will be evaluated first.</p>
</li>
<li>
<p>And if <code>MyFunction(X)</code> returns <code>false</code>, then the value of expression is known (the value of <code>false and whatever</code> is always <code>false</code>), and <code>MyOtherFunction(Y)</code> will not be executed at all.</p>
</li>
<li>
<p>Analogous rule is for <code>or</code> expression. There, if the expression is known to be <code>true</code> (because the 1st operand is <code>true</code>), the 2nd operand is not evaluated.</p>
</li>
<li>
<p>This is particularly useful when writing expressions like</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> (A &lt;&gt; <span>nil</span>) <span>and</span> A.IsValid <span>then</span>...</code></pre>
</div>
</div>
<p>This will work OK, even when <code>A</code> is <code>nil</code>.</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_testing_single_expression_for_multiple_values_case">2.5. Testing single expression for multiple values (case)</h3>
<p>If a different action should be executed depending on the value of some expression, then the <code>case .. of .. end</code> statement is useful.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>case</span> SomeValue <span>of</span>
  <span>0</span>: DoSomething;
  <span>1</span>: DoSomethingElse;
  <span>2</span>: <span>begin</span>
       IfItsTwoThenDoThis;
       AndAlsoDoThis;
     <span>end</span>;
  <span>3</span>..<span>10</span>: DoSomethingInCaseItsInThisRange;
  <span>11</span>, <span>21</span>, <span>31</span>: AndDoSomethingForTheseSpecialValues;
  <span>else</span> DoSomethingInCaseOfUnexpectedValue;
<span>end</span>;</code></pre>
</div>
</div>
<p>The <code>else</code> clause is optional. When no condition matches, and there’s no <code>else</code>, then nothing happens.</p>
<p>In you come from C-like languages, and compare this with <code>switch</code> statement in these languages, you will notice that there is no automatic <em>fall-through</em>. This is a deliberate blessing in Pascal. You don’t have to remember to place <code>break</code> instructions. In every execution, <em>at most one</em> branch of the <code>case</code> is executed, that’s it.</p>
</div>
<div>
<h3 id="_enumerated_and_ordinal_types_and_sets_and_constant_length_arrays">2.6. Enumerated and ordinal types and sets and constant-length arrays</h3>
<p>Enumerated type in Pascal is a very nice, opaque type. You will probably use it much more often than enums in other languages:)</p>
<div>
<div>
<pre><code data-lang="pascal"><span>type</span>
  TAnimalKind = (akDuck, akCat, akDog);</code></pre>
</div>
</div>
<p>The convention is to prefix the enum names with a two-letter shortcut of type name, hence <code>ak</code> = shortcut for <em>"Animal Kind"</em>. This is a useful convention, since the enum names are in the unit (global) namespace. So by prefixing them with <code>ak</code> prefix, you minimize the chances of collisions with other identifiers.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
The collisions in names are not a show-stopper. It’s Ok for different units to define the same identifier. But it’s a good idea to try to avoid the collisions anyway, to keep code simple to understand and grep.
</td>
</tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
You can avoid placing enum names in the global namespace by directive <code>{$scopedenums on}</code>. This means you will have to access them qualified by a type name, like <code>TAnimalKind.akDuck</code>. The need for <code>ak</code> prefix disappears in this situation, and you will probably just call the enums <code>Duck, Cat, Dog</code>. This is …</td></tr></tbody></table></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://newpascal.org/assets/modern_pascal_introduction.html">http://newpascal.org/assets/modern_pascal_introduction.html</a></em></p>]]>
            </description>
            <link>http://newpascal.org/assets/modern_pascal_introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742999</guid>
            <pubDate>Sun, 05 Jul 2020 23:15:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing a winning 4K intro in Rust]]>
            </title>
            <description>
<![CDATA[
Score 266 | Comments 63 (<a href="https://news.ycombinator.com/item?id=23742870">thread link</a>) | @Dowwie
<br/>
July 5, 2020 | https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html | <a href="https://web.archive.org/web/*/https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-4371838969983872321" itemprop="description articleBody">
<div><p><span>I recently wrote my first 4K intro in Rust and released it at the Nova 2020 where it took first place in the new school intro competition. Writing a 4K intro is quite involved and requires you to master many different areas at the same time. Here I will focus on what I learned about making Rust code as small as possible.</span></p><p><iframe allowfullscreen="" height="322" src="https://www.youtube.com/embed/SIkkYRQ07tU" width="387" youtube-src-id="SIkkYRQ07tU"></iframe></p><p>You can view the demo on<span>&nbsp;</span><a href="https://www.youtube.com/watch?v=SIkkYRQ07tU">youtube</a>, download the executable at<span>&nbsp;</span><a href="https://www.pouet.net/prod.php?which=85924">pouet</a><span>&nbsp;</span>or get the source code from<span>&nbsp;</span><a href="https://github.com/janiorca/sphere_dance">github</a></p><p>A 4K intro is a demo where the entire program ( including any data ) has two be 4096 bytes or less so it is important that the code is as space efficient as possible. Rust has a bit of a reputation for creating bloated executables so I wanted to find out if is possible to create very space efficient code with it.</p><p>The entire intro is written in a combination of Rust and glsl. Glsl is used for rendering everything on screen but Rust does everything else; world creation, camera and object control, creating instruments and playing music etc.</p><p>Some of the features I depend on, such as xargo, are not yet part of stable Rust so I use the nightly rust toolchain. To install and use the nightly toolchain as default you need the following rustup commands.</p><pre data-info="" data-role="codeBlock"><code>rustup toolchain install nightly
rustup default nightly
</code></pre><p>I use<span>&nbsp;</span><a href="http://crinkler.net/">crinkler</a><span>&nbsp;</span>to compress the object file generated by the rust compiler.</p><p>I also used<span>&nbsp;</span><a href="https://github.com/laurentlb/Shader_Minifier">shader minifier</a><span>&nbsp;</span>for pre-processing the<span>&nbsp;</span><code>glsl</code><span>&nbsp;</span>shader to make it smaller and more crinkler friendly. The shader minifier doesn't support output into<span>&nbsp;</span><code>.rs</code><span>&nbsp;</span>files so I ended up using its raw output and manually copying it into my<span>&nbsp;</span><a href="http://shader.rs/">shader.rs</a><span>&nbsp;</span>file. (In hindsight, I should have written something to automate that stage. Or even created a PR for shader minifier)</p><p>The starting point was the proof of concept code I developed earlier (<a href="https://www.codeslow.com/2020/01/writing-4k-intro-in-rust.html">https://www.codeslow.com/2020/01/writing-4k-intro-in-rust.html</a>) which I thought was pretty lean at the time. That article also goes into but more detail about setting up the<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file and how to use xargo for compiling tiny executable.</p><p>Many of the most effective size optimizations have nothing to do with clever hacks but are the result of rethinking the design.</p><p>My initial design had one part of the code creating the world, including placing the spheres and another part was responsible for moving the spheres. At some point I realized that the sphere placement and sphere moving code were doing very similar things and I could merge them into one sightly more complicated function that did both. Unfortunately, this type of optimization can make the code less elegant and readable.</p><p>At some point you have to look at the compiled assembly code to understand what the code gets compiled into and what size optimizations are worth it. The Rust compiler has a very useful option,<span>&nbsp;</span><code>--emit=asm</code><span>&nbsp;</span>for outputting assembler code. The following command creates a<span>&nbsp;</span><code>.s</code><span>&nbsp;</span>assembly file;</p><pre data-info="" data-role="codeBlock"><code>xargo rustc --release --target i686-pc-windows-msvc -- --emit=asm
</code></pre><p>It is not necessary to be an expert in assembler to benefit from studying the assembler output but it definitely helps to have a basic understanding assembler syntax. The release version uses<span>&nbsp;</span><code>opt-level = "z</code><span>&nbsp;</span>which causes the compiler to optimize for the smallest possible size. This can make it a bit tricky to work out which part of the assembly code corresponds to which part of the Rust code.</p><p>I discovered that the Rust compiler can be surprisingly good at minimizing code; getting rid of unused code and unnecessary parameters and folding code. It can also do some strange things which is why it is essential to occasionally study the resulting assembly code.</p><p>I worked with two versions of the code; one version does logging and allows the viewer to manipulate the camera which is used for creating interesting camera paths. Rust allows you to define<span>&nbsp;</span><strong>features</strong><span>&nbsp;</span>that you can use to optionally include bits of functionality. The<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file has a<span>&nbsp;</span><strong>[features]</strong><span>&nbsp;</span>section that lets you declare the available features and their dependencies. My 4K intro has the following section in the<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file;</p><pre data-info="toml" data-role="codeBlock"><span>[</span><span>features</span><span>]</span>
<span>logger</span> <span>=</span> <span>[</span><span>]</span>
<span>fullscreen</span> <span>=</span> <span>[</span><span>]</span>
</pre><p>Neither of the optional features has dependencies so they effectively work as being conditional compilation flags. The conditional blocks of code are preceded by<span>&nbsp;</span><code>#[cfg(feature)]</code><span>&nbsp;</span>statement. Using features in itself does not make the code smaller but it makes development process much nicer when you easily switch between different feature sets.</p><pre data-info="rust" data-role="codeBlock">        <span>#[cfg(feature = "fullscreen")]</span>
        <span>{</span>
            
        <span>}</span>

        <span>#[cfg(not(feature = "fullscreen"))]</span>
        <span>{</span>
            
        <span>}</span>
</pre><p>Having inspected the compiled code I am certain that only the selected features get included in the compiled code.</p><p>One of the main uses of<span>&nbsp;</span><strong>features</strong><span>&nbsp;</span>was to enable logging and error checking for the debug build. The code loading and compiling the glsl shader failed frequently and without useful error messages it would have been extremely painful to find the problems.</p><p>When putting code inside an<span>&nbsp;</span><code>unsafe{}</code><span>&nbsp;</span>block I sort of assumed that all safety checks would be disabled within this block but this is not true, all the usual checks are still applied and these checks can be expensive.</p><p>By default Rust range checks all array accesses. Take the following Rust code</p><pre data-info="rust" data-role="codeBlock">    delay_counter <span>=</span> sequence<span>[</span> play_pos <span>]</span><span>;</span>
</pre><p>Before doing the table look up the compiler would insert code that checks that play_pos is not indexing past the end of sequence and panic if that was the case. This adds considerable size to the code as there can be a lot of table look-ups like this.</p><p>Converting the above code into</p><pre data-info="rust" data-role="codeBlock">    delay_counter <span>=</span> <span>*</span>sequence<span>.</span><span>get_unchecked</span><span>(</span> play_pos <span>)</span><span>;</span>
</pre><p>tells the compiler to not perform any range checks and just do the table look-up. This is clearly a potentially dangerous operation and can thus only be performed within an<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code block</p><p>Initially all my loops used the idiomatic rust way of doing loops, using the<span>&nbsp;</span><code>for x in 0..10</code><span>&nbsp;</span>syntax which I just assumed would be compiled into tightest possible loop. Surprisingly, this was not the case. The simplest case;</p><pre data-info="rust" data-role="codeBlock"><span>for</span> x <span>in</span> <span>0</span><span>..</span><span>10</span> <span>{</span>
    
<span>}</span>
</pre><p>would get translated into assembly code that did the following;</p><pre data-info="" data-role="codeBlock"><code>    setup loop variable
loop:
    check for loop condition    
    if loop finished, jump to end
    // do code inside loop
    unconditionally jump to loop
end:
</code></pre><p>whereas if did the following rust code</p><pre data-info="rust" data-role="codeBlock"><span>let</span> x <span>=</span> <span>0</span><span>;</span>
<span>loop</span><span>{</span>
    
    x <span>+=</span> <span>1</span><span>;</span>
    <span>if</span> i <span>==</span> <span>10</span> <span>{</span>
        <span>break</span><span>;</span>
    <span>}</span>
<span>}</span>
</pre><p>would get directly compiled into;</p><pre data-info="" data-role="codeBlock"><code>    setup loop variable
loop:
    // do code inside loop
    check for loop condition    
    if loop not finished, jump to loop
end:
</code></pre><p>Note that the loop condition is checked at the end of each loop which makes the unconditional jump unnecessary. This is small space saving for one loop but they do add up when there are 30 loops in the program.</p><p>The other, much harder to understand, problem with the idiomatic Rust loop is that in some cases it the compiler would add some additional iterator setup code that really bloated the code. I never fully understood what triggered this additional iterator setup as it was always trivial to replace the<span>&nbsp;</span><code>for {}</code><span>&nbsp;</span>constructs with a<span>&nbsp;</span><code>loop{}</code><span>&nbsp;</span>construct.</p><p>I spent a lot of time optimizing the<span>&nbsp;</span><code>glsl</code><span>&nbsp;</span>code and one of the best class of optimizations ( which also usually made the code run faster) was to operate on an entire vector at a time instead of operating at a component at a time.</p><p>For example, the ray tracing code use a fast<span>&nbsp;</span><a href="http://www.cse.yorku.ca/~amana/research/grid.pdf">grid traversal algorithm</a><span>&nbsp;</span>to check which parts of the map each ray visits. The original algorithm considers each axis separately but it is possible to rewrite the algorithm so it considers all axes at the same time and does not need any branches. Rust doesn't really have a native vector type like glsl but you can use intrinsics to tell it to use SIMD instructions.</p><p>To use intrinsics I would convert the following code</p><pre data-info="rust" data-role="codeBlock">        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>0</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>0</span> <span>]</span><span>*</span>camera_speed<span>;</span>
        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>1</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>1</span> <span>]</span><span>*</span>camera_speed<span>;</span>
        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>2</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>2</span> <span>]</span><span>*</span>camera_speed<span>;</span>
</pre><p>into</p><pre data-info="rust" data-role="codeBlock">        <span>let</span> <span>mut</span> dst<span>:</span>x86<span>:</span><span>:</span>__m128 <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_load_ps</span><span>(</span>global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>let</span> <span>mut</span> src<span>:</span>x86<span>:</span><span>:</span>__m128 <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_load_ps</span><span>(</span>camera_rot_speed<span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>)</span><span>;</span>
        dst <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_add_ps</span><span>(</span> dst<span>,</span> src<span>)</span><span>;</span>
        core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_store_ss</span><span>(</span> <span>(</span><span>&amp;</span><span>mut</span> global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>)</span><span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>,</span> dst <span>)</span><span>;</span>
</pre><p>which would be quite a bit smaller ( but a lot less readable ). Sadly, for some reason this broke the debug build while working perfectly on the release build. Clearly, this is a problem with my intrinsics knowledge and not a problem with Rust. This is something I would spend more time on for my next 4K intro as the space saving were significant.</p><p>There are lot of standard Rust crates for loading OpenGL functions but by default they all load a very large set of OpenGL functions. Each loaded function takes up some space because the loader has to know its name. Crinkler does a very good job of compressing this kind of code but it is not able to completely get rid of the overhead so I had to create my own version<span>&nbsp;</span><code>gl.rs</code><span>&nbsp;</span>that only includes the OpenGL functions that are used in the code.</p><p>My first objective was to write a competitive proper 4K intro to prove that language was suitable for scenarios where every single byte counts and you really need low level control. Typically this has been the sole domain of assembler and C. The secondary objective was to write it using idiomatic Rust as much possible.</p><p>I think I was fairly successful on the first objective. At no point during the development did I feel that Rust was holding me back in any way or that I was sacrificing performance or capabilities because I was using Rust rather than C.</p><p>I was less successful on the second objective. There is far too much<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code that doesn't really need to be there.<span>&nbsp;</span><code>Unsafe</code><span>&nbsp;</span>has a corrupting effect; it is very easy to use<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code to quickly accomplish something (like using mutable statics) but once the unsafe code is there it begets more unsafe code and suddenly it …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html">https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html</a></em></p>]]>
            </description>
            <link>https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742870</guid>
            <pubDate>Sun, 05 Jul 2020 23:00:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Triplebyte data download doesn’t give you all your data]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23742473">thread link</a>) | @wolfgang42
<br/>
July 5, 2020 | https://www.linestarve.com/blog/post/triplebyte-data-download/ | <a href="https://web.archive.org/web/*/https://www.linestarve.com/blog/post/triplebyte-data-download/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="<%= page.layout %>-<%= page.slug %>" itemscope="" itemprop="blogPost">
	<div>
		<div>
			<div>
				<p>In May of last year I decided to start looking for a new job, and started by taking <a href="https://triplebyte.com/">Triplebyte</a>’s quiz. Having passed that, I spent the next three months going through the rest of their process, from a <a href="https://triplebyte.com/interview_guide">two-hour remote interview</a> all the way through to final negotiations with the company whose offer I selected. Throughout the process they were extremely competent and helpful, and at the end of it all I had only good things to say about them. They made the whole process go extremely smoothly, answered all the questions I had and gave me a ton of advice on the whole process, and their screening process was not only great from the my perspective but also gave me confidence in the quality of all their candidates.</p>

<p>Then, a little over a month ago, I got an email announcing the upcoming launch of Triplebyte’s new public profiles. I thought they were was a neat idea, and made a note that I should turn mine on next time I started a job hunt. Then someone <a href="https://news.ycombinator.com/item?id=23279837">posted the email on Hacker News</a>, pointing out that buried in the middle of the email was the fact that these new profiles were going to be opt-<em>out,</em> and unless I turned it off in the next week my profile would become public. This understandably caused an uproar, which the Triplebyte CEO Ammon <a href="https://news.ycombinator.com/item?id=23280460">completely misinterpreted</a>, posting a series of <a href="https://news.ycombinator.com/item?id=23280120">inflammatory comments</a> that <a href="https://news.ycombinator.com/item?id=23280472">misunderstood what people were upset about</a> before vanishing. A few days later he came back with a very apologetic email explaining that they weren’t going to go through with it after all, though it received <a href="https://news.ycombinator.com/item?id=23303037">mixed reactions</a>, with a lot of people being concerned that the idea had been considered at all.</p>

<p>In the midst of all this, I submitted a request through the <a href="https://triplebyte.com/privacy-center">Triplebyte privacy center</a> to download my data. (I considered deleting my account, but decided to give them the benefit of the doubt until things settled down.) After approving the request by clicking an email link, I was informed that it might take up to 30 days to complete my request, so I settled down to wait. As the weeks passed, I thought that the sudden influx of requests must have overwhelmed whoever was responsible for gathering the data from all the systems it was stored in.</p>

<p>Then, 36 days after I first submitted the request, I got an email informing me that my data was now ready to be downloaded. I clicked the link in the email, and then another link on the next page, and finally I got—</p>

<p>A 2,917 byte JSON blob.</p>

<p><em>Odd,</em> I thought, <em>that seems like an awfully long time for so little data.</em> (It’s just over 81 bytes per day, in fact, though I realize that’s a silly metric.) Still, I was relieved to know that they hadn’t been gathering reams of data about me behind my back. Scanning over the minified data, it looked like all they had was my address, some information I’d given them about my past jobs and preferred languages, and a couple of recent IP addresses. Seemingly they hadn’t even kept any information at all about my job search with them.</p>

<p>Then I opened up the file in a JSON viewer and gradually realized: <em>this was not all the information they had.</em> It wasn’t even all of the information they were <em>willing to admit</em> they had—it was missing some obvious things, like the text descriptions on the <code>education</code> and <code>work experience</code> objects, which were prominently displayed on my profile page. As far as I can tell, all I got was a sloppy attempt at making it look at a casual glance like they’d given me what I asked for.</p>

<p>This raises serious concerns for me about Triplebyte, even more so than their plan to make profiles public by default, which started this all. That may well have been born of overenthusiastic naïvité, and was quickly rescinded after being exposed to public comment. After that fiasco, though, I would have expected them to double down on making sure that they were taking privacy seriously. They had over a month before sending me this data to fix any issues with the system, and instead they sent me some slapdash attempt at maybe giving me a whiff of my data.</p>

<p>Triplebyte (as they explain in their privacy center) “care deeply about how your personal information is used and shared,” but apparently not enough to actually put effort into getting it right when you ask for it.</p>

<p>(I’ve sent them an email asking what happened to the rest of the data, and will update this post when I get a response. As it’s the weekend I may not hear back for a few days.)</p>

			</div>
		</div>
	</div>
</article></div>]]>
            </description>
            <link>https://www.linestarve.com/blog/post/triplebyte-data-download/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742473</guid>
            <pubDate>Sun, 05 Jul 2020 22:02:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contrarian view on closing files]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 90 (<a href="https://news.ycombinator.com/item?id=23742390">thread link</a>) | @coady
<br/>
July 5, 2020 | https://coady.github.io/posts/closing-files/ | <a href="https://web.archive.org/web/*/https://coady.github.io/posts/closing-files/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <div>
<div>

<div>
<div>
<h2 id="Contrarian-view-on-closing-files.">Contrarian view on closing files.<a href="#Contrarian-view-on-closing-files.">¶</a>
</h2>
<p>It has become conventional wisdom to always explicitly close file-like objects, via context managers.
The <a href="https://google.github.io/styleguide/pyguide.html#311-files-and-sockets">google style guide</a>
is representative:</p>
<blockquote>
<p>Explicitly close files and sockets when done with them.
Leaving files, sockets or other file-like objects open unnecessarily has many downsides, including:</p>
<p>They may consume limited system resources, such as file descriptors.</p>
<ul>
<li>Code that deals with many such objects may exhaust those resources unnecessarily if they're not returned to the system promptly after use.</li>
<li>Holding files open may prevent other actions being performed on them, such as moves or deletion.</li>
<li>Files and sockets that are shared throughout a program may inadvertantly be read from or written to after logically being closed. If they are actually closed, attempts to read or write from them will throw exceptions, making the problem known sooner.</li>
</ul>
<p>Furthermore, while files and sockets are automatically closed when the file object is destructed, tying the life-time of the file object to the state of the file is poor practice, for several reasons:</p>
<ul>
<li>There are no guarantees as to when the runtime will actually run the file's destructor. Different Python implementations use different memory management techniques, such as delayed Garbage Collection, which may increase the object's lifetime arbitrarily and indefinitely.</li>
<li>Unexpected references to the file may keep it around longer than intended (e.g. in tracebacks of exceptions, inside globals, etc).</li>
</ul>
<p>The preferred way to manage files is using the "with" statement:</p>

<pre><code>with open("hello.txt") as hello_file:
    for line in hello_file:
        print line</code></pre>
</blockquote>
<h3 id="In-theory">In theory<a href="#In-theory">¶</a>
</h3>
<p>Good points, and why limit this advice to file descriptors?  Any resource may be limited or require exclusivity;  that's why they're called resources.  Similarly one should always explicitly call <code>dict.clear</code> when finished with a <code>dict</code>.  After all, "there are no guarantees as to when the runtime will actually run the &lt;object's&gt; destructor.  And "code that deals with many such objects may exhaust those resources unnecessarily", such as memory, or whatever else is in the <code>dict</code>.</p>
<p>But in all seriousness, this advice is applying a notably higher standard of premature optimization to file descriptors than to any other kind of resource.  There are plenty of Python projects that are guaranteed to run on CPython for a variety of reasons, where destructors are immediately called.  And there are plenty of Python projects where file descriptor usage is just a non-issue.  It's now depressingly commonplace to see this in <code>setup.py</code> files:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>with</span> <span>open</span><span>(</span><span>"README.md"</span><span>)</span> <span>as</span> <span>readme</span><span>:</span>
    <span>long_description</span> <span>=</span> <span>readme</span><span>.</span><span>read</span><span>()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<p>Let's consider a practical example: a <code>load</code> function which is supposed to read and parse data given a file path.</p>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>import</span> <span>csv</span>
<span>import</span> <span>json</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""the supposedly bad way"""</span>
    <span>return</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>filepath</span><span>))</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""the supposedly good way"""</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>return</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>file</span><span>)</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""with a different file format"""</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>return</span> <span>csv</span><span>.</span><span>reader</span><span>(</span><span>file</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>Which versions work correctly?  Are you sure?  If it's not immediately obvious why one of these is broken, that's the point.  In fact, it's worth trying out before reading on.</p>
<p>...</p>
<p>The <code>csv</code> version returns an iterator over a closed file.  It's a violation of procedural abstraction to know whether the result of <code>load</code> is lazily evaluated or not; it's just supposed to implement an interface.  Moreover, according to this best practice, it's <em>impossible</em> to write the <code>csv</code> version correctly.  As absurd as it sounds, it's just an abstraction that can't exist.</p>
<p>Defiantly clever readers are probably already trying to fix it.  Maybe like this:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>yield from</span> <span>csv</span><span>.</span><span>reader</span><span>(</span><span>file</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>No, it will not be fixed.  This version only appears to work by <em>not</em> closing the file until the generator is exhausted or collected.</p>
<p>This trivial example has deeper implications.  If one accepts this practice, then one must also accept that storing a file handle anywhere, such as on an instance, is also disallowed.  Unless of course that object then virally implements it owns context manager, ad infinitum.</p>
<p>Furthermore it demonstrates that often the context is not being managed locally.  If a file object is passed another function, then it's being used outside of the context.  Let's revisit the <code>json</code> version, which works because the file is fully read.  Doesn't a json parser have some expensive parsing to do after it's read the file?  It might even throw an error.  And isn't it desirable, trivial, <a href="https://github.com/python/cpython/blob/master/Lib/json/__init__.py#L274">and likely</a> that the implementation releases interest in the file as soon as possible?</p>
<p>So in reality there are scenarios where the supposedly good way could keep the file open <em>longer</em> than the supposedly bad way.  The original inline version does exactly what it's supposed to do: close the file when all interested parties are done with it.  Python uses garbage collection to manage shared resources.  Any attempt to pretend otherwise will result in code that is broken, inefficient, or reinventing reference counting.</p>
<p>A true believer now has to accept that <code>json.load</code> is a useless and dangerous wrapper, and that the only correct implementation is:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>contents</span> <span>=</span> <span>file</span><span>.</span><span>read</span><span>()</span>
    <span>return</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>contents</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>This line of reasoning reduces to the absurd: a file should never be passed or stored anywhere.  Next an example where the practice has caused real-world damage.</p>
<h3 id="In-practice">In practice<a href="#In-practice">¶</a>
</h3>
<p><a href="https://requests.readthedocs.io/en/master/">Requests</a> is one of the most popular python packages, and <a href="https://docs.python.org/3/library/http.client.html#module-http.client">officially recommended</a>.  It includes a <a href="http://requests.readthedocs.org/en/latest/user/advanced/#session-objects">Session</a> object which supports closing via a context manager.  The vast majority of real-world code uses the the top-level functions or single-use sessions.</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>...</span><span>)</span>

<span>with</span> <span>requests</span><span>.</span><span>Session</span><span>()</span> <span>as</span> <span>session</span><span>:</span>
    <span>response</span> <span>=</span> <span>session</span><span>.</span><span>get</span><span>(</span><span>...</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>Sessions manage the connection pool, so this pattern of usage is establishing a new connection every time.  There are popular standard API clients which seriously do this, for every single request to the same endpoint.</p>
<p>Requests' documentation prominently states that "Keep-alive and HTTP connection pooling are 100% automatic".  So part of the blame may lay with that phrasing, since it's only "automatic" if sessions are reused.  But surely a large part of the blame is the dogma of closing sockets, and therefore sessions, explicitly.
The whole point of a connection pool is that it may leave connections open, so users who genuinely need this granularity are working at the wrong abstraction layer.  <code>http.client</code> is already builtin for that level of control.</p>
<p>Tellingly, requests' own top-level functions didn't always close sessions.  There's a long history to that code, including a <a href="https://github.com/kennethreitz/requests/commit/3155bc99362a8c6ab136b6a3bb999732617cd2e5">version that only closed sessions on success</a>.  An older version was <a href="https://github.com/kennethreitz/requests/issues/1882">causing warnings</a>, when run to check for such warnings, and was being blamed for the <em>appearance</em> of <a href="https://github.com/kennethreitz/requests/issues/1685">leaking memory</a>.  Those threads are essentially debating whether a resource pool is "leaking" resources.</p>

</div>
</div>
</div>
<div>

<div>
<div>
<h3 id="Truce">Truce<a href="#Truce">¶</a>
</h3>
<p>Prior to <code>with</code> being introduced in Python 2.5, it was <em>not</em> recommended that inlined reading of a file required a <code>try... finally</code> block.  Far from it, in the past idioms like <code>open(...).read()</code> and <code>for line in open(...)</code> were lauded for being succinct and expressive.  But if all this orphaned file descriptor paranoia was well-founded, it would have been a problem back then too.</p>
<p>Finally, let's address readability.  It could be argued (though it rarely is) that showing the reader when the file is closed has inherent value.  Conveniently, that tends to align with having opened the file for writing anyway, thereby needing an reference to it.  In which case, the readability is approximately equal, and potential pitfalls are more realistic.  But readability is genuinely lost when the file would have been opened in a inline expression.</p>
<p>The best practice is unjustifiably special-casing file descriptors, and not seeing its own reasoning through to its logical conclusion.  This author proposes advocating for <em>anonymous read-only</em> <code>open</code> expressions.  Your setup script is not going to run out of file descriptors because you wrote <code>open("README.md").read()</code>.</p>

</div>
</div>
</div>
</div>
    </div></div>]]>
            </description>
            <link>https://coady.github.io/posts/closing-files/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742390</guid>
            <pubDate>Sun, 05 Jul 2020 21:50:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Transport makes up only 6% of the greenhouse gas emissions from food]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 69 (<a href="https://news.ycombinator.com/item?id=23741040">thread link</a>) | @shafyy
<br/>
July 5, 2020 | https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/ | <a href="https://web.archive.org/web/*/https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>There's a common misconception that eating locally produced foods is important from an environmental point of view. Even the <a href="https://twitter.com/UN/status/1188622911080415235">UN tweeted about it.</a> This is wrong.</p><p>Transport makes up only 6% of the greenhouse gas emissions from food:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/How-much-of-GHGs-come-from-food-1-.png" alt="" width="3889" height="3935" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/How-much-of-GHGs-come-from-food-1-.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/How-much-of-GHGs-come-from-food-1-.png 1000w, https://blog.yeticheese.com/content/images/size/w1600/2020/07/How-much-of-GHGs-come-from-food-1-.png 1600w, https://blog.yeticheese.com/content/images/size/w2400/2020/07/How-much-of-GHGs-come-from-food-1-.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://ourworldindata.org/environmental-impacts-of-food">Our World in Data</a>.</figcaption></figure><p>The reason for this is that most foods are transported by ship and not plane. Only about 0.16% of food miles are done by plane:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/share-food-miles-by-method.png" alt="" width="3400" height="2400" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/share-food-miles-by-method.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/share-food-miles-by-method.png 1000w, https://blog.yeticheese.com/content/images/size/w1600/2020/07/share-food-miles-by-method.png 1600w, https://blog.yeticheese.com/content/images/size/w2400/2020/07/share-food-miles-by-method.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://ourworldindata.org/grapher/share-food-miles-by-method">Our World in Data</a>.</figcaption></figure><p>It makes sense to try and avoid foods that are transported by air. Typically, those are foods which are highly perishable, such as asparagus, green beans and berries.</p><p>In some cases, eating local food even has a more negative impact on the environment than buying something that has been produced half way around the world. For example, heated greenhouses are energy intensive and can produce more greenhouse gases than transporting something for thousands of kilometers by water or road.</p><p>It's clear that avoiding meat and dairy has a much bigger impact on reducing greenhouse gas emissions.</p><p>So, why do people keep saying that we should eat local?</p><p>It could just be ignorance. However, I think that it's often a straw man argument pushed by interest groups that want to keep selling meat and dairy. It is something that is easy to do and seems to make sense on the surface to many people. Let's take another look at that UN tweet from before:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png" alt="" width="1194" height="634" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 1000w, https://blog.yeticheese.com/content/images/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 1194w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://twitter.com/UN/status/1188622911080415235">Tweet from @UN</a> on Oct 28, 2019.</figcaption></figure><p>In addition to eating local food, they also recommend unplugging unused appliances and using less hot water. Like avoiding plastic bags or plastic straws, this is good advice but a long shot from making a meaningful impact on climate change.</p><p>Arguments like these try to shift away the spot light from big companies who collectively make up a large chunk of the greenhouse gas emissions to individuals. People think that they did something meaningful by buying local food, which, as we have seen, is not the case.</p><p>I'm not saying that we shouldn't do those things. We absolutely should, but it shouldn't be the main talking points of organizations like the UN or WWF.</p><p>To make real change, we must eat less meat and dairy, move to more renewable energy sources and reduce air and road travel significantly.</p><p>PS: I'm only talking about the impact on climate change in this article. Eating local and organic food has other benefits such as supporting the local economy and in most cases it's a good idea to do it.</p><p>Comments or questions? Join in on the discussion on <a href="https://twitter.com/yeticheeseparty/status/1279850824378781697?s=20">this Twitter thread</a>. </p><!--kg-card-begin: html--><!-- Begin Mailchimp Signup Form -->


<div id="mc_embed_signup">
<p>
    Our plant-based Yeti Feta will be available to order soon. Leave your email below and we'll let you know when it's ready. (No newsletters or other shenanigans)
</p>

</div>

<!--End mc_embed_signup--><!--kg-card-end: html-->
			</section></div>]]>
            </description>
            <link>https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741040</guid>
            <pubDate>Sun, 05 Jul 2020 19:05:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Purpose of Persuasion]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 40 (<a href="https://news.ycombinator.com/item?id=23740669">thread link</a>) | @apsec112
<br/>
July 5, 2020 | https://www.persuasion.community/p/the-purpose-of-persuasion | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/the-purpose-of-persuasion">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg&quot;,&quot;height&quot;:972,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6649732,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p>Friends,</p><p>I'm floored by the response of the past three days.</p><p>Once I hit <em>send</em>, this article will land in the inboxes of over 15,000 people. When we launched, my main fear was that the world would not be interested in a community that pledges to defend the values of a free society; now, my main fear is that we won't be able to live up to the hype.</p><p>So, here's my promise: We will do our very best to earn your trust. Some great articles will be coming your way soon. We are getting ready to announce more events and high-level members of the community. I hope you will join us for our inaugural townhall, which will take place next Sunday, July 12th, at 4pm EST. (Watch this space for the invite.) I'm very, very excited about what lies ahead. But I also know that it is hard to build this kind of community from the ground up, and that we will undoubtedly make mistakes along the way. Please bear with us when we do.</p><p>In the meanwhile, I want to take you a little deeper into the thinking that went into creating <em>Persuasion</em>. Why this project? Why now? And how can just a bunch of us—even if we are a much larger bunch than I could possibly have dreamed a few days ago—really make a difference to the future of free societies in the United States and around the world? The key to an answer lies in a short (and necessarily schematic) history of American intellectual life over the past half century. </p><p>Fifty years ago, the most important American institutions enjoyed a degree of legitimacy that is now hard to fathom. Nearly every American watched the news on one of the three network television stations. Nearly every American had a positive opinion of Princeton and Stanford. Nearly every Member of Congress believed that the advice of the Brookings Institution or the Council on Foreign Relations was to be taken seriously.&nbsp;</p><p>These institutions had much to recommend them: They gave the public a shared set of facts and assumptions, which could form the basis of political debate. And, though they never thought of their primary goal as fighting for the ideals of a free society, their operating system was philosophically liberal: From CBS to Harvard to Brookings, senior decision makers instinctively believed in values like free speech and due process.</p><p>However, these institutions also suffered from two important shortcomings. First, the people they admitted into their gilded halls only represented a small slice of America's population: sexism, racism and homophobia were <em>far</em> more prevalent in these institutions than they are today. The views they considered serious sometimes included the morally abhorrent.&nbsp;</p><p>Second, the realm of the “reasonable" was rather narrow. And, though this narrowness of debate constituted the lesser injustice, it was—at least in the short term—the cause of greater instability: Having come to believe that they could never quite speak in their own voices in the halls of the Brookings Institution or the column inches of the <em>New York Times</em>, a few assorted bands of malcontents started to cast around for an alternative. </p><p>Of these, the group that had the biggest impact on public life in America was a band of devoted conservatives, determined to create an ideological counter-establishment that could rival the mainstream.&nbsp;What started with <em>National Review</em>, an ideological fighting magazine, quickly grew into a sprawling and immensely powerful network of conservative institutions. The Heritage Foundation was set up to rival the influence of Brookings. The Federalist Society sought to change the ideological composition of America's judiciary. Fox News did its dismal best to spread the ideas of the conservative movement beyond the Beltway. A whole network of activist groups provided conservatives with an ideological foundation, a group of friends, and a professional home. Measured by its own ambitions,&nbsp;the movement was a staggering success.</p><p>Other minoritarian ideological movements took a page out of the same playbook. In 1960, a libertarian was a person with idiosyncratic views and no obvious political home. Then, the Institute for Humane Studies started to advocate libertarian ideas on college campuses, <em>Reason </em>took up their public defense, and a reinvigorated American Enterprise Institute set out to influence legislators on Capitol Hill. By 1980, the influence and intellectual self-confidence of libertarians had increased enormously.</p><p>The further left has always had its share of counter-establishment institutions. <em>The Nation</em>, after all, is one of the oldest magazines in the country, and some academic disciplines have long been at the forefront of leftist thought. But the left, too, has of late succeeded in building a more cohesive network of fighting institutions, as universities have become much more progressive, movements like the Democratic Socialists of America have awakened from decades of peaceful slumber, and publications like <em>Jacobin </em>have infused the movement with fresh intellectual energy.</p><p>Five or ten years ago, our potted history might have concluded here. Ideological movements from conservative to libertarian to leftist had fighting institutions of their own. Though philosophical liberals did not have a comparable home, they could confidently express their views within mainstream institutions.&nbsp;</p><p>But then those institutions started to change.</p><p>The story of that change has attracted an immense amount of attention over the past months. I won't bore you with a detailed recap of its most worrying manifestations, from the firing of James Bennet to the uncritical celebration of Robin DiAngelo. Nor do I want to suggest that these changes have completely delegitimized the mainstream: These institutions have not yet become wholly illiberal, and the advocates of a free society would be foolish to stop fighting for them.</p><p>But the erosion of values like free speech and due process within mainstream institutions does put philosophical liberals at a unique disadvantage. It is difficult to convey just how many amazing writers, journalists, and think-tankers—some young and some old, some relatively obscure and others very famous—have privately told me that they can no longer write in their own voices; that they are counting the days until they get fired; and that they don't know where to turn if they do. (Astonishingly, a number of them are far enough to the left to have supported Bernie Sanders in the primaries.)</p><p>This, to me, is a huge part of the reason why the defenders of the free society have seemed to lack conviction in recent months and years. Feeling, at best, begrudgingly tolerated by the institutions that employ them, they are always on the back foot: writing and speaking with one eye on Twitter, one eye on a hostile editor, and one eye on the attacks being shared on their own company’s Slack channel. (As you may have noticed, that requires too many eyes.)</p><p>But, if this situation helps to explain the collective lack of confidence among the advocates of a free society, it also points the way to an obvious solution. <strong>Instead of lamenting our loss of control over the establishment, we should follow the lead of other movements that have successfully built their own counter-establishment institutions.</strong><em>&nbsp;</em></p><p><em>That </em>is the goal I had in mind in starting <em>Persuasion</em>.</p><p>One core element of this project is a publishing platform explicitly devoted to debating, articulating, and defending the values of a free society. Emulating what <em>Reason</em>, <em>Jacobin,</em> and the <em>National Review</em> have accomplished within their own ideological traditions, I hope to create a space in which philosophical liberals can ask hard questions and come up with compelling answers. This requires both a commitment to a set of shared aspirations and enough diversity of opinion to force us to think very hard about how we can make the world a better place. This is a space for people who are open to changing their minds, but not their fundamental values.</p><p>But creating a modern reinvention of a fighting magazine, devoted to defending the ideals of a free society, is not my only ambition. If places like the <em>National Review</em> had a tremendous influence on our society, it is also because they became the nucleus of a cohesive community, which seeded a much wider archipelago of allied institutions. This is why I take the community element of <em>Persuasion</em>—all the live events, book clubs and social gatherings we'll experiment with over the coming months—so seriously. And it is also why I hope that this particular venture will spawn many formally independent organizations that share our founding values.</p><p>Before I close, let me say two quick words about some of the establishment institutions whose recent fate I have been lamenting. The first is that we must do what we can to preserve those universities, publications, and think tanks that still operate with fundamentally (small l) liberal assumptions. For example, I deeply love <em>The Atlantic</em>, and will continue to write for it. A small fighting institution that primarily addresses a devoted crowd of philosophical liberals neither is nor should be in competition with a large general interest magazine whose readership will always span a much broader ideological range. Part of the reason why we should articulate these values as clearly, forcefully, and persuasively as possible within these pages is to maximize the likelihood that they will continue to form the implicit operating system of vitally important publications like <em>The Atlantic</em>.&nbsp;</p><p>The second thing is that our ambition needs to extend beyond nostalgia. There is much to lament about the changes that have taken place in some of the country's most important institutions over the past years. But there is also much to criticize in what these institutions looked like at their supposed best. Our goal is not to return to a golden age that has, sadly, never existed; it is to build societies that live up to the noble and ambitious values of freedom and justice better than any society of the past.</p><p>The examples I have used here&nbsp;are very …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.persuasion.community/p/the-purpose-of-persuasion">https://www.persuasion.community/p/the-purpose-of-persuasion</a></em></p>]]>
            </description>
            <link>https://www.persuasion.community/p/the-purpose-of-persuasion</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740669</guid>
            <pubDate>Sun, 05 Jul 2020 18:16:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Deep Introduction to JIT Compilers: JITs are not very Just-in-time]]>
            </title>
            <description>
<![CDATA[
Score 278 | Comments 89 (<a href="https://news.ycombinator.com/item?id=23740655">thread link</a>) | @chrisseaton
<br/>
July 5, 2020 | https://carolchen.me/blog/jits-intro/ | <a href="https://web.archive.org/web/*/https://carolchen.me/blog/jits-intro/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <header>
    
    
  </header>

  <main id="main">
    
<article>
  <header>
    
    
  </header>
  <section id="js-article">
    
<p><em>If you are familiar with how JITs generally work (if you get what the title is referring to), I recommend skimming this or going straight to reading <a href="https://carolchen.me/blog/jits-impls">How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More</a></em> </p>
<p>My mentor, <a href="https://chrisseaton.com/">Chris</a>, who took me from “what is a JIT” to where I am now once told me that compilers were just bytes in bytes out and not at all low-level and scary. This is actually fairly true, and it's fun to learn about compiler internals and often useful for programmers everywhere!</p>
<p>This blog post gives background on how programming languages are implemented and how JITs work. It'll introduce the implementation details of the Julia language, though it won't talk about specific implementation details or optimizations made by more traditional JITs. Check out <a href="https://carolchen.me/blog/jits-impls">How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More</a> to read about how meta-tracing is implemented, how Graal supports C extensions, the relationship of JITs with LLVM and more!</p>
<h2 id="how-programming-languages-are-implemented">How Programming Languages are Implemented<a href="#how-programming-languages-are-implemented" aria-label="Anchor link for: how-programming-languages-are-implemented"> <i></i></a>
</h2>
<p>When we run a program, it’s either interpreted or compiled in some way. The compiler/interpreter is sometimes referred to as the "implementation" of a language, and one language can have many implementations. You may have heard things like "Python is interpreted", but that really means the reference(standard/default) implementation of Python is an interpreter. Python is a language specification and <em>CPython</em> is the interpreter and implementation of Python. </p>
<p>An interpreter is a program that directly executes your code. Well-known interpreters are usually written in C. Ruby, Python and PHP are written in C. Below is a function that loosely models how an interpreter might work:</p>
<pre><code><span>func </span><span>interpret</span><span>(</span><span>code </span><span>string</span><span>) {
  </span><span>if </span><span>code </span><span>== </span><span>"print('Hello, World!')" </span><span>{
    </span><span>print</span><span>(</span><span>"Hello, World"</span><span>);
  } </span><span>else if </span><span>code </span><span>==</span><span> “</span><span>x </span><span>= </span><span>0</span><span>; </span><span>x </span><span>+= </span><span>4</span><span>; </span><span>print</span><span>(</span><span>x</span><span>)” {
    variable_x </span><span>:= </span><span>0 
    </span><span>variable_x </span><span>+= </span><span>4
    </span><span>print</span><span>(</span><span>x</span><span>)
  }
}
</span></code></pre>
<p>A compiler is a program that translates code from some language to another language, though it usually refers to a destination language that is a machine code. Examples of compiled languages are C, Go and Rust.</p>
<pre><code><span>func </span><span>compile</span><span>(</span><span>code </span><span>string</span><span>) {
  []</span><span>byte </span><span>compiled_code </span><span>= </span><span>get_machine_code</span><span>(</span><span>code</span><span>);
  </span><span>write_to_executable</span><span>(</span><span>compiled_code</span><span>);
}
</span></code></pre>
<p>The difference between a compiled and interpreted language is actually much more nuanced. C, Go and Rust are clearly compiled, as they output a machine code file - which can be understood natively by the computer. The compile and run steps are fully distinct.</p>
<p>However, compilers can translate to any target language (this is sometimes called transpiling). Java for example, has a two-step implementation. The first is compiling Java source to bytecode, which is an Intermediate Representation (IR). The bytecode is then JIT compiled - which involves interpretation.</p>
<p>Python and Ruby also execute in two steps. Despite being known as interpreted languages, their reference implementations actually compile the source down to a bytecode. You may have seen .pyc files (not anymore in Python3) which contain Python bytecode! The bytecode is then interpreted by a virtual machine. These interpreters use bytecode because programmers tend to care less about compile time, and creating a bytecode language allows the engineers to specify a bytecode that is as efficient to interpret as possible. </p>
<p>Having bytecode is how languages check syntax before execution (though they could technically just do a pass before starting the interpreter). An example below shows why you would want to check syntax before runtime.</p>
<pre><code><span>sleep</span><span>(</span><span>1000</span><span>)
bad syntax beep boop beep boop
</span></code></pre>
<p>Another important note is that interpreted languages are typically slower for various reasons, the most obvious being that they're executed in a higher level language that has overhead execution time. The main reason is that the dynamic-ness of the languages they tend to implement means that they need many extra instructions to decide what to do next and how to route data. People still choose to build interpreters over compilers because they're easier to build and are more suited to handle things like dynamic typing, scopes etc (though you could build a compiler that has the same features). </p>
<h3 id="so-what-is-a-jit">So What is a JIT?<a href="#so-what-is-a-jit" aria-label="Anchor link for: so-what-is-a-jit"> <i></i></a>
</h3>
<p>A JIT compiler doesn't compile code Ahead-Of-Time (AOT), but still compiles source code to machine code and therefore is not an interpreter. JITs compile code at runtime, while your program is executing. This gives the JITs flexibility for dynamic language features, while maintaining speed from optimized machine code output. JIT-compiling C would make it slower as we'd just be adding the compilation time to the execution time. JIT-compiling Python would be fast, as compilation + executing machine code can often be faster than interpreting, especially since the JIT has no need to write to a file (disk writing is expensive, memory/RAM/register writing is fast). JITs also improve in speed by being able to optimize on information that is only available at runtime.</p>
<h3 id="julia-a-jit-compiler-that-s-just-in-time">Julia: a JIT Compiler that's Just-in-time<a href="#julia-a-jit-compiler-that-s-just-in-time" aria-label="Anchor link for: julia-a-jit-compiler-that-s-just-in-time"> <i></i></a>
</h3>
<p>A common theme between compiled languages is that they're statically typed. That means when the programmer creates or uses a value, they’re telling the computer what type it is and that information is guaranteed at compile time.</p>
<p>Julia is dynamically typed, but internally Julia is much closer to being statically typed.</p>
<pre><code><span>function </span><span>multiply</span><span>(x, y)
  x </span><span>*</span><span> y
</span><span>end
</span></code></pre>
<p>Here is an example of a Julia function, which could be used to multiply integers, floats, vectors, strings etc (Julia allows operator overloading). Compiling out the machine code for <em>all</em> these cases is not very productive for a variety of reasons, which is what we'd have to do if we wanted Julia to be a compiled language. Idiomatic programming means that the function will probably only be used by a few combinations of types and we don't want to compile something that we don't use yet since that's not very jitty (this is not a real term).</p>
<p>If I were to code <code>multiply(1, 2)</code>, then Julia will compile a function that multiplies integers. If I then wrote <code>multiply(2, 3)</code>, then the already-compiled code will be used. If I then added <code>multiply(1.4, 4)</code>, another version of the function will be compiled. We can observe what the compilation does with <code>@code_llvm multiply(1, 1)</code>, which generates LLVM Bitcode (not quite machine code, but a lower-level Intermediate Representation).</p>
<pre><code><span>define i64 @julia_multiply_17232(i64, i64) {
top</span><span>:</span><span>
; ┌ @ int</span><span>.</span><span>jl</span><span>:</span><span>54</span><span> within `*'
   </span><span>%</span><span>2 </span><span>=</span><span> mul i64 </span><span>%</span><span>1</span><span>, </span><span>%</span><span>0</span><span>
; └
  ret i64 </span><span>%</span><span>2</span><span>
}
</span></code></pre>
<p>And with <code>multiply(1.4, 4)</code>, you can see how complicated it can get to compile even one more function. In AOT compiled Julia, all (some optimizations can be made to reduce) of these combinations would have to live in the compiled code even if only one was used, along with the control flow to delegate. </p>
<pre><code><span>define double @julia_multiply_17042(double, i64) {
top</span><span>:</span><span>
; ┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>312</span><span> within `*'
; │┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>282</span><span> within `promote'
; ││┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>259</span><span> within `_promote'
; │││┌ @ number</span><span>.</span><span>jl</span><span>:</span><span>7</span><span> within `convert'
; ││││┌ @ float</span><span>.</span><span>jl</span><span>:</span><span>60</span><span> within `</span><span>Float64</span><span>'
       </span><span>%</span><span>2 </span><span>=</span><span> sitofp i64 </span><span>%</span><span>1</span><span> to double
; │└└└└
; │ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>312</span><span> within `*' @ float</span><span>.</span><span>jl</span><span>:</span><span>405
   </span><span>%</span><span>3 </span><span>=</span><span> fmul double </span><span>%</span><span>2</span><span>, </span><span>%</span><span>0</span><span>
; └
  ret double </span><span>%</span><span>3</span><span>
}
</span></code></pre>
<p>The general strategy of “assume a type and compile/behave based on that” is called type inferencing, which Julia mildly uses in the examples above. There are a lot of other compiler optimizations that are made, though none of them are very specific to JITs as Julia may be better described as a lazy AOT compiler.</p>
<p>The simplicity of this kind of jitting makes it easy for Julia to also supply AOT compilation. It also helps Julia to benchmark very well, definitely a tier above languages like Python and comparable to C (I'd cite numbers, but those are always nuanced and I don't want to get into that).</p>
<h3 id="so-what-is-a-jit-take-two">So What is a JIT? Take Two.<a href="#so-what-is-a-jit-take-two" aria-label="Anchor link for: so-what-is-a-jit-take-two"> <i></i></a>
</h3>
<p>Julia is actually the jittiest JIT I'll discuss, but not the most interesting as a "JIT". It actually compiles code right before the code needs to be used -- just in time. Most JITs however (Pypy, Java, JS Engines), are not actually about compiling code just-in-time, but compiling <em>optimal code</em> at an optimal time. In some cases that time is actually never. In other cases, compilation occurs more than once. In a vast majority of the cases compilation doesn't occur until after the source code has been executed numerous times, and the JIT will stay in an interpreter as the overhead to compilation is too high to be valuable.</p>
<p><img src="https://carolchen.me/blog/img/jits/jitbrr.jpg" alt=""></p>
<p>The other aspect at play is generating <em>optimal code</em>. Assembly instructions are not created equal, and compilers will put a lot of effort into generating well-optimized machine code. Usually, it is possible for a human to write better assembly than a compiler (though it would take a fairly smart and knowledgeable human), because the compiler cannot dynamically analyze your code. By that, I mean things like knowing the possible range of your integers or what keys are in your map, as these are things that a computer could only know after (partially) executing your program. A JIT compiler can actually do those things because it interprets your code first and gathers data from the execution. Thus, JITs are expensive in that they interpret, and add compilation time to execution time, but they make it up in highly optimised compiled code. With that, the timing of compilation is also dependent on whether the JIT has gathered enough valuable information.</p>
<p>The cool part about JITs is that I was sort of lying when I said a JIT implementation of C could not be faster than existing compiled implementations. It would not be feasible to try, but jit-compiling C in the way I just described is not a strict superset of compiling a language and thus it is not logically impossible to compile code fast enough to make up for the compile+profile+interpreting time. If I "JIT compiled" C similarly to how Julia does it (statically compile each function as it's called), it would be impossible to …</p></section></article></main></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://carolchen.me/blog/jits-intro/">https://carolchen.me/blog/jits-intro/</a></em></p>]]>
            </description>
            <link>https://carolchen.me/blog/jits-intro/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740655</guid>
            <pubDate>Sun, 05 Jul 2020 18:14:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Choosing a Rust web framework]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 30 (<a href="https://news.ycombinator.com/item?id=23740028">thread link</a>) | @LukeMathWalker
<br/>
July 5, 2020 | https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/ | <a href="https://web.archive.org/web/*/https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    

<blockquote>
<p><em>This post was originally meant as a section of <a href="https://www.lpalmieri.com/posts/2020-05-24-zero-to-production-0-foreword/"><strong>Zero To Production</strong></a> to explain the reasoning behind our technology choice. It eventually grew so large to be its own article!</em></p>

<p><em>You can discuss the article on <a href="https://news.ycombinator.com/item?id=23740028">HackerNews</a> or <a href="https://www.reddit.com/r/rust/comments/hlpsw5/choosing_a_rust_web_framework_2020_edition/">r/rust</a></em>.</p>
</blockquote>

<p>As of July 2020, the main web frameworks in the Rust ecosystem are:</p>

<ul>
<li><a href="https://actix.rs/"><code>actix-web</code></a>;<br></li>
<li><a href="https://rocket.rs/"><code>rocket</code></a>;<br></li>
<li><a href="https://github.com/http-rs/tide"><code>tide</code></a>;<br></li>
<li><a href="https://github.com/seanmonstar/warp"><code>warp</code></a>.</li>
</ul>

<p>Which one should you pick if you are about to start building a new <strong>production-ready</strong> API in Rust?</p>

<p>I will break down where each of those web frameworks stands when it comes to:</p>

<ul>
<li><a href="#1-comprehensiveness">Comprehensiveness</a>;<br></li>
<li><a href="#2-community-and-adoption">Community and adoption</a>;<br></li>
<li><a href="#3-sync-vs-async">Sync vs Async</a>, as well as their choice of <a href="#3-1-futures-runtime">futures runtime</a>;<br></li>
<li><a href="#4-documentation-tutorials-and-examples">Documentation, tutorials and examples</a>;<br></li>
<li><a href="#5-api-and-ergonomics">API and ergonomics</a>.</li>
</ul>

<p>I will in the end make <a href="#6-our-choice">my recommendation</a>.<br>
Worth remarking that there are no absolutes: different circumstances (and taste) might lead you to a different pick.</p>

<h2 id="1-comprehensiveness">1. Comprehensiveness</h2>

<p><code>actix-web</code>, <code>tide</code> and <code>warp</code> are <em>slim</em> web frameworks: they offer you an HTTP web server, routing logic, middleware infrastructure and basic building blocks and abstractions to parse, manipulate and respond to HTTP requests.</p>

<p><code>rocket</code> takes a different approach - it aims to be batteries-included: the most common needs should be covered by functionality provided out-of-the-box by <code>rocket</code> itself, with hooks for you to extend <code>rocket</code> if your usecase needs it.<br>
It should not come as a surprise then that <code>rocket</code> ships an easy-to-use <a href="https://rocket.rs/v0.4/guide/state/#databases">integration to manage connection pools</a> for several popular database (e.g. Postgres, Redis, Memcache, etc.) as well as its own <a href="https://rocket.rs/v0.4/guide/configuration/">configuration system</a> in <a href="https://api.rocket.rs/v0.4/rocket_contrib/"><code>rocket-contrib</code></a>, an ancillary crate hosted in <code>rocket</code>’s own repository.</p>

<p>We can compare them to frameworks available in other ecosystems:</p>

<ul>
<li><code>actix-web</code>, <code>tide</code> and <code>warp</code> are closer in spirit to <a href="https://palletsprojects.com/p/flask/"><code>Flask</code></a> from Python or <a href="https://expressjs.com/"><code>Express</code></a> from Javascript - they might be opinionated, but they do not ship a configuration management system or an ORM integration out of the box. You are in charge of structuring your API as you deem appropriate, bringing all the necessary crates and patterns into the picture;<br></li>
<li><code>rocket</code> is closer to <a href="https://www.djangoproject.com/"><code>Django</code></a> from Python or <a href="https://symfony.com/"><code>Symphony</code></a> from PHP: a stable and solid core with a set of high-quality in-tree components to fulfill your every day needs when building a solid web application. <code>rocket</code> has still a long way to go to match its peers in breadth and scope, but it is definitely off to a good start.</li>
</ul>

<p>Of course this is a snapshot of the landscape as of today, but the situation is continuously shifting according to the maintainers’ intentions - e.g. <code>actix-web</code> has slowly been accumulating more and more supporting functionality (from security to session management) in <a href="https://github.com/actix/actix-extras"><code>actix-extras</code></a>, under the umbrella of the <code>actix</code> GitHub organization.<br>
Furthermore, using a slim web framework does not force you to write everything from scratch as soon as the framework is falling short of your needs: you can leverage the ecosystem built by the community around it to avoid re-inventing the wheel on every single project.</p>

<h2 id="2-community-and-adoption">2. Community and adoption</h2>

<p>Numbers can be misleading, but they are a good conversation starting point. Looking at <a href="https://crates.io/">crates.io</a>, we have:</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th>Total Downloads</th>
<th>Daily Downloads</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>actix-web</code></td>
<td>~1250k</td>
<td>~3000</td>
</tr>

<tr>
<td><code>rocket</code></td>
<td>~525k</td>
<td>~1000</td>
</tr>

<tr>
<td><code>warp</code></td>
<td>~435k</td>
<td>~3000</td>
</tr>

<tr>
<td><code>tide</code></td>
<td>~47k</td>
<td>~300</td>
</tr>
</tbody>
</table>

<p>The number of total downloads is obviously influenced by how long a framework has been around (e.g. <code>actix-web:0.1.0</code> came out at the end of 2017!) while daily downloads are a good gauge for the current level of interest around it.</p>

<p>You should care about adoption and community size for a couple of reasons:</p>

<ul>
<li>consistent production usage over years makes it way less likely that you are going to be the first one to spot a major defect. Others cried so that you could smile (most of the time);<br></li>
<li>it correlates with the number of supporting crates for that framework;<br></li>
<li>it correlates with the amount of tutorials, articles and helping hands you are likely to find if you are struggling.</li>
</ul>

<p>The second point is particularly important for slim frameworks.<br>
You can get a feel of the impact of community size, once again, by looking at the number of results popping up on <a href="https://crates.io/">crates.io</a> when searching a framework name:</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th># results</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>rocket</code></td>
<td>178</td>
</tr>

<tr>
<td><code>actix-web</code></td>
<td>113</td>
</tr>

<tr>
<td><code>warp</code></td>
<td>57</td>
</tr>

<tr>
<td><code>tide</code></td>
<td>20</td>
</tr>
</tbody>
</table>

<p>Will all those crates be relevant? Unlikely.<br>
Will a fair share of them be outdated or unproven? Definitely.</p>

<p>Nonetheless it is a good idea, before starting a project, to have a quick look for functionality you know for a fact you will need. Let’s make a couple of quick examples with features we will be relying on in the email newsletter implementation we are building in <em>Zero To Production</em>:</p>

<ul>
<li>if you need to add Prometheus’ metrics to your API you can get off the ground in a couple of minutes with <a href="https://crates.io/crates/actix-web-prom"><code>actix-web-prom</code></a> or <a href="https://crates.io/crates/rocket_prometheus"><code>rocket-prometheus</code></a>, both with thousands of downloads. If you are using <code>warp</code> or <code>tide</code> you will have to write the integration from scratch;<br></li>
<li>if you want to add distributed tracing, <a href="https://crates.io/crates/actix-web-opentelemetry"><code>actix-web-opentelemetry</code></a> has your back. You will have to re-implement it if you choose any other framework.</li>
</ul>

<p>Most of these features are not too much work to implement, but the effort (especially maintenance) compounds over time. You need to choose your framework with your eyes wide open on the level of commitment it is going to require.</p>

<h2 id="3-sync-vs-async">3. Sync vs Async</h2>

<p>Rust landed its <code>async</code>/<code>await</code> syntax in version <code>1.39</code> - a game changer in terms of ergonomics for asynchronous programming.<br>
It took some time for the whole Rust ecosystem to catch up and adopt it, but it’s fair to say that crates dealing with IO-bound workloads are now generally expected to be async-first (e.g. <code>reqwest</code>).</p>

<p>What about web frameworks?<br>
<code>actix-web</code> adopted <code>async</code>/<code>await</code> with its <code>0.2.x</code> release, same as <code>warp</code>, while <code>tide</code> was using <code>async</code>/<code>await</code> before its stabilisation relying on the <code>nightly</code> Rust compiler.<br>
<code>rocket</code>, instead, still exposes a synchronous interface. <code>async</code>/<code>await</code> support is expected as part of its next <code>0.5</code> release, <a href="https://github.com/SergioBenitez/Rocket/issues/1065">in the making since last summer</a>.</p>

<p>Should you rule out <code>rocket</code> as a viable option because it does not yet support asynchronous programming?<br>
It depends.<br>
If you are implementing an application to handle high volumes of traffic with strict performance requirements it might be better to opt for an async web framework.<br>
If that is not the case, the lack of async support in <code>rocket</code> should not be one of your primary concerns.</p>

<h3 id="3-1-futures-runtime">3.1. Futures runtime</h3>

<p><code>async</code>/<code>await</code> is not all sunshine and roses.<br>
Asynchronous programming in Rust is built on top of the <code>Future</code> trait: a future exposes a <code>poll</code> method which has to be called to allow the future to make progress. You can think of Rust’s futures as <em>lazy</em>: unless polled, there is no guarantee that they will execute to completion.<br>
This is often been described as a <em>pull</em> model compared to the <em>push</em> model adopted by other languages<sup id="fnref:async-announcement"><a href="#fn:async-announcement">1</a></sup>, which has some interesting implications when it comes to performance and task cancellation.</p>

<p>Wait a moment though - if futures are lazy and Rust does not ship a runtime in its standard library, who is in charge to call the <code>poll</code> method?<br>
<strong>BYOR</strong> - <strong>B</strong>ring <strong>Y</strong>our <strong>O</strong>wn <strong>R</strong>untime!<br>
The async runtime is literally a dependency of your project, brought in as a crate.<br>
This provides you with a great deal of flexibility: you could indeed implement your own runtime optimised to cater for the specific requirements of your usecase (see <a href="http://smallcultfollowing.com/babysteps/blog/2019/12/09/async-interview-2-cramertj/#async-interview-2-cramertj">the Fuchsia project</a> or <a href="https://github.com/bastion-rs/bastion"><code>bastion</code></a>’s actor framework) or simply choose the most suitable on a case-by-case basis according to the needs of your application.<br>
That sounds amazing on paper, but reality is a bit less glamorous: interoperability between runtimes is quite poor at the moment; mixing runtimes can be painful, often causing issues that are not straight-forward either to triage, detect or solve.<br>
While most libraries should not depend on runtimes directly, relying instead on the interfaces exposed by the <a href="https://docs.rs/futures/0.3.5/futures/"><code>futures</code></a> crate, this is often not the case due to historical baggage (e.g. <code>tokio</code> was for a long time the only available runtime in the ecosystem), practical needs (e.g. a framework has to be able to spawn tasks) or lack of standardisation (e.g. the ongoing discussion on the <code>AsyncRead</code>/<code>AsyncWrite</code> traits - see <a href="http://smallcultfollowing.com/babysteps/blog/2020/01/20/async-interview-5-steven-fackler/">here</a> and <a href="http://smallcultfollowing.com/babysteps/blog/2020/03/10/async-interview-7-withoutboats/#async-interview-7-withoutboats">here</a>).<br>
Therefore picking an async web framework goes beyond the framework itself: you are choosing an ecosystem of crates, suddenly making it much more cumbersome to consume libraries relying on a different async runtime.</p>

<p>The current state of affairs is far from ideal, but if you are writing async Rust today I’d recommend you to make a <em>deliberate</em> choice when it comes to your async runtime.</p>

<p>The two main general-purpose async runtimes currently available in Rust are <a href="https://tokio.rs/"><code>tokio</code></a> and <a href="https://github.com/async-rs/async-std"><code>async-std</code></a>.<br>
<code>tokio</code> has been around for quite some time and it has seen extensive production usage. It is fairly tunable, although this results in a larger and more complex API surface.<br>
<code>async-std</code> was released almost a year ago, around the time of <code>async</code>/<code>await</code> stabilization. It provides great ergonomics, while leaving less room for configuration knobs.</p>

<p><a href="https://crates.io/">crates.io</a> can once again be used as a gauge for adoption and readiness:</p>

<table>
<thead>
<tr>
<th>Runtime</th>
<th>Total Downloads</th>
<th>Daily Downloads</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>tokio</code></td>
<td>~9600k</td>
<td>~30k</td>
</tr>

<tr>
<td><code>async-std</code></td>
<td>~600k</td>
<td>~4k</td>
</tr>
</tbody>
</table>

<p>How do frameworks map to runtimes?</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th>Runtime</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>actix-web</code></td>
<td><code>tokio</code></td>
</tr>

<tr>
<td><code>rocket</code> (<code>0.5.x</code>)</td>
<td><code>tokio</code></td>
</tr>

<tr>
<td><code>tide</code></td>
<td><code>async-std</code></td>
</tr>

<tr>
<td><code>warp</code></td>
<td><code>tokio</code></td>
</tr>
</tbody>
</table>

<h2 id="4-documentation-tutorials-and-examples">4. Documentation, tutorials and examples</h2>

<p>Having to dive into the source code to understand how something works can be fun (and educational!), but it should be a choice, not a necessity.<br>
In most situations I’d rather rely on the framework being well-documented, including non-trivial examples of relevant usage patterns.<br>
Good documentation, tutorials and fully-featured examples are <strong>mission-critical</strong> if you are working as part of a team, especially if one or more teammates are not experienced Rust developers.</p>

<p>Rust’s tooling treats documentation as a first class concept (just run <code>cargo doc --open</code> to get auto-generated docs for your project!) and it grew to be part of the culture of the Rust community itself. Library authors generally take it seriously and web frameworks are no exception to the general tendency: what you can find on <a href="https://docs.rs/">docs.rs</a> is quite thorough, with contextual examples …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/">https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/</a></em></p>]]>
            </description>
            <link>https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740028</guid>
            <pubDate>Sun, 05 Jul 2020 16:49:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scaling Pandas: Comparing Dask, Ray, Modin, Vaex, and Rapids]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 30 (<a href="https://news.ycombinator.com/item?id=23740012">thread link</a>) | @FHMS
<br/>
July 5, 2020 | https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray | <a href="https://web.archive.org/web/*/https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>Python and its most popular data wrangling library, Pandas, are soaring in popularity. Compared to competitors like Java, Python and Pandas make data exploration and transformation <strong>simple</strong>.</p><p>But both Python and Pandas are known to have issues around <strong>scalability</strong> and <strong>efficiency</strong>.</p><p>Python loses some efficiency right off the bat because it’s an interpreted, dynamically typed language. But more importantly, Python has always focused on simplicity and readability over raw power. Similarly, Pandas focuses on offering a simple, high-level API, largely ignoring performance. In fact, the creator of Pandas wrote “<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">The 10 things I hate about pandas</a>,” which summarizes these issues:</p><figure id="w-node-412b9aecdea3-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62e9007b2509635bd1ba2_image3.png" alt="Ten things Wes McKinney hates about Pandas."></p><figcaption>Performance issues and lack of flexibility are the main things Pandas’ own creator doesn’t like about the library. (<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">source</a>)</figcaption></figure><p>So it’s no surprise that many developers are trying to add more power to Python and Pandas in various ways. Some of the most notable projects are:</p><ul role="list"><li><a href="https://www.datarevenue.com/ml-tools/dask"><strong>Dask</strong></a><strong>:</strong> a low-level scheduler and a high-level partial Pandas replacement, geared toward running code on compute clusters.</li><li><strong>Ray:</strong> a low-level framework for parallelizing Python code across processors or clusters.</li><li><a href="https://www.datarevenue.com/ml-tools/modin"><strong>Modin</strong></a><strong>:</strong> a drop-in replacement for Pandas, powered by either <strong>Dask</strong> or <strong>Ray</strong>.</li><li><a href="https://www.datarevenue.com/ml-tools/vaex"><strong>Vaex</strong></a><strong>:</strong> a partial Pandas replacement that uses lazy evaluation and memory mapping to allow developers to work with large datasets on standard machines.</li><li><a href="https://www.datarevenue.com/ml-tools/rapids"><strong>RAPIDS</strong></a><strong>: </strong>a collection of data-science libraries that run on GPUs and include <a href="https://github.com/rapidsai/cudf">cuDF</a>, a partial replacement for Pandas.</li></ul><p>There are others, too. Below is an overview of the Python data wrangling landscape:</p><figure id="w-node-78c43e6cecae-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62eb85c7038610cea20d0_image2.png" alt="A graph showing how often popular data wrangling libraries are compared in Google searches."></p><figcaption>Dask, Modin, Vaex, Ray, and CuDF are often considered potential alternatives to each other. Source: Created with <a href="https://anvaka.github.io/vs/?query=Dask">this tool</a></figcaption></figure><p>So if you’re working with a lot of data and need faster results, which should you use?</p><h2><strong>Just tell me which one to try</strong></h2><p>Before you can make a decision about which tool to use, it’s good to have some more context about each of their approaches. We’ll compare each of them closely, but you’ll probably want to try them out in the following order:</p><ul role="list"><li><strong>Modin</strong>, with <strong>Ray</strong> as a backend. By installing these, you might see significant benefit by changing just a single line (`import pandas as pd` to `import modin.pandas as pd`). Unlike the other tools, Modin aims to reach full compatibility with Pandas.</li><li><strong>Dask</strong>,<strong> </strong>a larger and hence more complicated project. But Dask also provides <a href="https://docs.dask.org/en/latest/dataframe.html">Dask.dataframe</a>, a higher-level, Pandas-like library that can help you deal with <a href="https://en.wikipedia.org/wiki/External_memory_algorithm">out-of-core</a> datasets.</li><li><strong>Vaex, </strong>which is designed to help you work with large data on a standard laptop. Its Pandas replacement covers some of the Pandas API, but it’s more focused on exploration and visualization.</li><li><strong>RAPIDS, </strong>if you have access to NVIDIA graphics cards<strong>.</strong></li></ul><h2><strong>Quick comparison</strong></h2><p>Each of the libraries we examine has different strengths, weaknesses, and scaling strategies. The following table gives a broad overview of these. Of course, as with many things, most of the scores below are heavily dependent on your exact situation.&nbsp;</p><figure id="w-node-3fc1cb6579be-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62ef85090a97b0c469fa9_image5.png" alt="A table comparing the tools across maturity, popularity, ease of adoption, and other metrics."></p><figcaption>Dask and Ray are more mature, but Modin and Vaex are easier to get started with. Rapids is useful if you have access to GPUs.</figcaption></figure><p>These are subjective grades, and they may vary widely given your specific circumstances. When assigning these grades, we considered:</p><ul role="list"><li><strong>Maturity: </strong>The time since the first commit and the number of commits.</li><li><strong>Popularity: </strong>The number of GitHub stars.</li><li><strong>Ease of Adoption: </strong>The amount of knowledge expected from users, presumed hardware resources, and ease of installation.</li><li><strong>Scaling ability: </strong>The broad dataset size limits for each tool, depending on whether it relies mainly on RAM, hard drive space on a single machine, or can scale up to clusters of machines.&nbsp;</li><li><strong>Use case: </strong>Whether the libraries are designed to speed up Python software in general (“<strong>General</strong>”), are focused on data science and machine learning (“<strong>Data science</strong>”), or are limited to simply replacing Pandas’ ‘DataFrame’ functionality (“<strong>DataFrame</strong>”).</li></ul><h2><strong>CPUs, GPUs, Clusters, or Algorithms?</strong></h2><p>If your dataset is too large to work with efficiently on a single machine, your main options are to run your code across…</p><ul role="list"><li><strong>...multiple threads or processors:</strong> Modern CPUs have several independent cores, and each core can run many threads. Ensuring that your program uses all the potential processing power by parallelizing across cores is often the easiest place to start.</li><li><strong>...GPU cores: </strong>Graphics cards were originally designed to efficiently carry out basic operations on millions of pixels in parallel. However, developers soon saw other uses for this power, and “GP-GPU” (general processing on a graphics processing unit) is now a popular way to speed up code that relies heavily on matrix manipulations.</li><li><strong>...compute clusters: </strong>Once you hit the limits of a single machine, you need a networked cluster of machines, working cooperatively.</li></ul><p>Apart from adding more hardware resources, clever algorithms can also improve efficiency. Tools like Vaex rely heavily on <a href="https://en.wikipedia.org/wiki/Lazy_evaluation"><strong>lazy evaluation</strong></a><strong> </strong>(not doing any computation until it’s certain the results are needed) and <a href="https://en.wikipedia.org/wiki/Memory-mapped_file"><strong>memory mapping</strong></a><strong> </strong>(treating files on hard drives as if they were loaded into RAM).</p><p>None of these strategies is inherently better than the others, and you should choose the one that suits your specific problem.</p><p>Parallel programming (no matter whether you’re using threads, CPU cores, GPUs, or clusters) offers many benefits, but it’s also quite complex, and it makes tasks such as debugging far more difficult.</p><p>Modern libraries can hide some – but not all – of this added complexity. No matter which tools you use, you’ll run the risk of expecting everything to work out neatly (below left), but getting chaos instead (below right).</p><figure id="w-node-7b8872b99c95-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5efef852495a4ce0972910e9_image4_s.jpg" alt="Puppies in a row eating food from different bowls – and then chaos ensues."></p><figcaption>Parallel processing doesn’t always work out as neatly as you expect. (<a href="https://www.reddit.com/r/aww/comments/2oagj8/multithreaded_programming_theory_and_practice/">Source</a>)</figcaption></figure><h2><strong>Dask vs. Ray vs. Modin vs. Vaex vs. RAPIDS</strong></h2><p>While not all of these libraries are direct alternatives to each other, it’s useful to compare them each head-to-head when deciding which one(s) to use for a project.</p><p>Before getting into the details, note that:</p><ul role="list"><li>RAPIDS is a collection of libraries. For this comparison, we consider only the <strong>cuDF</strong> component, which is the RAPIDS equivalent of Pandas.</li><li>Dask is better thought of as two projects: a low-level Python scheduler (similar in some ways to Ray) and a higher-level Dataframe module (similar in many ways to Pandas).</li></ul><h3><strong>Dask vs. Ray</strong></h3><p>Dask (as a lower-level scheduler) and Ray overlap quite a bit in their goal of making it easier to execute Python code in parallel across clusters of machines. Dask focuses more on the data science world, providing higher-level APIs that in turn provide partial replacements for Pandas, NumPy, and scikit-learn, in addition to a low-level scheduling and cluster management framework.</p><p>The creators of Dask and Ray discuss how the libraries compare in <a href="https://github.com/ray-project/ray/issues/642">this GitHub thread</a>, and they conclude that the scheduling strategy is one of the key differentiators. Dask uses a centralized scheduler to share work across multiple cores, while Ray uses distributed bottom-up scheduling.</p><h3><strong>Dask vs. Modin</strong></h3><p>Dask (the higher-level Dataframe) acknowledges the limitations of the Pandas API, and while it partially emulates this for familiarity, it doesn’t aim for full Pandas compatibility. If you have complicated existing Pandas code, it’s unlikely that you can simply switch out Pandas for Dask.Dataframe and have everything work as expected. By contrast, this is exactly the goal Modin is working toward: 100% coverage of Pandas. Modin can run on top of Dask but was originally built to work with Ray, and that integration remains more mature.</p><h3><strong>Dask vs. Vaex</strong></h3><p>Dask (Dataframe) is not fully compatible with Pandas, but it’s pretty close. These close ties mean that Dask also carries some of the baggage inherent to Pandas. Vaex deviates more from Pandas (although for basic operations, like reading data and computing summary statistics, it’s very similar) and therefore is also less constrained by it.</p><p>Ultimately, Dask is more focused on letting you scale your code to compute clusters, while Vaex makes it easier to work with large datasets on a single machine. Vaex also provides features to help you easily visualize and plot large datasets, while Dask focuses more on data processing and wrangling.</p><h3><strong>Dask vs. RAPIDS (cuDF)</strong></h3><p>Dask and RAPIDS play nicely together via an integration <a href="https://rapids.ai/dask.html">provided by</a> RAPIDS. If you have a compute cluster, you should use Dask. If you have an NVIDIA graphics card, you should use RAPIDS. If you have a compute cluster of NVIDIA GPUs, you should use both.</p><h3><strong>Ray vs. Modin or Vaex or RAPIDS</strong></h3><p>It’s not that meaningful to compare Ray to Modin, Vaex, or RAPIDS. Unlike the other libraries, Ray doesn’t offer high-level APIs or a Pandas equivalent. Instead, Ray powers Modin and <a href="https://docs.ray.io/en/latest/tune.html">integrates with RAPIDS</a> in a similar way to Dask.</p><h3><strong>Modin vs. Vaex</strong></h3><p>As with the Dask and Vaex comparison, Modin’s goal is to provide a full Pandas replacement, while Vaex deviates more from Pandas. Modin should be your first port of call if you’re looking for a quick way to speed up existing Pandas code, while Vaex is more likely to be interesting for new projects or specific use cases (especially visualizing large datasets on a single machine).</p><h3><strong>Modin vs. RAPIDS (cuDF)</strong></h3><p>Modin scales Pandas code by using many CPU cores, via Ray or Dask. RAPIDS scales Pandas code by running it on GPUs. If you have GPUs available, give RAPIDS a try. But the easiest win is likely to come from Modin, and you should probably turn to RAPIDS only after you’ve tried Modin first.</p><h3><strong>Vaex vs. RAPIDS (cuDF)</strong></h3><p>Vaex and RAPIDS are similar in that they can both provide performance boosts on a single machine: Vaex by better utilizing your computer’s hard drive and processor cores, and RAPIDS by using your computer’s GPU (if it’s available and compatible). The RAPIDS project as a whole aims to be much broader than Vaex, letting you do machine learning end-to-end without the data leaving your GPU. Vaex is better for prototyping and data exploration, letting you explore large datasets on consumer-grade machines.</p><h2><strong>Final remarks: Premature optimization is the root of all evil</strong></h2><p>It’s fun to play with new, specialized tools. That said, many …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray">https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray</a></em></p>]]>
            </description>
            <link>https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740012</guid>
            <pubDate>Sun, 05 Jul 2020 16:47:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Andrew Wilkinson and Tiny Capital]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 16 (<a href="https://news.ycombinator.com/item?id=23739381">thread link</a>) | @colinkeeley
<br/>
July 5, 2020 | https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual | <a href="https://web.archive.org/web/*/https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-db1d94e86a488296a48d"><div><blockquote><p><em>“Let someone else run the marathon and incentivize them.”&nbsp;</em></p><p><em>-Andrew Wilkinson</em></p></blockquote><p><strong>What is Tiny?</strong></p><p><a href="http://tinycapital.com/">Tiny</a>&nbsp;is a long term holding company for internet businesses started by&nbsp;<a href="https://twitter.com/awilkinson">Andrew Wilkinson</a>&nbsp;and&nbsp;<a href="https://twitter.com/_sparling_?lang=en">Chris Sparling</a>. They take majority, generally whole, stakes in "profitable, simple, and often boring” internet businesses.&nbsp;</p><p><strong>Why are holding companies and micro private equity interesting?&nbsp;</strong></p><p>I suspect this is the most dependable way to become very wealthy. It isn’t as glamorous or as quick (potentially) as founding or investing in the next multi-billion dollar startup. This is a longer-term grind it out approach.&nbsp;</p><p>Starting companies is fun, but anyone who has done it knows it is a lot of work. Buying established businesses with existing cash flow isn’t as sexy so I suspect it is wildly underrated as a way of building wealth.&nbsp;</p><p>The reality is that it is easier to buy and improve businesses than to start them. It is easier to go from 3 to 10 than from 0 to 1. Even for the folks that have done it before.&nbsp;</p><p>There isn’t much info on how holding companies or micro-PEs like Tiny actually operate. I’ve listened to every podcast Andrew has been on and compiled these notes from them.&nbsp;</p><p>Here is what they are doing behind the scenes.</p><p><strong>How Andrew got started? Where the capital comes from?</strong></p><p>In 2006, Andrew founded&nbsp;<a href="http://metalab.co/">MetaLab</a>, a Victoria, Canada-based design agency shortly after high school. After rapid growth, he used the profits to diversify into a variety of businesses, which today form Tiny, a holding company he owns fully with his business partner Chris Sparling.&nbsp;</p><p>Agencies traditionally aren’t very profitable, but MetaLab is able to charge San Francisco agency rates and only pay Victoria, Canada wages.&nbsp;</p><p>Tiny shifted its focus from starting businesses to buying them in 2013 when MetaLab and all their other businesses combined were doing $7M/year in profit. Tiny is fully self-funded today.</p><p><strong>What’s the scale of Tiny now?</strong></p><p>Comfortably not tiny. It sounds like somewhere around $80-95M revenue per year (double-digit millions is what Andrew says) with highly profitable businesses. They have around 350-400 employees across 20ish companies.&nbsp;</p><p><strong>What Tiny looks for in businesses to buy?</strong></p><p>From their site:</p><blockquote><p><em>3-5+ years of operating history</em></p><p><em>Profits. A minimum $500k/year in annual profit, as high as $15MM.</em></p><p><em>A high-quality team in place. This is negotiable if the business is simple to operate and the team wants to leave.</em></p><p><em>We are open to owners sticking around, leaving cold turkey, or transitioning out over time. We'll work with you to transition.</em></p><p><em>Simple internet businesses that have high margins, don't require tons of people or complex technology, and have a competitive advantage that protects them from competitors. For example: A dominant brand, a large and loyal community, a niche vertical, or something similar.</em></p></blockquote><p>Andrew describes these businesses as "New Zealand companies.”</p><p>What is a New Zealand company?</p><ul data-rte-list="default"><li><p>It is in the middle of nowhere, nobody is paying attention to it, but it is quietly growing. It is not at risk of nuclear war.&nbsp;</p></li><li><p>It is self-sufficient and thriving. It’s food &amp; energy independent. A "safe" business isn't beholden to benevolent gatekeepers like Google or Facebook to reach their customer.&nbsp;</p></li></ul><p>Andrew is always worried about staying power.&nbsp;</p><p>An example of one of his New Zealand business is Dribbble:</p><ul data-rte-list="default"><li><p>Top 1,000 site on the internet&nbsp;</p></li><li><p>A huge community of designers</p></li><li><p>Profitable</p></li><li><p>Few competitors. Big companies are not trying to kill it or compete.&nbsp;</p></li><li><p>Not dependent on Facebook or Google for traffic. People type Dribbble.com into the address bar to visit.&nbsp;</p></li></ul><p><strong>Types of businesses Tiny has bought/started?</strong></p><p>I don’t know if this is by design, but it seems like Andrew has progressed from services to tools/products to platforms/communities to digital marketplaces.&nbsp;</p><ul data-rte-list="default"><li><p>Agencies: MetaLab (design agency), Double Up (podcast growth agency), 8020 (no-code agency)</p></li><li><p>SaaS tools:&nbsp;<a href="https://www.getflow.com/">Flow</a>&nbsp;(product management), Castro (podcast player), Supercast (podcast subscriptions)</p></li><li><p>Products: Caramba</p></li><li><p>Communities: Dribbble&nbsp;</p></li><li><p>Media: Designer News, RideHome (podcast network)</p></li><li><p>Job Boards:&nbsp;<a href="https://weworkremotely.com/">We Work Remotely</a></p></li><li><p>Digital goods marketplaces: Creative Market, Pixel Union</p></li></ul><p><strong>How Tiny companies operate?</strong></p><p>Tiny companies have fewer information responsibilities than typical PE-owned companies. There are no formal board meetings for example.&nbsp;</p><p>Once a month companies send Tiny a finance-only update with the P&amp;L, balance sheet, and KPIs. No operational info is included.&nbsp;</p><p>Once a quarter companies send Tiny a SWOT (strengths, weaknesses, opportunities, and threats) analysis.&nbsp;</p><p>Companies contact Tiny ASAP for emergencies, major news, or decisions.&nbsp;</p><p>Some CEOs will go 6 months or more without speaking with Andrew.&nbsp;</p><p><strong>How Tiny launches new businesses?</strong></p><p>Tiny’s primary business is buying majority stakes in businesses, not starting them. For a while Andrew would start a new business in any niche he was interested in. He tries to avoid that now and thinks it’s a lot better to buy something that is already good.</p><p>When Andrew does start a new business now, he delegates almost all aspects of it. He recently said he only spent something like 4 hours on each of the new businesses he has launched.&nbsp;</p><p>Andrew will pay for all the work to be done and the investment will form his stake in the business. He will find a CEO to run the business and pay the new CEOs a month or two of salaries to get things going. Then he’ll help with intros, but otherwise, he’ll be hands-off. All in he said it takes $10-50k to get off the ground with a great operator.</p><p><strong>Why do Founders sell to Tiny?</strong></p><p>Tiny is positioned as the good guys of private equity. The Berkshire Hathway of internet businesses.</p><p>They have become known for doing simple acquisitions. Andrew didn’t like the traditional acquisition process: long due diligence, and renegotiation of terms. Warren Buffet does deals in seven days and those are larger, more complex businesses. Smaller deals should be even quicker.</p><p>A challenge with this model is that it is difficult to acquire tech companies at reasonable prices. Acquiring boring traditional businesses is easier because the valuations are so much lower than tech companies. To successfully use this approach you need discipline around what you’re willing to pay for a business and a reputation for being easy to work with. Andrew gets deals by being a nice guy and offering a good home for businesses to live on. Contrast this with the typical PE approach of dramatically cutting costs (ie firing everyone) and squeezing as much profit out as possible. Some founders are looking more for freedom and an easy process than maximizing their financial outcome. </p><p>These smaller PE opportunities are underserved relative to the typical VC businesses. The lifestyle businesses that VC shuns are Andrew’s ideal companies. He is fishing in a less crowded pond. </p><p>Andrew will occasionally pay 10x for an amazing business, but that is rare.&nbsp;</p><p><strong>What happens to businesses after the sale?&nbsp;</strong></p><p>For the employees, it is business as usual for the most part. The goal is for the employees to not even notice.&nbsp; </p><p>The biggest difference is that Tiny becomes the bank. Cash is kept in the company based on historical working capital needs and any extra goes to the head office for new acquisitions.&nbsp; </p><p>Often Tiny buys product or designer-led startups that have grown organically. They will put standard best-practice marketing and sales processes in place and sometimes raise prices. Each company has its own CEO with a few exceptions like all job boards (5+) are under one CEO.&nbsp; </p><p>Tiny has a preference for remote companies where they can hire more affordably. Andrew estimates the cost of running a business in Canada can be 60-65% the cost of in California. Struggling American companies with inflated cost structures can reduce costs by moving to Canada. Canadian arbitrage includes lower salaries, not needing to pay medical benefits, SRED, and cheaper currency.</p><p><strong>Who runs the business after a sale?&nbsp;</strong></p><p>Often Andrew is buying from bootstrapped founders that have been at it for 5-10 years and want to move on.</p><p>Finding great people to run these companies is one of the hardest aspects of this model.&nbsp;</p><p>Andrew deals with this by paying up and hiring CEOs that have managed similar businesses at larger scales already before instead of trying to find underpriced less-experienced talent.&nbsp;</p><p>Months before closing on a deal Andrew works to identify opportunities for the business and a new leader to come in.&nbsp;</p><p>He finds these new CEOs through his existing CEOs by asking “we’re about to buy a business who’s the smartest person you know in the space."</p><p><strong>What does the operating company and Andrew do day-to-day?</strong></p><blockquote><p><em>“Entrepreneurship is just delegation”&nbsp;</em></p><p><em>-Andrew Wilkinson</em></p></blockquote><p>Andrew spends time looking for new deals and looking at their existing portfolio and thinking "how they could get fucked”.&nbsp;</p><p>Andrew says his strengths are:</p><ul data-rte-list="default"><li><p>Laser focused on problems for a short period of time. Moves fast.&nbsp;</p></li><li><p>Very good at 0 to 1. Burns bright for 15 days.&nbsp;</p></li><li><p>Inch deep and a mile wide</p></li><li><p>Not good at execution or day to day details</p></li></ul><p>Being comfortable with delegation is key to this model. Andrew is the owner, not the CEO. The owner can’t constantly be delegating what can or can’t be done or the CEO grows resentful. Some comfort with decisions being made that you don’t agree with comes with the territory. Large decisions that require more capital than usual are a discussion.&nbsp;</p><p><strong>How connected are businesses in the holding company?</strong></p><p>Tiny companies are not at all connected. They each operate independently.&nbsp;</p><p>CEOs will take calls and give advice on best practices, but nothing beyond small favors. Real work gets paid for. Tiny pays all companies for the work they do for the holding company and all work between companies is paid at the full rate.&nbsp;</p><p>Synergies are appealing, but they generally just make the CEOs resentful so they are avoided entirely.&nbsp;</p><p><strong>How much debt do they use?</strong></p><p>Tiny uses little debt for acquisitions (less than Berkshire Hathaway) and they like to pay off debt within 6 months. Debt comes from&nbsp;<a href="https://en.wikipedia.org/wiki/Business_Development_Bank_of_Canada"><strong>BDB of Canada</strong></a>, or traditional banks.</p><p><em>If you know of anything I should add to this please reach …</em></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual">https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual</a></em></p>]]>
            </description>
            <link>https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual</link>
            <guid isPermaLink="false">hacker-news-small-sites-23739381</guid>
            <pubDate>Sun, 05 Jul 2020 15:26:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust on the ESP32 (2019)]]>
            </title>
            <description>
<![CDATA[
Score 128 | Comments 42 (<a href="https://news.ycombinator.com/item?id=23737451">thread link</a>) | @lnyan
<br/>
July 5, 2020 | https://mabez.dev/blog/posts/esp32-rust/ | <a href="https://web.archive.org/web/*/https://mabez.dev/blog/posts/esp32-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	  <p>About six months ago, I made a <a href="https://www.reddit.com/r/rust/comments/ar2d3r/espressif_have_finally_released_an_llvm_fork_this/">post on reddit</a> highlighting the launch of Espressif's llvm xtensa fork, not too long after, I had a working <code>rustc</code> toolchain capable of generating xtensa assembly. At this point I had to put this project to the side to finish my final year of university. Funnily enough I didn't stray too far, my final year project used Rust to create a <a href="https://github.com/MWatch">'smartwatch'</a> (I may write about this in the future, if anyone is interested). </p>
<p>Since then I have seen a few posts utilising my fork to run Rust on the <a href="https://www.espressif.com/en/products/hardware/esp32/overview">ESP32</a> (<a href="https://dentrassi.de/2019/06/16/rust-on-the-esp-and-how-to-get-started/">see this great write up</a> by ctron, if you haven't already), most of which are building on top of <a href="https://github.com/espressif/esp-idf">esp-idf</a> which is written in C. In this post I'll be discussing the steps I took to generate valid binaries for the xtensa architecture with <code>rustc</code> and then write some <code>no_std</code> code to build a blinky program for the ESP32 only using Rust!</p>
<h2 id="hacking-the-compiler">Hacking the compiler</h2>
<p>In March of 2019, Espressif released their first run at an <a href="https://github.com/espressif/llvm-xtensa">llvm fork</a> to support the xtensa architecure. Shortly after I got to work bootstrapping Rust to use this newly created fork. Prior to this project, I'd had no experience with the compiler, fortunately I came across the <a href="https://github.com/rust-lang/rust/pull/52787">RISCV PR</a> which gave me a rough idea of what was required. After <em>many</em> build attempts I finally got it working; I was now able to generate xtensa assembly from Rust source code!</p>
<p>The next step was to assemble and link the generated assembly. The llvm fork in it's current state cannot perform object generation, so we must use an external assembler. Luckily Rust allows us to do so by specifying the <code>linker_flavor</code> as <code>gcc</code> and providing a path to the linker with the <code>linker</code> target option, in this case <code>xtensa-esp32-elf-gcc</code>. After that I created a few built-in targets (which you can see <a href="https://github.com/MabezDev/rust-xtensa/blob/ad570c5cb999f62a03156286fdb5d3d1bbd0fb8b/src/librustc_target/spec/xtensa_esp32_none_elf.rs">here</a>); <code>xtensa-esp32-none-elf</code> for the ESP32; <code>xtensa-esp8266-none-elf</code> for the ESP8266; finally the <code>xtensa-unknown-none-elf</code> target for a generic xtensa target.</p>
<h2 id="blinky-code">Blinky code</h2>
<p>Now lets try and get a ESP32 board to blink the onboard LED using just Rust. First off, we need our basic program structure. The <code>xtensa_lx6_rt</code> crate does most of the heavy lifting in this respect, we simply need to define an entry point and the panic handler. Some of this may look vaguely familiar if you have any experience with <code>cortex-m</code> development on Rust, I've tried to mirror the API as best as I can.</p>
<pre><span>#![</span><span>no_std</span><span>]
#![</span><span>no_main</span><span>]


</span><span>use</span><span> xtensa_lx6_rt as _;

</span><span>use </span><span>core::panic::PanicInfo;

</span><span>/// Entry point - called by xtensa_lx6_rt after initialisation
</span><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {
    </span><span>loop </span><span>{}
}

</span><span>/// Simple panic handler
</span><span>#[</span><span>panic_handler</span><span>]
</span><span>fn </span><span>panic</span><span>(</span><span>_info</span><span>: &amp;PanicInfo) -&gt; ! {
    </span><span>loop </span><span>{}
}
</span></pre>
<p>Now lets add some register definitions for the peripherals we want to use. For our blinky program, we will need to control the GPIO peripheral. In the ESP32 (and most modern processors) peripherals are mapped to memory adresses, commonly refered to as memory mapped peripherals. To control a peripheral we simply need to write values to the right addresses in memory, with respect to the reference manual supplied by the chip manufacturer.</p>
<pre><span>/// GPIO output enable reg
</span><span>const </span><span>GPIO_ENABLE_W1TS_REG</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44024</span><span>;

</span><span>/// GPIO output set register
</span><span>const </span><span>GPIO_OUT_W1TS_REG</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44008</span><span>;
</span><span>/// GPIO output clear register
</span><span>const </span><span>GPIO_OUT_W1TC_REG </span><span>: </span><span>u32 </span><span>= </span><span>0x3FF4400C</span><span>;

</span><span>/// The GPIO hooked up to the onboard LED
</span><span>const </span><span>BLINKY_GPIO</span><span>: </span><span>u32 </span><span>= </span><span>2</span><span>;

</span><span>/// GPIO function mode
</span><span>const </span><span>GPIO_FUNCX_OUT_BASE</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44530</span><span>;
</span><span>const </span><span>GPIO_FUNCX_OUT_SEL_CFG</span><span>: </span><span>u32 </span><span>= </span><span>GPIO_FUNCX_OUT_BASE </span><span>+ (</span><span>BLINKY_GPIO </span><span>* </span><span>4</span><span>);
</span></pre>
<p>Using these definitions it should be possible to change the gpio for your board<sup><a href="#gpio_pin">1</a></sup> by changing the <code>BLINKY_GPIO</code>; for my board (NODEMCU ESP-32S) it was GPIO2.</p>
<h3 id="initialisation">Initialisation</h3>
<p>Next lets setup the pin as a GPIO output. For the ESP32, this is a two step process<sup><a href="#gpio_pin">1</a></sup>. Firstly, its simply a case of setting a bit in the GPIO ouput enable register. Secondly the pin has to be configured in GPIO mode. There are not enough pins for all the possible peripherals in the chip, to combat this each pin can have multiple function modes. In the case of the ESP32, each pin has up to 256 different functions, although not all are mapped. To put the pin in GPIO mode, we need to put in mode 256 (0x100), we do this by writing to the function select register. After issuing those two register writes, we should be able to turn on the GPIO by setting the relevant bit inside the GPIO set register<sup><a href="#2">2</a></sup>.</p>
<pre><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {

    </span><span>// configure the pin as an output
    </span><span>unsafe </span><span>{
        core::ptr::write_volatile(</span><span>GPIO_ENABLE_W1TS_REG </span><span>as </span><span>*mut </span><span>_, </span><span>0x1 </span><span>&lt;&lt; </span><span>BLINKY_GPIO</span><span>);
        </span><span>// 0x100 makes this pin a simple gpio pin - see the technical reference for more info
        </span><span>core::ptr::write_volatile(</span><span>GPIO_FUNCX_OUT_SEL_CFG </span><span>as </span><span>*mut </span><span>_, </span><span>0x100</span><span>); 
    }
    </span><span>// turn on the LED
    </span><span>unsafe </span><span>{
        core::ptr::write_volatile(</span><span>GPIO_OUT_W1TS_REG </span><span>as </span><span>*mut </span><span>_, </span><span>0x1 </span><span>&lt;&lt; idx);           
    }
    </span><span>loop </span><span>{}
}
</span></pre><h3 id="delaying">Delaying</h3>
<p>For the next stage of our blinky program, we need a way to delay; a simple approach could use <code>for</code> loop like so.</p>
<pre><span>pub fn </span><span>delay</span><span>(</span><span>clocks</span><span>: </span><span>u32</span><span>) {
    </span><span>let</span><span> dummy_var: </span><span>u32 </span><span>= </span><span>0</span><span>;
    </span><span>for </span><span>_ in </span><span>0</span><span>..clocks {
        </span><span>unsafe </span><span>{ core::ptr::read_volatile(&amp;dummy_var) };
    }
}
</span></pre>
<p>We add the volatile read so that the compiler doesn't optimise our delay away. The problem with this approach is that depending of the optimisation level, the number of clock cycles each iteration of the loop changes. We need a cycle accurate way of delaying, fortunately the ESP32 has an internal clock counting register which can be accessed with the read special register <code>rsr</code> instruction. Now are delay function looks like this.</p>
<pre><span>/// cycle accurate delay using the cycle counter register
</span><span>pub fn </span><span>delay</span><span>(</span><span>clocks</span><span>: </span><span>u32</span><span>) {
    </span><span>// NOTE: does not account for rollover
    // ommitted: the asm to read the ccount
    </span><span>let</span><span> target = </span><span>get_ccount</span><span>() + clocks;
    </span><span>loop </span><span>{
        </span><span>if </span><span>get_ccount</span><span>() &gt; target {
            </span><span>break</span><span>;
        }
    }
}
</span></pre>
<p>Now we have cycle accurate counting we can delay for one second by waiting for the number of cycles the processor will do in one second. The default clock speed on most ESP boards is 40mhz, hence waiting for 40 million cycles equates to a one second delay.</p>
<p>Bringing the snippets together and cleaning up the code into functions, we now have <code>main</code> that looks like this.</p>
<pre><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {
    </span><span>// configure the pin as an output
    </span><span>configure_pin_as_output</span><span>(</span><span>BLINKY_GPIO</span><span>);

    </span><span>loop </span><span>{
        </span><span>set_led</span><span>(</span><span>BLINKY_GPIO</span><span>, </span><span>true</span><span>);
        </span><span>delay</span><span>(</span><span>CORE_HZ</span><span>);
        </span><span>set_led</span><span>(</span><span>BLINKY_GPIO</span><span>, </span><span>false</span><span>);
        </span><span>delay</span><span>(</span><span>CORE_HZ</span><span>);
    }
}
</span></pre>
<p>After flashing to the board, and firing up our JTAG debugger<sup><a href="#1">3</a></sup>, we are greeted with a blinking LED!</p>

<p>The full source can be found in the <a href="https://github.com/MabezDev/xtensa-rust-quickstart">the xtensa quickstart repo</a> if you wish to try it for yourself.</p>
<p>Now I know what most of you are thinking at this point, it's not very Rusty; it contains bundles of unsafe and there are no real abstractions here, and you are right; but it's something to get the ball rolling.</p>
<h2 id="limitations">Limitations</h2>
<p>There are a few small teething issues, but by far the biggest being issue is that the fork struggles with generating debug info; the external assembler does not support <a href="https://sourceware.org/binutils/docs-2.24/as/CFI-directives.html#CFI-directives">CFI directives</a> something that all llvm targets need to support. CFI directives can easily be removed with some preprocessing, but does of course add an extra step. After pushing past that issue, I was still getting relocation linker errors. I opened <a href="https://github.com/espressif/llvm-xtensa/issues/10">an issue</a> to document my findings in the hopes it can be sorted in the next iteration of the llvm fork.</p>
<h2 id="future-work">Future work</h2>
<p>Once the debuginfo issue is sorted, I hope to start developing an ecosystem of HAL's and drivers similar to the <a href="https://github.com/stm32-rs">stm32-rs</a> and <a href="https://github.com/nrf-rs">nrf-rs</a>; I've already started the <a href="https://github.com/esp-rs">esp-rs</a> organization which is where <code>xtensa-lx6-rt</code> currently resides. Espressif has started the upstream process, the first ten patches are now in review, there should be an update coming to their fork moving from the older llvm6 to llvm8 (and hopefully some other additions and fixes too!).</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://github.com/MabezDev/xtensa-rust-quickstart">xtensa-quickstart</a> - A quickstart project for using Rust on xtensa</li>
<li><a href="https://github.com/MabezDev/rust-xtensa">rust-xtensa</a> - The xtensa fork of Rust</li>
<li><a href="https://github.com/MabezDev">github</a> - My github</li>
</ul>
<br>
<hr>
<br>




	</div></div>]]>
            </description>
            <link>https://mabez.dev/blog/posts/esp32-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23737451</guid>
            <pubDate>Sun, 05 Jul 2020 09:00:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Beginner's Guide to Abstraction]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 79 (<a href="https://news.ycombinator.com/item?id=23735991">thread link</a>) | @jesseduffield
<br/>
July 4, 2020 | https://jesseduffield.com/beginners-guide-to-abstraction/ | <a href="https://web.archive.org/web/*/https://jesseduffield.com/beginners-guide-to-abstraction/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-75">
	<!-- .entry-header -->

	
	
	<div>
		<p>In <em>The Pragmatic Programmer</em>, Andrew Hunt and David Thomas introduced the DRY (Don't Repeat Yourself) principle. The rationale being that if you see the same code copy+pasted 10 times you should probably factor that code into its own method/class.</p>
<p>But then Sandi Metz came along and <a href="https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction">said</a>:</p>
<blockquote>
<p>Duplication is far cheaper than the wrong abstraction.</p>
</blockquote>
<p>And so the eternal war began.</p>
<h3>What is abstraction?</h3>
<p>For the purposes of this post I'm referring to the kind of abstraction as described in the <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)#Abstraction_in_object_oriented_programming">Abstraction Principle</a>, which Wikipedia describes like so:</p>
<blockquote>
<p>In software engineering and programming language theory, the abstraction principle (or the principle of abstraction) is a basic dictum that aims to reduce duplication of information in a program (usually with emphasis on code duplication) whenever practical by making use of abstractions provided by the programming language or software libraries</p>
</blockquote>
<p>This post has nothing to say about the conceptual kind of abstraction where from the concrete examples of 'Parrot' and 'Sparrow' you create an abstraction of 'Bird'. This post is about duplicated code, how to respond to it, and how to respond to other people's responses to it.</p>
<p>I define the verb 'abstraction' to be an <em>attempt</em> to reduce complexity by combining repeated commonality into some generalisation. And so, the noun 'abstraction' is the result of that attempt. If you're somebody who believes abstraction is by definition a <em>successful</em> attempt, feel free to substitute the term 'wrong abstraction' with 'failure to abstract' throughout this post.</p>
<p>The process of abstraction typically goes like this:<br>
1) you identify different chunks of code that you think are all essentially doing the same thing<br>
2) you create a method or a class with a narrow interface which can be substituted in for all the chunks of code you found<br>
3) you go and swap out the chunks of code with a call to your method/class</p>
<h3>Abstraction is always a gamble</h3>
<p>In the world of software engineering, when requirements are always changing, every abstraction is a gamble. When you make an abstraction over some concrete things, you're making a bet that the concrete things are more similar than they are different, and that their similarities are not mere coincidences: that there is a common purpose shared by the concrete things which would lead them to evolve in lockstep as requirements evolve. If you win the bet, your codebase will be easier to work in and adding new use cases via your abstraction will be trivially easy. If you lose, you'll see a flash of fear in your colleague's eyes whenever they're assigned a ticket to make yet another extension to the misfigured monster that the once-innocent abstraction has now become</p>
<p>But risk abounds everywhere, and leaving duplicated code unabstracted is its own gamble. You're betting that the chunks of code will evolve in separate directions as requirements change and that their current similarities are more coincidence than a reflection of their common purpose. Win the bet and your colleague gets to sleep soundly at night knowing they won't be facing the abstraction monster at work the next day. Lose, and code that should have evolved in lockstep is now implemented in completely different ways across different files, where a developer fixes a bug identified in one place, only for the same bug to be reported days later in a completely different file.</p>
<p>Your job is to get good at making the right bets.</p>
<h3>The right/wrong abstraction</h3>
<p>You'll know that you've made the <em>right</em> abstraction when a long time passes and you haven't needed to expand the interface (an example of expanding the interface is adding an optional flag argument). You'll also know you've made the right abstraction when another developer doesn't find it that much harder to understand how the code behaves for a given use case than if somebody had written the code to satisfy the use case without the abstraction.</p>
<p>You'll know you've made the <em>wrong</em> abstraction when after a while the interface has been expanded to support various optional flags, each for a different use case, and you need to be a genius to reason about what the code will actually do for a given use case. By the way, if you have a string arg that merely gets fed into a switch statement inside a method and for each new use case you come up with a new accepted value for it, you <em>are</em> expanding the implicit interface, even if that fact isn't captured in your type system.</p>
<p>There is plenty of daylight between the perfect abstraction and the completely wrong abstraction (perhaps the interface needs to be fundamentally changed but afterwards you're back to having a good abstraction), and so the point of this section isn't to prescribe how much you should be abstracting, but to encourage you to think about both perspectives and be able to make a case in a PR review for why you think an abstraction should/should-not exist.</p>
<h3>Do you over or under-abstract?</h3>
<p>Given it is impossible to make the right decision with regards to abstraction every time, you are probably either somebody who over-abstracts or somebody who under-abstracts.</p>
<p>If common feedback on your PR reviews is that you should DRY up your code, you could probably benefit from doing a scan for duplicated code before submitting a PR and considering whether it belongs in its own method/class.</p>
<p>If you commonly get feedback that your methods are hard to understand because they support too many disparate use cases at once, you are probaby over-abstracting and should consider whether you should increase your tolerance for duplication.</p>
<p>Note that it's not always as simple as under-abstracting vs over-abstracting. Sometimes abstraction is appropriate, but you might take the wrong approach. If an abstraction is deemed wrong by the team, that doesn't mean no abstraction is necessarily the best alternative.</p>
<h3>Under-abstraction examples</h3>
<p>The main sign that you could be under-abstracting is that you have a heap of code doing the exact same thing called in a heap of places with no obvious reason why anybody would want the code to diverge.</p>
<h4>Example: Hard-coded formulas</h4>
<h5>Bad:</h5>
<pre><code># sphere has radius of 11
sphere_volume = 4*Math::PI/3*11**3
puts "the volume of the sphere is #{sphere_volume} cm^3"
...

radius = calculate_radius
volume = 4*Math::PI/3*radius**3
sphere.volume = volume</code></pre>
<h5>Good:</h5>
<pre><code>def sphere_volume(radius)
  4*Math::PI/3*radius**3
end

# sphere has radius of 11
sphere_volume = sphere_volume(11)
puts "the volume of the sphere is #{sphere_volume} cm^3"
...

radius = calculate_radius
volume = sphere_volume(radius)
sphere.volume = volume</code></pre>
<p>Why is it a good idea to abstract the formula for a sphere's volume into its own method? Because if mathematicians ever found out they got the formula wrong, you would want to go through all the places in your code that you used the formula and update it to be correct. That is, we know ahead of time that we want the code to be in lockstep. This is as safe a gamble as you can get.</p>
<h3>Over-abstraction examples</h3>
<p>The main sign that you're over-abstracting is that your method accepts a bunch of optional args:</p>
<h4>Example: Bloated method</h4>
<h5>Bad:</h5>
<pre><code>def average(arr, type = Integer, ignore_nulls = false)
  if arr.any?(&amp;:nil?)
    if ignore_nulls
      arr = arr.compact
    else
      return nil
    end
  end

  if type == String
    arr = arr.map(&amp;:to_i)
  end

  arr.sum / arr.size
end

puts average([1,2,3])
=&gt; 2

puts average(['1','2','3'], String)
=&gt; 2

puts average(['1','2','3', nil], String, true)
=&gt; 2

puts average([1, 2, 3, nil], Integer, false)
=&gt; nil</code></pre>
<p>If you want to know how the <code>average</code> method behaves when you're dealing with an array of strings with no <code>nil</code> values, you have to read through the first if condition which has nothing to do with your use case before reaching the code that does. Likewise if you want to know how the <code>average</code> method behaves when the array contains either nils or integers, the second if condition is irrelevant, but you'll still need to read through that to understand how the whole thing works.</p>
<p>If each of the use cases came up dozens or hundreds of times, maybe then it would make sense to retain the abstraction, but when the number of optional arguments is roughly equal to the number of different use cases, chances are you've got the wrong abstraction.</p>
<h5>Good:</h5>
<pre><code>def average(arr)
  arr.sum / arr.size
end

puts average([1,2,3])
=&gt; 2

arr = ['1','2','3'].map(&amp;:to_i)
puts average(arr)
=&gt; 2

arr = ['1','2','3', nil].compact.map(&amp;:to_i)
puts average(arr)
=&gt; 2

arr = [1, 2, 3, nil]
if arr.any?(&amp;:nil?)
  puts nil
else
  puts average(arr)
end
=&gt; nil</code></pre>
<p>In this case we're not removing the abstraction altogether: we're just keeping the part that actually applies to all cases. Now understanding the logic of any one invocation of our <code>average</code> method is trivial.</p>
<p>We now have <code>.map(&amp;:to_i)</code> being duplicated whereas it only appeared once in the <code>Bad</code> alternative, but it's a small cost for a vast improvement.</p>
<p>Note that looking at the <code>Good</code> variant, it's clear that the behaviour is quite different from one use case to the next, but that is not at all clear in the <code>Bad</code> variant because the method calls all look so simple and it was anybody's guess how much code inside <code>average</code> applied to each use case.</p>
<p>This is why abstractions go bad over time: because as you expand the interface more and more, it becomes harder and harder to judge how appropriate the abstraction is to any given use case, and developers end up assuming that all that convoluted code is vaguely relevant to the majority of use cases when in fact it's not.</p>
<h4>Example: Awkward class</h4>
<h5>Bad:</h5>
<pre><code>class Shape
  def initialize(radius: nil, width: nil, type:)
    @radius = radius
    @width = width
    @type = type
  end

  def area
    case @type
    when :square
      @width ** 2
    when :circle
      (@radius ** 2) * Math::PI
    end
  end

  def perimeter
    case @type
    when :square
      @width * 4
    when :circle
      @radius * 2 * Math::PI
    end
  end

  def diameter
    case @type
    when :square
      nil
    when :circle
 …</code></pre></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jesseduffield.com/beginners-guide-to-abstraction/">https://jesseduffield.com/beginners-guide-to-abstraction/</a></em></p>]]>
            </description>
            <link>https://jesseduffield.com/beginners-guide-to-abstraction/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23735991</guid>
            <pubDate>Sun, 05 Jul 2020 01:12:40 GMT</pubDate>
        </item>
    </channel>
</rss>
