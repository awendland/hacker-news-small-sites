<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 23 Oct 2020 12:48:33 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Fri, 23 Oct 2020 12:48:33 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[What ORMs Have Taught Me: Just Learn SQL (2014)]]>
            </title>
            <description>
<![CDATA[
Score 208 | Comments 187 (<a href="https://news.ycombinator.com/item?id=24845300">thread link</a>) | @IA21
<br/>
October 20, 2020 | https://wozniak.ca/blog/2014/08/03/1/ | <a href="https://web.archive.org/web/*/https://wozniak.ca/blog/2014/08/03/1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<p>
I’ve come to the conclusion that, for me, ORMs are more detriment than
benefit. In short, they can be used to nicely augment working with SQL
in a program, but they should not replace it.
</p>

<p>
Some background: For the past 30 months I’ve been working with code
that has to interface with Postgres and to some extent, SQLite. Most
of that has been with <a href="http://sqlalchemy.org/">SQLAlchemy</a> (which I quite like) and <a href="http://hibernate.org/">Hibernate</a>
(which I don’t). I’ve worked with existing code and data models, as
well as designing my own. Most of the data is event-based storage
(“timelines”) with a heavy emphasis on creating reports.
</p>

<p>
Much has been written about the Object/Relational Impedance
Mismatch. It’s hard to appreciate it until you live it. Neward, in his
<a href="http://blogs.tedneward.com/post/the-vietnam-of-computer-science/">well known essay</a>, lays out many cogent reasons why ORMs turn into
quagmires. In my experience, I’ve had to deal directly with a fair
number of them: <i>entity identity issues</i>, <i>dual-schema problem</i>, <i>data
retrieval mechanism concern</i>, and the <i>partial-object problem</i>. I want to
talk briefly about my experiences with these issues and add one of my
own.
</p>

<div id="outline-container-orge4f50a6">
<h2 id="orge4f50a6">Partial objects, attribute creep, and foreign keys</h2>
<div id="text-orge4f50a6">
<p>
Perhaps the most subversive issue I’ve had with ORMs is “attribute
creep” or “wide tables”, that is, tables that just keep accruing
attributes. As much as I’d like to avoid it, sometimes it becomes
necessary (although things like <a href="http://www.postgresql.org/docs/9.3/interactive/hstore.html">Postgres’ hstore</a> can help). For
example, a client may be providing you with lots of data that they
want attached to reports based on various business logic. Furthermore,
you don’t have much insight into this data; you’re just schlepping it
around.
</p>

<p>
This in and of itself isn’t a terrible thing in a database. It becomes
a real pain point with an ORM. Specifically, the problem starts to
show up in any query that uses the entity directly to create the
query. You may have a Hibernate query like so early on in the project.
</p>

<pre>query(Foo.class).add(Restriction.eq("x", value))
</pre>

<p>
This may be fine when Foo has five attributes, but becomes a data fire
hose when it has a hundred. This is the equivalent of using <code>SELECT
*</code>, which is usually saying more than what is intended. ORMs, however,
encourage this use and often make writing precise projections as
tedious as they are in SQL. (I have optimized such queries by adding
the appropriate projection and reduced the run time from minutes to
seconds; all the time was spent translating the database row into a
Java object.)
</p>

<p>
Which leads to another bad experience: the pernicious use of foreign
keys. In the ORMs I’ve used, links between classes are represented in
the data model as foreign keys which, if not configured carefully,
result in a large number of joins when retrieving the object. (A
recent count of one such table in my work resulted in over 600
attributes and 14 joins to access a single object, using the preferred
query methodology.)
</p>

<p>
Attribute creep and excessive use of foreign keys shows me is that in
order to use ORMs effectively, you still need to know SQL. My
contention with ORMs is that, if you need to know SQL, just use SQL
since it prevents the need to know how non-SQL gets translated to SQL.
</p>
</div>
</div>

<div id="outline-container-org2d7e33d">
<h2 id="org2d7e33d">Data retrieval</h2>
<div id="text-org2d7e33d">
<p>
Knowing how to write SQL becomes even more important when you attempt
to actually write queries using an ORM. This is especially important
when efficiency is a concern.
</p>

<p>
From what I’ve seen, unless you have a really simple data model (that
is, you never do joins), you will be bending over backwards to figure
out how to get an ORM to generate SQL that runs efficiently. Most of
the time, it’s more obfuscated than actual SQL.
</p>

<p>
And if you elect to keep the query simple, you end up doing a lot of
work in the code that could be done in the database faster. <a href="https://en.wikipedia.org/wiki/Window_function_%28SQL%29#Window_function">Window
functions</a> are relatively advanced SQL that is painful to write with
ORMs. Not writing them into the query likely means you will be
transferring a lot of extra data from the database to your
application.
</p>

<p>
In these cases, I’ve elected to write queries using a templating
system and describe the tables using the ORM. I get the convenience of
an application level description of the table with direct use of
SQL. It’s a lot less trouble than anything else I’ve used so far.
</p>
</div>
</div>

<div id="outline-container-org05d550b">
<h2 id="org05d550b">Dual schema dangers</h2>
<div id="text-org05d550b">
<p>
This one seems to be one of those unavoidable redundancies.  If you
try to get rid of it, you only make more problems or add excessive
complexity.
</p>

<p>
The problem is that you end up having a data definition in two places:
the database and your application.  If you keep the definition
entirely in the application, you end up having to write the SQL Data
Definition Language (DDL) with the ORM code, which is the same
complication as writing advanced queries in the ORM.  If you keep it
in the database, you will probably want a representation in the
application for convenience and to prevent too much “string typing”.
</p>

<p>
I much prefer to keep the data definition in the database and read it
into the application.  It doesn’t solve the problem, but it makes it
more manageable.  I’ve found that reflection techniques to get the
data definition are not worth it and I succumb to managing the
redundancy of data definitons in two places.
</p>

<p>
But the damn migration issue is a real kick in the teeth: changing the
model is no big deal in the application, but a real pain in the
database.  After all, databases are persistent whereas application
data is not.  ORMs simply get in the way here because they don’t help
manage data migration at all.  I work on the principle that the
database’s data definitions aren’t things you should manipulate in the
application.  Instead, manipulate the results of queries.  That is,
the queries are your API to the database.  So instead of thinking
about objects, I think about functions with return types.
</p>

<p>
Thus, one is forced to ask, should you use an ORM for anything but
convenience in making queries?
</p>
</div>
</div>

<div id="outline-container-org7033826">
<h2 id="org7033826">Identities</h2>
<div id="text-org7033826">
<p>
Dealing with entity identities is one of those things that you have to
keep in mind at all times when working with ORMs, forcing you to write
for two systems while only have the expressivity of one.
</p>

<p>
When you have foreign keys, you refer to related identities with an
identifier. In your application, “identifier” takes on various
meanings, but usually it’s the memory location (a pointer). In the
database, it’s the state of the object itself. These two things don’t
really get along because you can really only use database identifiers
in the database (the ultimate destination of the data you’re working
with).
</p>

<p>
What this results in is having to manipulate the ORM to get a database
identifier by manually flushing the cache or doing a partial commit to
get the actual database identifier.
</p>

<p>
I can’t even call this a leaky abstraction because the work “leak”
implies small amounts of the contents escaping relative to the source.
</p>
</div>
</div>

<div id="outline-container-orgddbdda4">
<h2 id="orgddbdda4">Transactions</h2>
<div id="text-orgddbdda4">
<p>
Something that Neward alludes to is the need for developers to handle
transactions. Transactions are dynamically scoped, which is a powerful
but mostly neglected concept in programming languages due to the
confusion they cause if overused.  This leads to a lot of boilerplate
code with exception handlers and a careful consideration of where
transaction boundaries should occur.  It also makes you pass session
objects around to any function/method that might have to communicate
with the database.
</p>

<p>
The concept of a transaction translates poorly to applications due to
their reliance on context based on time. As mentioned, dynamic scoping
is one way to use this in a program, but it is at odds with lexical
scoping, the dominant paradigm. Thus, you must take great care to know
about the “when” of a transaction when writing code that works with
databases and can make modularity tricky (“Here’s a useful function
that will only work in certain contexts”).
</p>

<p>
Where do I see myself going?
</p>

<p>
At this point, I’m starting to question the wisdom behind the outright
rejection of <a href="http://c2.com/cgi/wiki?StoredProcedures">stored procedures</a>.  It sounds <a href="http://c2.com/cgi/wiki?StoredProceduresAreEvil">heretical</a>, but it may work
for my use cases.  (And hey, with the advent of “devops”, the divide
between the developer and the database administrator is basically
non-existent.)
</p>

<p>
I’ve found myself thinking about the database as just another data
type that has an API: the queries.  The queries return values of some
type, which are represented as some object in the program. By moving
away from thinking of the objects in my application as something to be
stored in a database (the raison d’être for ORMs) and instead thinking
of the database as a (large and complex) data type, I’ve found working
with a database from an application to be much simpler. And wondering
why I didn’t see it earlier.
</p>

<p>
(It should be made clear that I am not claiming this is how all
applications should deal with a database.  All I am saying is that
this fits my use case based on the data I am working with.)
</p>

<p>
Regardless of whether I find that stored procedures aren’t actually
that evil or whether I keep using templated SQL, I do know one thing:
I won’t fall into the “ORMs make it easy” trap. They are an acceptable
way to represent a data definition, but a poor way to write queries
and a bad way to store object state. If you’re using an RDBMS, bite
the bullet and learn SQL.
</p>

<p>
August 03, 2014
</p>
</div>
</div>
</div></div>]]>
            </description>
            <link>https://wozniak.ca/blog/2014/08/03/1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24845300</guid>
            <pubDate>Wed, 21 Oct 2020 06:37:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Julia GPU]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24840972">thread link</a>) | @cdsousa
<br/>
October 20, 2020 | https://notamonadtutorial.com/julia-gpu-98a461d33e21 | <a href="https://web.archive.org/web/*/https://notamonadtutorial.com/julia-gpu-98a461d33e21">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><div><div><div><p><a href="https://medium.com/@federicocarrone?source=post_page-----98a461d33e21--------------------------------" rel="noopener"><img alt="Federico Carrone" src="https://miro.medium.com/fit/c/96/96/2*p2NbnNI4sEc75QvzOZ1gaA.jpeg" width="48" height="48"></a></p></div></div></div></div><figure><div><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/2816/1*KJX3T1Y9T1Cj0aV3m-A22w.png" width="1408" height="528" srcset="https://miro.medium.com/max/552/1*KJX3T1Y9T1Cj0aV3m-A22w.png 276w, https://miro.medium.com/max/1104/1*KJX3T1Y9T1Cj0aV3m-A22w.png 552w, https://miro.medium.com/max/1280/1*KJX3T1Y9T1Cj0aV3m-A22w.png 640w, https://miro.medium.com/max/1400/1*KJX3T1Y9T1Cj0aV3m-A22w.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*KJX3T1Y9T1Cj0aV3m-A22w.png?q=20"></p></div></div></div></figure><p id="58dc">We are living in a time where more and more data is being created every day as well as new techniques and complex algorithms that try to extract the most out of it. As such, CPU capabilities are approaching a bottleneck in their computing power. GPU computing opened its way into a new paradigm for high-performance and parallel computation a long time ago, but it was not until recently that it become massively used for data science.<br>In this interview, <a href="https://twitter.com/maleadt" rel="noopener">Tim Besard</a>, one of the main contributors to the JuliaGPU project, digs into some of the details about GPU computing and the features that make Julia a language suited for such tasks, not only from a performance perspective but also from a user one.</p></div></div></section><section><div><div><p id="38e1"><em>Join the Not a Monad Tutorial Telegram </em><a href="https://t.me/notamonadtutorial" rel="noopener"><em>group</em></a><em> or </em><a href="https://t.me/channel_notamonadtutorial" rel="noopener"><em>channel</em></a><em> to talk about programming, computer science and papers. See you there!</em></p><p id="b3d6"><em>If you are looking for good engineers send me an email to mail@fcarrone.com or you can also reach me via twitter at </em><a href="https://twitter.com/federicocarrone" rel="noopener"><em>@federicocarrone</em></a><em>.</em></p></div></div></section><section><div><div><h2 id="6d6e">Please tell us a bit about yourself. What is your background? what is your current position?</h2><p id="b291">Iâ€™ve always been interested in systems programming, and after obtaining my CS degree I got the opportunity to start a PhD at Ghent University, Belgium, right when Julia was first released around 2012. The language seemed intriguing, and since I wanted to gain some experience with LLVM, I decided to port some image processing research code from MATLAB and C++ to Julia. The goal was to match performance of the C++ version, but some of its kernels were implemented in CUDA Câ€¦ So obviously Julia needed a GPU back-end!</p><p id="7f14">That was easier said than done, of course, and much of my PhD was about implementing that back-end and (re)structuring the existing Julia compiler to facilitate these additional back-ends. Nowadays Iâ€™m at Julia Computing, where I still work on everything GPU-related.</p><h2 id="8f7f">What is JuliaGPU? What is the goal of the project?</h2><p id="43d9">JuliaGPU is the name we use to group GPU-related resources in Julia: Thereâ€™s a <a href="https://github.com/JuliaGPU" rel="noopener">GitHub organization</a> where most packages are hosted, a <a href="https://juliagpu.org/" rel="noopener">website</a> to point the way for new users, we have <a href="https://github.com/JuliaGPU/gitlab-ci" rel="noopener">CI infrastructure</a> for JuliaGPU projects, thereâ€™s a Slack channel and Discourse category, etc.</p><p id="ceaa">The goal of all this is to make it easier to use GPUs for all kinds of users. Current technologies often impose significant barriers to entry: CUDA is fairly tricky to install, C and C++ are not familiar to many users, etc. With the software we develop as part of the JuliaGPU organization, we aim to make it easy to use GPUs, without hindering the ability to optimize or use low-level features that the hardware has to offer.</p><h2 id="5da7">What is GPU computing? How important is it nowadays?</h2><p id="4de0">GPU computing means using the GPU, a device originally designed for graphics processing, to perform general-purpose computations. It has grown more important now that CPU performance is not improving as steadily as it used to. Instead, specialized devices like GPUs or FPGAs are increasingly used to improve the performance of certain computations. In the case of GPUs, the architecture is a great fit to perform highly-parallel applications. Machine learning networks are a good example of such parallel applications, and their popularity is one of the reasons GPUs have become so important.</p><h2 id="f596">Do you think Julia is an appropriate language to efficiently use GPU capabilities? Why?</h2><p id="775f">Juliaâ€™s main advantage is that the language was designed to be compiled. Even though the syntax is high-level, the generated machine code is<br>compact and has great performance characteristics (for more details, see <a href="http://janvitek.org/pubs/oopsla18b.pdf" rel="noopener">this paper</a>). This is crucial for GPU execution, where we are required to run native binaries and cannot easily (or efficiently) interpret code as is often required by other languageâ€™s semantics.</p><p id="b90b">Because weâ€™re able to directly compile Julia for GPUs, we can use almost all of the languageâ€™s features to build powerful abstractions. For example, you can define your own types, use those in GPU arrays, compose that with existing abstractions like lazy "Transpose" wrappers, access those on the GPU while benefiting from automatic bounds-checking (if needed), etc.</p><h2 id="824e">From a Python programmer perspective, how does CUDA.jl compare to PyCUDA? Are their functionalities equivalent?</h2><p id="cf3f">PyCUDA gives the programmer access to the CUDA APIs, with high-level Python functions that are much easier to use. CUDA.jl provides the same, but in Julia. The `hello world` from PyCUDAâ€™s home page looks almost identical in Julia:</p><pre><span id="a986">using CUDA</span><span id="03e4">function multiply_them(dest, a, b)<br> i = threadIdx().x<br> dest[i] = a[i] * b[i]<br> return<br>end</span><span id="9e7d">a = CuArray(randn(Float32, 400))<br>b = CuArray(randn(Float32, 400))</span><span id="5833">dest = similar(a)<br>@cuda threads=400 multiply_them(dest, a, b)</span><span id="1372">println(dest-a.*b)</span></pre><p id="396f">Thereâ€™s one very big difference: "multiply_them" here is a function written in Julia, whereas PyCUDA uses a kernel written in CUDA C. The reason is straightforward: Python is not simple to compile. Of course, projects like Numba prove that it is very much possible to do so, but in the end those are separate compilers that try to match the reference Python compilers as closely as possible. With CUDA.jl, we integrate with that reference compiler, so itâ€™s much easier to guarantee consistent semantics and follow suit when the language changes (for more details,<br>refer to <a href="https://arxiv.org/abs/1712.03112" rel="noopener">this paper</a>).</p><h2 id="5cf6">Are the packages in the JuliaGPU organization targeted to experienced programmers only?</h2><p id="6590">Not at all. CUDA.jl targets different kinds of (GPU) programmers. If you are confident writing your own kernels, you can do so, while using all of the low-level features CUDA GPUs have to offer. But if you are new to the world of GPU programming, you can use high-level array operations that use existing kernels in CUDA.jl. For example, the above element-wise multiplication could just as well be written as:</p><pre><span id="9c1f">using CUDA</span><span id="476a">a = CuArray(randn(Float32, 400))<br>b = CuArray(randn(Float32, 400))</span><span id="efdf">dest = a .* b</span></pre><h2 id="4c26">Is it necessary to know how to code in CUDA.jl to take full advantage of GPU computing in Julia?</h2><p id="208b">Not for most users. Julia has a powerful language of generic array operations ("map", "reduce", "broadcast", "accumulate", etc) which can be applied to all kinds of arrays, including GPU arrays. That means you can often re-use your codebase developed for the CPU with CUDA.jl (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0965997818310123" rel="noopener">this paper</a> shows some powerful examples). Doing so often requires minimal changes: changing the array type, making sure you use array operations instead of for loops, etc.</p><p id="672d">Itâ€™s possible you need to go beyond this style of programming, e.g., because your application doesnâ€™t map cleanly onto array operations, to use specific GPU features, etc. In that case, some basic knowledge about CUDA and the GPU programming model is sufficient to write kernels in CUDA.jl.</p><h2 id="1a33">How is the experience of coding a kernel in CUDA.jl in comparison to CUDA C and how transferable is the knowledge to one another?</h2><p id="6a08">Itâ€™s very similar, and thatâ€™s by design: We try to keep the kernel abstractions in CUDA.jl close to their CUDA C counterparts such that the programming environment is familiar to existing GPU programmers. Of course, by using a high-level source language thereâ€™s many quality-of-life improvements. You can allocated shared memory, for example, statically and dynamically as in CUDA C, but instead of a raw pointers we use an N-dimensional array object you can easily index. An example from the <a href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/" rel="noopener">NVIDIA developer blog</a>:</p><pre><span id="ef8a">__global__ void staticReverse(int *d, int n)<br>{<br> __shared__ int s[64];<br> int t = threadIdx.x;<br> int tr = n-t-1;<br> s[t] = d[t];<br> __syncthreads();<br> d[t] = s[tr];<br>}</span></pre><p id="9cb0">The CUDA.jl equivalent of this kernel looks very familiar, but uses array objects instead of raw pointers:</p><pre><span id="996c">function staticReverse(d)<br> s = @cuStaticSharedMem(Int, 64)<br> t = threadIdx().x<br> tr = length(d)-t+1<br> s[t] = d[t]<br> sync_threads()<br> d[t] = s[tr]<br> return<br>end</span></pre><p id="9a14">Using array objects has many advantages, e.g. multi-dimensional is greatly simplified and we can just do "d[i,j]". But itâ€™s also safer, because these accesses are bounds checked:</p><pre><span id="3d65">julia&gt; a = CuArray(1:64)<br>64-element CuArray{Int64,1}:<br> 1<br> 2<br> 3<br> â‹®<br> 62<br> 63<br> 64</span><span id="0d9c">julia&gt; @cuda threads=65 staticReverse(a)<br>ERROR: a exception was thrown during kernel execution.<br>Stacktrace:<br> [1] throw_boundserror at abstractarray.jl:541</span></pre><p id="c17a">Bounds checking isnâ€™t free, of course, and once weâ€™re certain our code is correct we can add an "@inbounds" annotation to our kernel and get the high-performance code we expect:</p><pre><span id="6846">julia&gt; @device_code_ptx @cuda threads=64 staticReverse(a)<br>.visible .entry staticReverse(.param .align 8 .b8 d[16]) {<br> .reg .b32 %r&lt;2&gt;;<br> .reg .b64 %rd&lt;15&gt;;<br> .shared .align 32 .b8 s[512];</span><span id="09c9">mov.b64 %rd1, d;<br> ld.param.u64 %rd2, [%rd1];<br> ld.param.u64 %rd3, [%rd1+8];<br> mov.u32 %r1, %tid.x;<br> cvt.u64.u32 %rd4, %r1;<br> mul.wide.u32 %rd5, %r1, 8;<br> add.s64 %rd6, %rd5, -8;<br> add.s64 %rd7, %rd3, %rd6;<br> ld.global.u64 %rd8, [%rd7+8];<br> mov.u64 %rd9, s;<br> add.s64 %rd10, %rd9, %rd6;<br> st.shared.u64 [%rd10+8], %rd8;<br> bar.sync 0;<br> sub.s64 %rd11, %rd2, %rd4;<br> shl.b64 %rd12, %rd11, 3;<br> add.s64 %rd13, %rd9, %rd12;<br> ld.shared.u64 %rd14, [%rd13+-8];<br> st.global.u64 [%rd7+8], %rd14;<br> ret;<br>}</span><span id="4489">julia&gt; a<br>64-element CuArray{Int64,1}:<br> 64<br> 63<br> 62<br> â‹®<br> 3<br> 2<br> 1</span></pre><p id="3cee">Tools like "@device_code_ptx" make it easy for an experienced developer to inspect generated code and ensure the compiler does what he wants.</p><h2 id="164a">Why does having a compiler have such an impact in libraries like CUDA.jl? (How was the process of integrating it to the Julia compiler?)</h2><p id="e360">Because we have a compiler at our disposal, we can rely on higher-order functions and other generic abstractions that specialize based on the arguments that users provide. That greatly simplifies our library, but also gives the user very powerful tools. As an example, we have carefully implemented a `mapreduce` function that uses shared memory, warp intrinsics, etc to perform a high-performance reduction. The implementation is generic though, and will automatically re-specialize (even at run time) based on the arguments to the function:</p><pre><span id="9da7">julia&gt; mapreduce(identity, +, CuArray([1,2,3]))<br>6</span><span id="c680">julia&gt; mapreduce(sin, *, CuArray([1.1,2.2,3.3]))<br>-0.11366175839582586</span></pre><p id="52e8">With this powerful `mapreduce` …</p></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://notamonadtutorial.com/julia-gpu-98a461d33e21">https://notamonadtutorial.com/julia-gpu-98a461d33e21</a></em></p>]]>
            </description>
            <link>https://notamonadtutorial.com/julia-gpu-98a461d33e21</link>
            <guid isPermaLink="false">hacker-news-small-sites-24840972</guid>
            <pubDate>Tue, 20 Oct 2020 18:48:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Palo Alto Networks sends cease-and-desist letter to take down review videos]]>
            </title>
            <description>
<![CDATA[
Score 431 | Comments 116 (<a href="https://news.ycombinator.com/item?id=24840119">thread link</a>) | @bonfire
<br/>
October 20, 2020 | https://orca.security/cybersecurity-community-transparency/ | <a href="https://web.archive.org/web/*/https://orca.security/cybersecurity-community-transparency/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main">

				
							<div data-elementor-type="single" data-elementor-id="1240" data-elementor-settings="[]">
		<div>
			<div>
						<section data-id="665134c3" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="12af1bce" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
			<div>
					<div>
				<div data-id="19996313" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			<p><em>Abstract: A few weeks ago, Orca Security published a comparison between the Orca Cloud Security Platform and a few other cloud security tools—including a&nbsp;<a href="https://orca.security/prisma-cloud-security/">comparison with Palo Alto Networks Prisma</a>. In response, Palo Alto Networks sent a <a href="https://orca.security/wp-content/uploads/PANW-Legal-Letter-Cease-Desist.pdf" target="_blank" rel="noopener noreferrer">cease and desist letter</a>, demanding the comparison be removed immediately. Here is my response. I urge you to see&nbsp;<a href="https://www.youtube.com/playlist?list=PLDnqJTEi6ynvsGkSJHYeEmhz6_NfzKgbf">the videos in question</a>&nbsp;and if you, like me, believe the cybersecurity community deserves transparency and vendors shouldn’t be allowed to prevent publishing reviews or benchmarks via legal threats, then please share this post. You can also leave your own comments down below.</em></p>
<p><strong>To: Palo Alto Networks</strong></p>
<p><strong>CC: The cybersecurity community</strong></p>
<p><strong>Subject: The Cybersecurity community demands transparency, not legal threats&nbsp;</strong></p>
<p>Security has always been about transparency. The concept of security by obscurity was frowned upon as early as 1851—even before the invention of electricity—when&nbsp;<a href="https://en.wikipedia.org/wiki/Alfred_Charles_Hobbs">Alfred Hobbs</a>, a Massachusetts-based locksmith, demonstrated how then state-of-the-art locks could be picked. He explained that exposing the information would make the public more secure, as rogues already knew the deficiencies. The public needed to be educated, and he’d pursue better locks. Today’s locks are more advanced, but the principle is the same.</p>
<p>The cybersecurity community preaches about many products. All come with their own advantages and disadvantages, capabilities, and limitations. I believe that the only way practitioners can choose the tools that fit their environments best is by viewing factual evidence—not by relying solely on marketing materials. This is why we launched our&nbsp;<a href="https://orca.security/cloud-security-solutions/">Cloud Security Punch-Out! Series</a>, where we deploy a few tools—including Orca Security—on the exact same environment and share the results with viewers who deserve to see them. I urge you to take&nbsp;<a href="https://orca.security/prisma-cloud-security/">a look at the one we did with Palo Alto Networks;</a>&nbsp;as you’ll see we don’t hide those areas where Palo Alto Networks shines.</p>
<p>Unfortunately, Palo Alto Networks is now trying to use legal threats to prevent us from publishing&nbsp;these video&nbsp;reviews. In <a href="https://orca.security/wp-content/uploads/PANW-Legal-Letter-Cease-Desist.pdf" target="_blank" rel="noopener noreferrer">its letter</a>, Palo Alto Networks does not point to any factual inaccuracies in the reviews of its products’ performance. Instead, it premises its threats on flimsy, boilerplate contract terms that prohibit reviews and comparisons of its products and hollow trademark allegations purporting that Palo Alto Networks is sponsoring the videos.</p>
<p>It’s outrageous that the world’s largest cybersecurity vendor (its products being used by over 65,000 organizations according to its website), believes that its users aren’t entitled to share any benchmark or performance comparison of its products. According to its boilerplate contract terms that prohibit “disclosing, publishing, or otherwise making publicly available any benchmark, performance, or comparison tests” of its products, you’re in violation even if you publish the results of an internal comparison of Palo Alto Networks against other products as part of your procurement process. The same goes for the hundreds of Palo Alto Networks reviews on various sites that include G2 Crowd, Capterra, and Gartner Peer Insights. It means that only benchmarks approved by Palo Alto Networks can be published.</p>
<p>Palo Alto Networks appears oblivious to the fact that the New York Attorney General’s office&nbsp;<a href="https://www.leagle.com/decision/2003579195misc2d3841519">sued and won an injunction</a>&nbsp;against McAfee from enforcing its contractual restrictions against publishing reviews or comparisons of its products without its consent more than 17 years ago. In enacting the&nbsp;<a href="https://www.law.cornell.edu/uscode/text/15/45b">Consumer Review Fairness Act</a>, Congress has also prohibited businesses from including contract terms that prohibit consumers from reviewing products or services they purchase.</p>
<blockquote><p>Palo Alto Networks, do you think your products are flawless or that the bad guys will follow along, not openly talking about products’ deficiencies? If the answer is no to both, then why resort to legal threats to remove such benchmarks and comparisons? I refuse to accept a world where any vendor believes it has the right to prevent the free flow of information, and control which product reviews are made publicly available.</p></blockquote>
<p>I urge you to make your products better and focus your marketing efforts on demonstrating that, rather than throwing away money on ill-conceived gag efforts. Such action doesn’t benefit anyone. If you believe we missed something in our test, then tell us so we can make adjustments—we’ll happily integrate your comments and suggestions.</p>
<p>We could contract an objective third party to conduct additional tests. You could conduct your own tests with Palo Alto Networks and Orca Security’s products, then let the audience see and decide for themselves. All such actions would be far more beneficial to the industry, permitting both companies to learn and improve our products for the sake of customers.</p>
<p>As we all recently learned too well,&nbsp;<a href="https://www.contagionlive.com/news/sunlight-inactivates-the-airborne-virus-that-causes-covid19">sunlight is the best disinfectant</a>. The cybersecurity community deserves better than a vendor’s lack of transparency while wielding dubious legal methods. Palo Alto Networks is the worlds’ largest cybersecurity vendor; with great power comes great responsibility. Your products are great—but nothing is perfect, and the public should have free access to all of the facts.</p>
<p>Yours faithfully,<br>
Avi Shua, CEO and Co-Founder<br>
Orca Security</p>
		</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
					</div>
		</div>
		</div>
		
					
					
				
			</div></div>]]>
            </description>
            <link>https://orca.security/cybersecurity-community-transparency/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24840119</guid>
            <pubDate>Tue, 20 Oct 2020 17:31:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My platonic ideal for how engineering hiring should work]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 260 (<a href="https://news.ycombinator.com/item?id=24840013">thread link</a>) | @leeny
<br/>
October 20, 2020 | http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/ | <a href="https://web.archive.org/web/*/http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>I’ve been in and around eng hiring for the past 13 years, as an engineer, a recruiter, and a founder of a technical recruiting marketplace (interviewing.io). Over the course of those 13 years, I’ve become increasingly disgruntled at the state of hiring, and now I’m mad enough to write this blog post.</p>



<p>If you’ve ever been on either end of the table, you’re probably mad at the state of hiring, too. Whether you have given it a lot of thought or whether you just feel it deep down, something about the whole process feels off.</p>



<p>But we’ve been doing it this way for so long that we probably take much of how hiring works as gospel, and it’s really hard to tease apart all the different components of the process and examine why they are the way they are. In this post, I’d like to challenge many of the things we assume about hiring, and, perhaps most importantly, I’d like to lay out my platonic ideal for how eng hiring should work. It’s a simple axiom, really:</p>



<p><em>It should be easy for smart people to talk to other smart people.</em></p>



<p>Or, another way to put it … if I’m a good engineer, it should be easy for me to talk to a hiring manager at a company I might be interested in, at a time of my choosing. But that’s simply not possible today. Despite the refrain that we’re in a candidate’s market and that there’s a shortage of good candidates, which should mean that candidates should have the power to call the shots, today’s hiring process couldn’t be further removed from this ideal. And it’s not just broken for a specific type of candidate. It’s broken for <em>everyone.</em></p>



<p>If you’re reading this, you might be an engineering manager, a senior engineer with stellar credentials, a recent bootcamp grad, an engineer from a background traditionally underrepresented in tech, or some combination of these. <strong>What’s truly messed up about the status quo is that, regardless of which of these groups you fall into, your journey will be unnecessarily unpleasant. Though the degree of unpleasantness will not always be the same, it’s not about race, seniority, pedigree, or gender … or even which side of the table you’re on. Hiring, in its current incarnation, is broken for everybody.</strong></p>



<p>Why? Let us go then, you and I, into the bowels of the status quo.</p>



<h2>A candidate and a hiring manager, never the twain shall meet</h2>



<p>Let’s say that I’m a competent generalist engineer who looks good on paper, and I’m thinking that it’s time to look for a new job. What happens next? The idea of having to mount a full-on job search is so daunting.&nbsp;</p>



<p>I could try some job boards to see which companies are out there. But what would I filter on? I know a lot of programming languages but am not set on having to work in a specific one. How can I tell if I’ll hit it off with the team? I’m applying via a job board to a position I know next to nothing about — will anyone even respond?</p>



<p>Suppose I find some companies where I might want to work. If I’m lucky enough to know someone there, I’ll have to get them to refer me, even though that may not actually do much to speed things along. And if I don’t know anyone there, applying will be an exhausting long shot. Odds are no one will look at my application, and having to redo my resume — or worse, write cover letters — seems like the most tedious kind of busywork.</p>



<p>I guess I can always dig through the recruiter spam I’ve gotten. But do those recruiters still work at the company? If they do, how long will it actually take to get into the process?</p>



<p>Breaking character for a moment, a friend of mine recently got this recruiting email from Google, who has elevated gaslighting to an art form: somehow the fact that it takes two months to get through their process has become a <em>selling point.</em></p>



<figure><img loading="lazy" width="750" height="319" src="http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-750x319.png" alt="" srcset="http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-750x319.png 750w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-450x191.png 450w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail-768x327.png 768w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Fwd__Hello_From_Google__-_aline_interviewing_io_-_Interviewing_io_Mail.png 1004w" sizes="(max-width: 750px) 100vw, 750px"></figure>



<p>Once I do get into the process, why do I have to endure the same intro call ten times with different recruiters who can’t tell me anything about what I’d be working on at any level of depth?</p>



<p>Do I join some platform, create a profile that I copy-paste everywhere (with writing that was just as painful as the aforementioned resume/cover letter) and sort of hope that decent companies contact me … only to have to begin the same recruiter calls over and over again, as above?</p>



<p>Will I have to take some quizzes that drill me on obscure syntax or make me solve toy problems that have no bearing on my engineering ability before I even get to have the aforementioned inane conversation with a recruiter?</p>



<p><strong>If I’m actually good at my job, why can’t I just set up some conversations with companies I think are cool and see if it’s a fit? Why do I have to subject myself and others to an endless parade of vapid conversations and the inevitable busywork that precedes them?</strong></p>



<p>Here’s the truth. Even if I look good on paper and am well-connected, hiring still sucks because of all the noise, uncertainty, and time wasted … but at least I have options. They might not be exactly the right options for me, but at least they exist. On the other hand, if I’m an engineer without a pedigree or a network, my choices are extremely limited, no matter how good I am. Recruiters aren’t reaching out to me, I’m not well-networked enough to have friends refer me, and I <em>definitely </em>don’t hear back when I apply.</p>



<hr>



<p>Let’s take a look at the other side of the table. Let’s say I’m an eng manager who needs to hire more competent generalists for my team. Having worked as both an eng manager and a recruiter, I can tell you that what happens next isn’t particularly inspiring.</p>



<p>As an eng manager, I sit down with a recruiter and try to explain what I’m looking for. Nine times out of ten, I want a <a href="https://www.joelonsoftware.com/2007/06/05/smart-and-gets-things-done/">smart person who can get shit done</a>. But, after a farcical game of telephone, somehow those criteria get warped into years of experience with a specific technology or requirements about where the candidate went to school. I also end up with an uninspired, sterile job description that fails to capture the imagination of any candidates who might unwittingly stumble upon it.</p>



<p><strong>My recruiter then goes to any number of sourcing tools of which LinkedIn Recruiter is the ubiquitous, lackluster market leader. They type in keywords I didn’t ask for and filter on credentials I don’t care about to come up with the same homogenous list of candidates every other recruiter at every other tech company is chasing.</strong></p>



<p>They then contact these candidates en masse with generic copy about my team and the hard problems we’re solving. They celebrate single-digit response rates and spend the minimal time left over to give a cursory glance at candidates applying directly.</p>



<h2>Why is hiring broken?</h2>



<p>So therein lies the ineffectual dance. This is the process we’ve come to accept. As far as I can tease out, the axioms that underlie today’s recruiting best practices go something like this (some of these were told to me verbatim when I was starting out as a recruiter, even):</p>



<ol><li><strong>Thou shalt not engage with active candidates. After all, in this market, strong candidates aren’t looking. Good recruiters build relationships so that when a good candidate does decide to enter the market, the recruiter is there, behind the next doorway, ready to spring!</strong></li><li><strong>Engineering time is expensive, so it’s critical to do as much top-of-funnel filtering as possible to make sure that it’s spent on the right candidates.</strong></li></ol>



<p>Are these axioms wrong? The sad truth is … not really. I’ve written in a <a href="http://blog.alinelerner.com/the-unvarnished-unbundled-guide-to-hiring-tools/">previous post about how market forces rule everything around me</a>, and recruiting best practices are no exception. In an economy with a surplus of jobs and a shortage of talent, it follows that the best talent is going to be harder to find, engineering time will be expensive, and recruiters in their current incarnation are, dare I say it, a necessary evil. <sup><a href="#footnote_0_2455" id="identifier_0_2455" title="From what you’ve read up until this point, you might think that I hate recruiters and find them useless. Not so, dear reader! I hate bad recruiters. And, unfortunately, most of them are bad. What’s sad is that the good ones, instead of spending time on tasks for which they’re uniquely qualified and well-suited, are instead stuck at the top of the funnel sourcing engineers whose qualifications they don’t have the domain expertise to evaluate and selling them on roles they don’t have the domain expertise to describe. The best recruiters I’ve worked with are singularly amazing at shepherding candidates through the process, tirelessly stewarding a company’s employer brand, advising hiring managers on the best ways to close, keeping an analytical eye on the funnel to identify issues before they even arise, and much more.">1</a></sup></p>



<p>The data supports our current world view. According to Lever (one of the two application tracking systems widely used by startups, Greenhouse is the other), here’s a breakdown of how many candidates from each source it takes to make a hire. Note that here, larger numbers are bad — for many companies, internal referrals are the best source and inbound applications are the worst.</p>



<figure><img loading="lazy" width="750" height="375" src="http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-750x375.png" alt="" srcset="http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-750x375.png 750w, http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-450x225.png 450w, http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2-768x384.png 768w, http://blog.alinelerner.com/wp-content/uploads/2020/10/graphLever2.png 1008w" sizes="(max-width: 750px) 100vw, 750px"><figcaption>Source: <a href="https://www.lever.co/recruiting-resources/articles/recruitment-process/" target="_blank" rel="noreferrer noopener">https://www.lever.co/recruiting-resources/articles/recruitment-process/</a></figcaption></figure>



<p>Looking at this data, you can see why recruiters simply ignore online applications. The same dynamics also apply to platforms such as AngelList — like any jobs board, it’s noisy and probably full of candidates who don’t have much leverage (e.g., juniors/bootcamp grads and people requiring visa sponsorship).</p>



<p>As for the value of eng time, guarding it carefully isn’t exactly wrong either. In fact, if you look at what a typical hiring process looks like today, you’ll see that most of the time spent is by engineers conducting interviews.</p>



<figure><table><thead><tr><th><strong>Hiring process stage</strong></th><th>Who does it?</th><th>How long does it take?</th></tr></thead><tbody><tr><td>Resume review</td><td>Recruiter</td><td>10-30 seconds</td></tr><tr><td>Recruiter screen</td><td>Recruiter</td><td>45 min</td></tr><tr><td>Technical phone screen</td><td>Engineer</td><td>1 hour</td></tr><tr><td>Onsite – Eng portion</td><td>Engineer</td><td>6 hours</td></tr><tr><td>Onsite – Recruiter portion</td><td>Recruiter</td><td>1 hour</td></tr><tr><td>Offer</td><td>Recruiter OR Eng mgr</td><td>1 hour</td></tr></tbody></table></figure>



<p>Engineering salaries are high, so given that most of the time spent on a single candidate is with engineers, it’s <em>rational</em> to put some recruiter gates at the top of the funnel to protect eng time. The idea is that recruiters will effectively screen out most candidates and only pass on the most promising ones to the eng team.</p>



<p>Unfortunately, when you look at an actual typical <em>funnel</em>, you’ll see that despite attempts to gate the top with recruiters filtering resumes and making intro calls, it’s not really working. Below is what a typical funnel looks like.</p>



<figure><img loading="lazy" width="750" height="89" src="http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-750x89.png" alt="" srcset="http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-750x89.png 750w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-450x53.png 450w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42-768x91.png 768w, http://blog.alinelerner.com/wp-content/uploads/2020/10/Screenshot-2019-03-23-17.34.42.png 1092w" sizes="(max-width: 750px) 100vw, 750px"></figure>



<p>If you <a href="https://blog.interviewing.io/you-probably-dont-factor-in-engineering-time-when-calculating-cost-per-hire-heres-why-you-really-should/" target="_blank" rel="noreferrer noopener">do the math</a> and look at how many hours are spent — not per candidate but per hire (more useful because hires are ultimately what we want) — you’ll see that despite attempts to save eng time, recruiters spend roughly 15 hours a hire <sup><a href="#footnote_1_2455" id="identifier_1_2455" title="If we add in time to review resumes, it’s an extra five hours (at most).">2</a></sup> and engineers spend about 40. In a process where you don’t make an offer 50% of the time and only convert those offers to hires 50% of …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/">http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/</a></em></p>]]>
            </description>
            <link>http://blog.alinelerner.com/ive-been-an-engineer-and-a-recruiter-hiring-is-broken-heres-why-and-heres-what-it-should-be-like-instead/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24840013</guid>
            <pubDate>Tue, 20 Oct 2020 17:22:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS and their billions in IPv4 addresses]]>
            </title>
            <description>
<![CDATA[
Score 165 | Comments 159 (<a href="https://news.ycombinator.com/item?id=24839887">thread link</a>) | @bgpdude
<br/>
October 20, 2020 | https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html | <a href="https://web.archive.org/web/*/https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div id="post-body"><p>Earlier this week, I was doing some work on AWS and wanted to know what IP addresses were being used. Luckily for me, AWS publishes this all here <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="nofollow noopener">https://ip-ranges.amazonaws.com/ip-ranges.json</a>. When you go through this list, you’ll quickly see that AWS has a massive asset of IPv4 allocations. Just counting quickly I noticed a lot of big prefixes.</p><figure><blockquote data-width="550"><p lang="en" dir="ltr">Ever wondered what all of the AWS network ranges are? You can find them all here:<a href="https://t.co/NBaBF6w0la">https://t.co/NBaBF6w0la</a><br>That's *a lot* of big prefixes!<br>4x /11, 14x /12, 30x /13, 78x /14, 184x /15, 278x /16</p>— Andree Toonk, Adelante! (@atoonk) <a href="https://twitter.com/atoonk/status/1316098702260359168?ref_src=twsrc%5Etfw">October 13, 2020</a></blockquote>

</figure><p>However, the IPv4 ranges on that list are just the ranges that are in use and allocated today by AWS. Time to dig a bit deeper.</p><h3 id="ipv4-address-acquisitions-by-aws">IPv4 address acquisitions by AWS</h3><p>Over the years, AWS has acquired a lot of IPv4 address space. Most of this happens without gaining too much attention, but there were a few notable acquisitions that I’ll quickly summarize below.</p><h4 id="2017-mit-selling-8-million-ipv4-addresses-to-aws">2017: MIT selling 8 million IPv4 addresses to AWS</h4><p>In 2017 <a href="https://www.internetsociety.org/blog/2017/05/mit-goes-on-ipv4-selling-spree/" rel="noopener">MIT sold half of its 18.0.0.0/8</a> allocation to AWS. This 18.128.0.0/9 range holds about 8 million IPv4 addresses.</p><h4 id="2018-ge-sells-3-0-0-0-8-to-aws">2018: GE sells 3.0.0.0/8 to AWS</h4><p>In 2018 the IPv4 prefix 3.0.0.0/8 was transferred from GE to AWS. With this, AWS became the proud owner of its first /8! That’s sixteen million new IPv4 addresses to feed us hungry AWS customers. <a href="https://news.ycombinator.com/item?id=18407173" rel="nofollow noopener">https://news.ycombinator.com/item?id=18407173</a></p><h4 id="2019-aws-buys-amprnet-44-192-0-0-10">2019: AWS buys AMPRnet 44.192.0.0/10</h4><p>In 2019 AWS bought a /10 from AMPR.org, the Amateur Radio Digital Communications (ARDC). The IPv4 range 44.0.0.0/8 was an allocation made to the Amateur Radio organization in 1981 and known as the AMPRNet. This sell caused a fair bit of discussion, check out the <a href="https://mailman.nanog.org/pipermail/nanog/2019-July/thread.html#102103" rel="noopener">nanog discussion here.</a></p><p>Just this month, it <a href="http://www.southgatearc.org/news/2020/october/sale-of-amateur-radio-amprnet-tcp-ip-addresses.htm" rel="noopener">became public knowledge</a> AWS paid $108 million for this /10. That’s $25.74 per IP address.</p><p>These are just a few examples. Obviously, AWS has way more IP addresses than the three examples I listed here. The IPv4 transfer market is very active. Check out this website to get a sense of all transfers: <a href="https://account.arin.net/public/transfer-log#NRPM-8.3IPv4" rel="noopener">https://account.arin.net/public/transfer-log</a></p><h3 id="all-aws-ipv4-addresses">All AWS IPv4 addresses</h3><p>Armed with the information above it was clear that not all of the AWS owned ranges were in the <a href="https://ip-ranges.amazonaws.com/ip-ranges.json">JSON</a> that AWS published. For example, parts of the 3.0.0.0/8 range are missing. Likely because some of it is reserved for future use.</p><p>Combining all those IPv4 prefixes, removing duplicates and overlaps by aggregating them results in the following list of unique IPv4 address owned by AWS: <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes" rel="nofollow noopener">https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#all-prefixes</a></p><p>The total number of IPv4 addresses in that list is just over 100 Million (100,750,168). That’s <strong>the equivalent of just over six /8’s,</strong> not bad!</p><p>If we break this down by allocation size, we see the following:</p><pre><code>1x /8     =&gt; 16,777,216 IPv4 addresses
1x /9     =&gt; 8,388,608 IPv4 addresses
4x /10    =&gt; 16,777,216 IPv4 addresses
5x /11    =&gt; 10,485,760 IPv4 addresses
11x /12   =&gt; 11,534,336 IPv4 addresses
13x /13   =&gt; 6,815,744 IPv4 addresses
34x /14   =&gt; 8,912,896 IPv4 addresses
53x /15   =&gt; 6,946,816 IPv4 addresses
182x /16  =&gt; 11,927,552 IPv4 addresses
&lt;and more&gt;</code></pre><p>A complete breakdown can be found here: <a href="https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#breakdown-by-ipv4-prefix-size" rel="nofollow noopener">https://gist.github.com/atoonk/b749305012ae5b86bacba9b01160df9f#breakdown-by-ipv4-prefix-size</a></p><h3 id="putting-a-valuation-on-aws-ipv4-assets">Putting a valuation on AWS’ IPv4 assets</h3><blockquote>Alright.. this is just for fun…</blockquote><p>Since AWS is (one of) the largest buyers of IPv4 addresses, they have spent a significant amount on stacking up their IPv4 resources. It’s impossible, as an outsider, to know how much AWS paid for each deal. However, we can for fun, try to put a dollar number on AWS’ current IPv4 assets.</p><p>The average price for IPv4 addresses has gone up over the years. From ~$10 per IP a few years back to ~$25 per IP <a href="https://auctions.ipv4.global/" rel="noopener">nowadays</a>. <br>Note that these are market prices, so if AWS would suddenly decide to sell its IPv4 addresses and overwhelm the market with supply, prices would drop. But that won’t happen since we’re all still addicted to IPv4 ;)</p><p>Anyway, let’s stick with $25 and do the math just for fun.</p><pre><code>100,750,168 ipv4 addresses x $25 per IP = $2,518,754,200</code></pre><p>Just<strong> over $2.5 billion worth of IPv4 addresses,</strong> not bad! </p><h3 id="peeking-into-the-future">Peeking into the future</h3><p>It’s clear AWS is working hard behind the scenes to make sure we can all continue to build more on AWS. One final question we could look at is: <em>how much buffer does AWS have?</em> ie. how healthy is their IPv4 reserve?</p><p>According to their <a href="https://ip-ranges.amazonaws.com/ip-ranges.json" rel="noopener">published data</a>, they have allocated roughly 53 Million IPv4 addresses to existing AWS services. We found that all their IPv4 addresses combined equates to approximately 100 Million IPv4 addresses. That means they still have ~47 Million IPv4 addresses, or 47% available for future allocations. That’s pretty healthy! And on top of that, I’m sure they’ll continue to source more IPv4 addresses. The IPv4 market is still hot!</p></div>
    </div></div>]]>
            </description>
            <link>https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24839887</guid>
            <pubDate>Tue, 20 Oct 2020 17:12:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Predicting Football Results with Statistical Modelling]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 30 (<a href="https://news.ycombinator.com/item?id=24838958">thread link</a>) | @henrik_w
<br/>
October 20, 2020 | https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/ | <a href="https://web.archive.org/web/*/https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      

      <section itemprop="text">
        <p>Football (or soccer to my American readers) is full of clichés: “It’s a game of two halves”, “taking it one game at a time” and “Liverpool have failed to win the Premier League”. You’re less likely to hear “Treating the number of goals scored by each team as independent Poisson processes, statistical modelling suggests that the home team have a 60% chance of winning today”. But this is actually a bit of cliché too (it has been discussed <a href="https://www.pinnacle.com/en/betting-articles/soccer/how-to-calculate-poisson-distribution">here</a>, <a href="https://help.smarkets.com/hc/en-gb/articles/115001457989-How-to-calculate-Poisson-distribution-for-football-betting">here</a>, <a href="http://pena.lt/y/2014/11/02/predicting-football-using-r/">here</a>, <a href="http://opisthokonta.net/?p=296">here</a> and <a href="https://dashee87.github.io/data%20science/football/r/predicting-football-results-with-statistical-modelling/">particularly well here</a>). As we’ll discover, a simple Poisson model is, well, overly simplistic. But it’s a good starting point and a nice intuitive way to learn about statistical modelling. So, if you came here looking to make money, <a href="http://www.make5000poundspermonth.co.uk/">I hear this guy makes £5000 per month without leaving the house</a>.</p>

<h2 id="poisson-distribution">Poisson Distribution</h2>

<p>The model is founded on the number of goals scored/conceded by each team. Teams that have been higher scorers in the past have a greater likelihood of scoring goals in the future. We’ll import all match results from the recently concluded Premier League (2016/17) season. There’s various sources for this data out there (<a href="https://www.kaggle.com/hugomathien/soccer">kaggle</a>, <a href="http://www.football-data.co.uk/englandm.php">football-data.co.uk</a>, <a href="https://github.com/jalapic/engsoccerdata">github</a>, <a href="http://api.football-data.org/index">API</a>). I built an <a href="https://github.com/dashee87/footballR">R wrapper for that API</a>, but I’ll go the csv route this time around.</p>

<div><div><pre><code><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>seaborn</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>poisson</span><span>,</span><span>skellam</span>

<span>epl_1617</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>"http://www.football-data.co.uk/mmz4281/1617/E0.csv"</span><span>)</span>
<span>epl_1617</span> <span>=</span> <span>epl_1617</span><span>[[</span><span>'HomeTeam'</span><span>,</span><span>'AwayTeam'</span><span>,</span><span>'FTHG'</span><span>,</span><span>'FTAG'</span><span>]]</span>
<span>epl_1617</span> <span>=</span> <span>epl_1617</span><span>.</span><span>rename</span><span>(</span><span>columns</span><span>=</span><span>{</span><span>'FTHG'</span><span>:</span> <span>'HomeGoals'</span><span>,</span> <span>'FTAG'</span><span>:</span> <span>'AwayGoals'</span><span>})</span>
<span>epl_1617</span><span>.</span><span>head</span><span>()</span>
</code></pre></div></div>

<div>
<table>
  <thead>
    <tr>
      <th></th>
      <th>HomeTeam</th>
      <th>AwayTeam</th>
      <th>HomeGoals</th>
      <th>AwayGoals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Burnley</td>
      <td>Swansea</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Crystal Palace</td>
      <td>West Brom</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Everton</td>
      <td>Tottenham</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hull</td>
      <td>Leicester</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Man City</td>
      <td>Sunderland</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>We imported a csv as a pandas dataframe, which contains various information for each of the 380 EPL games in the 2016-17 English Premier League season. We restricted the dataframe to the columns in which we’re interested (specifically, team names and numer of goals scored by each team). I’ll omit most of the code that produces the graphs in this post. But don’t worry, you can find that code on <a href="https://github.com/dashee87/blogScripts/blob/master/Jupyter/2017-06-04-predicting-football-results-with-statistical-modelling.ipynb">my github page</a>. Our task is to model the final round of fixtures in the season, so we must remove the last 10 rows (each gameweek consists of 10 matches).</p>

<div><div><pre><code><span>epl_1617</span> <span>=</span> <span>epl_1617</span><span>[:</span><span>-</span><span>10</span><span>]</span>
<span>epl_1617</span><span>.</span><span>mean</span><span>()</span>
</code></pre></div></div>

<div><div><pre><code>HomeGoals    1.591892
AwayGoals    1.183784
dtype: float64
</code></pre></div></div>

<p>You’ll notice that, on average, the home team scores more goals than the away team. This is the so called ‘home (field) advantage’ (discussed <a href="https://jogall.github.io/2017-05-12-home-away-pref/">here</a>) and <a href="http://bleacherreport.com/articles/1803416-is-home-field-advantage-as-important-in-baseball-as-other-major-sports">isn’t specific to soccer</a>. This is a convenient time to introduce the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a>. It’s a discrete probability distribution that describes the probability of the number of events within a specific time period (e.g 90 mins) with a known average rate of occurrence. A key assumption is that the number of events is independent of time. In our context, this means that goals don’t become more/less likely by the number of goals already scored in the match. Instead, the number of goals is expressed purely as function an average rate of goals. If that was unclear, maybe this mathematical formulation will make clearer:</p>



<p> represents the average rate (e.g. average number of goals, average number of letters you receive, etc.). So, we can treat the number of goals scored by the home and away team as two independent Poisson distributions. The plot below shows the proportion of goals scored compared to the number of goals estimated by the corresponding Poisson distributions.</p>

<p><img src="https://dashee87.github.io/images/home_away_goals_python.png" alt=""></p>

<p>We can use this statistical model to estimate the probability of specfic events.</p>



<p>The probability of a draw is simply the sum of the events where the two teams score the same amount of goals.</p>



<p>Note that we consider the number of goals scored by each team to be independent events (i.e. P(A n B) = P(A) P(B)). The difference of two Poisson distribution is actually called a <a href="https://en.wikipedia.org/wiki/Skellam_distribution">Skellam distribution</a>. So we can calculate the probability of a draw by inputting the mean goal values into this distribution.</p>

<div><div><pre><code><span># probability of draw between home and away team</span>
<span>skellam</span><span>.</span><span>pmf</span><span>(</span><span>0.0</span><span>,</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>0</span><span>],</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>1</span><span>])</span>
</code></pre></div></div>



<div><div><pre><code><span># probability of home team winning by one goal</span>
<span>skellam</span><span>.</span><span>pmf</span><span>(</span><span>1</span><span>,</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>0</span><span>],</span>  <span>epl_1617</span><span>.</span><span>mean</span><span>()[</span><span>1</span><span>])</span>
</code></pre></div></div>



<p><img src="https://dashee87.github.io/images/skellam_goals_python.png" alt=""></p>

<p>So, hopefully you can see how we can adapt this approach to model specific matches. We just need to know the average number of goals scored by each team and feed this data into a Poisson model. Let’s have a look at the distribution of goals scored by Chelsea and Sunderland (teams who finished 1st and last, respectively).</p>

<p><img src="https://dashee87.github.io/images/chelsea_sunderland_goals_python.png" alt=""></p>

<h2 id="building-a-model">Building A Model</h2>

<p>You should now be convinced that the number of goals scored by each team can be approximated by a Poisson distribution. Due to a relatively sample size (each team plays at most 19 home/away games), the accuracy of this approximation can vary significantly (especially earlier in the season when teams have played fewer games). Similar to before, we could now calculate the probability of various events in this Chelsea Sunderland match. But rather than treat each match separately, we’ll build a more general Poisson regression model (<a href="https://en.wikipedia.org/wiki/Poisson_regression">what is that?</a>).</p>

<div><div><pre><code><span># importing the tools required for the Poisson regression model</span>
<span>import</span> <span>statsmodels.api</span> <span>as</span> <span>sm</span>
<span>import</span> <span>statsmodels.formula.api</span> <span>as</span> <span>smf</span>

<span>goal_model_data</span> <span>=</span> <span>pd</span><span>.</span><span>concat</span><span>([</span><span>epl_1617</span><span>[[</span><span>'HomeTeam'</span><span>,</span><span>'AwayTeam'</span><span>,</span><span>'HomeGoals'</span><span>]]</span><span>.</span><span>assign</span><span>(</span><span>home</span><span>=</span><span>1</span><span>)</span><span>.</span><span>rename</span><span>(</span>
            <span>columns</span><span>=</span><span>{</span><span>'HomeTeam'</span><span>:</span><span>'team'</span><span>,</span> <span>'AwayTeam'</span><span>:</span><span>'opponent'</span><span>,</span><span>'HomeGoals'</span><span>:</span><span>'goals'</span><span>}),</span>
           <span>epl_1617</span><span>[[</span><span>'AwayTeam'</span><span>,</span><span>'HomeTeam'</span><span>,</span><span>'AwayGoals'</span><span>]]</span><span>.</span><span>assign</span><span>(</span><span>home</span><span>=</span><span>0</span><span>)</span><span>.</span><span>rename</span><span>(</span>
            <span>columns</span><span>=</span><span>{</span><span>'AwayTeam'</span><span>:</span><span>'team'</span><span>,</span> <span>'HomeTeam'</span><span>:</span><span>'opponent'</span><span>,</span><span>'AwayGoals'</span><span>:</span><span>'goals'</span><span>})])</span>

<span>poisson_model</span> <span>=</span> <span>smf</span><span>.</span><span>glm</span><span>(</span><span>formula</span><span>=</span><span>"goals ~ home + team + opponent"</span><span>,</span> <span>data</span><span>=</span><span>goal_model_data</span><span>,</span> 
                        <span>family</span><span>=</span><span>sm</span><span>.</span><span>families</span><span>.</span><span>Poisson</span><span>())</span><span>.</span><span>fit</span><span>()</span>
<span>poisson_model</span><span>.</span><span>summary</span><span>()</span>
</code></pre></div></div>

<table>
<caption>Generalized Linear Model Regression Results</caption>
<tbody><tr>
  <th>Dep. Variable:</th>        <td>goals</td>      <th>  No. Observations:  </th>  <td>   740</td> 
</tr>
<tr>
  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   700</td> 
</tr>
<tr>
  <th>Model Family:</th>        <td>Poisson</td>     <th>  Df Model:          </th>  <td>    39</td> 
</tr>
<tr>
  <th>Link Function:</th>         <td>log</td>       <th>  Scale:             </th>    <td>1.0</td>  
</tr>
<tr>
  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -1042.4</td>
</tr>
<tr>
  <th>Date:</th>           <td>Sat, 10 Jun 2017</td> <th>  Deviance:          </th> <td>  776.11</td>
</tr>
<tr>
  <th>Time:</th>               <td>11:17:38</td>     <th>  Pearson chi2:      </th>  <td>  659.</td> 
</tr>
<tr>
  <th>No. Iterations:</th>         <td>8</td>        <th>                     </th>     <td> </td>   
</tr>
</tbody></table>
<table>
<tbody><tr>
               <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P&gt;|z|</th> <th>[95.0% Conf. Int.]</th> 
</tr>
<tr>
  <th>Intercept</th>                  <td>    0.3725</td> <td>    0.198</td> <td>    1.880</td> <td> 0.060</td> <td>   -0.016     0.761</td>
</tr>
<tr>
  <th>team[T.Bournemouth]</th>        <td>   -0.2891</td> <td>    0.179</td> <td>   -1.612</td> <td> 0.107</td> <td>   -0.641     0.062</td>
</tr>
<tr>
  <th>team[T.Burnley]</th>            <td>   -0.6458</td> <td>    0.200</td> <td>   -3.230</td> <td> 0.001</td> <td>   -1.038    -0.254</td>
</tr>
<tr>
  <th>team[T.Chelsea]</th>            <td>    0.0789</td> <td>    0.162</td> <td>    0.488</td> <td> 0.626</td> <td>   -0.238     0.396</td>
</tr>
<tr>
  <th>team[T.Crystal Palace]</th>     <td>   -0.3865</td> <td>    0.183</td> <td>   -2.107</td> <td> 0.035</td> <td>   -0.746    -0.027</td>
</tr>
<tr>
  <th>team[T.Everton]</th>            <td>   -0.2008</td> <td>    0.173</td> <td>   -1.161</td> <td> 0.246</td> <td>   -0.540     0.138</td>
</tr>
<tr>
  <th>team[T.Hull]</th>               <td>   -0.7006</td> <td>    0.204</td> <td>   -3.441</td> <td> 0.001</td> <td>   -1.100    -0.302</td>
</tr>
<tr>
  <th>team[T.Leicester]</th>          <td>   -0.4204</td> <td>    0.187</td> <td>   -2.249</td> <td> 0.025</td> <td>   -0.787    -0.054</td>
</tr>
<tr>
  <th>team[T.Liverpool]</th>          <td>    0.0162</td> <td>    0.164</td> <td>    0.099</td> <td> 0.921</td> <td>   -0.306     0.338</td>
</tr>
<tr>
  <th>team[T.Man City]</th>           <td>    0.0117</td> <td>    0.164</td> <td>    0.072</td> <td> 0.943</td> <td>   -0.310     0.334</td>
</tr>
<tr>
  <th>team[T.Man United]</th>         <td>   -0.3572</td> <td>    0.181</td> <td>   -1.971</td> <td> 0.049</td> <td>   -0.713    -0.002</td>
</tr>
<tr>
  <th>team[T.Middlesbrough]</th>      <td>   -1.0087</td> <td>    0.225</td> <td>   -4.481</td> <td> 0.000</td> <td>   -1.450    -0.568</td>
</tr>
<tr>
  <th>team[T.Southampton]</th>        <td>   -0.5804</td> <td>    0.195</td> <td>   -2.976</td> <td> 0.003</td> <td>   -0.963    -0.198</td>
</tr>
<tr>
  <th>team[T.Stoke]</th>              <td>   -0.6082</td> <td>    0.197</td> <td>   -3.094</td> <td> 0.002</td> <td>   -0.994    -0.223</td>
</tr>
<tr>
  <th>team[T.Sunderland]</th>         <td>   -0.9619</td> <td>    0.222</td> <td>   -4.329</td> <td> 0.000</td> <td>   -1.397    -0.526</td>
</tr>
<tr>
  <th>team[T.Swansea]</th>            <td>   -0.5136</td> <td>    0.192</td> <td>   -2.673</td> <td> 0.008</td> <td>   -0.890    -0.137</td>
</tr>
<tr>
  <th>team[T.Tottenham]</th>          <td>    0.0532</td> <td>    0.162</td> <td>    0.328</td> <td> 0.743</td> <td>   -0.265     0.371</td>
</tr>
<tr>
  <th>team[T.Watford]</th>            <td>   -0.5969</td> <td>    0.197</td> <td>   -3.035</td> <td> 0.002</td> <td>   -0.982    -0.211</td>
</tr>
<tr>
  <th>team[T.West Brom]</th>          <td>   -0.5567</td> <td>    0.194</td> <td>   -2.876</td> <td> 0.004</td> <td>   -0.936    -0.177</td>
</tr>
<tr>
  <th>team[T.West Ham]</th>           <td>   -0.4802</td> <td>    0.189</td> <td>   -2.535</td> <td> 0.011</td> <td>   -0.851    -0.109</td>
</tr>
<tr>
  <th>opponent[T.Bournemouth]</th>    <td>    0.4109</td> <td>    0.196</td> <td>    2.092</td> <td> 0.036</td> <td>    0.026     0.796</td>
</tr>
<tr>
  <th>opponent[T.Burnley]</th>        <td>    0.1657</td> <td>    0.206</td> <td>    0.806</td> <td> 0.420</td> <td>   -0.237     0.569</td>
</tr>
<tr>
  <th>opponent[T.Chelsea]</th>        <td>   -0.3036</td> <td>    0.234</td> <td>   -1.298</td> <td> 0.194</td> <td>   -0.762     0.155</td>
</tr>
<tr>
  <th>opponent[T.Crystal Palace]</th> <td>    0.3287</td> <td>    0.200</td> <td>    1.647</td> <td> 0.100</td> <td>   -0.062     0.720</td>
</tr>
<tr>
  <th>opponent[T.Everton]</th>        <td>   -0.0442</td> <td>    0.218</td> <td>   -0.202</td> <td> 0.840</td> <td>   -0.472     0.384</td>
</tr>
<tr>
  <th>opponent[T.Hull]</th>           <td>    0.4979</td> <td>    0.193</td> <td>    2.585</td> <td> 0.010</td> <td>    0.120     0.875</td>
</tr>
<tr>
  <th>opponent[T.Leicester]</th>      <td>    0.3369</td> <td>    0.199</td> <td>    1.694</td> <td> 0.090</td> <td>   -0.053     0.727</td>
</tr>
<tr>
  <th>opponent[T.Liverpool]</th>      <td>   -0.0374</td> <td>    0.217</td> <td>   -0.172</td> <td> 0.863</td> <td>   -0.463     0.389</td>
</tr>
<tr>
  <th>opponent[T.Man City]</th>       <td>   -0.0993</td> <td>    0.222</td> <td>   -0.448</td> <td> 0.654</td> <td>   -0.534     0.335</td>
</tr>
<tr>
  <th>opponent[T.Man United]</th>     <td>   -0.4220</td> <td>    0.241</td> <td>   -1.754</td> <td> 0.079</td> <td>   -0.894     0.050</td>
</tr>
<tr>
  <th>opponent[T.Middlesbrough]</th>  <td>    0.1196</td> <td>    0.208</td> <td>    0.574</td> <td> 0.566</td> <td>   -0.289     0.528</td>
</tr>
<tr>
  <th>opponent[T.Southampton]</th>    <td>    0.0458</td> <td>    0.211</td> <td>    0.217</td> <td> 0.828</td> <td>   -0.369     0.460</td>
</tr>
<tr>
  <th>opponent[T.Stoke]</th>          <td>    0.2266</td> <td>    0.203</td> <td>    1.115</td> <td> 0.265</td> <td>   -0.172     0.625</td>
</tr>
<tr>
  <th>opponent[T.Sunderland]</th>     <td>    0.3707</td> <td>    0.198</td> <td>    1.876</td> <td> 0.061</td> <td>   -0.017     0.758</td>
</tr>
<tr>
  <th>opponent[T.Swansea]</th>        <td>    0.4336</td> <td>    0.195</td> <td>    2.227</td> <td> 0.026</td> <td>    0.052     0.815</td>
</tr>
<tr>
  <th>opponent[T.Tottenham]</th>      <td>   -0.5431</td> <td>    0.252</td> <td>   -2.156</td> <td> 0.031</td> <td>   -1.037    -0.049</td>
</tr>
<tr>
  <th>opponent[T.Watford]</th>        <td>    0.3533</td> <td>    0.198</td> <td>    1.782</td> <td> 0.075</td> <td>   -0.035     0.742</td>
</tr>
<tr>
  <th>opponent[T.West Brom]</th>      <td>    0.0970</td> <td>    0.209</td> <td>    0.463</td> <td> 0.643</td> <td>   -0.313     0.507</td>
</tr>
<tr>
  <th>opponent[T.West Ham]</th>       <td>    0.3485</td> <td>    0.198</td> <td>    1.758</td> <td> 0.079</td> <td>   -0.040     0.737</td>
</tr>
<tr>
  <th>home</th>                       <td>    0.2969</td> <td>    0.063</td> <td>    4.702</td> <td> 0.000</td> <td>    0.173     0.421</td>
</tr>
</tbody></table>

<p>If you’re curious about the <code>smf.glm(...)</code> part, you can find more information <a href="http://www.statsmodels.org/stable/examples/notebooks/generated/glm_formula.html">here</a> (edit: earlier versions of this post had erroneously employed a Generalised Estimating Equation (GEE)- <a href="https://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models">what’s the difference?</a>). I’m more interested in the values presented in the <code>coef</code> column in the model summary table, which are analogous to the slopes in linear regression. Similar to <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, we take the <a href="http://www.lisa.stat.vt.edu/sites/default/files/Poisson.and_.Logistic.Regression.pdf">exponent of the parameter values</a>. A positive value implies more goals (), while values closer to zero represent more neutral effects (). Towards the bottom of the table you might notice that <code>home</code> has a <code>coef</code> of 0.2969. This captures the fact that home teams generally score more goals than the away team (specifically, =1.35 times more likely). But not all teams are created equal. Chelsea has a <code>coef</code> of 0.0789, while the corresponding value for Sunderland is -0.9619 (sort of saying Chelsea (Sunderland) are better (much worse!) scorers than average). Finally, the <code>opponent*</code> values …</p></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/">https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/</a></em></p>]]>
            </description>
            <link>https://dashee87.github.io/football/python/predicting-football-results-with-statistical-modelling/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24838958</guid>
            <pubDate>Tue, 20 Oct 2020 16:02:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why build another website builder?]]>
            </title>
            <description>
<![CDATA[
Score 76 | Comments 85 (<a href="https://news.ycombinator.com/item?id=24837331">thread link</a>) | @apledger3
<br/>
October 20, 2020 | https://www.makeswift.com/blog/why-build-another-website-builder | <a href="https://web.archive.org/web/*/https://www.makeswift.com/blog/why-build-another-website-builder">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div width="[object Object]" data-slate-editor="true" data-key="8882" autocorrect="on" spellcheck="true" data-gramm="false"><p data-slate-object="block" data-key="8883"><span data-slate-object="text" data-key="8884"><span data-slate-leaf="true" data-offset-key="8884:0"><span><span data-slate-string="true">Name something expensive that's difficult to build and trashed after each use.</span></span></span></span></p><p data-slate-object="block" data-key="8887"><span data-slate-object="text" data-key="8888"><span data-slate-leaf="true" data-offset-key="8888:0"><span><span data-slate-string="true">Did you say rockets? No, SpaceX figured out how to reuse those in 2015. </span></span></span></span></p><p data-slate-object="block" data-key="8891"><span data-slate-object="text" data-key="8892"><span data-slate-leaf="true" data-offset-key="8892:0"><span><span data-slate-string="true">I'm talking about your company's website. Completely overhauling the website has become so routine that we've become numb to the pain of throwing it all away, over and over again.</span></span></span></span></p><p data-slate-object="block" data-key="8895"><span data-slate-object="text" data-key="8896"><span data-slate-leaf="true" data-offset-key="8896:0"><span><span data-slate-string="true">Let's explore why this happens.</span></span></span></span></p><p data-slate-object="block" data-key="8899"><span data-slate-object="text" data-key="8900"><span data-slate-leaf="true" data-offset-key="8900:0"><span><span data-slate-string="true">Imagine you're the cofounder of a new company. You need to set up a website, but your product partner is already behind on features so that leaves it solely up to you. What's your plan? Given the tight budget and schedule, the disciplined move would be to quickly use a template in an inexpensive website builder. You can graduate to something a little more custom once you have more resources and your brand, positioning, and voice figured out.</span></span></span></span></p><p data-slate-object="block" data-key="8903"><span data-slate-object="text" data-key="8904"><span data-slate-leaf="true" data-offset-key="8904:0"><span><span data-slate-string="true">Sounds like a good plan, but anyone who's gone down this path knows it's never as simple as it seems.</span></span></span></span></p><p data-slate-object="block" data-key="8907"><span data-slate-object="text" data-key="8908"><span data-slate-leaf="true" data-offset-key="8908:0"><span><span data-slate-string="true">For one, the template never lasts as long as you need it to. You realize the design isn't working, or your competitor just unexpectedly made a move. So you start trying to make changes. Swapping out the text and images comes easy, but as soon as you start adjusting the layout, frustration begins to set in. You've got a million other things to do and for some reason you can't get the page to look right on mobile. </span></span></span></span></p><p data-slate-object="block" data-key="8911"><span data-slate-object="text" data-key="8912"><span data-slate-leaf="true" data-offset-key="8912:0"><span><span data-slate-string="true">As it turns out, the same features and guard rails that made it easy to stand up the template have now become the reason it's hard to iterate. You're stuck, and whether or not you were ready, the time has come</span></span></span></span><span data-slate-object="text" data-key="8913"><span data-slate-leaf="true" data-offset-key="8913:0"><span><span data-slate-string="true"> to deal with the classic website building dilemma:</span></span></span></span></p><p data-slate-object="block" data-key="8916"><span data-slate-object="text" data-key="8917"><span data-slate-leaf="true" data-offset-key="8917:0"><span><span data-slate-string="true">Do you stay put and compromise on your vision, or invest in another solution that </span></span></span></span><span data-slate-object="text" data-key="8918"><span data-slate-leaf="true" data-offset-key="8918:0"><span><span data-slate-string="true">might</span></span></span></span><span data-slate-object="text" data-key="8919"><span data-slate-leaf="true" data-offset-key="8919:0"><span><span data-slate-string="true"> be better?</span></span></span></span></p><p data-slate-object="block" data-key="8922"><span data-slate-object="text" data-key="8923"><span data-slate-leaf="true" data-offset-key="8923:0"><span><span data-slate-string="true">After researching more advanced solutions for a few days, it starts to become apparent that you're out of your depth. There are so many different products, and every time you begin stepping away from the template you're consistently met with a giant learning curve. Frustration sets in again so you decide to call that technical friend. You're in luck! She's got her favorite tool and she's available to help.</span></span></span></span></p><p data-slate-object="block" data-key="8926"><span data-slate-object="text" data-key="8927"><span data-slate-leaf="true" data-offset-key="8927:0"><span><span data-slate-string="true">Unfortunately, all you're about to do is move your bottleneck. You may think once things are set up you can say your goodbyes, but the reality is you're never free. As your team and ideas grow you'll need more and more help, and she'll be the only one who can make the big changes. Your projects will start moving slower and slower. But on the bright side, at least you'll be able to get it done... eventually.</span></span></span></span></p><p data-slate-object="block" data-key="8930"><span data-slate-object="text" data-key="8931"><span data-slate-leaf="true" data-offset-key="8931:0"><span><span data-slate-string="true">But what happens if she leaves? Nobody likes stepping into someone else's mess, so it ends up being cheaper to just rebuild using whatever tool the new technical expert is comfortable with. </span></span></span></span><span data-slate-object="text" data-key="8932"><span data-slate-leaf="true" data-offset-key="8932:0"><span><span data-slate-string="true">Is it any mystery, then, why we're stuck in this constant build, trash, move, build cycle?</span></span></span></span><span data-slate-object="text" data-key="8933"><span data-slate-leaf="true" data-offset-key="8933:0"><span><span data-slate-string="true"> </span></span></span></span><span data-slate-object="text" data-key="8934"><span data-slate-leaf="true" data-offset-key="8934:0"><span><span data-slate-string="true">The solutions have changed, but the underlying trade-off hasn't. </span></span></span></span><span data-slate-object="text" data-key="8935"><span data-slate-leaf="true" data-offset-key="8935:0"><span><span data-slate-string="true">All options are either too basic and restrictive, or too technical and complex.</span></span></span></span></p><p data-slate-object="block" data-key="8938"><span data-slate-object="text" data-key="8939"><span data-slate-leaf="true" data-offset-key="8939:0"><span><span data-slate-string="true">Founders building out early marketing for their startups are not the only ones who experience this. At some point, all marketers feel this pain.</span></span></span></span></p><p data-slate-object="block" data-key="8942"><span data-slate-object="text" data-key="8943"><span data-slate-leaf="true" data-offset-key="8943:0"><span><span data-slate-string="true">The problem is so widespread that it's given birth to a whole category of products called landing page builders.</span></span></span></span></p><p data-slate-object="block" data-key="8946"><span data-slate-object="text" data-key="8947"><span data-slate-leaf="true" data-offset-key="8947:0"><span><span data-slate-string="true">Marketers are so fed up with being sidelined that they're willing to duct tape their site with </span></span></span></span><span data-slate-object="text" data-key="8948"><span data-slate-leaf="true" data-offset-key="8948:0"><span><span data-slate-string="true">mostly</span></span></span></span><span data-slate-object="text" data-key="8949"><span data-slate-leaf="true" data-offset-key="8949:0"><span><span data-slate-string="true"> on-brand pages, just to have some operational independence when it comes to web content. Sure, landing page builders can have extra features bolted on like A/B testing and analytics, but they aren't the real reason marketers are out shopping for a solution. Marketers are looking for a place where they can get creative and not break anything.</span></span></span></span></p><p data-slate-object="block" data-key="8952"><span data-slate-object="text" data-key="8953"><span data-slate-leaf="true" data-offset-key="8953:0"><span><span data-slate-string="true">We know this because we built and eventually sunset a landing page builder, Landing Lion. Many of our customers wished they could build full websites in it, and some actually did. Despite its many shortcomings around bulk management, a surprising number of customers were willing to put in extra manual work to build and maintain full websites. The reason? Our user experience was actually designed for them—intelligent and tech-savvy generalists who wanted to break free from the template without having to go get a degree. They could finally build exactly what they envisioned and they could do it by themselves, quickly.</span></span></span></span></p><p data-slate-object="block" data-key="8956"><span data-slate-object="text" data-key="8957"><span data-slate-leaf="true" data-offset-key="8957:0"><span><span data-slate-string="true">The real problem was that for the rest of our customers, their websites were locked down. They'd been delicately constructed by the real owners, the technical experts. But who is ultimately responsible for the </span></span></span></span><span data-slate-object="text" data-key="8958"><span data-slate-leaf="true" data-offset-key="8958:0"><span><span data-slate-string="true">entire</span></span></span></span><span data-slate-object="text" data-key="8959"><span data-slate-leaf="true" data-offset-key="8959:0"><span><span data-slate-string="true"> brand experience, website included? The marketers.</span></span></span></span></p><p data-slate-object="block" data-key="8962"><span data-slate-object="text" data-key="8963"><span data-slate-leaf="true" data-offset-key="8963:0"><span><span data-slate-string="true">That's why we're building Makeswift.</span></span></span></span></p></div></div><div><div width="[object Object]" data-slate-editor="true" data-key="8965" autocorrect="on" spellcheck="true" data-gramm="false"><p data-slate-object="block" data-key="8966"><span data-slate-object="text" data-key="8967"><span data-slate-leaf="true" data-offset-key="8967:0"><span><span data-slate-string="true">Marketers need a website builder designed just for them. Our mission is to tackle the issues holding back marketing teams from building, shipping, and iterating on </span></span></span></span><span data-slate-object="text" data-key="8968"><span data-slate-leaf="true" data-offset-key="8968:0"><span><span data-slate-string="true">all</span></span></span></span><span data-slate-object="text" data-key="8969"><span data-slate-leaf="true" data-offset-key="8969:0"><span><span data-slate-string="true"> web content, on their own schedule.</span></span></span></span></p><p data-slate-object="block" data-key="8972"><span data-slate-object="text" data-key="8973"><span data-slate-leaf="true" data-offset-key="8973:0"><span><span data-slate-string="true">To do this, we need to fix the user experience. More specifically, we need to move away from the split "template &amp; content" user experience found in nearly all advanced website solutions. This approach exposes two distinct experiences to the end user: One for a technical specialist to build a template, and another basic experience, usually a form, to plug in content. This creates a problem where the people responsible for the content can't change the template—sound familiar?</span></span></span></span></p><p data-slate-object="block" data-key="8976"><span data-slate-object="text" data-key="8977"><span data-slate-leaf="true" data-offset-key="8977:0"><span><span data-slate-string="true">To move as fast as possible, the people in charge of </span></span></span></span><span data-slate-object="text" data-key="8978"><span data-slate-leaf="true" data-offset-key="8978:0"><span><span data-slate-string="true">what</span></span></span></span><span data-slate-object="text" data-key="8979"><span data-slate-leaf="true" data-offset-key="8979:0"><span><span data-slate-string="true"> to build need to also control </span></span></span></span><span data-slate-object="text" data-key="8980"><span data-slate-leaf="true" data-offset-key="8980:0"><span><span data-slate-string="true">how</span></span></span></span><span data-slate-object="text" data-key="8981"><span data-slate-leaf="true" data-offset-key="8981:0"><span><span data-slate-string="true"> it's built.</span></span></span></span></p><p data-slate-object="block" data-key="8984"><span data-slate-object="text" data-key="8985"><span data-slate-leaf="true" data-offset-key="8985:0"><span><span data-slate-string="true">That's why we're focused on designing a single, elegant user experience that can be easily taught to an entire team of tech-savvy generalists. The challenge is to provide enough power for advanced use cases without making the product difficult to </span></span></span></span><span data-slate-object="text" data-key="8986"><span data-slate-leaf="true" data-offset-key="8986:0"><span><span data-slate-string="true">learn</span></span></span></span><span data-slate-object="text" data-key="8987"><span data-slate-leaf="true" data-offset-key="8987:0"><span><span data-slate-string="true">. Learning to build completely custom websites should be no more complicated than learning to design a slide show presentation.</span></span></span></span></p><p data-slate-object="block" data-key="8990"><span data-slate-object="text" data-key="8991"><span data-slate-leaf="true" data-offset-key="8991:0"><span><span data-slate-string="true">So why build another website builder? With what feels like a new website builder popping up everyday, it's apparent that we're all still searching for something better. If you're interested in shaping the way we build, ship, and iterate on web content, please sign up for our early access program.</span></span></span></span></p></div></div></div>]]>
            </description>
            <link>https://www.makeswift.com/blog/why-build-another-website-builder</link>
            <guid isPermaLink="false">hacker-news-small-sites-24837331</guid>
            <pubDate>Tue, 20 Oct 2020 13:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Differential Dataflow]]>
            </title>
            <description>
<![CDATA[
Score 112 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24837031">thread link</a>) | @timhigins
<br/>
October 20, 2020 | https://timelydataflow.github.io/differential-dataflow/introduction.html | <a href="https://web.archive.org/web/*/https://timelydataflow.github.io/differential-dataflow/introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                
                

                
                
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        
<p>In this book we will work through the motivation and technical details behind <a href="https://github.com/frankmcsherry/differential-dataflow">differential dataflow</a>, a computational framework build on top of <a href="https://github.com/frankmcsherry/timely-dataflow">timely dataflow</a> intended for efficiently performing computations on large amounts of data and <em>maintaining</em> the computations as the data change.</p>
<p>Differential dataflow programs look like many standard "big data" computations, borrowing idioms from frameworks like MapReduce and SQL. However, once you write and run your program, you can <em>change</em> the data inputs to the computation, and differential dataflow will promptly show you the corresponding changes in its output. Promptly meaning in as little as milliseconds.</p>
<p>This relatively simple set-up, write programs and then change inputs, leads to a surprising breadth of exciting and new classes of scalable computation. We will explore it in this document!</p>
<hr>
<p>Differential dataflow arose from <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/naiad_sosp2013.pdf">work at Microsoft Research</a>, where we aimed to build a high-level framework that could both compute and incrementally maintain non-trivial algorithms.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        
                            <a rel="next" href="https://timelydataflow.github.io/differential-dataflow/chapter_0/chapter_0.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>
                        

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                

                
                    <a rel="next" href="https://timelydataflow.github.io/differential-dataflow/chapter_0/chapter_0.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
                
            </nav>

        </div>

        

        

        

        
        
        

        

        
        
        
        
        

        
        
        

        <!-- Custom JS scripts -->
        

        

    

</div>]]>
            </description>
            <link>https://timelydataflow.github.io/differential-dataflow/introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24837031</guid>
            <pubDate>Tue, 20 Oct 2020 13:31:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The surprising impact of medium-size texts on PostgreSQL performance]]>
            </title>
            <description>
<![CDATA[
Score 171 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24836979">thread link</a>) | @haki
<br/>
October 20, 2020 | https://hakibenita.com/sql-medium-text-performance | <a href="https://web.archive.org/web/*/https://hakibenita.com/sql-medium-text-performance">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article data-progress-indicator="">
        <hr>
<p>Any database schema is likely to have plenty of text fields. In this article, I divide text fields into three categories:</p>
<ol>
<li>
<p><strong>Small texts</strong>: names, slugs, usernames, emails, etc. These are text fields that usually have some low size limit, maybe even using <code>varchar(n)</code> and not <code>text</code>.</p>
</li>
<li>
<p><strong>Large texts</strong>: blog post content, articles, HTML content etc. These are large pieces of free, unrestricted text that is stored in the database.</p>
</li>
<li>
<p><strong>Medium texts</strong>: descriptions, comments, product reviews, stack traces etc. These are any text field that is between the small and the large. These type of texts would normally be unrestricted, but naturally smaller than the large texts.</p>
</li>
</ol>
<p><strong>In this article I demonstrate the surprising impact of medium-size texts on query performance in PostgreSQL.</strong></p>
<figure><img alt="Sliced bread... it gets better<br><small>Photo by <a href=&quot;https://unsplash.com/photos/WHJTaLqonkU&quot;>Louise Lyshøj</a></small>" src="https://hakibenita.com/images/00-sql-medium-text-performance.jpg"><figcaption>Sliced bread... it gets better<br><small>Photo by <a href="https://unsplash.com/photos/WHJTaLqonkU">Louise Lyshøj</a></small></figcaption>
</figure>
<details open="">
    <summary>Table of Contents</summary>

</details>
<hr>
<h2 id="understanding-toast"><a href="#understanding-toast">Understanding TOAST</a></h2>
<p>When talking about large chunks of text, or any other field that may contain large amounts of data, we first need to understand how the database handles the data. Intuitively, you might think that the database is storing large pieces of data inline like it does smaller pieces of data, but in fact, <a href="https://www.postgresql.org/docs/current/storage-toast.html" rel="noopener">it does not</a>:</p>
<blockquote>
<p>PostgreSQL uses a fixed page size (commonly 8 kB), and does not allow tuples to span multiple pages. Therefore, it is not possible to store very large field values directly.</p>
</blockquote>
<p>As the documentation explains, PostgreSQL can't store rows (tuples) in multiple pages. So how does the database store large chunks of data?</p>
<blockquote>
<p>[...] large field values are compressed and/or broken up into multiple physical rows. [...] The technique is affectionately known as TOAST (or “the best thing since sliced bread”).</p>
</blockquote>
<p>OK, so how is this TOAST working exactly?</p>
<blockquote>
<p>If any of the columns of a table are TOAST-able, the table will have an associated TOAST table</p>
</blockquote>
<p>So TOAST is a separate table associated with our table. It is used to store large pieces of data of TOAST-able columns (the <code>text</code> datatype for example, is TOAST-able).</p>
<p>What constitutes a large value?</p>
<blockquote>
<p>The TOAST management code is triggered only when a row value to be stored in a table is wider than TOAST_TUPLE_THRESHOLD bytes (normally 2 kB). The TOAST code will compress and/or move field values out-of-line until the row value is shorter than TOAST_TUPLE_TARGET bytes (also normally 2 kB, adjustable) or no more gains can be had</p>
</blockquote>
<p>PostgreSQL will try to compress a the large values in the row, and if the row can't fit within the limit, the values will be stored out-of-line in the TOAST table.</p>
<h3 id="finding-the-toast"><a href="#finding-the-toast">Finding the TOAST</a></h3>
<p>Now that we have <em>some</em> understanding of what TOAST is, let's see it in action. First, create a table with a text field:</p>
<div><pre><span></span><span>db=#</span> <span>CREATE</span> <span>TABLE</span> <span>toast_test</span> <span>(</span><span>id</span> <span>SERIAL</span><span>,</span> <span>value</span> <span>TEXT</span><span>);</span>
<span>CREATE TABLE</span>
</pre></div>


<p>The table contains an id column, and a value field of type <code>TEXT</code>. Notice that we did not change any of the default storage parameters.</p>
<p>The text field we added supports TOAST, or is TOAST-able, so PostgreSQL should create a TOAST table. Let's try to locate the TOAST table associated with the table <code>toast_test</code> in <a href="https://www.postgresql.org/docs/current/catalog-pg-class.html" rel="noopener"><code>pg_class</code></a>:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>relname</span><span>,</span> <span>reltoastrelid</span> <span>FROM</span> <span>pg_class</span> <span>WHERE</span> <span>relname</span> <span>=</span> <span>'toast_test'</span><span>;</span>
<span>  relname   │ reltoastrelid</span>
<span>────────────┼───────────────</span>
<span> toast_test │        340488</span>

<span>db=#</span> <span>SELECT</span> <span>relname</span> <span>FROM</span> <span>pg_class</span> <span>WHERE</span> <span>oid</span> <span>=</span> <span>340488</span><span>;</span>
<span>     relname</span>
<span>─────────────────</span>
<span> pg_toast_340484</span>
</pre></div>


<p>As promised, PostgreSQL created a TOAST table called <code>pg_toast_340484</code>.</p>
<h3 id="toast-in-action"><a href="#toast-in-action">TOAST in Action</a></h3>
<p>Let's see what the TOAST table looks like:</p>
<div><pre><span></span><span>db=#</span> <span>\d</span> <span>pg_toast.pg_toast_340484</span>
<span>TOAST table "pg_toast.pg_toast_340484"</span>
<span>   Column   │  Type</span>
<span>────────────┼─────────</span>
<span> chunk_id   │ oid</span>
<span> chunk_seq  │ integer</span>
<span> chunk_data │ bytea</span>
</pre></div>


<p>The TOAST table contains three columns:</p>
<ul>
<li><code>chunk_id</code>: A reference to a toasted value.</li>
<li><code>chunk_seq</code>: A sequence within the chunk.</li>
<li><code>chunk_data</code>: The actual chunk data.</li>
</ul>
<p>Similar to "regular" tables, the TOAST table also has the same restrictions on inline values. To overcome this restriction, large values are split into chunks that can fit within the limit.</p>
<p>At this point the table is empty:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span><span>;</span>
<span> chunk_id │ chunk_seq │ chunk_data</span>
<span>──────────┼───────────┼────────────</span>
<span>(0 rows)</span>
</pre></div>


<p>This makes sense because we did not insert any data yet. So next, insert a small value into the table:</p>
<div><pre><span></span><span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>'small value'</span><span>);</span>
<span>INSERT 0 1</span>

<span>db=#</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span><span>;</span>
<span> chunk_id │ chunk_seq │ chunk_data</span>
<span>──────────┼───────────┼────────────</span>
<span>(0 rows)</span>
</pre></div>


<p>After inserting the small value into the table, the TOAST table remained empty. This means the small value was small enough to be stored inline, and there was no need to move it out-of-line to the TOAST table.</p>
<figure>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 176.3 111" height="10em"><path d="M9 47c62-3 116 2 153-2M13 47c49 0 93 0 150 2m-3-4c3 17 2 22 4 53m-2-52c-3 14-2 31 0 56m2-4c-37 1-78 7-150 5m149-4H13m0 4c-3-15 1-28-4-56m3 55c2-11-1-25 1-54" stroke="currentColor" fill="none"></path><text y="15" font-size="16" fill="currentColor" transform="translate(33 60)">1</text><text y="15" font-size="16" fill="currentColor" transform="translate(61 61)">"small value"</text><path d="M10 12l155-1v37L12 46" stroke-width="0" fill="#f2f2f2"></path><path d="M10 9c43-1 84 0 156 2M11 11c36 1 75 0 155-1m1-2l-2 38m1-36v39m1 0c-39 2-78 1-159 0m158-2c-39 2-77 2-155 0m-3 1c0-8 3-20 3-36m-1 37V10" stroke="currentColor" fill="none"></path><path d="M52 16l4 81m0-84c-1 19-5 36-3 88" stroke="currentColor" fill="none"></path><text y="15" font-size="16" transform="translate(23 18)">id</text><g><text x="20" y="15" font-size="16" text-anchor="middle" transform="translate(68 19)">value</text></g></svg>
<figcaption>Small text stored inline</figcaption>
</figure>

<p>Let's insert a large value and see what happens:</p>
<div><pre><span></span><span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>'n0cfPGZOCwzbHSMRaX8 ... WVIlRkylYishNyXf'</span><span>);</span>
<span>INSERT 0 1</span>
</pre></div>


<p>I shortened the value for brevity, but that's a random string with 4096 characters. Let's see what the TOAST table stores now:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>*</span> <span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span><span>;</span>
<span> chunk_id │ chunk_seq │ chunk_data</span>
<span>──────────┼───────────┼──────────────────────</span>
<span>   995899 │         0 │ \x30636650475a4f43...</span>
<span>   995899 │         1 │ \x50714c3756303567...</span>
<span>   995899 │         2 │ \x6c78426358574534...</span>
<span>(3 rows)</span>
</pre></div>


<p>The large value is stored out-of-line in the TOAST table. Because the value was too large to fit inline in a single row, PostgreSQL split it into three chunks. The <code>\x3063...</code> notation is how psql displays binary data.</p>
<figure>
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 435.8 254" height="20em"><path d="M9 43c58 5 116 4 154 6M14 48c34-1 70 1 148-1m2 0c-5 34-4 68-3 92m0-94c1 34 4 69 2 96m-4 0c-27-2-54-2-143-5m145 3c-47 2-96 2-147 1m-2 3c5-29 2-60 0-94m2 89c1-36-2-72-3-90" stroke="currentColor" fill="none"></path><path d="M10 95c45-2 79 3 150 0M12 96c53 0 106-3 147-3" stroke="currentColor" fill="none"></path><text y="15" font-size="16" fill="currentColor" transform="translate(33 60)">1</text><text y="15" font-size="16" fill="currentColor" transform="translate(61 61)">"small value"</text><text y="15" font-size="16" fill="currentColor" transform="translate(34 106)">2</text><path d="M108 109l4 3 3 5-2 4-5 4h-4l-4-3v-7c0-2 1-2 3-3l6-3 1 1m-4-2l3 3 3 5 1 4c0 2-1 2-2 3l-6 3-4-5c-1-2-3-4-2-5l2-4 4-4 1 3" stroke-width="0" fill="#f41d92"></path><path d="M105 110h5l2 4 2 5c0 2-3 3-4 4l-4 2c-2 0-2-1-3-2l-4-6 2-4 4-5 1 2m-1 0l6 1 3 3-1 4-1 4-6 3-3-1-3-5v-7l6-2-2-1M115 115c30 2 64 1 141 0m-141 1c34-1 70-3 142-1" stroke="currentColor" fill="none"></path><path d="M229 125c5-1 13-4 26-11m-26 11l27-10" stroke="currentColor" fill="none"></path><path d="M229 104c5 4 13 6 26 10m-26-9c6 3 14 4 27 10" stroke="currentColor" fill="none"></path><path d="M275 104h152v137l-151 3" stroke-width="0" fill="#f41d92"></path><path d="M276 99c36 2 66 5 146 7m-148-3c47-2 87-3 150 0m2 4c-5 40-4 85-2 135m2-139c-2 41-1 78 0 140m1-3c-53 5-105-2-149 0m146 4c-45 1-90 0-146-2m1-1c-4-56-3-110 0-135m-5 135c5-48 3-100 2-138" stroke="currentColor" fill="none"></path><path d="M314 107c1 30 5 66 8 136m-3-136v137" stroke="currentColor" fill="none"></path><path d="M279 153c36 6 77 8 145 2m-145 0c50-2 102-2 146-2M271 194c46 0 91 2 151 5m-148-2c29-3 59-1 146-1" stroke="currentColor" fill="none"></path><text y="15" font-size="16" fill="currentColor" transform="translate(294 165)">2</text><text y="15" font-size="16" fill="currentColor" transform="translate(295 119)">1</text><text y="15" font-size="16" fill="currentColor" transform="translate(292 207)">3</text><text y="15" font-size="16" fill="currentColor" transform="translate(329 118)">\x.....</text><text y="15" font-size="16" fill="currentColor" transform="translate(331 164)">\x.....</text><text y="15" font-size="16" fill="currentColor" transform="translate(332 206)">\x.....</text><g><path d="M12 9l153 1-1 36-153 1" stroke-width="0" fill="#f2f2f2"></path><path d="M11 12c50 0 100-4 157-2M9 11c48-3 97-3 158-1m1 0c-3 13 0 29-2 39m0-39v39m-1-1c-41-2-80 1-154-2m156 2c-62 2-122 2-158 0m2 1c1-12-2-25-3-40m1 39c1-12 2-23 1-38" stroke="currentColor" fill="none"></path></g><g><path d="M55 11c-1 39 2 67 0 130M54 16c4 36 3 74 1 120" stroke="currentColor" fill="none"></path></g><g><text y="15" font-size="16" transform="translate(23 18)">id</text></g><g><text x="20" y="15" font-size="16" text-anchor="middle" transform="translate(68 19)">value</text></g></svg>
<figcaption>Large text stored out-of-line, in the associated TOAST table</figcaption>
</figure>

<p>Finally, execute the following query to summarize the data in the TOAST table:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>
<span> chunk_id │ chunks │ pg_size_pretty</span>
<span>──────────┼────────┼────────────────</span>
<span>   995899 │      3 │ 4096 bytes</span>
<span>(1 row)</span>
</pre></div>


<p>As we've already seen, the text is stored in three chunks.</p>
<div>
<p>size of database objects</p>
<p>There are several ways to get the <a href="https://www.postgresql.org/docs/current/functions-admin.html#FUNCTIONS-ADMIN-DBSIZE" rel="noopener">size of database objects in PostgreSQL</a>:</p>
<ul>
<li><code>pg_table_size</code>: Get the size of the table including TOAST, but excluding indexes</li>
<li><code>pg_relation_size</code>: Get the size of just the table</li>
<li><code>pg_total_relation_size</code>: Get the size of the table, including indexes and TOAST</li>
</ul>
<p>Another useful function is <code>pg_size_pretty</code>: used to display sizes in a friendly format.</p>
</div>
<h3 id="toast-compression"><a href="#toast-compression">TOAST Compression</a></h3>
<p>So far I refrained from categorizing texts by their size. The reason for that is that the size of the text itself does not matter, what matters is its size after compression.</p>
<p>To create long strings for testing, we'll implement a function to generate random strings at a given length:</p>
<div><pre><span></span><span>CREATE</span> <span>OR</span> <span>REPLACE</span> <span>FUNCTION</span> <span>generate_random_string</span><span>(</span>
  <span>length</span> <span>INTEGER</span><span>,</span>
  <span>characters</span> <span>TEXT</span> <span>default</span> <span>'0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'</span>
<span>)</span> <span>RETURNS</span> <span>TEXT</span> <span>AS</span>
<span>$$</span>
<span>DECLARE</span>
  <span>result</span> <span>TEXT</span> <span>:=</span> <span>''</span><span>;</span>
<span>BEGIN</span>
  <span>IF</span> <span>length</span> <span>&lt;</span> <span>1</span> <span>then</span>
      <span>RAISE</span> <span>EXCEPTION</span> <span>'Invalid length'</span><span>;</span>
  <span>END</span> <span>IF</span><span>;</span>
  <span>FOR</span> <span>__</span> <span>IN</span> <span>1..</span><span>length</span> <span>LOOP</span>
    <span>result</span> <span>:=</span> <span>result</span> <span>||</span> <span>substr</span><span>(</span><span>characters</span><span>,</span> <span>floor</span><span>(</span><span>random</span><span>()</span> <span>*</span> <span>length</span><span>(</span><span>characters</span><span>))</span><span>::</span><span>int</span> <span>+</span> <span>1</span><span>,</span> <span>1</span><span>);</span>
  <span>end</span> <span>loop</span><span>;</span>
  <span>RETURN</span> <span>result</span><span>;</span>
<span>END</span><span>;</span>
<span>$$</span> <span>LANGUAGE</span> <span>plpgsql</span><span>;</span>
</pre></div>


<p>Generate a string made out of 10 random characters:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>generate_random_string</span><span>(</span><span>10</span><span>);</span>
<span> generate_random_string</span>
<span>────────────────────────</span>
<span> o0QsrMYRvp</span>
</pre></div>


<p>We can also provide a set of characters to generate the random string from. For example, generate a string made of 10 random digits:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>generate_random_string</span><span>(</span><span>10</span><span>,</span> <span>'1234567890'</span><span>);</span>
<span> generate_random_string</span>
<span>────────────────────────</span>
<span> 4519991669</span>
</pre></div>


<p>PostgreSQL TOAST uses the <a href="https://doxygen.postgresql.org/pg__lzcompress_8c_source.html" rel="noopener">LZ family of compression</a> techniques. Compression algorithms usually work by identifying and eliminating repetition in the value. A long string containing fewer characters should compress very well compared to a string made of many different characters when encoded into bytes.</p>
<p>To illustrate how TOAST uses compression, we'll clean out the <code>toast_test</code> table, and insert a random string made of many possible characters:</p>
<div><pre><span></span><span>db=#</span> <span>TRUNCATE</span> <span>toast_test</span><span>;</span>
<span>TRUNCATE TABLE</span>

<span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>generate_random_string</span><span>(</span><span>1024</span> <span>*</span> <span>10</span><span>));</span>
<span>INSERT 0 1</span>
</pre></div>


<p>We inserted a 10kb value made of random characters. Let's check the TOAST table:</p>
<div><pre><span></span><span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>

<span> chunk_id │ chunks │ pg_size_pretty</span>
<span>──────────┼────────┼────────────────</span>
<span>  1495960 │      6 │ 10 kB</span>
</pre></div>


<p>The value is stored out-of-line in the TOAST table, and we can see it is not compressed.</p>
<p>Next, insert a value with a similar length, but made out of fewer possible characters:</p>
<div><pre><span></span><span>db=#</span> <span>INSERT</span> <span>INTO</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>VALUES</span> <span>(</span><span>generate_random_string</span><span>(</span><span>1024</span> <span>*</span> <span>10</span><span>,</span> <span>'123'</span><span>));</span>
<span>INSERT 0 1</span>

<span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>

<span> chunk_id │ chunks │ pg_size_pretty</span>
<span>──────────┼────────┼────────────────</span>
<span>  1495960 │      6 │ 10 kB</span>
<span>  1495961 │      2 │ 3067 bytes</span>
</pre></div>


<p>We inserted a 10K value, but this time it only contained 3 possible digits: <code>1</code>, <code>2</code> and <code>3</code>. This text is more likely to contain repeating binary patterns, and should compress better than the previous value. Looking at the TOAST, we can see PostgreSQL compressed the value to ~3kB, which is a third of the size of the uncompressed value. Not a bad compression rate!</p>
<p>Finally, insert a 10K long string made of a single digit:</p>
<div><pre><span></span><span>db=#</span> <span>insert</span> <span>into</span> <span>toast_test</span> <span>(</span><span>value</span><span>)</span> <span>values</span> <span>(</span><span>generate_random_string</span><span>(</span><span>1024</span> <span>*</span> <span>10</span><span>,</span> <span>'0'</span><span>));</span>
<span>INSERT 0 1</span>

<span>db=#</span> <span>SELECT</span> <span>chunk_id</span><span>,</span> <span>COUNT</span><span>(</span><span>*</span><span>)</span> <span>as</span> <span>chunks</span><span>,</span> <span>pg_size_pretty</span><span>(</span><span>sum</span><span>(</span><span>octet_length</span><span>(</span><span>chunk_data</span><span>)</span><span>::</span><span>bigint</span><span>))</span>
<span>FROM</span> <span>pg_toast</span><span>.</span><span>pg_toast_340484</span> <span>GROUP</span> <span>BY</span> <span>1</span> <span>ORDER</span> <span>BY</span> <span>1</span><span>;</span>

<span> chunk_id │ chunks │ pg_size_pretty</span>
<span>──────────┼────────┼────────────────</span>
<span>  1495960 │      6 │ 10 kB</span>
<span>  1495961 │      2 │ 3067 bytes</span>
</pre></div>


<p>The string was compressed so well, that the database was able to store it in-line.</p>
<h3 id="configuring-toast"><a href="#configuring-toast">Configuring TOAST</a></h3>
<p>If you are interested in configuring TOAST for a table you can do that by setting storage parameters at <code>CREATE TABLE</code> or <code>ALTER …</code></p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://hakibenita.com/sql-medium-text-performance">https://hakibenita.com/sql-medium-text-performance</a></em></p>]]>
            </description>
            <link>https://hakibenita.com/sql-medium-text-performance</link>
            <guid isPermaLink="false">hacker-news-small-sites-24836979</guid>
            <pubDate>Tue, 20 Oct 2020 13:25:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raymarching with Fennel and LÖVE]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24835766">thread link</a>) | @forgotpwd16
<br/>
October 20, 2020 | https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/ | <a href="https://web.archive.org/web/*/https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>Previously I’ve decided to implement a rather basic <a href="https://andreyorst.gitlab.io/posts/2020-06-04-simple-ray-casting-with-clojurescript/">raycasting engine in ClojureScript</a>.
It was a lot of fun, an interesting experience, and ClojureScript was awesome.
I’ve implemented small <a href="https://andreyorst.gitlab.io/posts/2020-06-04-simple-ray-casting-with-clojurescript/#labyrinth-game">labyrinth game</a>, and thought about adding more features to the engine, such as camera shake, and wall height change.
But when I’ve started working on these, I quickly understood, that I’d like to move on to something more interesting, like real 3D rendering engine, that also uses rays.</p>
<p>Obviously, my first though was about writing a ray-tracer<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
This technique is wide known, and gained a lot of traction recently.
With native hardware support for ray tracing, a lot of games are using it, and there are a lot of tutorials teaching how to implement one<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.
In short, we cast a bunch of rays in 3D space, and calculate their trajectories, looking for what ray will hit and bounce off.
Different materials have different bounce properties, and by tracing rays from camera to the source of light, we can imitate illumination.
There are also a lot of different approaches how to calculate bouncing, e.g. for global illumination, and ambient light, but I’ve felt that it is a rather complicated task, for a weekend post.
And unlike raycasting, most ray-tracers require polygonal information in order to work, where raycasting only need to know wall start and end points.</p>
<p>I’ve wanted a similar approach for 3D rendering, where we specify an object in terms of it’s mathematical representation.
Like for sphere, we’ll just specify coordinate of a center, and a radius, and our rays will find intersection points with it, providing us a sufficient data to draw this sphere on screen.
And recently, I’ve read about a similar technique, that uses rays for drawing on screen, but instead of casting infinite rays as in raycasting, it marches a ray in terms of steps.
And it also uses a special trick, to make this process very optimized, therefore we can use it for rendering real 3D objects.</p>
<p>I’ve decided to structure this post similarly to the one about raycasting, so this will be another long-read, often more about Fennel rather than raymarching, but at the end I promise that we’ll get something that looks like this:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/end-result.png"> 
</figure>

<p>So, just as in raycasting, first we need to do is to understand how raymarching engine works <em>on paper</em>.</p>
<h2 id="raymarching-basics">Raymarching basics</h2>
<p>Raymarching can be illustrated similarly to raycaster, except it requires more steps until we could render our image.
First, we need a camera, and an object to look at:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle.svg"> 
</figure>

<p>Our first step would to cast a ray, however, unlike with raycasting, we’ll cast a portion of a ray:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-short-ray.svg"> 
</figure>

<p>We then check, if the ray intersects with the sphere.
It’s not, so we do one more step:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-two-steps.svg"> 
</figure>

<p>It’s not intersecting yet, so we repeat again:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-ray-overshoot.svg"> 
</figure>

<p>Oops, ray overshoot, and is now inside the sphere.
This is not really good option for us, as we want for our rays to end directly at the object’s surface, without calculating intersection point with the object itself.
We can fix this by casting shorter ray:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/camera-and-circle-small-rays.svg"> 
</figure>

<p>However, this is very inefficient!
And besides, if we’ll change the angle a bit or move the camera, we will overshoot again.
Which means that we’ll either have incorrect result, or require a very small step size, which will blow up computation process.
How we can fix this?</p>
<h3 id="distance-estimation">Distance estimation</h3>
<p>The solution to this is a signed distance function, or a so called Distance Estimator.
Imagine if we knew how far we are from the object at any point of time?
This would mean that we can shoot a ray of this length in any direction and still don’t hit anything.
Let’s add another object to the scene:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/scene-with-two-objects.svg"> 
</figure>

<p>Now, let’s draw two circles, which will represent distances from the objects, to the point from where we’ll cast rays:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/distances.svg"> 
</figure>

<p>We can see, that there are two circles, and one is bigger than another.
This means, that if we choose the shortest safe distance, we can safely cast ray in any direction and not overshoot anything.
For example, let’s cast a ray towards the square:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/ray-in-safe-zone.svg"> 
</figure>

<p>We can see, that we haven’t reached the square, but more importantly we did not overshoot it.
Now we need to march the ray again, but what distance should it cover?
To answer this question, we need to take another distance estimation from ray end to the objects in the scene:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/second-safe-march.svg"> 
</figure>

<p>Once again we choose shorter distance, and march towards the square, then get the distance again, and repeat the whole process:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/another-safe-match.svg"> 
</figure>

<p>You can see that with each step the distance to the object becomes smaller, and thus we will never overshoot the object.
However this also means, that we will take a lot of really small steps, until we finally fully hit the object, if we ever do.
This is not a good idea, because it is even more inefficient than using fixed distance, and produces too accurate results, which we don’t really need.
So instead of marching up until we exactly hit the object, we will march <em>enough</em> times.
E.g. until the distance to the object is small enough, then there’s no real point to continue marching, as it is clear that we will hit the object soon.
But this also means, that if the ray goes near the edge of an object, we do a lot of expensive steps of computing distance estimations.</p>
<p>Here’s a ray that is parallel to the side of the square, and marches towards the circle:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/expensive-marching.svg"> 
</figure>

<p>We do a lot of seemingly pointless measurements, and if a ray was closer to the square’s side, we would do even more steps.
However this also means, that we can use this data (since we’re already computed it) to render such things as glow, or ambient occlusion.
But more on this later.</p>
<p>Once ray hit an object we have all the data we need.
Ray represents a point on the screen, and the more rays we cast the higher resolution of our image will be.
And since we’re not using triangles to represent objects, our spheres will always be smooth, no matter how close we are to it, because there’s no polygons involved.</p>
<p>This is basically it.
Ray marching is quite simple concept, just like raycaster, although it’s a bit more complicated, as we do have to compute things in 3D space now.
So let’s begin implementing it by installing required tools, and setting up the project.</p>
<h2 id="project-structure">Project structure</h2>
<p>As you know from the title we will use two main tools to create ray-marcher, which are <a href="https://love2d.org/">LÖVE</a>, a free game engine, and <a href="https://fennel-lang.org/">Fennel</a> the programming language.
I’ve chosen Fennel, because it is a Lisp like language, that compiles to Lua, and I’m quite a fan of Lisps.
But we also needed to draw somewhere, and I know no GUI toolkit for Lua.
But there is LÖVE - a game engine that runs Lua code, which is capable on running on all systems, thus a perfect candidate for our task.</p>
<p>Installation steps may differ per operating system, so please refer to manuals<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup><sup>, </sup><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.
At the time of writing this post I’m using Fedora GNU/Linux, so for me it means:</p>
<div><pre><code data-lang="sh">$ sudo dnf install love luarocks readline-devel
$ luarocks install --local fennel
$ luarocks install --local readline <span># requires readline-devel</span>
$ <span>export</span> <span>PATH</span>=<span>"</span><span>$PATH</span><span>:</span><span>$HOME</span><span>/.luarocks/bin"</span>
</code></pre></div><p>It’s better to permanently add <code>$HOME/luarocks/bin</code> (or another path, if your installation differs) to the <code>PATH</code> variable in your shell, in order to be able to use installed utilities without specifying full path every time.
You can test if everything is installed correctly, by running <code>fennel</code> in you command line.</p>
<div><pre><code data-lang="sh">$ fennel
Welcome to Fennel 0.5.0 on Lua 5.3!
Use (doc something) to view documentation.
&gt;&gt; (+ 1 2 3)
6
&gt;&gt;
</code></pre></div><p>For other distributions installation steps may vary, and for Windows, I think it’s safe to skip the <code>readline</code> part, which is fully optional, but makes editing in a REPL a bit more comfortable.</p>
<p>Once everything is installed, let’s create the project directory, and the <code>main.fnl</code> file, where we will write our code.</p>
<div><pre><code data-lang="sh">$ mkdir love_raymarching
$ <span>cd</span> love_raymarching
$ touch main.fnl
</code></pre></div><p>And that’s it!
We can test if everything works by adding this code to <code>main.fnl</code>:</p>
<div><pre><code data-lang="clojure">(<span>fn </span><span>love.draw</span> []
  (<span>love.graphics.print</span> <span>"It works!"</span>))
</code></pre></div><p>Now we can compile it with <code>fennel --compile main.fnl &gt; main.lua</code>, thus producing the <code>main.lua</code> file, and run <code>love .</code> (dot is intentional, it indicates current directory).</p>
<p>A window should appear, with white text <code>It works!</code> in upper left corner:</p>
<figure>
    <img src="https://andreyorst.gitlab.io/2020-10-15-raymarching-with-lua-and-love/love-works.png"> 
</figure>

<p>Now we can begin implementing our raymarcher.</p>
<h2 id="scene-setup">Scene setup</h2>
<p>Just as in raycaster, we need a camera that will shoot rays, and some objects to look at.
Let’s begin by creating a camera object, that will store coordinates and rotation information.
We can do so, by using <code>var</code> to declare a variable that is local to our file, and that we can later change with <code>set</code><sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>:</p>

<div><pre><code data-lang="clojure">(<span>var </span><span>camera</span> {<span>:pos</span> [0.0 0.0 0.0]
             <span>:x-rotate</span> 0.0
             <span>:z-rotate</span> 0.0})
</code></pre></div><blockquote>
<p>For those unfamiliar with Lisps, and especially Clojure, let me quickly explain what this syntax is.
If you know this stuff, feel free to <a href="#org6f2d291">skip this part</a>.</p>
<p>We start by using a <code>var</code> special form, that binds a value to a name like this: <code>(var name value)</code>.
So if we start the REPL, using <code>fennel</code> command in the shell, and write <code>(var a 40)</code>, a new variable <code>a</code> will be created.
We then can check, that it has the desired value by typing <code>a</code>, and pressing return:</p>
<p>We can then alter the contents of this variable by using <code>set</code> special form, which works like this <code>(set name new-value)</code>:</p>
<div><pre><code data-lang="clojure"><span>&gt;&gt;</span> (<span>set </span><span>a</span> (<span>+ </span><span>a</span> 2))
<span>&gt;&gt;</span> <span>a</span>
42
</code></pre></div><p>Now to curly and square brackets.
Everything enclosed in curly braces is a hashmap.
We can use any Lua value as our key, and the most common choice is a string, but Fennel has additional syntax for defining keys - a colon followed by a word: <code>:a</code>.
This is called a keyword, and in Fennel it is essentially the same as <code>"a"</code>, but we don’t need to write a pair of quotes.
However keywords can’t contain spaces, and some other symbols.</p>
<p>So writing this <code>{:a 0 :b 2 :c :hello}</code> in the REPL will make a new table, that holds three key value pairs, which we can later get with another syntax - the dot <code>.</code>.
Combining it with <code>var</code>, we can see that it works:</p>
<div><pre><code data-lang="clojure"><span>&gt;&gt;</span> (<span>var </span><span>m</span> {<span>:a</span> 1 <span>:b</span> 2 <span>:c</span> <span>:hello</span>})
<span>&gt;&gt;</span> (<span>. </span><span>m</span> <span>:b</span>)
2
</code></pre></div><p>There’s also a shorthand for this …</p></blockquote></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/">https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/</a></em></p>]]>
            </description>
            <link>https://andreyorst.gitlab.io/posts/2020-10-15-raymarching-with-fennel-and-love/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24835766</guid>
            <pubDate>Tue, 20 Oct 2020 10:26:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Assorted Thoughts on Zig and Rust]]>
            </title>
            <description>
<![CDATA[
Score 325 | Comments 278 (<a href="https://news.ycombinator.com/item?id=24835357">thread link</a>) | @ikskuh
<br/>
October 20, 2020 | https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/ | <a href="https://web.archive.org/web/*/https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>I've been using <a href="https://ziglang.org/">zig</a> for ~4 months worth of side projects, including a <a href="https://git.sr.ht/%7Ejamii/focus/tree">toy text editor</a> and an <a href="https://git.sr.ht/%7Ejamii/imp">interpreter for a relational language</a>. I've written ~10kloc.</p>
<p>That's not nearly enough time to form a coherent informed opinion. So instead here is an incoherent assortment of thoughts and experiences, in no particular order :)</p>
<p>This is not meant to be an introduction to zig - check out the excellent <a href="https://ziglang.org/documentation/master/">language docs</a> or the new <a href="https://ziglearn.org/">ziglearn.org</a> instead. I'll try to focus instead on things that are not immediately obvious from reading intro material.</p>
<p>The obvious point of comparison is to rust. For context, I've been using rust <a href="https://scattered-thoughts.net/writing/three-months-of-rust/">since 2015</a>. Mostly in research positions writing throwaway code, but also ~14 months working on <a href="https://materialize.io/">a commercial database</a> which is ~100kloc.</p>
<hr>
<p>Zig is dramatically simpler than rust. It took a few days before I felt proficient vs a month or more for rust.</p>
<p>Most of this difference is <strong>not</strong> related to lifetimes. Rust has patterns, traits, dyn, modules, declarative macros, procedural macros, derive, associated types, annotations, cfg, cargo features, turbofish, autoderefencing, deref coercion etc. I encountered most of these in the first week. Just understanding how they all work is a significant time investment, let alone learning when to use each and how they affect the available design space.</p>
<p>I still haven't internalized the full rule-set of rust enough to be able predict whether a design in my head will successfully compile. I don't remember the order in which methods are resolved during autoderefencing, or how module visibility works, or how the type system determines if one impl might <a href="https://github.com/Ixrec/rust-orphan-rules#what-are-the-orphan-rules">overlap another or be an orphan</a>. There are frequent moments where I know what I want the machine to do but struggle to encode it into traits and lifetimes.</p>
<p>Zig manages to provide many of the same features with a single mechanism - compile-time execution of regular zig code. This comes will all kinds of pros and cons, but one large and important pro is that I already know how to write regular code so it's easy for me to just write down the thing that I want to happen.</p>
<hr>
<p>One of the key differences between zig and rust is that when writing a generic function, rust will prove that the function is type-safe for every possible value of the generic parameters. Zig will prove that the function is type-safe only for each parameter that you actually call the function with.</p>
<p>On the one hand, this allows zig to make use of arbitrary compile-time logic where rust has to restrict itself to structured systems (traits etc) about which it can form general proofs. This in turn allows zig a great deal of expressive power and also massively simplifies the language.</p>
<p>On the other hand, we can't type-check zig libraries which contain generics. We can only type-check specific uses of those libraries.</p>
<pre><span>// This function is typesafe if there exist no odd perfect numbers
// https://en.wikipedia.org/wiki/Perfect_number#Odd_perfect_numbers
fn foo(comptime n: comptime_int, i: usize) usize {
  const j = if (comptime is_odd_perfect_number(n)) "surprise!" else 1;
  return i + j;
}
</span></pre>
<p>This means zig also doesn't get the automatic, machine-checked documentation of type constraints that rust benefits from and may face more challenges providing IDE support.</p>
<p>This might harm the zig ecosystem by making it harder to compose various libraries. But <a href="https://julialang.org/">julia</a> has a similar model and in practice it has worked very well (<a href="https://youtu.be/dmWQtI3DFFo?t=1710">eg</a>, <a href="https://www.oxinabox.net/2020/02/09/whycompositionaljulia.html">eg</a>).</p>
<hr>
<p>Zig's comptime allows expressing <a href="https://scattered-thoughts.net/writing/open-multiple-dispatch-in-zig/">open multiple dispatch</a> as a library.</p>
<p>It should be relatively trivial to implement specialization the same way, which has been a <a href="https://github.com/rust-lang/rust/issues/31844">work in progress</a> in rust for years and is critical to many optimizations in julia's math libraries.</p>
<p>Julia chose dynamic typing because it's very difficult to encode the types of various mathematical operations into a general schema (eg fortress <a href="https://youtu.be/EZD3Scuv02g?t=3011">struggled with this</a>). Zig's approach of not requiring general schemas but still type-checking individual cases may be an interesting sweet spot.</p>
<hr>
<p>I used the <a href="https://cwe.mitre.org/data/definitions/1350.html">2020 CWE Top 25 Most Dangerous Software Weaknesses</a> to get a sense of the relative frequency of different causes of memory unsafety.</p>
<p>(I'm assuming that the zig programmer is using release-safe mode instead of the unfortunately named release-fast mode which disables all runtime safety checks.)</p>
<ul>
<li>Out-of-bounds Write (787/1350)</li>
<li>Out-of-bounds Read (125/1350)</li>
<li>Improper Restriction of Operations within the Bounds of a Memory Buffer (119/1350)</li>
</ul>
<p>Both languages primarily use bounds-checked slices and relegate pointer arithmetic to a separate type (<code>*T</code> in rust, <code>[*]T</code> in zig).</p>
<ul>
<li>NULL Pointer Dereference (476/1350)</li>
</ul>
<p>Both languages require explicit annotations for nulls (<code>Option&lt;T&gt;</code> in rust, <code>?T</code> in zig) and require code to either handle the null case or safely crash on null (<code>x.unwrap()</code> in rust, <code>x.?</code> in zig).</p>
<p>Dereferencing/casting a null c pointer is undefined behavior in both languages, but is checked at runtime in zig.</p>
<ul>
<li>Integer Overflow or Wraparound (190/1350)</li>
</ul>
<p>Rust catches overflow in debug and wraps in release. Zig catches overflow in debug/release-safe and leaves behavior undefined in release-fast.</p>
<p>Both languages allow explicitly asking for wraparound (<code>x.wrapping_add(1)</code> in rust, <code>x +% 1</code> in zig).</p>
<ul>
<li>Use After Free (416/1350)</li>
</ul>
<p>As long as all unsafe code obeys the aliasing and lifetime rules, rust protects completely against UAF.</p>
<p>Zig has little protection. The recently merged
<a href="https://github.com/ziglang/zig/blob/575fbd5e3592cff70cbfc5153884d919e6bed89f/lib/std/heap/general_purpose_allocator.zig">GeneralPurposeAllocator</a> avoids reusing memory regions (which prevents freed data from being overwritten) and reusing pages (which means that UAF will eventually result in a page fault). But this comes at the cost of fragmentation and lower performance and it also won't provide protection for child allocators using the GPA as a backing allocator.</p>
<hr>
<p>Both languages will insert implicit casts between primitive types and pointers whenever it is safe to do so, and require explicit casts otherwise. (With the odd exception that rust will not implicitly upcast numbers).</p>
<p>Both languages support generics which almost entirely avoids the need to cast void pointers.</p>
<hr>
<p>In rust the Send/Sync traits flag types which are safe to move/share across threads. In the absence of unsafe code it should be impossible to cause data races.</p>
<p>Zig has no comparable protection. It's possible to implement the same logic as Send/Sync in comptime zig, but without the ability to track ownership the rules would have to be much more restrictive.</p>
<hr>
<p>Rust prevents having multiple mutable references to the same memory region at the same time.</p>
<p>This means that eg iterator invalidation is prevented at compile time, because the borrow checker won't allow mutating a data-structure while an iterator is holding a reference to the data-structure. Similarly for resizing a data-structure while holding a reference to the old allocation. Both examples are easy sources of UAF in zig.</p>
<hr>
<p>Neither language is able to produce stack traces for stack overflows at the moment (<a href="https://github.com/rust-lang/rust/issues/51405">rust</a>, <a href="https://github.com/ziglang/zig/issues/1616">zig</a>)</p>
<p>In the future zig is <a href="https://github.com/ziglang/zig/issues/1006">intended</a> to statically check the maximum stack usage of your program and force recursive code to explicitly allocate space on the heap, so that stack overflows produce a recoverable OutOfMemory error rather than a crash.</p>
<p>This is not an academic problem - I've seen real-world crashes from recursive tree transformations in compilers (<a href="https://github.com/MaterializeInc/materialize/pull/3996">eg</a>) and it's often painful to write the same logic without recursion.</p>
<hr>
<p>Undefined behavior in rust is defined <a href="https://doc.rust-lang.org/nomicon/what-unsafe-does.html">here</a>. It's worth noting that breaking the aliasing rules in unsafe rust can cause undefined behavior but these rules are not yet well-defined. So far this hasn't caused me any problems but it is a little unnerving.</p>
<p><a href="https://github.com/rust-lang/miri">Miri</a> is an interpreter for rusts Mid-level Intermediate Representation which will detect many (but not all) cases of undefined behavior in unsafe rust. It's far too slow to use for the whole materialize test suite but was useful for unit-testing an unsafe module.</p>
<p>Undefined behavior in zig is defined <a href="https://ziglang.org/documentation/master/#Undefined-Behavior">here</a>. This list is <a href="https://github.com/ziglang/zig/issues/1966">probably incomplete</a> given that the core language is still under development.</p>
<p>Zig <a href="https://github.com/ziglang/zig/issues/2301">aspires</a> to insert runtime checks for almost all undefined behavior when compiling in debug mode. So far all the easy cases are handled, which is already a dramatic improvement over c.</p>
<p>Zigs compile-time partial evaluation is done by an IR interpreter - it seems plausible that this could also be used as a miri-like tool in the future.</p>
<hr>
<p><code>@import</code> takes a path to a file and turns the whole file into a struct. So modules are just structs. And vice-versa - if you have a large struct declaration you can move it into a file to reduce the indentation.</p>
<p>Zig doesn't care at all where you put files on the filesystem.</p>
<p><code>@import</code> is part of the compile-time execution system so things like platform-specific modules and configurable features can be specified in regular code rather than rust's limited set of <code>#[cfg(...)]</code> macros.</p>
<hr>
<p>Array, struct, enum and union literals can be anonymous - <code>.{.Constant = 1.0}</code> is an anonymous union with it's own type, but can be implicitly cast to any union with a <code>Constant: f64</code> field because they share the same structure.</p>
<p>In rust my code is littered with <code>use Expr::*</code> and I'm careful to avoid name collisions between different enums that I might want to import in the same functions. In zig I just use anonymous literals everywhere and don't worry about it.</p>
<hr>
<p>Anonymous literals are also nice when using structs to simulate keyword arguments. No need to find and import the correct type:</p>
<pre><span>fn do_things(config: struct {
  max_things: usize = 1000, // default value
  flavor: Flavor,
}) void {
  ...
}

do_things(.{.flavor = .Strawberry});
</span></pre>
<hr>
<p>There is a pattern that shows up a lot in the materialize codebase:</p>
<pre><span>let</span><span> constant </span><span>= </span><span>if let </span><span>Expr</span><span>::</span><span>Constant(constant) </span><span>=</span><span> expr { constant } </span><span>else </span><span>{ </span><span>panic!</span><span>() }</span><span>;
</span></pre>
<p>It's common enough that many types have methods like <code>expr.unwrap_constant()</code>.</p>
<p>In zig:</p>
<pre><span>const constant = expr.Constant;
</span></pre>
<p>A similar pattern is:</p>
<pre><span>if</span><span> some_condition {
    </span><span>if let </span><span>Expr</span><span>::</span><span>Constant(</span><span>_</span><span>) </span><span>=</span><span> expr {
        </span><span>...
    </span><span>}
}
</span></pre>
<p>Again, many types get methods like <code>expr.is_constant()</code>.</p>
<pre><span>if</span><span> some_condition </span><span>&amp;&amp;</span><span> expr</span><span>.</span><span>is_constant</span><span>(…</span></pre></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/">https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/</a></em></p>]]>
            </description>
            <link>https://scattered-thoughts.net/writing/assorted-thoughts-on-zig-and-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24835357</guid>
            <pubDate>Tue, 20 Oct 2020 09:22:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Design Stripe or Hacker News-like favicons in seconds]]>
            </title>
            <description>
<![CDATA[
Score 181 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24835219">thread link</a>) | @hosshams
<br/>
October 20, 2020 | https://formito.com/tools/favicon | <a href="https://web.archive.org/web/*/https://formito.com/tools/favicon">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Copy the following code and put it inside the<!-- --> <code>&lt;head&gt;</code> tag of your website.</p><pre><code></code></pre><p>Or, download the SVG file, add the following code to<!-- --> <code>&lt;head&gt;</code> tag of your website, and replace<!-- --> <code>href</code> attribute with URL to your SVG file.</p><pre><code>&lt;link rel="icon" type="image/svg+xml" href="favicon.svg" /&gt;</code></pre></div></div>]]>
            </description>
            <link>https://formito.com/tools/favicon</link>
            <guid isPermaLink="false">hacker-news-small-sites-24835219</guid>
            <pubDate>Tue, 20 Oct 2020 08:56:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Discipline Doesn’t Scale]]>
            </title>
            <description>
<![CDATA[
Score 211 | Comments 104 (<a href="https://news.ycombinator.com/item?id=24834965">thread link</a>) | @ingve
<br/>
October 20, 2020 | https://www.sicpers.info/2020/10/discipline-doesnt-scale/ | <a href="https://web.archive.org/web/*/https://www.sicpers.info/2020/10/discipline-doesnt-scale/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>If programmers were just more disciplined, more <em>professional</em>, they’d write better software. All they need is a <a href="https://www.pearson.com/us/higher-education/program/Martin-Clean-Coder-The-A-Code-of-Conduct-for-Professional-Programmers/PGM8366.html">code of conduct</a> telling them how to work like those of us who’ve worked it out.</p>
<p>The above statement is true, which is a good thing for those of us interested in improving the state of software and in helping our fellow professionals to improve their craft. However, it’s also very difficult and inefficient to apply, in addition to being entirely unnecessary. In the common parlance of our industry, “discipline doesn’t scale”.</p>
<p>Consider the trajectory of object lifecycle management in the Objective-C programming language, particularly the NeXT dialect. Between 1989 and 1995, the dominant way to deal with the lifecycle of objects was to use the <tt>+new</tt> and <tt>-free</tt> methods, which work much like <tt>malloc/free</tt> in C or <tt>new/delete</tt> in C++. Of course it’s <em>possible</em> to design a complex object graph using this ownership model, it just needs discipline, that’s all. Learn the heuristics that the experts use, and the techniques to ensure correctness, and get it correct.</p>
<p>But you know what’s better? Not having to get that right. So around 1994 people introduced new tools to do it an easier way: reference counting. With NeXTSTEP Mach Kit’s <tt>NXReference</tt> protocol and OpenStep’s <tt>NSObject</tt>, developers no longer need to know when <em>everybody</em> in an app is done with an object to destroy it. They can indicate when a reference is taken and when it’s relinquished, and the object itself will see when it’s no longer used and free itself. Learn the heuristics and techniques around auto releasing and unretained references, and get it correct.</p>
<p>But you know what’s better? Not having to get that right. So a couple of other tools were introduced, so close together that they were probably developed in parallel[*]: Objective-C 2.0 garbage collection (2006) and Automatic Reference Counting (2008). ARC “won” in popular adoption so let’s focus there: developers no longer need to know exactly when to retain, release, or autorelease objects. Instead of describing the edges of the relationships, they describe the <em>meanings</em> of the relationships and the compiler will automatically take care of ownership tracking. Learn the heuristics and techniques around weak references and the “weak self” dance, and get it correct.</p>
<p>[*] I’m ignoring here the significantly earlier integration of the Boehm conservative GC with Objective-C, because so did everybody else. That in itself is an important part of the technology adoption story.</p>
<p>But you know what’s better? You get the idea. You see similar things happen in other contexts: for example C++’s move from <tt>new/delete</tt> to smart pointers follows a similar trajectory over a similar time. The reliance on an entire programming community getting some difficult rules right, when faced with the alternative of using different technology <em>on the same computer</em> that follows the rules for you, is a tough sell.</p>
<p>It seems so simple: computers exist to automate repetitive information-processing tasks. Requiring programmers who have access to computers to recall and follow repetitive information processes is wasteful, when the computer can do that. So give those tasks to the computers.</p>
<p>And yet, for some people the problem with software isn’t a lack of automation but a lack of discipline. Software would be better if only people knew the rules, honoured them, and slowed themselves down so that instead of cutting corners they just chose to ignore important business milestones instead. Back in my day, everybody knew “no Markdown around town” and “don’t code in an IDE after Labour Day”, but now the kids do whatever they want. The motivations seem different, and I’d like to sort them out.</p>
<p>Let’s start with hazing. A lot of the software industry suffers from “I had to go through this, you should too”. Look at software engineering interviews, for example. I’m not sure whether anybody actually believes “I had to deal with carefully ensuring NUL-termination to avoid buffer overrun errors so you should too”, but I do occasionally still hear people telling less-experienced developers that they should learn C to learn more about how their computer works. <a href="https://queue.acm.org/detail.cfm?id=3212479">Your computer is not a fast PDP-11</a>, all you will learn is how the C virtual machine works.</p>
<p>Just as Real Men Don’t Eat Quiche, so <a href="https://www.codeproject.com/articles/927/real-programmers-don-t-use-pascal">real programmers don’t use Pascal</a>. Real Programmers use FORTRAN. This motivation for sorting discipline from rabble is based on the idea that if it isn’t at least as hard as it was when <em>I</em> did this, it isn’t hard enough. And that means that the goalposts are movable, based on the orator’s experience.</p>
<p>This is often related to the <em>term</em> of their experience: you don’t need TypeScript to write good React Native code, just Javascript and some discipline. You don’t need React Native to write good front-end code, just JQuery and some discipline. You don’t need JQuery…</p>
<p>But along with the term of experience goes the breadth. You see, the person who learned reference counting in 1995 and thinks that you can only <em>really</em> understand programming if you manually type out your own reference-changing events, presumably didn’t go on to use garbage collection in Java in 1996. The person who thinks you can only <em>really</em> write correct software if every case is accompanied by a unit test presumably didn’t learn Eiffel. The person who thinks that you can only <em>really</em> design systems if you use the Haskell type system may not have tried OCaml. And so on.</p>
<p>The conclusion is that for this variety of disciplinarian, the appropriate character and quantity of discipline is whatever they had to deal with at some specific point in their career. Probably a high point: after they’d got over the tricky bits and got productive, and after you kids came along and ruined everything.</p>
<p>Sometimes the reason for suggesting the disciplined approach is entomological in nature, as in the case of the eusocial insect the “performant” which, while not a real word, exists in greater quantities in older software than in newer software, apparently. The performant is capable of making software faster, or use less memory, or more concurrent, or less dependent on I/O: the specific characteristics of the performant depend heavily on context.</p>
<p>The performant is often not talked about in the same sentences as its usual companion species, the irrelevant. Yes, there may be opportunities to shave a few percent off the runtime of that algorithm by switching from the automatic tool to the manual, disciplined approach, but does that matter (yet, or at all)? There are software-construction domains where specific performance characteristics are desirable, indeed that’s true across a lot of software. But it’s typical to focus performance-enhancing techniques on the bits where they enhance performance that needs enhancing, not to adopt them across the whole system on the basis that it was better when everyone worked this way. You might save a few hundred cycles writing native software instead of using a VM for that UI method, but if it’s going to run after a network request completes over EDGE then trigger a 1/3s animation, nobody will notice the improvement.</p>
<p>Anyway, whatever the source, the problem with calls for discipline is that there’s no strong motivation to <em>become</em> more disciplined. I can use these tools, and my customer is this much satisfied, and my employer pays me this much. Or I can learn from you how I’m <em>supposed</em> to be doing it, which will slow me down, for…your satisfaction? So you know I’m doing it the way it’s supposed to be done? Or so that I can tell everyone else that they’re doing it wrong, too? Sounds like a great deal.</p>
<p>Therefore discipline doesn’t scale. Whenever you ask some people to slow down and think harder about what they’re doing, some fraction of them will. Some will wonder whether there’s some other way to get what you’re peddling, and may find it. Some more will not pay any attention. The dangerous ones are the ones who thought they <em>were</em> paying attention and yet still end up not doing the disciplined thing you asked for: they either torpedo your whole idea or turn it into not doing the thing (see OOP, Agile, Functional Programming). And still more people, by far the vast majority, just weren’t listening at all, and you’ll never reach them.</p>
<p>Let’s flip this around. Let’s look at where we <em>need</em> to be disciplined, and ask if there are gaps in the tool support for software engineers. Some people want us to always write a failing test and make it pass before adding any code (or want us to write a passing test and revert our changes if it accidentally fails): does that mean our tools should not let us write code for which there’s no test? Does the same apply for acceptance tests? Some want us to refactor mercilessly; does that mean our design tools should always propose more parsimonious alternatives for passing the same tests? Some say we should get into the discipline of writing code that always reveals its intent: should the tools make a crack at interpreting the intention of the code-as-prose?</p>
	</div></div>]]>
            </description>
            <link>https://www.sicpers.info/2020/10/discipline-doesnt-scale/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834965</guid>
            <pubDate>Tue, 20 Oct 2020 08:11:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Long Road to HTTP/3]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 72 (<a href="https://news.ycombinator.com/item?id=24834767">thread link</a>) | @todsacerdoti
<br/>
October 20, 2020 | https://scorpil.com/post/the-long-road-to-http3/ | <a href="https://web.archive.org/web/*/https://scorpil.com/post/the-long-road-to-http3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wrapper"><div id="container"><div><article itemscope="" itemtype="http://schema.org/BlogPosting"><div itemprop="articleBody"><p>While HTTP/3 specification is still in the draft stage, the latest version of the Chrome browser already <a href="https://blog.chromium.org/2020/10/chrome-is-deploying-http3-and-ietf-quic.html" target="_blank" rel="nofollow">supports it by default</a>
. With Chrome holding around 70% of browser market share, you could say HTTP/3 has gone mainstream.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/quic-logo.png" alt="QUIC logo"></p><p>The new revision of this foundational protocol aims to make the web more efficient, secure, and shorten the content-delivery latencies. In some ways, it’s a braver take of HTTP2: similar goals addressed by replacing the underlying TCP protocol with a new, purpose-built protocol QUIC. The best way to explain the benefits of QUIC is to illustrate where TCP falls short as a transport for HTTP requests. And to do that, we’ll start at the very beginning.</p><h3 id="http-the-original">HTTP. The Original.</h3><p>When Sir Tim Berners-Lee formalized the design of a simple <a href="https://www.w3.org/Protocols/HTTP/AsImplemented.html" target="_blank" rel="nofollow">one-line hyper-text-exchange protocol</a>
in 1991, TCP was already an old, reliable protocol. The original definition document of what later became known as HTTP 0.9 specifically mentions TCP as a preferred, albeit not exclusive, transport protocol:</p><blockquote><p>Note: HTTP currently runs over TCP, but could run over any connection-oriented service.</p></blockquote><p>Of course, this proof-of-concept version of HTTP had very few similarities to HTTP we now know and love today. There were no headers and no status codes. The typical request was as simple as <code>GET /path</code>. The response contained only HTML and ended with the closing of the TCP connection.
Since browsers were not yet a thing, user was supposed to read HTML directly. It was possible to link to other resources, but none of the tags present in this early version of HTML requested additional resources asynchronously. A single HTTP request delivered a complete, self-sufficient page.</p><h3 id="emergence-of-http10">Emergence of HTTP/1.0</h3><p>In subsequent years the internet has exploded, and HTTP evolved to be an extendable and flexible general-purpose protocol, although transporting HTML remained its chief specialty. There are three critical updates to HTTP that enabled this evolution:</p><ul><li>introduction of methods allowed the client to identify the type of action it wants to perform. For example, POST was created to allow client sending data to the server to process and store</li><li>status codes provided a way for client to confirm that the server has processed the request successfully, and if not - to understand what kind of error has occured</li><li>headers added an ability to attach structured textual metadata to requests and responses that could modify the behavior of the client or server. Encoding and content-type headers, for example, allowed HTTP to transfer not just HTML, but any type of payload. “Compression” header allowed the client and server to negotiate supported compression formats, thus reducing the amount of data to transfer over the connection</li></ul><p>At the same time, HTML advanced to support images, styles, and other linked resources. Browsers were now forced to perfrom multiple requests to display a single web page, which the original connection-per-request architecture was not designed to handle. Establishing and ending a TCP connection involves a lot of back-and-forth packet exchange, so it is relatively expensive in terms of latency overhead. It didn’t matter much when a web-page consisted of a single text file, but as the number of requests per page increased, so did the latency.</p><p>The picture below illustrates how much overhead was involved in establishing a new TCP connection per request.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http1-tcp-overhead.png" alt="TCP connection requires three requests to establish connection and four to close it cleanly"></p><p>A “connection” header was created to address this problem. Client sends a request with “connection: keep-alive” header to signal intent to keep the TCP connection open for subsequent requests. If server understands this header and agrees to respect it, its response will also contain the “connection: keep-alive” header. This way, both parties maintain TCP channel open and use it for subsequent communication until either party decides to close it. This became even more important with the spread of SSL/TLS encryption, because negotianting an encryption algorithm and exchanging cryptographic keys requires an additional request/response cycle on each connection.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http1-keepalive.png" alt="A single TCP connection can be reused for multiple requests with “connection: keep-alive” header"></p><p>At the time, many of the HTTP improvements appeared spontaneously. When a popular browser or a server app saw a need for a new HTTP feature, they would simply implement it themselves and hoped that other parties would follow the suit. Ironically, a decentralized web needed a centralized governing body to avoid fragmentation into incompatible pieces. Tim Berners-Lee, the original creator of the protocol, recognized the danger and founded the World Wide Web Consortium (W3C) in 1994, which together with the Internet Engineering Task Force (IETF) worked on formalizing stack of internet technologies. As the initial step to bring more structure to the existing environment, they documented the most common features used in HTTP at the time and named the resulting protocol HTTP/1.0. However, because this “specification” described varied, often inconsistent techniques as seen “in the wild”, it never received a status of a standard. Instead, the work on the new version of the HTTP protocol has begun.</p><h3 id="standardization-of-http11">Standardization of HTTP/1.1</h3><p>HTTP/1.1 fixed inconsistencies of HTTP/1.0 and adjusted the protocol to be more performant in the new web ecosystem. Two of the most critical changes introduced were the use of persistent TCP connections (keep-alive’s) by default and HTTP pipelining.</p><p>HTTP pipelining simply means that client does not need to wait for the server to respond to a request before sending subsequent HTTP requests. This feature resulted in even more efficient use of bandwidth and reduced latencies, but it could be improved even more. HTTP pipelining still requires from server to respond in the order of requests received, so if a single request in a pipeline is slow to fulfill, all subsequent responses to a client will be delayed accordingly. This problem is known as head-of-the-line blocking.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http11-blocking.png" alt="Since large-picture.jpg was requested first, it’s blocking the delivery of the style.css"></p><p>At this point in time, the web is gaining more and more interactive capabilities. Web 2.0 is just around the corner, some webpages include dozens or even hundreds of external resources. To work around the head-of-the-line blocking, and to decrease page loading speeds, clients establish multiple TCP connections per host. Of course, the connection overhead never went anywhere. In reality, it got worse, since more and more applications encrypt HTTP traffic with SSL/TLS. So most browsers set the limit of maximal possible simultaneous connections in an attempt to strike a delicate balance.</p><p>Many of the larger web-services have recognized that existing limitations are too restricting for their exceptionally heavy interactive web-applications, so they “gamed the system” by distributing their app through multiple domain names. It all worked, somehow, but the solution has been far from elegant.</p><p>Despite a few shortcomings, the simplicity of HTTP/1.0 and HTTP/1.1 has made them widely successful, and for over a decade no one has made a serious attempt to change them.</p><h3 id="spdy-and-http2">SPDY and HTTP/2</h3><p>In 2008 Google released the Chrome browser, which rapidly gained popularity for being quick and innovative. It has given Google a strong vote on matters of internet technologies. In the early 2010s, Google adds support for its web protocol SPDY to Chrome.</p><p>HTTP/2 standard was based on SPDY with some improvements. HTTP/2 solved the head-of-the-line blocking problem by multiplexing the HTTP requests over a single open TCP connection. This allowed server to answer requests in any order, client could then re-assemble the responses as it received them, making the whole exchange faster within a single connection.</p><p><img src="https://scorpil.com/img/the-long-road-to-http3/http2-multiplexing.png" alt="style.css was returned before the large-picture.jpg, becuase of HTTP/2 multiplexing"></p><p>In fact, with HTTP/2 server can serve the resources to a client before it even asked for it! To give an example, if the server knows that client will most likely need a stylesheet to display an HTML page, it can “push” the CSS to the client without waiting for a corresponding request. While beneficial in theory, this feature rarely seen in practice, since it requires a server to understand the structure of the HTML it serves, which is rarely the case.</p><p>HTTP/2 also allows compressing request headers in addition to the request body, which further reduces the amount of data transferred over the wire.</p><p>HTTP/2 solved a lot of problems for the web, but not all of them. A similar type of head-of-the-line problem is still present on the level of TCP protocol, which remains a foundational building block of the web. When a TCP packet gets lost in transit, the receiver can’t acknowledge incoming packages until the lost package is re-sent by a server. Since TCP is by design oblivious to higher-level protocols like HTTP, a single lost packet will block the stream for all in-flight HTTP requests until the missing data is re-sent. This problem is especially prominent on an unreliable connection, which is not rare in the age of ubiquitous mobile devices.</p><h3 id="http3-revolution">HTTP/3 revolution</h3><p>Since issues with HTTP/2 can not be resolved purely on the application layer a new iteration of the protocol must update the transport layer. However, creating a new transport-layer protocol is not an easy task. Transport protocols need to be supported by hardware vendors and deployed by the majority of network operators, which are reluctant to update because of the costs and efforts involved. Take IPv6 as an example: it was introduced 24 years ago and is still far from being universally supported.</p><p>Fortunately, there is another option. UDP protocol is as widely supported as TCP but is simple enough to serve as a building block for custom protocols running on top of it. UDP packets are fire-and-forget: there are no handshakes, persistent connections, or error-correction. The primary idea behind HTTP3 is to abandon TCP in favor of a UDP-based QUIC protocol. QUIC adds the necessary features (those that were previously provided by TCP, and more) in a way that makes sense for the web environment.</p><p>Unlike HTTP2, which technically allowed an unencrypted communication, QUIC strictly requires encryption to establish a connection. Additionally, encryption is applied to all data …</p></div></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://scorpil.com/post/the-long-road-to-http3/">https://scorpil.com/post/the-long-road-to-http3/</a></em></p>]]>
            </description>
            <link>https://scorpil.com/post/the-long-road-to-http3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834767</guid>
            <pubDate>Tue, 20 Oct 2020 07:37:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adolphe Sax, Inventor of the Saxophone]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24834716">thread link</a>) | @bobf
<br/>
October 20, 2020 | http://www.dinant.be/en/inheritance/adolphe-sax | <a href="https://web.archive.org/web/*/http://www.dinant.be/en/inheritance/adolphe-sax">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tbody>
<tr>
<td>


<h2><span>Adolphe Sax, a Dinantais of genius</span></h2>
<blockquote>
<p>Along with Joachim Patenier (1485-1524), the creator of landscape painting; with Antoine Wiertz (1806-1865), the lyrical painter; with a plethora of sculptors, painters, musicians, brassworkers and others, Dinant can legitimately pride itself on having been the birthplace on 6 November 1814 of Antoine-Joseph, or Adolphe, Sax, a prolific and inspired inventor in the manufacture of musical instruments.</p>
<p>In 1860, the diarist, Oscar Comettant, wrote, "In the services that he has rendered to musical art, in the battles he has had to go through to bring his discoveries to the light of day and defend them from despoilment and in the rewards he has been the object of from all the industrial nations, [Sax's life] rises to the heights of a social event. Novelists will draw from this strange life mysterious and moving episodes (we would add: the legal world will find in the account of the "Sax trial" a vast domain for a case law study) and the moralists will find in it the features of self-denial, physical courage and perseverance, of which only a lifted soul and a great heart are capable.</p>
</blockquote>
<a name="Enfance"></a>
<h2><span>An Agitated Childhood</span></h2>

<blockquote>
<p>Antoine-Joseph Sax was born in the street that has borne his name since 1896, in a modest house, which was destroyed in 1914, and which was built on the present site of an important commercial building.</p>
<p>In its façade, there is a stained-glass window and an inscription chiselled into the stonework: "Adolphe Sax, 1814-1894, was born here". This window was solemnly inaugurated on 27 June 1954, on the initiative of the Tourist Information Centre, under the mayorship of Mr Léon Sasserath. It is the work of Mr Jean Jadin, who designed the cartoon, and Miss Maggy Arzée. Both were taught by Miss Yvonne Gérard and Mr Perot, teachers of graphic art and decoration at the Fine Arts Academy in Namur, which was then directed by Mr Lambeau. It was created under the direction of Mr Van de Capelle.</p>
<p>Son of Charles-Joseph Sax (1791-1865) and Marie-Joseph Masson (1813-1861), Antoine-Joseph was the eldest of eleven children (six boys and five girls, only four of whom survived, the others dying between the ages of 20 and 25).</p>
<p>His childhood was tragic. Hardly able to stand, Antoine-Joseph fell from a height of three floors, seriously bumping his head against a stone: he was believed dead. At the age of three, he swallowed a bowl of vitriolized water, and then a pin. Later, he was seriously burned in a gunpowder explosion; he fell onto a cast iron frying pan and burned himself on one side. Three times he escaped poisoning and asphyxiation in his bedroom, where varnished items were lying about during the night. Another time, he was hit on the head by a cobblestone; he fell into a river and was saved by the skin of his teeth.</p>
<p>"<em>He's a child condemned to misfortune; he won't live," his mother said. In the district, they called him "little Sax, the ghost</em>".</p>
<p>These initial serious incidents were, alas, but the prelude to an eventful existence such as only a few have known. In 1858, Adolphe Sax was miraculously saved from a cancer of the lip by a black doctor who knew the properties of certain Indian plants. What would the future have been but for this intervention?</p>
</blockquote>
<a name="Charles-Joseph"></a>
<h2><span>Charles-Joseph Sax</span></h2>

<blockquote>
<p>A joiner-cabinetmaker, Charles-Joseph Sax quickly launched himself, with success, into the manufacture of musical instruments. In the "New Street" he ran a large workshop. In this trade, he acquired such a reputation that, in 1815 (his eldest son was only one year old), he also set up a workshop in Brussels (where Antoine-Joseph's brothers and sisters were to be born), where he was summoned by William I of Orange (we were then under Dutch occupation). The latter appointed him as maker to the Court and entrusted him with the task of supplying suitable instruments to Belgium regimental music corps.</p>
<p>A self-taught man, therefore, Charles-Joseph Sax made woodwind and brass instruments, even violins and pianos. He registered a dozen patents and brought his instruments to perfection. He successfully participated in numerous exhibitions, where he was awarded flattering distinctions.</p>
<p>At the time when he could have spend the day playing, laughing and having fun, Antoine-Joseph observed the work in his father's workshop, besides being given instruction by one of his uncles, a teacher in Dinant. He was intelligent and his inventive mind was already showing itself, thanks to his love for music (whilst very young, he took singing and flute lessons). Thereafter, he was given lessons by his father, who quickly appreciated his abilities and did all he could to develop them.</p>
<p>Far from disregarding his son's aspirations, Charles-Joseph Sax made him his apprentice and, from a young age, he was conscious of the importance of his work, as though he were anticipating his destiny.</p>
<p>In 1853, after the death of seven of his eleven children, and following financial worries at his Brussels business, Charles-Joseph joined his son in Paris. The master was to become the servant, and was from then on in charge of making saxophones until his death in 1865.</p>
</blockquote>
<a name="Jeunesse"></a>
<h2><span>A productive childhood</span></h2>

<blockquote>
<p>Supported and assisted by his father, the youth worked. He created, he perfected instruments and he played them. He was 16 when he went to the Industrial Exposition in Brussels to present flutes and ivory clarinets. At the age of 20, he made an entirely new clarinet, with 24 keys, a work of imagination and a masterpiece of manual work. Then, a new bass clarinet, which incited enthusiasm in Habeneck, the leader of the orchestra at the Paris Opera House, who was passing through Brussels, and who called the other clarinets "barbarian instruments".</p>
<p>Even at that early stage, this creation provoked jealousy in the soloist at the "Great Royal Harmony" in Brussels, who refused to use it because, he said, it had come from "that weedy little pupil, Sax". "Play your clarinet, then" Sax answered, " and I shall play mine." The challenge accepted, Sax triumphed in front of four thousand people. He became a soloist. Works were written for him that, after his departure, were no longer played because they were so difficult!</p>
<p>The young genius pursued his work. He invented a sound reflector, a new double-bass clarinet, a piano-tuning process that remained the inventor's secret and who probably was unable to exploit it for want of money, a steam organ "capable of being heard throughout the province": now that just shows Sax's tendency to think big!</p>
<p>Sax's beginnings throw a very curious light on his character (we shall call him Adolphe from now on): energy, courage, dynamism, total self-confidence. He refused to go and set up a business in St Petersburg, rejected an offer to set up in London. That means that his reputation exceeded frontiers. Sax was conscious of all his possibilities and his talent; he conceived the work that he felt the call to achieve; he was full of hope and he believed he had every chance of success; he had great visions, he believed in what he saw. He suffocated in his little country.</p>
<p>In 1840, he presented nine inventions at the Belgian Exhibition. He was denied the first medal on the plea of his young age; there would be nothing left to offer him the year after. He was thwarted in his true-love, if not in his pride. He refused the vermeil medal he was awarded, replying with pride, "<em>If they think me too young to deserve the gold medal, I myself think me too old to accept this vermeil one</em>."</p>
</blockquote>
<a name="Paris"></a>
<h2><span>The Call to Paris</span></h2>

<blockquote>
<p>Europe's centre of attraction, Paris haunted him, Paris called him.</p>
<p>The composer Halévy wrote to him of the hope that composers had in his inventions: "<em>Hurry and finish your new family of instruments (saxophones) and come and succour to the poor composers that are looking for something new and to the public that is demanding it, if not to the world itself.</em>"</p>
<p>Let us add to this call and the snub in Brussels the fact of his family trials, and the decision was made: Adolphe Sax left for Paris "rich in ideas and light in cash": he had thirty francs in his pocket!</p>
<p>The year 1842 formed the turning point in Sax's life, possessing as he did his new invention: the saxophone and its family.</p>
<p><img src="http://www.dinant.be/uploads/pages/286/sax_atel.jpg" alt="">Moreover, in 1841, had he not presented it anonymously in Brussels, behind a curtain, so as not to disclose it and avoid the risk of plagiarism?</p>
<p>Adolphe Sax was almost thirty, "the age at which man's creative character affirms itself, at which the human personality is drawn." At the age of 27, Napoleon won his first battle in Italy; Newton was 24 and Einstein 26 when they devised their theories. Mozart died aged 35 and Schubert at 31. Examples of precocious geniuses are manifold.</p>
<p>As one former inhabitant of Dinant once rightfully said (1) "<em>a distinction has to be drawn here between a man who draws from his own abstract thoughts the stuff that his genius will knead, him for whom symbols and signs are sufficient to bring forth a thought laden with restrained life and latent splendours; and the other man for whom a technique, slow and tenacious apprenticeship on a complicated apparatus is necessary for him to be able to physically achieve the formal idea. Count, for example, how many early-developing mathematicians there are compared with child physicists. The former exist, the latter are nowhere to be found. Sax is of the category of intellectuals that concentrated on matter and not pure form"</em>.</p>
<p>In 1842, there was Adolphe Sax living in a simple shed in Rue Saint-Georges, Paris. To set up business, he had to borrow money from a musician acquaintance.</p>

</blockquote>
<a name="Berlioz"></a>
<h2><span>Thanks to Berlioz</span></h2>
<blockquote>
<div>
<p><span><strong>About the saxophone, he said "<em>Its principal merit in my view is the varied beauty of its accent, sometimes serious, sometimes calm, sometimes impassioned, dreamy or melancholic, or vague, like the weakened echo of an echo, like the indistinct plaintiff moans of the breeze in the woods and, even better, like the mysterious vibrations of a bell, long after it has been struck; there does not exist another musical instrument …</em></strong></span></p></div></blockquote></td></tr></tbody></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.dinant.be/en/inheritance/adolphe-sax">http://www.dinant.be/en/inheritance/adolphe-sax</a></em></p>]]>
            </description>
            <link>http://www.dinant.be/en/inheritance/adolphe-sax</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834716</guid>
            <pubDate>Tue, 20 Oct 2020 07:28:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My chatbot is dead – Why yours should probably be too]]>
            </title>
            <description>
<![CDATA[
Score 168 | Comments 157 (<a href="https://news.ycombinator.com/item?id=24834552">thread link</a>) | @raphaelsaunier
<br/>
October 19, 2020 | https://azumbrunnen.me/blog/my-chatbot-is-dead-%C2%B7-why-yours-should-probably-be-too/ | <a href="https://web.archive.org/web/*/https://azumbrunnen.me/blog/my-chatbot-is-dead-%C2%B7-why-yours-should-probably-be-too/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
                
<audio controls="controls"><source src="https://azumbrunnen.me/audio/chatbot-is-dead" type="audio/mpeg"></audio>



<p>Personal websites are usually like old books in a shelf. They languish, accumulate dust, and their wrinkles and cracks become more apparent over time. About 3 years ago I embarked on a simple experiment that would end up prolonging the shelf-life of my website by an unusually large margin.</p>



<p>Back in 2017, it seemed like Conversational UI was poised to take over the world. We saw Quartz turning news into a conversation, WeChat being featured as the poster-child of a post application world, iMessage turning into an unnecessarily complex mess, and chatbots popping up like mushrooms in moist forests.</p>



<p>Of course, any trend gaining so much traction and interest needs to be taken seriously. As such, I decided to familiarize myself with the topic, and turned my website into a chatbot.</p>



<p>Instead of being greeted by the internationally standardized greeting every designer used at some point in their career, there was no bold, dramatically oversized, and deep black sans-serif reading: <em>Hi, I’m a designer.</em> (To be fair, I didn’t use Proxima Nova either)</p>



<p>Instead, a couple of chat bubbles exuberantly ushered onto the canvas to greet users as if we had all been long time friends.</p>



<figure><video autoplay="" loop="" muted="" src="https://azumbrunnen.me/wp-content/uploads/chatbot-animated.mp4" playsinline=""></video><figcaption>Conversational intro</figcaption></figure>



<p>It was witty, new, and slightly awkward. People would send messages that ranged from simple chit chat, to deep philosophical topics, to downright disturbing and ridiculous insults.</p>



<p>The experiment got featured on Hackernews, Medium, was used in psychological studies conducted by Dan Ariely’s team, and the source code was ripped and edited by various startups to fit their needs. One business in the Bay Area had an idea to use it to sell flowers in a conversational way. It looks like they went out of business.</p>



<p>The reaction and feedback was surprising to say the least. It was an idea so simple, so silly, that the outcome was in many ways unexpected. After all, the only one who really cares about your website, is usually yourself.</p>



<p>That didn’t stop me from revamping my website and kill the very thing that had turned it into a micro-celebrity before. With the death of my old chatbot, some angry emails by schools who are using it as a reference for “creative” web design, and a good amount of time that has passed ever since, I wanted to take a step back and set the record straight.</p>



<hr>



<h2>When chatbots matter</h2>



<p>So let’s be honest with ourselves for a moment: <em>when did you actually ever enjoy talking to a chat bot?</em> And I’m not talking about the type of bots you talk to when you’re bored, but about those that provide a deeper purpose.</p>



<p>It turns out that the answer is, at least for most of us, almost never.</p>



<p>I love you Intercom, except when I don’t. 99% of time I don’t want to talk to a silly and obtrusive avatar popping up from some corner of the screen before I even had a chance to check out what’s going on. Somehow, I can’t help but think others feel the same.</p>



<p>In fact, we do know that others feel the same. Chat heads jumping at us unasked, are the quintessential equivalent of the infamous sales clerk who eagerly talks to us upon entering a store.</p>



<p>To further add to the challenges: as soon as users go off-script, chat bot’s don’t just become awkward and unpredictable—they turn into little sociopaths that might rub users the wrong way.</p>



<figure><img loading="lazy" width="400" height="346" src="https://azumbrunnen.me/wp-content/uploads/grandma.png" alt="" srcset="https://azumbrunnen.me/wp-content/uploads/grandma.png 400w, https://azumbrunnen.me/wp-content/uploads/grandma-300x260.png 300w" sizes="(max-width: 400px) 100vw, 400px"><figcaption>UX Chat.me —&nbsp;Conversational UX News</figcaption></figure>



<p>The moment you create a chat bot is the moment you allow customers to have a conversation with <em>your brand</em>. Not with yourself, not with your friend, but with an uber entity—a symbol—that represents everything you and your team stand for. That’s not a step to be taken lightly.</p>



<p>This simple conversational entity can be a fun tool to engage with people but depending on how the conversation goes, it can quickly turn into a misrepresentation of the values of your team and your company. So building a chat bot should never be the default choice, but an intentional one.</p>



<p>That’s why it’s worth asking yourself the following three questions before venturing into this space:</p>



<h3>1. Is your use case simple enough to be solved through chat?</h3>



<p>Conversation is incredibly complex and it’s challenging enough to keep it on track in the real world. If the use case isn’t simple, chances are, chat bots are not the right tool for the job.</p>



<h3>2. Is your NLP capable and sophisticated enough?</h3>



<p>There are two types of bots: pre-scripted bots with a range of default answers users can choose from, and Natural Language Processing based ones.</p>



<p>Choosing the right one is hard. While pre-scripted can feel too limiting, NLP can break at every corner. Often times, teams quickly fall into the trap of spending a huge amount of time focusing on personality and silly jokes, instead of solving the problem users hired you for in the first place.</p>



<p>Therefore, building on top of the first point above, within the conversational landscape, simple always wins.</p>



<h3>3. Are your users actually in chat based environments?</h3>



<p>Chat bots work best where users already are. If your users are primarily spending time in messaging platforms where bots and micro-apps can be seamlessly embedded, great. That can serve as an effective and natural way to engage with your audience because it matches the “be where users already are” principle.</p>



<p>If on the other hand, people come to your website, a medium that has made great strides to provide content in a non-linear and quick way, it often unnecessarily slows users down. </p>



<hr>



<h2>Farewell chatbot</h2>



<p>I don’t want to discredit chat bots as a paradigm. They have their use in certain industries, medium, and work well for a specific set of use cases. The important part is being deliberate, rather than jumping ship blindfolded.</p>



<p>So whereas turning my website into a chat was a fun experiment, I ultimately feel like it has slowly turned into a fad. I got fooled by the trend, and as a by-product became part of the trend itself.  Fads come and go, and as they get refined and re-interpreted, they ultimately find their true purpose. What we’re left with is the age old insight that it’s only through experimentation, that we can unlock concepts and ideas that last.</p>



<p>Rest in peace chat bot, long live chat bots.</p>
            </article></div>]]>
            </description>
            <link>https://azumbrunnen.me/blog/my-chatbot-is-dead-%C2%B7-why-yours-should-probably-be-too/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834552</guid>
            <pubDate>Tue, 20 Oct 2020 06:52:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Disney Animation data sets (2018)]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24834299">thread link</a>) | @ascorbic
<br/>
October 19, 2020 | https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html | <a href="https://web.archive.org/web/*/https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>

            
            <span>July 03, 2018 
                 | Tags: 
                     
                        Datasets
                      
                </span>

            <section>
                <p>Today at <a href="https://cg.ivd.kit.edu/egsr18/">EGSR 2018</a>, Walt Disney Animation Studios announced the release of two large, production quality/scale data sets for rendering research purposes.
The data sets are available on a new <a href="https://disneyanimation.com/data-sets/">data sets page on the official Disney Animation website</a>.
The first data set is the Cloud Data Set, which contains a large and highly detailed volumetric cloud data set that we used for our “<a href="https://blog.yiningkarlli.com/2017/07/spectral-and-decomposition-tracking.html">Spectral and Decomposition Tracking for Rendering Heterogeneous Volumes</a>” SIGGRAPH 2017 paper, and the second data set is the Moana Island Scene, which is a full production scene from <a href="https://blog.yiningkarlli.com/2016/11/moana.html">Moana</a>.</p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/shotCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/shotCam_hyperion.jpg" alt="Figure 1: The Moana Island Data Set, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/wdas_cloud_hyperion_render.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/wdas_cloud_hyperion_render.jpg" alt="Figure 2: The Cloud Data Set, rendered using Disney's Hyperion Renderer."></a></p>

<p>In this post, I’ll share some personal thoughts, observations, and notes.
The release of these data sets was announced by my teammate, Ralf Habel, at EGSR today, but this release has been in the works for a very long time now, and is the product of the collective effort of an enormous number of people across the studio.
A number of people deserve to be highlighted: Rasmus Tamstorf spearheaded the entire effort and was instrumental in getting the resources and legal approval needed for the Moana Island Scene.
Heather Pritchett is the TD that did the actual difficult work of extracting the Moana Island Scene out of Disney Animation’s production pipeline and converting it from proprietary data formats into usable, industry-standard data formats.
Sean Palmer and Jonathan Garcia also helped in resurrecting the data from Moana.
Hyperion developers Ralf Habel and Peter Kutz led the effort to get the Cloud Data Set approved and released; the cloud itself was made by artists Henrik Falt and Alex Nijmeh.
On the management side of things, technology manager Rajesh Sharma and Disney Animation CTO, <a href="https://twitter.com/ncannon?lang=en">Nick Cannon</a>, provided crucial support and encouragement.
Matt Pharr has been crucial in collaborating with us to get these data sets released.
Matt was highly accommodating in helping us get the Moana Island Scene into a PBRT scene; I’ll talk a bit more about this later.
Intel’s Embree team also gave significant feedback.
My role was actually quite small; along with other members of the Hyperion development team, I just provided some consultation throughout the whole process.</p>

<p>Please note the licenses that the data sets come with.
The Cloud Data Set is licensed under a <a href="https://disney-animation.s3.amazonaws.com/uploads/production/data_set_asset/6/asset/License_Cloud.pdf">Creative Commons Attribution ShareAlike 3.0 Unported License</a>; the actual cloud is based on a photograph by Kevin Udy on his <a href="https://coclouds.com/436/cumulus/%202012-07-26/">Colorado Clouds Blog</a>, which is also licensed under the same Creative Commons license.
The Moana Island Scene is licensed under a more restrictive, custom Disney Enterprises <a href="https://disney-animation.s3.amazonaws.com/uploads/production/data_set_asset/4/asset/License_Moana.pdf">research license</a>.
This is because the Moana Island Scene is a true production scene; it was actually used to produce actual frames in the final film.
As such, the data set is being released only for pure research and development purposes; it’s not meant for use in artistic projects.
Please stick to and follow the licenses these data sets are released under; if people end up misusing these data sets, then it makes releasing more data sets into the community in the future much harder for us.</p>

<p>This entire effort was sparked two years ago at SIGGRAPH 2016, when Matt Pharr made an appeal to the industry to provide representative production-scale data sets to the research community.
I don’t know how many times I’ve had conversations about how well new techniques or papers or technologies will scale to production cases, only to have further discussion stymied by the lack of any true production data sets that the research community can test against.
We decided as a studio to answer Matt’s appeal, and last year at SIGGRAPH 2017, Brent Burley and Rasmus Tamstorf announced our intention to release both the Cloud and Moana Island data sets.
It’s taken nearly a year from announcement to release because the process has been complex, and it was very important to the studio to make sure the release was done properly.</p>

<p>One of the biggest challenges was getting all of the data out of the production pipeline and our various proprietary data formats into something that the research community can actually parse and make use of.
Matt Pharr was extremely helpful here; over the past year, Matt has added support for <a href="http://ptex.us/">Ptex</a> textures and implemented the <a href="http://blog.selfshadow.com/publications/s2015-shading-course/burley/s2015_pbs_disney_bsdf_notes.pdf">Disney Bsdf</a> in <a href="https://github.com/mmp/pbrt-v3">PBRT v3</a>.
Having Ptex and the Disney Bsdf available in PBRT v3 made PBRT v3 the natural target for an initial port to a renderer other than Hyperion, since internally all of Hyperion’s shading uses the Disney Bsdf, and all of our texturing is done through Ptex.
Our texturing also relies heavily on procedural <a href="https://www.disneyanimation.com/technology/seexpr.html">SeExpr</a> expressions; all of the expression-drive texturing had to be baked down into Ptex for the final release.</p>

<p>Both the Cloud and Moana Island data sets are, quite frankly, enormous.
The Cloud data set contains a single OpenVDB cloud that weighs in at 2.93 GB; the data set also provides versions of the VDB file scaled down to half, quarter, eighth, and sixteenth scale resolutions.
The Moana Island data set comes in three parts: a base package containing raw geometry and texture data, an animation package containing animated stuff, and a PBRT package containing a PBRT scene generated from the base package.
These three packages combined, uncompressed, weigh in at well over 200 GB of disk space; the uncompressed PBRT package along weighs in at around 38 GB.</p>

<p>For the Moana Island Scene, the provided PBRT scene requires a minimum of around 90 GB if RAM to render.
This many seem enormous for consumer machines, because it is.
However, this is also what we mean by “production scale”; for Disney Animation, 90 GB is actually a fairly mid-range memory footprint for a production render.
On a 24-core, dual-socket Intel Xeon Gold 6136 system, the PBRT scene took me a little over an hour and 15 minutes to render from the ‘shotCam’ camera.
Hyperion renders the scene faster, but I would caution against using this data set to do performance shootouts between different renders.
I’m certain that within a short period of time, enthusiastic members of the rendering community will end up porting this scene to Renderman and Arnold and Vray and Cycles and every other production renderer out there, which will be very cool!
But keep in mind, this data set was authored very specifically around Hyperion’s various capabilities and constraints, which naturally will be very different from how one might author a complex data set for other renderers.
Every renderer works a bit differently, so the most optimal way to author a data set for every renderer will be a bit different; this data set is no exception.
So if you want to compare renderers using this data set, make sure you understand the various ways how the way this data set is structured impacts the performance of whatever renderers you are comparing.</p>

<p>For example, Hyperion subdivides/tessellates/displaces everything to as close to sub-poly-per-pixel as it can get while still fitting within computational resources.
This means our scenes are usually very heavily subdivided and tessellated.
However, the PBRT version of the scene doesn’t come with any subdivision; as a result, silhouettes in the following comparison images don’t fully match in some areas.
Similarly, PBRT’s lights and lighting model differ from Hyperion’s, and Hyperion has various artistic controls that are unique to Hyperion, meaning the renders produced by PBRT versus Hyperion differ in many ways:</p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/shotCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/shotCam_hyperion.jpg" alt="Figure 3a: 'shotCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/shotCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/shotCam_pbrt.jpg" alt="Figure 3b: 'shotCam' camera angle, rendered using PBRT v3."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/beachCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/beachCam_hyperion.jpg" alt="Figure 4a: 'beachCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/beachCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/beachCam_pbrt.jpg" alt="Figure 4b: 'beachCam' camera angle, rendered using PBRT v3."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/dunesACam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/dunesACam_hyperion.jpg" alt="Figure 5a: 'dunesACam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/dunesACam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/dunesACam_pbrt.jpg" alt="Figure 5b: 'dunesACam' camera angle, rendered using PBRT v3. Some of the plants are in slightly different locations than the Hyperion render; this was just a small change that happened in data conversion to the PBRT scene."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/flowersCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/flowersCam_hyperion.jpg" alt="Figure 6a: 'flowersCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/flowersCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/flowersCam_pbrt.jpg" alt="Figure 6b: 'flowersCam' camera angle, rendered using PBRT v3. Note that the silhouette of the flowers is different compared to the Hyperion render because the Hyperion render subdivides the flowers, whereas the PBRT render displays the base cage."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/grassCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/grassCam_hyperion.jpg" alt="Figure 7a: 'grassCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/grassCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/grassCam_pbrt.jpg" alt="Figure 7b: 'grassCam' camera angle, rendered using PBRT v3. The sand dune in the background looks particularly different from the Hyperion render due to subdivision and displacement."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/palmsCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/palmsCam_hyperion.jpg" alt="Figure 8a: 'palmsCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/palmsCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/palmsCam_pbrt.jpg" alt="Figure 8b: 'palmsCam' camera angle, rendered using PBRT v3. The palm leaves look especially different due to differences in artistic lighting shaping and curve shading differences. Most notably, the look in Hyperion depends heavily on attributes that vary along the length of the curve, which is something PBRT doesn't support yet. Some more work is needed here to get the palm leaves to look more similar between the two renders."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/rootsCam_hyperion.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/rootsCam_hyperion.jpg" alt="Figure 9a: 'rootsCam' camera angle, rendered using Disney's Hyperion Renderer."></a></p>

<p><a href="https://blog.yiningkarlli.com/content/images/2018/Jul/rootsCam_pbrt.png"><img src="https://blog.yiningkarlli.com/content/images/2018/Jul/preview/rootsCam_pbrt.jpg" alt="Figure 9b: 'rootsCam' camera angle, rendered using PBRT v3. Again, the significant difference in appearance in the rocks is probably just due to subdivision/tesselation/displacement."></a></p>

<p>Another example of a major difference between the Hyperion renders and the PBRT renders is in the water, which Hyperion renders using photon mapping to get the caustics.
The provided PBRT scenes use unidirectional pathtracing for everything including the water, hence the very different caustics.
Similarly, the palm trees in the ‘palmsCam’ camera angle look very different between PBRT and Hyperion because Hyperion’s lighting controls are very different from PBRT; Hyperion’s lights include various artistic controls for custom shaping and whatnot, which aren’t necessarily fully physical.
Also, the palm leaves are modeled using curves, and the shading depends on varying colors and attributes along the length and width of the curve, which PBRT doesn’t support yet (getting the palm leaves is actually the top priority for if more resources are freed up to improve the data set release).
These difference between renderers don’t necessarily mean that one renderer is better than the other; they simply mean that the renderers are different.
This will be true for any pair of renderers that one wants to compare.</p>

<p>The Cloud Data Set includes an example render from Hyperion, which implements our Spectral and Decomposition Tracking paper in its volumetric rendering system to efficiently render the cloud with thousands of bounces.
This render contains no post-processing; what you see in the provided image is exactly what Hyperion outputs.
The VDB file expresses the cloud as a field of heterogeneous densities.
Also provided is an example <a href="https://www.mitsuba-renderer.org/">Mitsuba</a> scene, renderable using the <a href="https://github.com/zhoub/mitsuba-vdb">Mitsuba-VDB plugin that can be found on Github</a>.
Please consult the README file for some modifications in Mitsuba that are necessary to render the cloud.
Also, please note that the Mitsuba example will take an extremely long time to render, since Mitsuba isn’t really meant to render high-albedo heterogeneous volumes.
With proper acceleration structures and algorithms, rendering the cloud only takes us a few minutes using Hyperion, and should be similarly fast in any modern production renderer.</p>

<p>One might wonder just why production data sets in general are so large.
This is an interesting question; the short answer across the industry basically boils down to “artist time is more expensive and valuable than computer hardware”.
We could get these scenes to fit into much smaller …</p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html">https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html</a></em></p>]]>
            </description>
            <link>https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24834299</guid>
            <pubDate>Tue, 20 Oct 2020 05:54:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Moana Motunui Renderer on GPU]]>
            </title>
            <description>
<![CDATA[
Score 325 | Comments 47 (<a href="https://news.ycombinator.com/item?id=24833218">thread link</a>) | @Impossible
<br/>
October 19, 2020 | https://www.render-blog.com/2020/10/03/gpu-motunui/ | <a href="https://web.archive.org/web/*/https://www.render-blog.com/2020/10/03/gpu-motunui/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
  
  <p><span>03 Oct 2020</span></p><p>Disney Animation’s Moana island dataset is a production-scale scene with memory requirements that make it challenging to render. This post summarizes some of those challenges, and describes how the <a href="https://github.com/chellmuth/gpu-motunui">GPU-Motunui</a> project is able to efficiently render the scene on a consumer-grade GPU with less than 8GB of memory. <a href="#renders">Click here</a> to skip ahead to the results.</p>

<h2 id="the-moana-island">The Moana island</h2>

<p>In 2018, Disney Animation released the Moana island dataset to the rendering research community. Compared to traditional research scenes, the scale of the Moana island scene is massive: the scene contains 90 million quad primitives, 5 million curves, and more than 28 million instances. All told, the island consists of over 15 billion primitives, weighing in at just under 30GB of geometry files.</p>

<p>The shots included with the dataset are beautiful, and showcase the amazing imagery that can be created by combining the best artists in the world with path tracing techniques and modern hardware. Here are two reference images, rendered with Disney’s proprietary Hyperion renderer:</p>

<div>
  <p><img src="https://www.render-blog.com/assets/hyperion-reference-shotCam.png" alt="Hyperion shotCam reference"></p><p>Hyperion shotCam reference</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/hyperion-reference-beachCam.png" alt="Hyperion beachCam reference"></p><p>Hyperion beachCam reference</p>
</div>

<h2 id="gpu-motunui-project">GPU-Motunui Project</h2>

<p>The goal of the GPU-Motunui project is to render all the Moana shots efficiently and accurately on a consumer-grade graphics card. There are two main challenges to accomplishing this with the Moana dataset. First, with a typical graphics card having only 8GB of memory, an out-of-core rendering solution is required to handle the large amounts of geometry. Second, the scene’s textures are provided in the Ptex format, and Ptex doesn’t have a publicly available CUDA implementation. This project currently only solves the first problem, and Ptex texture lookup is done on the CPU (although conveniently its cost is fully hidden by being computed concurrently with GPU shadow ray tracing).</p>

<p>The Hyperion reference images are impossible to match exactly; for example the varying brown and green colors along the palm tree fronds in the palmsCam shot are not provided in the dataset. Other features of the scene are possible to render but out of my initial scope, notably subdivision surfaces and their displacement maps, and a full Disney BSDF implementation.</p>

<div>
  <p><img src="https://www.render-blog.com/assets/hyperion-unique-palmsCam.png" alt="Example of an unreproducible material variation on the palm tree frond"></p><p>Example of an unreproducible material variation on the palm tree frond</p>
</div>

<p>All ray tracing operations are run through Nvidia’s OptiX 7 API. This means GPU-Motunui gets the full benefits of available RT cores and a world-class BVH implementation. The following sections describe how GPU-Motunui maps dataset assets to OptiX data structures, and how GPU-Motunui’s out-of-core rendering solution works.</p>

<h3 id="scene-representation">Scene representation</h3>

<p>The Moana scene makes widespread use of multi-level instancing. In OptiX, this requires a three-level hierarchy of acceleration structures to manage: two levels of IASs, and a base level of GASs (Instance Acceleration Structures and Geometry Acceleration Structures, respectively). GPU-Motunui makes use of OptiX’s AS compaction and relocation APIs to further reduce memory usage.</p>

<p>The isHibiscus element makes a good example of how a typical element in the scene is organized and built. The tree is assembled from a base model in one Wavefront .obj file (containing the trunk and branches), and four primitives: one flower and three leaf models (each with their own .obj file).</p>

<div>

<p>Left: The four simple primitives that will be instanced to fill out the hibiscus tree <br>Right: The base trunk and branches model </p>
</div>

<p>In OptiX, each of these models has an associated GAS, and each GAS can be subdivided into multiple build inputs. Build inputs are used to map sections of the model to information needed at shading time by indexing into OptiX’s shader binding table. These GASs form the bottom level of the hierarchy.</p>

<p>Next, an IAS is used to build the full isHibiscus element. This IAS is in the middle level of the hierarchy. The figure below shows each primitive’s instances in isolation, and combined to make the full element:</p>

<div>

<p>Left: Isolated instances for each primitive<br>Right: Full isHibiscus element</p>
</div>

<p>Finally, a second IAS is built to track all of the element’s instances present in the scene. This second IAS is the top level of the instance hierarchy.</p>

<div>
  <p><img src="https://www.render-blog.com/assets/isHibiscus-instanced-elements.png" alt="The shotCam view rendered with only isHibiscus instances"></p><p>The shotCam view rendered with only isHibiscus instances</p>
</div>

<p>Although the isHibiscus element has a typical structure, there are some more complicated elements included in the dataset. The isCoral element, for example, has different base geometry and instanced primitives for each of its element instances, but the underlying primitive geometries are shared across all the element instances.</p>

<p>The Moana GAS and IASs alone require 18.5 GB, well past the memory budget of my 8GB RTX 2070. Because OptiX has no native support for out-of-core rendering, the traditional OptiX pipeline had to be put aside for a custom-made solution.</p>

<h3 id="out-of-core-rendering">Out-of-core rendering</h3>

<p>To solve the out-of-core rendering problem, GPU-Motunui divides the scene’s geometry into different sections, and ray traces each separately, while tracking the closest hit. Replacing a traditional device trace call with a host loop comes with many design consequences to the renderer, from asset loading to the core path tracing loop that sends rays through the scene.</p>

<p>Before rendering, the asset loading process allocates a large chunk of GPU memory (currently 6.7GB). A custom allocator is implemented that manages this block of memory. It is responsible for allocating two types of memory: output and temporary. Output memory is allocated from the left of the block, and is used for OptiX structures. Temporary memory is managed on a stack from the right end of the memory block. Managing the temporary memory this way ensures that the output structures are always tightly packed.</p>

<p>After elements are processed into their accelerator structures on the GPU, their used memory is snapshotted onto the host, and the allocator is cleared. The process is repeated until all of the scene’s geometry is processed, resulting in the host managing a list of GPU memory snapshots. The figure below shows an example layout of GPU memory that could be snapshotted:</p>

<div>
  <p>GPU memory layout after loading the isHibiscus element.<br>(Dotted arrows show that an IAS holds instances of the pointed-at AS)</p>

</div>

<p>As mentioned above, when it comes time to ray trace, each snapshot is processed in a loop. This means a call to <code>cudaMemcpy</code> and <code>optixLaunch</code> for each snapshot. A global buffer is maintained that indicates the depth of the current closest intersection. This value is used as the <code>tmax</code> parameter for the CUDA kernel’s call to <code>optixTrace</code>, and a successful intersection will update the depth buffer for the next launch.</p>

<p>In a traditional OptiX path tracer, the entire render loop can run in device code inside a single call to <code>optixLaunch</code>; i.e., a successful intersection will lead to more BSDF and shadow rays being traced in the same kernel launch. Because GPU-Motunui’s design mandates multiple launches for tracing each path segment, the render loop is pulled out into host code. While this potentially diminishes OptiX’s ability to efficiently schedule program execution, it also opens up opportunties for optimization, such as running Ptex texture lookups on the CPU concurrently with GPU kernels and I/O.</p>

<h3 id="shading">Shading</h3>
<p>As with any OptiX application, GPU-Motunui makes use of the shader binding table (SBT). SBT records contain pointers to normal buffers and material attributes. The underlying data for the normal buffers is stored alongside OptiX acceleration structures and included in geometry snapshots. This ensures that GPU memory is never wasted on unreachable normal buffer data.</p>

<h2 id="renders">Renders</h2>
<p>Included below are GPU-Motunui renders of the six scenes included in the dataset. shotCam is the slowest to render at 18.2 seconds per sample at 1024x429 resolution, and took just over five hours total for the final image. All shots are 1024spp, capped at a maximum of five bounces, and were run on an Nvidia RTX 2070.</p>
<div>
  <p><img src="https://www.render-blog.com/assets/ours-shotCam.png" alt="shotCam"></p><p>shotCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-beachCam.png" alt="beachCam"></p><p>beachCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-dunesACam.png" alt="dunesACam"></p><p>dunesACam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-palmsCam.png" alt="palmsCam"></p><p>palmsCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-birdseyeCam.png" alt="birdseyeCam"></p><p>birdseyeCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-rootsCam.png" alt="rootsCam"></p><p>rootsCam</p>
</div>

<div>
  <p><img src="https://www.render-blog.com/assets/ours-grassCam.png" alt="grassCam"></p><p>grassCam</p>
</div>

<h2 id="optimization">Optimization</h2>
<p>The initial implementation of the renderer required 42.6 seconds per 1spp on the shotCam scene. A few optimizations combined to make significant reductions in rendering time, cutting each pass down to 18.2 seconds (a 57.3% reduction).</p>

<h4 id="cpugpu-concurrency">CPU/GPU concurrency</h4>
<p>Tracing shadow rays on the GPU in parallel with Ptex lookups on the CPU cut rendering time by 23.4%. It was disappointing to be forced to do texture lookups on the CPU, but the time savings make up for it.</p>

<h4 id="multiple-ptex-caches">Multiple Ptex caches</h4>
<p>Parallelizing the Ptex lookups and using multiple Ptex caches eliminated texture lookups as a bottleneck to the system; shadow ray casting time fully dominates the texture lookup. Empirically, spawning two threads per core (totaling 12 on an Intel i7-8700K) and sharing three Ptex caches comfortably reduced the texture lookup time beneath the shadow ray budget. This improved the time savings to a 33.9% reduction over the baseline.</p>

<h4 id="pinned-memory">Pinned memory</h4>
<p>The acceleration structure snapshots are all saved to pinned host memory. Switching from normal to pinned host memory increased the transfer throughput from 7.73 GB/s to 11.84 GB/s, cutting the baseline render time by 19.5%.</p>

<h2 id="future-steps">Future Steps</h2>
<p>Getting this scene running on my RTX 2070 card was a very fun and rewarding project, but there are still many improvements to be made:</p>
<ul>
  <li>Implementing the Disney BSDF</li>
  <li>Rendering subdivision surfaces along with displacement mapping</li>
  <li>More efficiently packing the acceleration structures, and optimizing ray tracing throughput</li>
  <li>Experimenting with how various research results hold up on production scenes (e.g., testing select path guiding techniques)</li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://github.com/chellmuth/gpu-motunui/">GPU-Motunui</a></li>
  <li><a href="https://technology.disneyanimation.com/islandscene/">Moana Island Scene</a></li>
  <li><a href="https://pharr.org/matt/blog/2018/07/16/moana-island-pbrt-all.html">Swallowing the elephant</a> - Matt Pharr</li>
  <li><a href="https://blog.yiningkarlli.com/2018/07/disney-animation-datasets.html">Disney Animation Data Sets</a> - Yining Karl Li</li>
  <li><a href="https://schuttejoe.github.io/post/disneypostmortem/">Rendering the Moana Island Scene Part 2: A production scene from a hobby renderer</a> - Joe Schutte</li>
  <li><a href="https://ingowald.blog/2020/01/09/digesting-the-elephant/">Digesting the elephant</a> - Ingo Wald</li>
  <li>Brent Burley and Dylan Lacewell. <a href="http://ptex.us/ptexpaper.html">Ptex: …</a></li></ul></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.render-blog.com/2020/10/03/gpu-motunui/">https://www.render-blog.com/2020/10/03/gpu-motunui/</a></em></p>]]>
            </description>
            <link>https://www.render-blog.com/2020/10/03/gpu-motunui/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24833218</guid>
            <pubDate>Tue, 20 Oct 2020 01:45:55 GMT</pubDate>
        </item>
    </channel>
</rss>
