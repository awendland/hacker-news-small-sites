<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 02 Jul 2020 12:20:05 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 02 Jul 2020 12:20:05 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Implementing a System Call for OpenBSD]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 14 (<a href="https://news.ycombinator.com/item?id=23685279">thread link</a>) | @soheilpro
<br/>
June 29, 2020 | https://poolp.org/drafts/2020-05-28-015100-copy/ | <a href="https://web.archive.org/web/*/https://poolp.org/drafts/2020-05-28-015100-copy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://poolp.org/drafts/2020-05-28-015100-copy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23685279</guid>
            <pubDate>Tue, 30 Jun 2020 00:43:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Things to know before starting a Patreon page]]>
            </title>
            <description>
<![CDATA[
Score 186 | Comments 53 (<a href="https://news.ycombinator.com/item?id=23680591">thread link</a>) | @colebowl
<br/>
June 29, 2020 | http://pencerw.com/feed/2020/6/24/three-things-all-creators-should-know-about-patreon?mc_cid=cb178f4bd4&mc_eid=5ac3a20371 | <a href="https://web.archive.org/web/*/http://pencerw.com/feed/2020/6/24/three-things-all-creators-should-know-about-patreon?mc_cid=cb178f4bd4&mc_eid=5ac3a20371">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-0e74a9a2acebd70c05fe"><div><p>For the three years starting in April of 2017, I ran much of <a href="http://theprepared.org/newsletter" target="_blank">The Prepared’s</a> (and ultimately my family’s) income through Patreon. I started doing so as an experiment - one that by any measure has been a success. But while Patreon was instrumental in that process, I recommend that creators <strong>not</strong> structure their incomes and careers around Patreon. Here’s why.</p><p>Like many creators, I chose Patreon’s “pay by the creation” (rather than “pay by the month) mode. This directly incentivizes creators to continue doing the actual work, and keeps them accountable to the commitments they make. </p><p>But what Patreon doesn’t tell you is that fans can optionally set a monthly cap on their spending, and that cap can be arbitrarily low - even <strong>less than your per-creation commitment level.</strong> In other words, a reader of my weekly newsletter could pledge $5 per newsletter, but then set a $2 monthly cap. The worst part about this is that there’s literally nowhere in the Patreon backend that I can see this cap. I spoke to Patreon’s product team about this in late 2018, and they told me that the best thing I could do is to look at my creation-by-creation analytics at the end of the month and see which of my patrons paid for which creations; if a person doesn’t show up at the end of the month, then they must have set a cap.</p><p>This is a totally unscalable solution, and it makes the process of issuing patron rewards excruciatingly hard to manage. Creators need the ability to quickly and easily determine <strong>who</strong> is paying them for <strong>what</strong>; Patreon makes this impractically hard.</p><p>Further:</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1593267194369_49688"><div><ul data-rte-list="default"><li><p>Patreon provides email alerts for when a new patron makes a pledge, but has <strong>no email or push notifications for when patrons delete pledges.</strong> </p></li><li><p>Patreon has no creator-side notification system for declined charges or charges that are flagged for fraud. Worse yet, their patron-side notification system appears to be totally ineffective; many long time patrons (and personal friends of mine) were genuinely shocked to hear, many months later, that their monthly charges had been declined - leading to their pledges being automatically canceled by Patreon. Even worse, Patreon’s “Declines” page, which shows the total declined amount on a month by month basis, has no way of showing which patrons’ pledges were declined - you instead need to go into the “Relationship Manager” and filter by “Declined” to see whose charges have gone through, and when.</p></li><li><p>If you, as a creator, go through all of the effort to find charges that<em> </em>have been declined or marked as fraud, it can then be really difficult to recoup that revenue. This is mostly a result of the fact that most Patreon creators charge a small amount of money (a couple dollars) per month. In theory you could email or message the patron when their charge doesn’t go through, but in practice it feels a bit weird to be hounding someone over (say) $4. If the pledge was billed on an annual basis, though, it might be a big enough sum to warrant the effort. </p></li><li><p>Patreon uses accounting terms with little regard for their generally accepted meaning. See the screenshot above, which is titled “Earnings Projections” but then actually lists <em>gross revenue.</em> In accounting, <em>earnings</em> is the same as <em>profit - </em>it’s what a company has left <strong>after every expense is paid, </strong>whereas <em>gross revenue</em> is the total amount that a company takes in and doesn’t take into account expenses at all. In other words, Patreon is suggesting that the numbers here are what will be deposited into my bank account - but once Patreon takes their platform fees, it’ll actually be significantly less. This kind of sloppy terminology is all over Patreon’s creator backend, and no matter how you slice it is either the result of gross incompetence or a deliberate desire to deceive creators.</p></li></ul><p>Between credit card processing fees (2.9% plus $0.30 per transaction) and Patreon’s cut (between 5% and a whopping 12%), your earnings will be <em>significantly</em> less than your top line pledged amount. In practice, I saw total fees of between 8-12%. (Note: I signed up for Patreon before they shifted to tiered pricing, and now have a “Founder” Pro plan at a 5% platform fee rate. If you signed up for a Pro or Premium account today, you’d pay Patreon 3% or 7% more than I do, respectively.)</p><p>If Patreon were actively bringing customers to me - if normal people were just out there browsing Patreon for awesome things to support - then that might make sense. <strong>But the reality is that success on Patreon is inextricably tied to having your own platform and community.</strong> All Patreon does is manage recurring payment processing - a commodity service that many companies do for a drastically lower fee structure. Sure, ostensibly you can also be having conversations with patrons, generating some kind of community there, etc - but every step you take to encourage users to interact with you on Patreon, the more you undermine your own platform. In other words, Patreon engages in rent seeking - but they ultimately do it on <strong>your</strong> platform, and don’t bring a built-in audience with which to raise you higher.</p><p>When I transitioned off of Patreon, I moved to a combination of Quickbooks Online ($645/year; note that <a href="https://www.propublica.org/article/inside-turbotax-20-year-fight-to-stop-americans-from-filing-their-taxes-for-free" target="_blank">Intuit is a terrible company</a>) and Squarespace’s ($480/year) recurring products feature. The result is that my processing fees dropped dramatically. At my peak Patreon earnings, I was spending almost $300/month ($3600/year) on Patreon’s platform fees. My current revenue is roughly 3x what it was then, but I’m paying 68% less than I used to be.<strong> My current payments, web hosting, and accounting software outlay is $1,125 a year; if I had remained on Patreon my annual fees would be about $10,000.</strong></p><p>Okay, you’re saying - so Patreon isn’t the perfect all-in-one platform that will allow me to bill, chat with, and build my audience. But maybe it’s a piece of a larger puzzle?</p><p>It’s a great idea, but unfortunately Patreon does a terrible job integrating with the other services that I use to run my business.</p><p>The first thing I’d want from Patreon is an easy way to automatically share my content (which most creators distribute elsewhere - for me, it’s Mailchimp) to Patreon. But while Patreon does have a public API, it’s poorly developed (there is no sandbox/testing area, and the most recent updates to <a href="https://github.com/Patreon/patreon-python" target="_blank">their API libraries</a> are from January of 2019) and only allows browsing/looking up data on Patreon; you cannot post content to your Patreon account via the API. This lack of functionality also exists in Zapier’s implementation of the Patreon API: You can use Patreon as a trigger, but not as an action.</p><p>What this means is that creators are inherently tied to Patreon’s terrible, horrible, clicky clicky GUI. You are completely tied to the limitations that are built into Patreon’s web product, and don’t have the ability to build automations that’ll speed up your content and customer management.</p><p>Patreon also fails to integrate well with accounting software - something that flies in the face of their promise to give creators “the stability you need to build an independent creative&nbsp;career.” Their API (and Zapier’s implementation of it) only provides <em>pledge</em> activity, and is therefore inaccurate (caps, declines, and fraud aren’t factored in - it’s a guesstimate at what you might make in the future) in all of the ways described above.</p><p>I really can’t stress this enough: <strong>If your intention is to build a meaningful income, there are much better options out there than Patreon. </strong>What Patreon <em>does</em> offer is a quick way to see whether people on the internet will pay you a little money for something that you’re already doing for free. </p><p>This is a nontrivial thing, but it’s something that you should really think through before you start a Patreon page. If it’s a success, then it’ll likely make a lot of sense for you to transition <em>off</em> of Patreon at some point in the foreseeable future. That might be fine - especially if you’re really early on and success feels like a longshot - but The Prepared’s transition off of Patreon required a lot of management on my part and resulted in roughly 1/3 of my patrons dropping their pledges. </p><p>To be clear: I’m deeply appreciative of all of the people and companies who supported me through Patreon, and it really is true that those first couple of dollars made a big impact in the path of my career. But Patreon as a platform did remarkably little to support me along that journey, even after I became a moderately successful creator and took quite a bit of time to explain my frustrations to both their customer service &amp; user research teams.</p></div></div></div>]]>
            </description>
            <link>http://pencerw.com/feed/2020/6/24/three-things-all-creators-should-know-about-patreon?mc_cid=cb178f4bd4&amp;mc_eid=5ac3a20371</link>
            <guid isPermaLink="false">hacker-news-small-sites-23680591</guid>
            <pubDate>Mon, 29 Jun 2020 18:23:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Deep Chernoff Faces]]>
            </title>
            <description>
<![CDATA[
Score 135 | Comments 32 (<a href="https://news.ycombinator.com/item?id=23679014">thread link</a>) | @pxx
<br/>
June 29, 2020 | https://www.ihatethefuture.com/2020/06/deep-chernoff-faces.html | <a href="https://web.archive.org/web/*/https://www.ihatethefuture.com/2020/06/deep-chernoff-faces.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-7486848344071731398" itemprop="description articleBody"><p>
One of my favorite<a href="#foot1" id="cite1">¹</a> concepts for multi-dimensional data visualization is the <a href="https://en.wikipedia.org/wiki/Chernoff_face">Chernoff Face</a>. The idea here is that for a dataset with many dependent variables, it is often difficult to immediately understand the influences one variable may have on another. However, humans are great at recognizing small differences in faces, so maybe we can leverage that!</p><h2>
Tired: traditional plotting</h2><p>
The example from Wikipedia plots the differences between a few judges on some rating dimensions:</p><p><a href="https://1.bp.blogspot.com/-I_lJGg7QnmM/Xvl1rXrnm-I/AAAAAAAA534/0wm9LMMmQ0kgtxdNq9exqELo-jgSsqCMACLcBGAsYHQ/s1600/1000px-Chernoff_faces_for_evaluations_of_US_judges.svg.png"><img data-original-height="750" data-original-width="1000" height="240" src="https://1.bp.blogspot.com/-I_lJGg7QnmM/Xvl1rXrnm-I/AAAAAAAA534/0wm9LMMmQ0kgtxdNq9exqELo-jgSsqCMACLcBGAsYHQ/s320/1000px-Chernoff_faces_for_evaluations_of_US_judges.svg.png" width="320"></a></p><p>
which already improves on the "traditional" way to present such data, which is something similar to a line chart:</p><p><a href="https://1.bp.blogspot.com/-vFSg9Y-NJYg/XvmB6rUCFmI/AAAAAAAA54g/pXmUUEPqWf4QXqrasbL_WxSjN3zWu1_TwCLcBGAsYHQ/s1600/x.png"><img data-original-height="353" data-original-width="583" height="193" src="https://1.bp.blogspot.com/-vFSg9Y-NJYg/XvmB6rUCFmI/AAAAAAAA54g/pXmUUEPqWf4QXqrasbL_WxSjN3zWu1_TwCLcBGAsYHQ/s320/x.png" width="320"></a></p>
<p>
or a radar chart:</p><p><a href="https://www.blogger.com/blogger.g?blogID=2308943333455824725&amp;useLegacyBlogger=true"></a><a href="https://www.blogger.com/blogger.g?blogID=2308943333455824725&amp;useLegacyBlogger=true"></a><a href="https://1.bp.blogspot.com/-Dh3hoPt0f5A/XvmBsL-eM7I/AAAAAAAA54c/r0KXd9byh1wQ06Yp5Y3Ds-LV44p_DEfpQCLcBGAsYHQ/s1600/x.png"><img data-original-height="356" data-original-width="591" height="192" src="https://1.bp.blogspot.com/-Dh3hoPt0f5A/XvmBsL-eM7I/AAAAAAAA54c/r0KXd9byh1wQ06Yp5Y3Ds-LV44p_DEfpQCLcBGAsYHQ/s320/x.png" width="320"></a></p>


<p>
Clearly<a href="#foot2" id="cite2">²</a> the Chernoff faces are the best way to present this data, but they leave some of the gamut unexplored.</p>

<h2>
Wired: using GANs to synthesize faces</h2>


<p>
The GAN technique is pretty easily summarized. You set up two neural networks, a generator, which tries to generate realistic images, and a discriminator, which tries to distinguish between the output of the generator and a real corpus of images. By training the networks together, adding more layers, fooling around with hyperparameters and overall "the scientific process", you manage to get results where the generator network is able to fool the discriminator (and humans) with high-quality images of faces that do not actually map to anybody in the real world. StyleGAN improves on some of the basic structure here by using a novel generator architecture that stacks a bunch of layers together and ends up with an intermediate latent space that has "nice" properties.</p>

<p><a href="https://1.bp.blogspot.com/-JxAnoglX3VU/Xvl-BPsI1XI/AAAAAAAA54I/HTbKb0qKkhE88y4HzqyHwele2NAzZM0WgCLcBGAsYHQ/s1600/n9fgba8b0qr01.png"><img data-original-height="1461" data-original-width="1600" height="292" src="https://1.bp.blogspot.com/-JxAnoglX3VU/Xvl-BPsI1XI/AAAAAAAA54I/HTbKb0qKkhE88y4HzqyHwele2NAzZM0WgCLcBGAsYHQ/s320/n9fgba8b0qr01.png" width="320"></a></p>
<p><i>Basically how it works.</i></p>
<br>
<div><p>
A trained StyleGAN (1 or 2; the architecture for the dimensions does not change between versions), at the end of the day, takes a 512 element vector in the latent space "Z", then sends it through some </p><strike>nonsense</strike><p> fully-connected layers to form a "dlatent"<a href="#foot3" id="cite3">³</a> vector of size 18x512. The 18 here indicates how many layers there are in the generator proper—the trained networks that NVIDIA provides produce 1024x1024 images; there are 2 layers for each of the dimensions from 2^2=4 to 2^10=1024.</p></div>

<p><a href="https://1.bp.blogspot.com/-RLpfhFZBSuo/Xvl-Wzo0B2I/AAAAAAAA54Q/ztjxSsS_JJYSz9MZBXnXBYuGC2IkT9SAACLcBGAsYHQ/s1600/x.png"><img data-original-height="767" data-original-width="267" height="320" src="https://1.bp.blogspot.com/-RLpfhFZBSuo/Xvl-Wzo0B2I/AAAAAAAA54Q/ztjxSsS_JJYSz9MZBXnXBYuGC2IkT9SAACLcBGAsYHQ/s320/x.png" width="111"></a></p>
<p><i>What part of STACK MORE LAYERS did you not get?</i></p>
<br>
<div>
<p><a href="https://www.blogger.com/blogger.g?blogID=2308943333455824725&amp;useLegacyBlogger=true"></a><a href="https://www.blogger.com/blogger.g?blogID=2308943333455824725&amp;useLegacyBlogger=true"></a>The upshot is that the 18x512 space has nice linearity properties that will be useful to us when we want to generate photorealistic faces that only differ on one axis of importance. The authors of the NVIDIA paper call each of the 18 layers a "style", and the observation is that copying qualities from each style gives qualitatively different results.</p><p>

The styles that correspond to coarse resolutions bring high-level aspects like pose, hair style, face shape; the styles that correspond to fine resolutions bring low-level aspects like color scheme. But they only roughly correspond to these, so it's going to be somewhat annoying to play around with the latent space! What we need is some way to automatically classify these images for the properties we want to use in our Chernoff faces...</p></div>
<p>
<br>
<h3>
Putting it all together</h3>
</p>
<div><p>
What I did was take an <a href="https://github.com/rosasalberto/StyleGAN2-TensorFlow-2.x">unofficial re-implementation of StyleGAN2</a> and run it to generate 4096 random images (corresponding actually to random seeds 0 - 4095 in the original repository). The reason why we're using an unofficial implementation is that the unofficial implementation ports everything to TensorFlow 2, which also enabled me to run it with (a) less GPU RAM (I only have a GTX 1080 at home) and (b) on the CPU if needed for a future project where I serve these images dynamically.</p><p>

I was also too lazy to train a network to recognize any features, so I fed these images through <a href="https://www.kairos.com/pricing">Kairos</a>'s free trial, receiving API responses that roughly looked like <a href="https://pastebin.com/PXWDKkYf">this</a>. (There's no code for this part, you can just do it with cURL<a href="#foot4" id="cite4">⁴</a>).</p><p>

<a href="https://www.blogger.com/blogger.g?blogID=2308943333455824725&amp;useLegacyBlogger=true"></a><a href="https://www.blogger.com/blogger.g?blogID=2308943333455824725&amp;useLegacyBlogger=true"></a>Then, somewhere in this <a href="https://github.com/patrickxia/StyleGAN2-TensorFlow-2.x/blob/master/play_around_with_directions.ipynb">gigantic unorganized notebook</a>&nbsp;that I need to clean up, I train some <a href="https://en.wikipedia.org/wiki/Support_vector_machine#Linear_SVM">linear SVMs</a> to split our sample of the latent space; eventually I will clean this up and make it so it's a little more automated than the crazy experimentation I was doing. After I train the SVMs and receive the normal vector from their separating hyperplanes, I manually explore the latent space by considering only one style's worth of elements from each of the normal vectors, plotting the results, and seeing what changes about the photos (which could be different than the feature from the API response!). This could probably be automated; I should likely use the <a href="https://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a> of the Kairos API responses given the SVM result to rank which styles are most important and then automatically return that instead of manually pruning styles.<br>
<span><br></span>
<span>I ended up with seven usable properties plus one I threw out before I got tired of doing this by hand.</span></p><ul>
<li>yaw</li>
<li>eye squint</li>
<li>age</li>
<li>smile</li>
<li>skin tone</li>
<li>gender</li>
<li>hair length</li>
<li>quality of photo</li>
</ul>
<div><p>
I decided not to use quality of photo (you can see the results in the notebook results) because I didn't want half the photos to just look terrible. The good news is that I made <a href="https://github.com/patrickxia/StyleGAN2-TensorFlow-2.x/blob/master/chernoff_faces.ipynb">a new notebook</a> for generating the actual faces, one that should actually work for you if you clone the repo.</p><p>

After generating a few images and using the `montage` command from ImageMagick, we get our preliminary results!</p><p><a href="https://1.bp.blogspot.com/-BUwEV9KxEHU/XvmKsnDLQdI/AAAAAAAA54w/LRRwtnI9zl80FV5sIQMEQfcmGFcX8cYCQCLcBGAsYHQ/s1600/out.png"><img data-original-height="426" data-original-width="512" height="331" src="https://1.bp.blogspot.com/-BUwEV9KxEHU/XvmKsnDLQdI/AAAAAAAA54w/LRRwtnI9zl80FV5sIQMEQfcmGFcX8cYCQCLcBGAsYHQ/s400/out.png" width="400"></a></p>

<br></div>
<p>
Now that's the future!</p>



<p><a href="#cite1" id="foot1">¹</a>&nbsp;"Favorite" might be code for "useless," going with the theme of this blog.<br>
<a href="#cite2" id="foot2">²</a>&nbsp;Clearly.<br>
<a href="#cite3" id="foot3">³</a>&nbsp;It's called a dlatent in the code, but the paper calls it the space W and the vectors w. I don't know.<br>
<a href="#cite4" id="foot4">⁴</a></p><tt>for q in `seq 0 4095`; do i=$(printf "%04d" $q); curl -d '{"image": "http://cantina.patrickxia.com/faces/seed'$i'.png"}' -H "app_id: xxx" -H "app_key: xxx" -H 'store_image: "false"' -H "Content-Type: application/json" http://api.kairos.com/detect &gt; $i.json; sleep 1; done</tt><p> and something similar for the `media` API, which gives you landmarks. If you care enough, I've hosted the images <a href="http://cantina.patrickxia.com/faces">here</a>, which lets you just look through them if you want.</p></div>

</div></div>]]>
            </description>
            <link>https://www.ihatethefuture.com/2020/06/deep-chernoff-faces.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23679014</guid>
            <pubDate>Mon, 29 Jun 2020 16:21:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The History of Usenet and FidoNet]]>
            </title>
            <description>
<![CDATA[
Score 109 | Comments 28 (<a href="https://news.ycombinator.com/item?id=23678687">thread link</a>) | @cfmcdonald
<br/>
June 29, 2020 | https://technicshistory.com/2020/06/25/the-era-of-fragmentation-part-4-the-anarchists/ | <a href="https://web.archive.org/web/*/https://technicshistory.com/2020/06/25/the-era-of-fragmentation-part-4-the-anarchists/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>Between roughly 1975 and 1995, access to computers accelerated much more quickly than access to computer networks. First in the United States, and then in other wealthy countries, computers became commonplace in the homes of the affluent, and nearly ubiquitous in institutions of higher education. But if users of those computers wanted to connect their machines together – to exchange email, download software, or find a community where they could discuss their favorite hobby, they had few options. Home users could connect to services like CompuServe. But, until the introduction of flat monthly fees in the late 1980s, they charged by the hour at rates relatively few could afford. Some university students and faculty could connect to a packet-switched computer network, but many more could not. By 1981, only about 280 computers had access to ARPANET. CSNET and BITNET would eventually connect hundreds more, but they only got started in the early 1980s. At that time the U.S. counted more than 3,000 institutions of higher education, virtually all of which would have had multiple computers, ranging from large mainframes to small workstations.</p>
<p>Both communities, home hobbyists and those academics who were excluded from the big networks, turned to the same technological solution to connect to one another. They hacked the plain-old telephone system, the Bell network, into a kind of telegraph, carrying digital messages instead of voices, and relaying messages from computer to computer across the country and the world.</p>
<p>These were among the earliest peer-to-peer computer networks. Unlike CompuServe and other such centralized systems, onto which home computers latched to drink down information like so many nursing calves, information spread through these networks like ripples on a pond, starting from anywhere and ending up everywhere. Yet they still became rife with disputes over politics and power. In the late 1990s, as the Internet erupted into popular view, many claimed that it would flatten social and economic relations. By enabling anyone to connect with anyone, the middle men and bureaucrats who had dominated our lives would find themselves cut out of the action. A new era of direct democracy and open markets would dawn, where everyone had an equal voice and equal access. Such prophets might have hesitated had they reflected on what happened on Usenet and Fidonet in the 1980s. Be its technical substructure ever so flat, every computer network is embedded within a community of human users. And human societies, no matter how one kneads and stretches, always seem to keep their lumps.</p>
<h2>Usenet</h2>
<p>In the summer of 1979, Tom Truscott was living the dream life for a young computer nerd. A grad student in computer science at Duke University with an interest in computer chess, he landed an internship at Bell Labs’ New Jersey headquarters, where he got to rub elbows with the creators of Unix, the latest craze to sweep the world of academic computing.</p>
<p>The origins of Unix, like those of the Internet itself, lay in the shadow of American telecommunications policy. Ken Thompson and Dennis Ritchie of Bell Labs decided in the late 1960s to build a leaner, much pared-down version of the massive MIT Multics system to which they had contributed as software developers. The new operating system quickly proved a hit within the labs, popular for its combination of low overhead (allowing it to run on even inexpensive machines) and high flexibility. However, AT&amp;T could do little to profit from their success. A 1956 agreement with the Justice Department required AT&amp;T to license non-telephone technologies to all comers at a reasonable rate, and to stay out of all business sectors other than supplying common carrier communications.</p>
<p>So AT&amp;T began to license Unix to universities for use in academic settings on very generous terms. These early licensees, who were granted access to the source code, began building and selling their own Unix variants, most notably the Berkeley Software Distribution (BSD) Unix created at the the University of California’s flagship campus. The new operating system quickly swept academia. Unlike other popular operating systems, such as the DEC TENEX / TOPS-20, it could run on hardware from a variety of vendors, many of them offering very low-cost machines. And Berkeley distributed the software for only a nominal fee, in addition to the modest licensing fee from AT&amp;T.<sup id="fnref-13802-fee"><a href="#fn-13802-fee">1</a></sup></p>
<p>Truscott felt that he sat at the root of all things, therefore, when he got to spend the summer as Ken Thompson’s intern, playing a few morning rounds of volleyball before starting work at midday, sharing a pizza dinner with his idols, and working late into the night slinging code on Unix and the C programming language. He did not want to give up the connection to that world when his internship ended, and so as soon as he returned to Duke in the fall, he figured out how to connect the computer science department’s Unix-equipped PDP 11/70 back to the mothership in Murray Hill, using a program written by one of his erstwhile colleagues, Mike Lesk. It was called <em><span>uucp</span></em> – Unix to Unix copy – and it was one of a suite of “uu” programs new to the just-released Unix Version 7, which allowed one Unix system to connect to another over a modem. Specifically, <em>uucp</em> allowed one to copy files back and forth between the two connected computers, which allowed Truscott to exchange email with Thompson and Ritchie.</p>
<figure data-shortcode="caption" id="attachment_14009" aria-describedby="caption-attachment-14009"><img data-attachment-id="14009" data-permalink="https://technicshistory.com/2020/06/25/the-era-of-fragmentation-part-4-the-anarchists/truscott/" data-orig-file="https://technicshistory.files.wordpress.com/2020/06/truscott.jpg" data-orig-size="171,187" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="truscott" data-image-description="" data-medium-file="https://technicshistory.files.wordpress.com/2020/06/truscott.jpg?w=171" data-large-file="https://technicshistory.files.wordpress.com/2020/06/truscott.jpg?w=171" src="https://technicshistory.files.wordpress.com/2020/06/truscott.jpg?w=739" alt="truscott" srcset="https://technicshistory.files.wordpress.com/2020/06/truscott.jpg 171w, https://technicshistory.files.wordpress.com/2020/06/truscott.jpg?w=137 137w" sizes="(max-width: 171px) 100vw, 171px"><figcaption id="caption-attachment-14009">Undated photo of Tom Truscott</figcaption></figure>
<p>It was Truscott’s fellow grad student, Jim Ellis, who had installed the new Version 7 on the Duke computer, but even as the new upgrade gave with one hand, it took away with the other. The news program that was distributed by the Unix users’ group, USENIX, which would broadcast news items to all users of a given Unix computer system, no longer worked on the new operating ssytem. Truscott and Ellis decided they would replace it with their own 7-compatible news program, with more advanced features, and return their improved software back to the community for a little bit of prestige.</p>
<p><span>At this same time, Truscott was also using <em>uucp</em> to connect with a Unix machine at the University of North Carolina ten miles to the southwest in Chapel Hill, and talking to a grad student there named Steve Bellovin.<sup id="fnref-13802-bellovin"><a href="#fn-13802-bellovin">2</a></sup> Bellovin had also started building his own news program, which notably included the concept of topic-based <em>newsgroups</em>, to which one could subscribe, rather than only having a single broadcast channel for all news. Bellovin, Truscot and Ellis decided to combine their efforts and build a networked news system with newsgroups, that would use <em>uucp</em> to share news between sites. They intended to distributed provide Unix-related news for USENIX members, so they called their system Usenet.&nbsp;</span></p>
<p>Duke would serve as the central clearinghouse at first, using its auto-dialer and <em>uucp</em> to connect to each other site on the network at regular intervals, in order to pick up it local news updates and deposit updates from its peers. Bellovin wrote the initial code, but it used shell scripts that operated very slowly, so Stephen Daniel, another Duke grad student, rewrote the program in C. Daniel’s version became know as A News. Ellis promoted the program at the January 1980 Usenix conference in Boulder, Colorado, and gave away all eighty copies of the software that he had brought with him. By the next Usenix conference that summer, the organizers had added A News to the general software package that they distributed to all attendees.</p>
<p>The creators described the system, cheekily, as a “poor man’s ARPANET.” Though one may not be accustomed to thinking of Duke as underprivileged, it did not have the clout in the world of computer science necessary at the time to get a connection to that premiere American computer network. But access to Usenet required no one’s permission, only a Unix system, a modem, and the ability to pay the phone bills for regular news transfers, requirements that virtually any institution of higher education could meet by the early 1980s.</p>
<p>Private companies also joined up with Usenet, and helped to facilitate the spread of the network. Digital Equipment Corporation (DEC) agreed to act as an intermediary between Duke and UC Berkeley, footing the long-distance telephone bills for inter-coastal data transfer. This allowed Berkeley to become a second, west-coast hub for Usenet, connecting up UC San Francisco, UC San Diego, and others, including Sytek, an early LAN business. The connection to Berkeley, an ARPANET site, also enabled cross-talk between ARPANET and Usenet (after a second re-write by Mark Horton and Matt Glickman to create B News). ARPANET sites began picking up Usenet content and vice versa, though ARPA rules technically forbid interconnection with other networks. The network grew rapidly, from fifteen sites carrying ten posts a day in in 1980, to 600 sites and 120 posts in 1983, and 5000 sites and 1000 posts in 1987.<sup id="fnref-13802-sitesandposts"><a href="#fn-13802-sitesandposts">3</a></sup></p>
<p>Its creators had originally conceived Usenet as a way to connect the Unix user community and discuss Unix developments, and to that end they created two groups, <em>net.general</em> and <em>net.v7bugs</em> (the latter for discussing problems with the latest version of Unix). However they left the system entirely open for expansion. Anyone was free to create a new group under “net”, and users very quickly added non-technical topics such as <em>net.jokes</em>. Just as one was free to send whatever one chose, recipients could also ignore whatever groups they chose, e.g. a system could join Usenet and request data only for <em>net.v7bugs,</em> ignoring the rest of the content. Quite unlike the carefully planned ARPANET, Usenet self-organized, and grew in an anarchic way overseen by no central authority.</p>
<p>Yet out of this superficially democratic medium a hierarchical order quickly emerged, with a certain subset of highly-connected, high-traffic sites recognized as the “backbone” of the system. This process developed fairly naturally. Because each transfer of data from one …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://technicshistory.com/2020/06/25/the-era-of-fragmentation-part-4-the-anarchists/">https://technicshistory.com/2020/06/25/the-era-of-fragmentation-part-4-the-anarchists/</a></em></p>]]>
            </description>
            <link>https://technicshistory.com/2020/06/25/the-era-of-fragmentation-part-4-the-anarchists/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23678687</guid>
            <pubDate>Mon, 29 Jun 2020 15:52:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Worrying about the NPM Ecosystem]]>
            </title>
            <description>
<![CDATA[
Score 148 | Comments 69 (<a href="https://news.ycombinator.com/item?id=23678409">thread link</a>) | @diiq
<br/>
June 29, 2020 | https://sambleckley.com/writing/npm.html | <a href="https://web.archive.org/web/*/https://sambleckley.com/writing/npm.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h3 id="tldr">TL;DR</h3>
<p>The <acronym>npm</acronym> ecosystem seems unwell. If you are concerned with security, reliability, or long-term maintenance, it is almost impossible to pick a suitable package to use — both because there are 1.3 million packages available, and even if you find one that is well documented and maintained, it might depend on hundreds of other packages, with dependency trees stretching ten or more levels deep — as one developer, it’s impossible to validate them all.</p>

<p>I spend some time measuring the extent of the problem.</p>

<p>I suggest that this is a social problem, more than a technical one, and propose a semi-social solution: a human-maintained subset of the total registry, based on shared criteria by which a “healthy” package can receive a seal of approval. One criterion would be to only depend on other approved packages.</p>

<h3 id="the-premise">The premise</h3>

<p>I don’t like the way I feel when I’m installing packages with npm. Selecting a package, installing it, discovering the 93 additional packages that were installed along with it, and hoping all of <em>them</em> are also suitable for my project… it feels out of control.</p>

<p>I feel unhappy because picking dependencies is hard, so I blame npm, and that way my problems are not my fault.</p>

<p>Is there some way for me to measure this badness, and thus more thoroughly escape the blame?</p>

<h3 id="thinking-a-bit-like-a-scientist-but-not-too-much">Thinking a bit like a scientist, but not too much</h3>

<p>Can we make meaningful empirical measurements of the health of the <acronym>npm</acronym> registry? I think so; but before I do, in order to maintain the slightest semblance of impartiality, I want to lay out what I <em>expect</em> a healthy package registry to look like. If it turns out that npm, taken as a whole, mostly looks the way I expect a healthy registry to look, I’ll have to put my tail between my legs and take ownership of my struggles. If it looks wildly different, I’ll still have hope for blaming the system.</p>

<h3 id="imagining-a-healthy-repository">Imagining a healthy repository</h3>

<p>In my ideal world there are, mostly, 4 types of packages:</p>

<ul>
  <li>
    <p><strong>Utilities:</strong></p>

    <p>A utility is a simple package, with no dependencies, that accomplishes a single small but onerous task. For example: lodash is a collection of utilities (and you can install each one separately). Controversially, left-pad.</p>
  </li>
  <li>
    <p><strong>Libraries:</strong></p>

    <p>A library is one step up in abstraction. It may depend on a handful of utilities, and it accomplishes a whole set of related tasks. For example: urlib has just a few simple dependencies, and while it does many things, it respects a clear unifying principle.</p>
  </li>
  <li>
    <p><strong>Frameworks:</strong></p>

    <p>A framework provides scaffolding for an entire project, and may depend on multiple libraries and utilities. You should only ever need zero or one of these in your project. Examples: React, angular, express.</p>
  </li>
  <li>
    <p><strong>Plugins:</strong></p>

    <p>A plugin enhances a framework with additional, special-use functionality. For a plugin, the framework should be a peer dependency, not a true dependency. It might also depend directly on a library, or a handful of utilities. Examples: an angular component or a jquery plugin.</p>
  </li>
</ul>

<p>Obviously, the world is messy! I don’t expect 100% of packages to fit into such simple categories, nor are any of these definitions strict and unyielding. But overall, I’d hope <em>many</em> or even <em>most</em> packages to <em>mostly</em> conform to a categorization <em>something</em> like that, with most packages being utilities and libraries, and the fewest packages being frameworks.</p>

<h3 id="what-would-that-mean-statistically">What would that mean statistically?</h3>

<p>In a world where packages fit mostly into that hierarchy, a registry of many packages would have these qualities:</p>

<ul>
  <li>No dependency cycles</li>
  <li>Most packages would have dependency trees 0-4 levels deep, leaning towards lower numbers</li>
  <li>Even the deepest and broadest frameworks would depend on fewer than 250 packages, including dependencies of dependencies (3-4 steps deep, with 3-4 dependencies per package, &lt;= 4<sup>4</sup> max). Most packages would install well under 30 other packages. These numbers are extremely generous; smaller numbers would make me even happier.</li>
</ul>

<h3 id="what-do-we-actually-see">What do we actually see?</h3>

<p>I downloaded the metadata for all 1.3 million packages in the npmjs.org repository and attempted to crunch some numbers. (For more technical details on how I did this, see the final section, titled “Appendix: Methods”)</p>

<p>I’m going to use “the number of other packages that depend on this one” as a poor-man’s proxy for how popular a package is. It’s not a <em>good</em> proxy, but neither is the other common choice: the number of downloads (see the section <a href="https://packaging.python.org/guides/analyzing-pypi-package-downloads/#background">“Background”</a>, from PyPI). The number of downloads would have been much more computationally expensive to acquire, so I used dependents.</p>

<h3 id="circular-dependencies">Circular dependencies</h3>

<p>Of those 1.3 million packages, 1,700 depend directly on themselves, either perfectly circularly, or a different version of the same package. I have no explanation for that.</p>

<p>2500 packages are part of a two-package dependency cycle.</p>

<p>Then ~500, ~125, and ~25 are part of 3-, 4-, and 5-package cycles, respectively. (These are not always simple circles; it may be three codependent packages in any loopy arrangement)</p>

<p>My immediate reaction was that those numbers seemed like good news; out of a million packages, only a tiny percentage of oddball packages have circular dependencies, and the rest are fine. Right?</p>

<p>Unfortunately, what I discovered was that almost 150,000 packages — more than 1 in 10 — had at least one of these circular dependencies <em>somewhere</em> in their dependency graph. That means at least a few of those “oddballs” are actually major, highly-referenced packages. Some examples:</p>

<ul>
  <li><code>babel-core</code> depends on <code>babel-register</code>, which depends on <code>babel-core</code>.</li>
  <li><code>yeoman-generator</code> depends on <code>yeoman-environment</code> which depends on <code>yeoman-generator</code>.</li>
</ul>

<p>Those are not isolated instances; I plucked them as the most easily recognizable out of a list of dozens of highly-depended-upon packages with circular dependencies.</p>

<p>I need to be clear, here, that this is not <em>technically</em> a problem! <acronym>npm</acronym> can install these packages just fine. They can circularly-depend on one another; there’s no technical problem. They work. Obviously. If <code>babel-core</code> didn’t work, someone would have said something by now.</p>

<p>But I’m not particularly comfortable with it. I’m especially uncomfortable when the cycles involved are longer than two packages, which makes them seem less intentional. Maybe there’s a good reason; these folks are smart, and I always try to assume that odd choices were made for important reasons. And yet…</p>

<h3 id="what-about-devdependencies">What about <code>devDependencies</code>?</h3>

<p>Among the most-depended-upon packages are</p>

<ul>
  <li><code>@types/</code>[something]</li>
  <li>compilers like <code>typescript</code> and <code>babel</code></li>
  <li>build systems like <code>gulp</code> and <code>webpack</code>.</li>
</ul>

<p>I’m <strong>not</strong> measuring that using <code>devDependencies</code> — <code>typescript</code> appears in over 12,000 “dependency” lists.</p>

<p>So while I am less concerned with packages having large and deep <code>devDependency</code> trees (I <em>am</em> still concerned, but less so), it seems that a large proportion of packages aren’t making use of the distinctions between <code>dependencies</code> and <code>devDependencies</code> in the first place. That <em>is</em> concerning.</p>

<h3 id="dependency-tree-depths">Dependency tree depths</h3>

<p>I am defining the depth of a package’s dependency tree as the longest dependency-of-a-dependency-of-a-dependency chain I can find. Especially deep dependency trees are a problem because of how difficult they make it to audit all the packages that will get installed when including a single new package.</p>

<p>The <em>average</em> dependency tree depth in npmjs.org is just under 4. Which doesn’t sound too bad!</p>

<p>As often happens, though, the mean does not tell the whole story. Almost half of all packages have no dependencies at all — which is a good thing! — but all those zeros dramatically drop the average of the packages that <em>do</em> have dependencies.</p>

<p>If we charted of the number of packages with each dependency tree depth greater than zero, then based on the idealized registry I imagined above, here’s what I’d hope to see:</p>

<p><strong>Imagined:</strong></p>

<p><img src="https://sambleckley.com/assets/images/npm_packages_per_tree_depth_ideal.svg" alt="Imaginary chart of package counts per tree depth"></p>

<p>And here’s what we actually get:</p>

<p><strong>Reality:</strong></p>

<p><img src="https://sambleckley.com/assets/images/npm_packages_per_tree_depth.svg" alt="Real chart of package counts per tree depth"></p>

<p>Remember that we were hoping for mostly 2, 3, and 4. Instead, there is still a long tail of packages with tree depths <em>above 20</em>. 20 is… much larger than I was expecting, and I was expecting to be disappointed.</p>

<p>But let’s re-use the “oddball” theory: perhaps all those packages with extremely deep dependency trees are rarely used, and not worth worrying about. Let’s check.</p>

<p>Here’s a scatterplot, where each package is placed based on its tree depth plotted and how many other packages reference it. On the right of the plot are the most-referenced (~popular) packages; on the top are the deepest dependency trees. Again, my hopes first:</p>

<p><strong>Imagined:</strong></p>

<p><img src="https://sambleckley.com/assets/images/npm_packages_per_tree_depth_and_dependents_ideal.svg" alt="Imaginary scatterplot of package counts per tree depth"></p>

<p>Followed by reality:</p>

<p><strong>Reality:</strong></p>

<p><img src="https://sambleckley.com/assets/images/npm_packages_per_tree_depth_and_dependents.svg" alt="Real scatterplot of package counts per tree depth"></p>

<p>There are heaps of 10+ tree-depths up among even the most popular packages, and a few even reach 20+. Extremely deep trees are not just a problem of “oddball” packages.</p>

<h3 id="direct-dependencies-branching-factor">Direct dependencies (branching factor)</h3>

<p>The average number of direct dependencies (among packages with any dependencies at all) is 5. That, by itself, doesn’t seem to bad. It feels a little alarming when combined with high tree depths, though. Does that mean some of these packages have 5<sup>10</sup> total dependencies? (Spoiler: no.)</p>

<p>Here’s a graph of how many packages have 1 dependency, 2 dependencies… up to 30 — a nice, neat exponential decay.</p>

<p><img src="https://sambleckley.com/assets/images/npm_packages_per_direct_dependencies.svg" alt="Chart of package counts per direct dependencies"></p>

<p>This curve is clean enough that I would not be surprised to see something pretty similar in any package registry — maybe not the exact same parameters, but a similar shape. Not shown here is an <em>incredibly</em> long tail; there are 4 packages tied for the most direct dependencies with exactly 1000, and there are runners-up spread pretty smoothly up to that maximum.</p>

<h3 id="indirect-dependencies">Indirect dependencies</h3>

<p>Knowing the average depth and branching factor, you have to imagine that counting the total dependencies of each package, including dependencies-of-dependencies, is not going to yield good news. But many of the branches of a large dependency tree are shared — multiple packages in the tree all depend on the same library. And the tree depth I have measured is the <em>maximum</em> depth for each package — not the average. So the picture is not necessarily as dire as an initial …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://sambleckley.com/writing/npm.html">https://sambleckley.com/writing/npm.html</a></em></p>]]>
            </description>
            <link>https://sambleckley.com/writing/npm.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23678409</guid>
            <pubDate>Mon, 29 Jun 2020 15:21:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Klutz Press: books built for learning stuff]]>
            </title>
            <description>
<![CDATA[
Score 311 | Comments 93 (<a href="https://news.ycombinator.com/item?id=23676862">thread link</a>) | @whatrocks
<br/>
June 29, 2020 | https://www.charlieharrington.com/create-wonderful-things-be-good-have-fun | <a href="https://web.archive.org/web/*/https://www.charlieharrington.com/create-wonderful-things-be-good-have-fun">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><blockquote>
<p>Create wonderful things, be good, have fun</p>
</blockquote>
<p>This is the credo of Klutz Press, the most important book publisher of my childhood. It being summer and all, Hobbes, ol' buddy... let's going exploring!</p>
<h2>What makes a Klutz Press book so good for learning stuff?</h2>
<p>If you've heard of Klutz, then you've likely seen their debut: <a href="https://www.amazon.com/gp/product/0932592007/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0932592007&amp;linkId=adaa512e8af09feab7c571ec8f2863cc">Juggling For the Complete Klutz</a>. </p>
<p><span>
      <a href="https://www.charlieharrington.com/static/c28fcf440265c6bea76d51f90e51b462/1cfc2/juggling.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="juggling" title="juggling" src="https://www.charlieharrington.com/static/c28fcf440265c6bea76d51f90e51b462/a6d36/juggling.png" srcset="https://www.charlieharrington.com/static/c28fcf440265c6bea76d51f90e51b462/222b7/juggling.png 163w,
https://www.charlieharrington.com/static/c28fcf440265c6bea76d51f90e51b462/ff46a/juggling.png 325w,
https://www.charlieharrington.com/static/c28fcf440265c6bea76d51f90e51b462/a6d36/juggling.png 650w,
https://www.charlieharrington.com/static/c28fcf440265c6bea76d51f90e51b462/1cfc2/juggling.png 900w" sizes="(max-width: 650px) 100vw, 650px" loading="lazy">
  </a>
    </span></p>
<p>If not, I highly suggest seeking out a copy. Keep in mind, it's more than just a <em>book</em> -- <em>Juggling for the Complete Klutz</em> has these amazing attributes:</p>
<ul>
<li>It's spiral-bound</li>
<li>It has hilarious drawings</li>
<li>It comes attached with three real-life bean bags!</li>
</ul>
<p>These are book super-powers, in my book (a Klutz-worthy pun?). In our day, my sister and I owned, devoured, and treasured these Klutz titles:</p>
<ul>
<li><a href="https://www.amazon.com/gp/product/0932592082/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0932592082&amp;linkId=5bb878d785a02edc2b01eeffd63f9a76">Country and Blues Harmonica for the Musically Hopeless</a> (comes with instructional cassette and a gen-u-ine Hohner harmonica)</li>
<li><a href="https://www.amazon.com/gp/product/1591747007/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1591747007&amp;linkId=bf0dda95c016b59824316ebe42f872ec">Friendship Bracelets Craft Kit</a> (comes with string and supplies for making friendship bracelets)</li>
<li><a href="https://www.amazon.com/gp/product/1878257501/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1878257501&amp;linkId=7315967f85fba1c812b20b480b0bd966">Table Top Football: A Guide to the Classic Lunchroom Sport</a> (comes with an amazing leather-ish table top football)</li>
<li><a href="https://www.amazon.com/gp/product/1878257536/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1878257536&amp;linkId=58c3235cd298baf5ec29fb13ee806ced">Cats Cradle</a> (comes with a tie-die cat's cradle string)</li>
<li><a href="https://www.amazon.com/gp/product/1591745047/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1591745047&amp;linkId=905fbe99f4844c375f00baa92f1beee0">Bead Loom Bracelets: Learn to Make Beautiful Beaded Bracelets</a> (comes with beads and string)</li>
<li><a href="https://www.amazon.com/gp/product/1878257412/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1878257412&amp;linkId=d283508248a4016cd908bd8e37fcea68">Kids Shenanigans</a> (comes with a Whoopie cushion!)</li>
<li><a href="https://www.amazon.com/gp/product/1878257749/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1878257749&amp;linkId=99f448476dd4b6ce6baae76dbf048446">Earthsearch: A Kid's Geography Museum in a Book</a> (comes with interactive spinners, a sand-powered clock, and a real-life penny)</li>
<li><a href="https://www.amazon.com/gp/product/1878257145/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=1878257145&amp;linkId=6b93493aa3d2cd5660e5e24c404ad5e6">Explorabook: A Kid's Science Museum in a Book</a> (comes with a bunch more interactive projects and whatnots inside the book, like mirrors, spinners, and a packet of algae that you can grow)</li>
</ul>
<p>As a kid, there was nothing better than getting a new Klutz book (ok, maybe a Super Nintendo game). But unlike a replay of <em>Super Mario RPG</em>, these Klutz books require no nostaglia goggles. Here's why I think they're magic:</p>
<h3>Klutz books are spiral-bound</h3>
<p>Books for learning stuff should be able to open up and stay flat. The old 1980s computer manuals for computers like the Commodore VIC-20, the Commodore 64, and <a href="https://www.charlieharrington.com/my-new-old-apple-iie-computer">my new old Apple IIe</a> knew this much -- their manuals were spiral-bound and spell-binding.</p>
<p>So, why don't we see more spiral-bound books? Without knowing that much about printing costs, I imagine they're more expensive. Also, they do look slightly worse on a bookshelf, especially if you're going for that 'grammable color pattern look (so, just don't do this).</p>
<h3>Klutz books come with the required materials</h3>
<p>The little "paper" football that came with the <em>Table Top Football: A Guide to the Classic Lunchroom Sport</em> was a revered grail of mine. I remember that my dad took a sheetrock knife and made a small incision in its plastic case attached to the book so that the football could be slid in and out, with the explicit rule that the football must either be <em>in the case</em> or <em>being used in a game</em>. This is much like the inexorable <a href="https://hypercritical.co/">Jon Siracusan</a> rule for Airpods. Obey, or the Airpods will be instantly lost forever.</p>
<p><span>
      <a href="https://www.charlieharrington.com/static/e4a351feccb1c6d3e7524fa014860c7c/20f07/football.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="football" title="football" src="https://www.charlieharrington.com/static/e4a351feccb1c6d3e7524fa014860c7c/20f07/football.jpg" srcset="https://www.charlieharrington.com/static/e4a351feccb1c6d3e7524fa014860c7c/d2f63/football.jpg 163w,
https://www.charlieharrington.com/static/e4a351feccb1c6d3e7524fa014860c7c/c989d/football.jpg 325w,
https://www.charlieharrington.com/static/e4a351feccb1c6d3e7524fa014860c7c/20f07/football.jpg 334w" sizes="(max-width: 334px) 100vw, 334px" loading="lazy">
  </a>
    </span></p>
<p>Side note that there are some people who just love making small cuts into the plastic cases for things, so that you easily return them to their "pristine" condition. I, myself, don't understand these people. I like wripping these cases to shreds instantly.</p>
<p>Anyway, back to these Klutz books. By including juggling bean bags, yarn for friendship bracelets, or a real harmonica, Klutz Press books gave you everything you needed to get your hands dirty. "Active learning", or something like that. Playing = learning. Etcetera.</p>
<h3>Klutz books have hilarious art</h3>
<p><span>
      <a href="https://www.charlieharrington.com/static/6c99ba6e2419ae976e32b7e293532a7a/20e5d/shenanigan.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="shenanigan" title="shenanigan" src="https://www.charlieharrington.com/static/6c99ba6e2419ae976e32b7e293532a7a/20e5d/shenanigan.jpg" srcset="https://www.charlieharrington.com/static/6c99ba6e2419ae976e32b7e293532a7a/d2f63/shenanigan.jpg 163w,
https://www.charlieharrington.com/static/6c99ba6e2419ae976e32b7e293532a7a/c989d/shenanigan.jpg 325w,
https://www.charlieharrington.com/static/6c99ba6e2419ae976e32b7e293532a7a/20e5d/shenanigan.jpg 450w" sizes="(max-width: 450px) 100vw, 450px" loading="lazy">
  </a>
    </span></p>
<p>Just look at that paper airplane stuck in the teacher's hair!</p>
<p>Klutz had a particular art direction that spoke to me as a child. The goofy people in their guides made me feel like it was <em>okay to be a klutz</em>. </p>
<p>Which brings me to the most important reason that Klutz books are special.</p>
<h3>Klutz books embrace the Beginner's Mindset</h3>
<blockquote>
<p>In the beginner's mind there are many possibilities. In the experts mind there are few - Shunryū Suzuki</p>
</blockquote>
<p>Everyone starts out as a klutz. No matter what. That means it's okay to make mistakes. It can even be funny - in fact, it should be funny! Because it's fun to learn new things.</p>
<p>Being a klutz, making mistakes, having fun, this is the path to wonderful things.</p>
<h2>Who's behind Klutz Press?</h2>
<p>According to <a href="https://en.wikipedia.org/wiki/Klutz_Press">Wikipedia</a>, Klutz Press was founded in 1977 by three friends in Palo Alto.</p>
<p>The apocryphal story is that <a href="https://en.wikipedia.org/wiki/John_Cassidy_(author)">John Cassidy</a>, a recent Stanford grad working as a high school teacher, brought a bucket of tennis balls and some hand-written instructions for juggling to his remedial reading class. The laughs and learning and genuine reading and genuine juggling that ensued inspired Cassidy and his buddies from Stanford to publish <em>Juggling For the Complete Klutz</em> under their new company: Klutz Press.</p>
<p><em>Juggling For the Complete Klutz</em> has sold over 2.5 million copies. But Googling for Klutz Press is somewhat challenging these days. In 2000, Klutz was acquired by a company called Nelvana for $74 million, and in 2002 Klutz became a subsidiary of Scholastic, Inc. This latter merger is a good match in my book, as the Scholastic Book Fair also ranks heavily in my childhood memories of learning to love reading. The only other company I'd feel comfortable with owning Klutz Press is Pizza Hut - thanks to their delicious <a href="https://www.bookitprogram.com/">BOOK-IT!</a> reading program, which brought me dozens of delicious cheese Personal Pan Pizzas during the 1990s. My parents couldn't decide if they hated or loved Pizza Hut for this.</p>
<p>Nowadays, Klutz.com redirects to the Scholastic website, and it's unclear what's out-of-print or available from the voluminous Klutz catalog. So if you do find a genuine Klutz book and kit, I'd snag them quickly!</p>
<p>Luckily, I was able to find a good interview with Cassidy from 1995 on the <a href="https://web.archive.org/web/20110616182712/http://findarticles.com/p/articles/mi_m1154/is_n5_v83/ai_16857996/">Wayback Machine</a>. In 1995, Klutz was at the height of their power and influence in kid's minds. Here are some choice quotes from Cassidy:</p>
<p>On their company culture at Klutz Press:</p>
<blockquote>
<p>"In terms of being laid back, we take a back seat to nobody."</p>
</blockquote>
<p>On their "teaching" style:</p>
<blockquote>
<p>"Talk to a kid about fun and math, and it's like you're talking about two different sides of the universe. If we can climb this mountain, there's nothing we can't tackle."</p>
</blockquote>
<blockquote>
<p>"Kids don't learn all that much by listening or reading. They need to get elbow-deep in a subject and touch it, feel it, and smell it."</p>
</blockquote>
<p>This reminds me of Seymour Paypert's <a href="https://www.amazon.com/gp/product/0465046746/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;tag=whatrocks09-20&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0465046746&amp;linkId=2949aefc36d4bd9d9f632170d2ac23de">Mindstorms book</a> about his work with LOGO and the Turtle machine (you can check out <a href="https://www.charlieharrington.com/mindstorms">my notes</a> on the book)</p>
<p>The article explains a bit about their business:</p>
<ul>
<li>All of Klutz' books sell for less than $20</li>
<li>They can have low prices because: (1) the books (with their accompanying "stuff") are viewed as more than books - but "toys/novelties", so more retailers than booksellers are interesting in carring them, and (2) they do large printing runs (150k copies vs the usual 10k for children's books)</li>
</ul>
<p>And, importantly, the final word from Cassidy:</p>
<blockquote>
<p>"I can hang a spoon off my nose," Cassidy boasts, "and I take a lot of pride in that".</p>
</blockquote>
<h2>Create wonderful things, be good, have fun</h2>
<p>I just wanted to write that out again. I've decided to adopt their credo as my own life motto.</p>
<p>I learned so much from Klutz Press as a kid. I'm still learning now. Thank you, John Cassidy and team, for making these wonderful books. My juggling is finally starting to get pretty good, but I'll always be a klutz.</p></div></div>]]>
            </description>
            <link>https://www.charlieharrington.com/create-wonderful-things-be-good-have-fun</link>
            <guid isPermaLink="false">hacker-news-small-sites-23676862</guid>
            <pubDate>Mon, 29 Jun 2020 12:18:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Neurons that fire together, wire together, but how?]]>
            </title>
            <description>
<![CDATA[
Score 124 | Comments 41 (<a href="https://news.ycombinator.com/item?id=23676233">thread link</a>) | @Anon84
<br/>
June 29, 2020 | http://dissociativediaries.com/neurons-that-fire-together-wire-together-ok-but-how/ | <a href="https://web.archive.org/web/*/http://dissociativediaries.com/neurons-that-fire-together-wire-together-ok-but-how/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<div id="et-boc">
			
			<div><div>
				
				
				
				
					<div>
				<div>
				
				
				<div>
				
				
				<div>
					
<p>One of my little pet projects is a neurology book for psychologists, coaches and the like. What most people in these fields imagine about the brain is somewhere between perplexing, preposterous and potentially poisonous. (Seriously, I could tell you stories…) So I kind of wanted to write something like this for them.</p>
<p>Now I do consider it a tall order for me – I have some knowledge of neurology, but actually writing a SENSIBLE, understandable guidebook to neurology, with some practical applications too, that’s a challenge. (Senseless guidebooks to neurology are a dime a dozen, of course. In fact, I’ve walled in at least two people in one library, using only senseless pop-neurology books and as far I can tell, the rotting skeletons still haven’t been found. But then again the rotting corpse smell really doesn’t stand out much in most libraries I’ve been to, so I shouldn’t be surprised.)</p>
<p><a href="http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920.jpg"><img src="http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920.jpg" alt="" width="1920" height="1280" srcset="http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920.jpg 1920w, http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920-300x200.jpg 300w, http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920-768x512.jpg 768w, http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920-1024x683.jpg 1024w, http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920-1080x720.jpg 1080w, http://dissociativediaries.com/wp-content/uploads/2019/02/neuron-3567980_1920-610x407.jpg 610w" sizes="(max-width: 1920px) 100vw, 1920px"></a></p>
<p>But back to my main point. The great thing about trying to write a book like that, even attempting to prepare for doing it at one point in the future, when you know enough, is finding all the lovely little holes in how you’re telling the story to yourself. The teeny-tiny little jumps in understanding, the „lies-to-children”, the simplifications.</p>

<p>And one of these, for me, was how neurons get to connect.</p>

<p>And when I asked around, it turned out I wasn’t the only one missing this little bit.</p>

<p>I mean, yeah, we all know Hebb. „Neurons that fire together, wire together”, the neurology mantra in flesh. I can just imagine NeuroZen masters, walking around meditating neurologists, „fiiiiiiiiireeee toooooogeeeether” instead of „ommmm” going around.</p>

<p>That’s how neurons connect. When two fire at once, their connection becomes stronger. The synapse gains long term potentiation/LTP, by both an increase in the neurotransmitters produced, and an increase in the postsynaptic receptors for the neurotransmitters. Short-term it happens through utilizing some of the additional AMPA receptors available near to the synaptic membrane in the postsynaptic cell, long term by changes in protein synthesis and gene expression, in order to ensure a larger number of receptors. So far so good, in one way or another pretty much anyone who understands anything about neurology understands this bit. It might be more or less simplistic, you might have the AMPA receptors memorized or not, but you get the gist of it.&nbsp;(Although this begs the question: does Dale’s principle means that the increase in presynaptic neurotransmitter production influence not just this single synapse, but also the concentration of neurotransmitters in all synaptic connections of the presynaptic neuron? Or perhaps does a reverse mechanism happen, with other connections of the presynaptic neuron being stripped bare of their neurotransmitters? Something I need to check! )</p>

<p>Now here’s the kicker: how do neurons that AREN’T already connected connect? How does their “fire/wire” rule go?</p>

<p>I mean, that still applies, doesn’t it?</p>

<p>Actually, it doesn’t apply in the original Hebb’s law, since that required the neurons to be connected already and a couple of other things as well. But still, the general principle is how we tend to explain things like associative learning, conditioning, etc. The direct wording of Hebb’s law can’t apply there, because then we’d need every neural network to be connected to every other neural network at once, and that’d be a mess that wouldn’t really work all that well. But the general principle seems to apply.</p>

<p>And when I started asking around, no one could tell me how exactly.</p>

<p>How does a neuron know to extend new axonal or dendric connections towards another neuron, so that they might actually form a synapse between the two? Because this has to happen somehow, and this has to happen in a fairly structured way. If it didn’t happen, if we were only left to the remains of the synaptic pruning we got since birth, then our neural networks would become pretty much immovable after were what, twelve? I mean yeah, I’ve met these people, and so have you, but they’re in the small minority… well, large minority… well, OK, a small majority. But even so, there are quite a few people who aren’t that way. And if the neural growth was haphazard, then we’d get a far more random jumble of associations, then the fairly clean ones we tend to get in any conditioning/associative learning experiences.</p>

<p>So I knew there must be some kind of system for neurons to detect other neurons it should be „aiming for” in their development.</p>

<p>And – this was about two or three years ago as of writing this – no one I asked could tell me what it was. I’ve reached out to a lot of people I’ve considered very competent in the field, including some of my old Uni professors, and no one could direct me. Nor could I find the answer in any of the textbooks I’ve referred to.</p>

<p>As it turns out, the answer is fairly recent, as articles about it only started to appear in the 2000s. Since it took me some time to find it back then, I figured I’d make this article to share it now, making the whole thing easier to find for others.</p>

<p>We’ve known from about 1890 that the endings of both the axon and the dendrites are covered in something known as a growth cone. Like the tip of the branch, this is the part where the cell can further develop and extend, either in original growth, in regeneration, or as a reaction to some factors (including a decrease in overall electric activity which causes a form of synaptic scaling – hence explaining the „devouring” of unused neural pathways of amputated body parts by nearby active structures). But how do the growth cones know how exactly to grow? The brain is a complex three-dimensional web of interconnected neurons, so „hitting” the right point is definitely too hard if the neurons just kept on growing until they hit anything at all.</p>

<p>Now, the synaptic scaling process is still a piece of the answer – even if a neuron forms a new synaptic connection with a different neuron, if the target neuron already has too many connections, it will tend to remove the weakest ones, and this includes the most recent ones. The scaling goes both ways after all – it goes for more synapses when it starts with too few, but for less, if it starts with too many.</p>

<p>But synaptic scaling is not everything. As it turns out, the tips of the growth cone constantly produce structures called filopodia, and these react to specific chemical attractants and repellents. These chemicals are produced by both cells at the target area, and by so-called guidepost cells along the way. There are suggestions that the system for such targeting is fairly robust, especially in early development (and its limitations in later life might explain why spinal cord injuries and the like are so hard to fix).</p>

<p>(There is one other process for directing cells, the growth along the radial glia, but since it mainly refers to embryonic development, it’s really not all that interesting for us in this topic.)</p>

<p>Now, the more perceptive of you will already have noticed that while what I’ve given is AN answer, it is not THE answer to the question I’ve originally asked. Or at least, not necessarily.</p>

<p>It might be possible, that the interaction of the three systems:<br>– a pre-set network of attractor/repellent chemical pathways for guiding axon/dendrite growth,<br>– the synaptic scaling mechanism of homeostatic plasticity, which limits the max connections and promotes new connections if existing neural excitability falls below what is the baseline for the network,<br>– and the good old Hebbian mechanism for strengthening existing neural connections,<br>is in fact enough. That the interplay of these systems does answer our basic question, and simple enough rules result in sufficient flexibility to explain what is observed.</p>

<p>But, obviously, the process might be a bit more complicated. There isn’t that much literature about it, unfortunately, so what I’m doing here is more of an (un)educated guess, but since guidepost cells tend to be neurons which have yet to develop an axon – but already have dendrites and can receive signals, a slightly more complex – but better targeted – system can be imagined, where guidepost cells would be a part of a wider neural network, the stimulation of which „calls forth” further cells to build connections. This, in my understanding (Again, limited, so don’t take my word for it. I mean it.) would be a more effective explanation of how we can rapidly learn to connect fairly new information, even across huge (neurologically-scaled) swaths of neural space. A strongly agitated system produces a stronger „call for connections”, hence mediating the faster development of new and broader associations.</p>

<p>So there you have it, a quick summary of one part of neural connectivity I’ve yet to see described in a textbook about the brain, but which really should be given out there, along with the classic Hebbian principle. Hope it’s useful 🙂</p>
				</div>
			</div> <!-- .et_pb_text -->
			</div> <!-- .et_pb_column -->
				
				
			</div> <!-- .et_pb_row -->
				
				
			</div> <!-- .et_pb_section -->			</div>
			
		</div>					</div></div>]]>
            </description>
            <link>http://dissociativediaries.com/neurons-that-fire-together-wire-together-ok-but-how/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23676233</guid>
            <pubDate>Mon, 29 Jun 2020 10:40:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Has GitHub been down more since its acquisition by Microsoft?]]>
            </title>
            <description>
<![CDATA[
Score 239 | Comments 138 (<a href="https://news.ycombinator.com/item?id=23676199">thread link</a>) | @tdrnd
<br/>
June 29, 2020 | https://nimbleindustries.io/2020/06/04/has-github-been-down-more-since-its-acquisition-by-microsoft/ | <a href="https://web.archive.org/web/*/https://nimbleindustries.io/2020/06/04/has-github-been-down-more-since-its-acquisition-by-microsoft/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<div>
				    
				<div>
    <article id="post-775">
	
	            
            

                
        <div>
                        
            
            
<p>Two years ago, on June 4th of 2018, Microsoft announced its acquisition of GitHub, unicorn darling of the developer tools startup ecosystem, <a rel="noreferrer noopener nofollow" href="https://techcrunch.com/2018/06/04/microsoft-has-acquired-github-for-7-5b-in-microsoft-stock/" target="_blank" data-wpel-link="external">for $7.5B in stock</a>. The announcement unearthed a wide range of <a href="https://news.ycombinator.com/item?id=17221527" data-wpel-link="external" target="_blank" rel="nofollow">opinions and pontifications</a>, ranging from “GitHub is doomed” to “Microsoft is smart”, with many predictions about GitHub’s future. Some thought Microsoft’s growing investments in its cloud offering, Azure, might help GitHub. Could an investment by Microsoft improve GitHub’s reliability or harden them against outages like <a href="https://www.wired.com/story/github-ddos-memcached/" data-wpel-link="external" target="_blank" rel="nofollow">DDOSes</a>? Have any of these predictions come true?</p>



<p>We set out to analyze one angle of the GitHub acquisition: Has GitHub become more reliable since its acquisition by Microsoft? Our service, <a href="https://statusgator.com/" data-wpel-link="internal" rel="follow">StatusGator</a>, monitors more than 700 status pages of cloud providers and SaaS companies large and small. We aggregate and normalize status page data and make it available to our subscribers however they need: in notifications by email, Slack, Teams, or webhook, and in a <a href="https://nimbleindustries.io/2020/01/24/how-to-save-precious-minutes-during-incident-response/" data-wpel-link="internal" rel="follow">unified status dashboard</a> for all service dependencies.</p>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-1024x613.png" alt="" srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-1024x613.png 1024w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-300x180.png 300w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-768x460.png 768w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-1536x919.png 1536w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-720x431.png 720w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-580x347.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1-320x192.png 320w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-header-1.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>For more than 5 years, we have analyzed the <a href="https://www.githubstatus.com/" data-wpel-link="external" target="_blank" rel="nofollow">GitHub status page</a> constantly. Every 5 minutes, StatusGator takes a screenshot and <a href="https://statusgator.com/services/github" data-wpel-link="internal" rel="follow">collects relevant data</a> about their service status. That means we are uniquely positioned to offer analysis of the downtime that GitHub themselves announce via their status page.</p>



<p>What does the data tell us? In the two years since the acquisition announcement, GitHub has reported a 41% increase in status page incidents. Furthermore, there has been a 97% increase in incident minutes, compared to the two years prior to the announcement. Does this actually point to a decrease in reliability? We can’t say. This could simply mean GitHub has increased its transparency, publishing to their status page more frequently.</p>



<h2>Incident Counts</h2>



<p>We calculated an incident count in the 24 months preceding the announcement and the 24 months after. We <a href="https://nimbleindustries.io/2019/12/26/under-the-hood-inside-a-status-page-aggregator/" data-wpel-link="internal" rel="follow">classify status pages</a> into four states: <em>up</em>, <em>warn</em>, and <em>down</em>, and <em>maintenance</em>. GitHub does not expose scheduled maintenance on their status page. For these calculations we consider an incident to be any change in status between <em>up</em> and <em>warn</em> or <em>down</em>.</p>



<p>Before the acquisition, there were 89 incidents published on the GitHub status page. After, there were 126 incidents. A 41% increase:</p>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-1024x422.png" alt="89 Incidents before the acquisition, 126 incidents after, a 41% increase." srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-1024x422.png 1024w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-300x124.png 300w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-768x316.png 768w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-1536x633.png 1536w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-2048x844.png 2048w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-1920x791.png 1920w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-720x297.png 720w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-580x239.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-counts-1-320x132.png 320w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>In the graph below, we’ve charted the incident counts by month. The left side shows the 24 months before and the right side shows the 24 months after:</p>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-1024x533.png" alt="Graph showing GitHub incidents by month, before and after their acquisition by Microsoft." srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-1024x533.png 1024w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-300x156.png 300w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-768x400.png 768w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-1536x799.png 1536w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-720x375.png 720w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-580x302.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph-320x167.png 320w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-incident-graph.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<h2>Incident Minutes</h2>



<p>We calculated incident minutes by subtracting the start and end time of time of incidents. Although not 100% realtime, StatusGator checks frequently: every 5 minutes, so status page changes are detected quickly. We counted time where the page was not in an overall <em>up</em> state.</p>



<p>In the 24 months prior to the acquisition announcement, there were 6,110 minutes of downtime. During the 24 months after, there were 12,074 minutes of downtime, a 97% increase:</p>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-1024x420.png" alt="6,110 Incidents before the acquisition, 12,074 incidents after, a 97% increase." srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-1024x420.png 1024w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-300x123.png 300w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-768x315.png 768w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-1536x630.png 1536w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-2048x840.png 2048w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-1920x788.png 1920w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-720x295.png 720w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-580x238.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-counts-1-320x131.png 320w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>In the graph below, we’ve charted the incident minutes by month. The left side shows the 24 months before and the right side shows the 24 months after:</p>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-1024x531.png" alt="Graph showing GitHub downtime minutes by month, before and after their acquisition by Microsoft." srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-1024x531.png 1024w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-300x156.png 300w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-768x399.png 768w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-1536x797.png 1536w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-2048x1063.png 2048w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-1920x997.png 1920w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-720x374.png 720w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-580x301.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-downtime-minute-graph-1-320x166.png 320w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h2>Status Page Evolution</h2>



<p>During these four years, GitHub has made enormous improvements in their status page information granularity and design. In December 2018, they switched from a home grown status page to one operated by Atlassian’s StatusPage service, <a href="https://nimbleindustries.io/2019/03/11/2019-statusgator-status-page-awards/" data-wpel-link="internal" rel="follow">the most popular status page provider</a>. In doing so, they added numerous <a href="https://nimbleindustries.io/2019/11/22/component-status-filtering-is-here/" data-wpel-link="internal" rel="follow">individual component statuses</a>. Here’s what GitHub’s status page looked like before their switch to Atlassian StatusPage:</p>



<h3>GitHub’s Old Status Page</h3>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-status-page-old.png" alt="GitHub status page showing only a single status across all GitHub services." srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-status-page-old.png 645w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-status-page-old-300x187.png 300w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-status-page-old-580x361.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-status-page-old-320x199.png 320w" sizes="(max-width: 645px) 100vw, 645px"><figcaption>GitHub status page showing only a single status across all GitHub services.</figcaption></figure>



<p>When they switched to their new status page format, GitHub took a huge step towards increased accountability and transparency by detailing the following individual service components:</p>



<ul><li>Git Operations</li><li>API Requests</li><li>Issues, PRs, Dashboard, Projects</li><li>Notifications</li><li>Gists</li><li>GitHub Pages</li></ul>



<p>Overtime, they have expanded and refined their component statuses. They also started showing historical data right on their status page. As you can see in their newest and most detailed status page format, it shows the following service component statuses:</p>



<ul><li>Git Operations</li><li>API Requests</li><li>Webhooks</li><li>Issues, PRs, Projects</li><li>GitHub Actions</li><li>GitHub Packages</li><li>GitHub Pages</li></ul>



<h3>GitHub’s New Status Page</h3>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-944x1024.png" alt="GitHub status page showing detailed statuses of each of the major components of their service." srcset="https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-944x1024.png 944w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-277x300.png 277w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-768x833.png 768w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-720x781.png 720w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-580x629.png 580w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new-320x347.png 320w, https://nimbleindustries.io/wp-content/uploads/2020/05/github-satus-page-new.png 1280w" sizes="(max-width: 944px) 100vw, 944px"><figcaption>GitHub status page showing detailed statuses of each of the major components of their service.</figcaption></figure>



<p>They also moved their status page to a dedicated domain, <a href="https://www.githubstatus.com/" data-wpel-link="external" target="_blank" rel="nofollow">githubstatus.com</a>, <a href="https://nimbleindustries.io/2020/04/21/your-status-page-deserves-its-own-domain/" data-wpel-link="internal" rel="follow">which follows a best practice we recommend</a> to anyone who hosts a status page. All of this additional transparency, details, and historical data is a commendable effort to relay the most up-to-date information about the status of all GitHub systems. More providers of critical cloud infrastructure should emulate what GitHub has done. <a href="https://nimbleindustries.io/2019/12/07/your-status-page-is-useless-if-you-dont-use-it/" data-wpel-link="internal" rel="follow">Your status page is useless if you don’t use it.</a></p>



<h2>Conclusion</h2>



<p>What can we conclude from all of this data? Objectively, we can conclude that GitHub has published to their status page more frequently in the two years after their acquisition announcement. They have posted more incidents of disruption and downtime. Those incidents have been longer in duration. According to the data they provided, GitHub has been down more since the acquisition by Microsoft. </p>



<p>But that could be all a part of coordinated effort to be more transparent about their service status, an effort that should be applauded.</p>



<p>Our goal at StatusGator is not to shame anyone for disruptions and outages. Everyone experiences unexpected downtime. We simply strive to make status page data available and accessible in more useful ways. From Slack and <a href="https://nimbleindustries.io/2020/04/03/status-page-monitoring-in-microsoft-teams/" data-wpel-link="internal" rel="follow">Microsoft Teams</a>, to <a href="https://nimbleindustries.io/2020/04/30/status-page-webhooks-new-and-improved/" data-wpel-link="internal" rel="follow">webhooks</a>, an API, and more. StatusGator aggregates status page data and empowers you to keep your team informed. </p>



<h2>Try StatusGator</h2>



<figure><img src="https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-1024x175.png" alt="StatusGator logo" srcset="https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-1024x175.png 1024w, https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-300x51.png 300w, https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-768x131.png 768w, https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-1920x329.png 1920w, https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-720x123.png 720w, https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-580x99.png 580w, https://nimbleindustries.io/wp-content/uploads/2019/03/logo-large-320x55.png 320w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Is your team dependent on GitHub? Consider trying out <a href="https://statusgator.com/" data-wpel-link="internal" rel="follow">StatusGator</a>,&nbsp;free for 30 days. You can get notifications about GitHub and more than 670 other services with status pages we monitor. You can receive notifications in Microsoft Teams, Slack, by email, SMS, or webhook. Our favorite feature is a Slack integration with a <code>/statuscheck</code> slash command that allows querying the status of any service, right from where your team hangs out.</p>



<p><a href="https://statusgator.com/" data-wpel-link="internal" rel="follow">Try a 30 day free trial</a>&nbsp;of StatusGator and let us know what you think.</p>

                        
                            
            
        </div>
        
                             
    </article>
</div>
				
								
							</div>

		
	



			

		</div></div>]]>
            </description>
            <link>https://nimbleindustries.io/2020/06/04/has-github-been-down-more-since-its-acquisition-by-microsoft/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23676199</guid>
            <pubDate>Mon, 29 Jun 2020 10:31:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Arduino FIDO2 Authenticator]]>
            </title>
            <description>
<![CDATA[
Score 170 | Comments 87 (<a href="https://news.ycombinator.com/item?id=23676006">thread link</a>) | @snakeye
<br/>
June 29, 2020 | https://en.ovcharov.me/2020/06/29/uru-card-arduino-fido2-authenticator/ | <a href="https://web.archive.org/web/*/https://en.ovcharov.me/2020/06/29/uru-card-arduino-fido2-authenticator/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<section>
<div>
<div>
<section>



<article>
<p>After publishing the URU Key project people keep asking me to make it open source. I have tried to organize sources in a more readable way but I still think that plain C and ESP IDF are too difficult for the broad audience. And, unfortunately, the biometrics part is covered by NDA and can not be published.</p>
<p>Therefore I am starting a new project to address these and other issues.</p>
<h2 id="size-and-form-factor">Size and form factor</h2>
<p>A few months ago I have finalized the <a href="https://en.ovcharov.me/2020/04/06/uru-key-final-hardware-design/">URU Key hardware design</a> and started working on housing for it. Yes, it is very small and lightweight but carrying it around is a kind of a problem. The device is too fragile to be worn on the keyring and a bit thick to put in the pocket. The necklace is not my style.</p>
<figure>
<img src="https://d33wubrfki0l68.cloudfront.net/31ed3b469871b100ae43a0dbae4930d2f3fb272b/61808/uploads/resized/uru-key-w800.jpg" data-src="https://d33wubrfki0l68.cloudfront.net/31ed3b469871b100ae43a0dbae4930d2f3fb272b/61808/uploads/resized/uru-key-w800.jpg" data-srcset="/uploads%2Fresized%2Furu-key-w150.jpg 150w, /uploads%2Fresized%2Furu-key-w300.jpg 300w, /uploads%2Fresized%2Furu-key-w600.jpg 600w, /uploads%2Fresized%2Furu-key-w800.jpg 800w, /uploads%2F2020%2F06%2F29%2Furu-key.jpg 1200w" sizes="(max-width: 230px) 150px, (max-width: 380px) 300px, (max-width: 680px) 600px, (max-width: 880px) 800px, 1200px" alt="URU Key with the battery" title="URU Key with the battery" srcset="https://en.ovcharov.me/uploads%2Fresized%2Furu-key-w150.jpg 150w, https://en.ovcharov.me/uploads%2Fresized%2Furu-key-w300.jpg 300w, https://en.ovcharov.me/uploads%2Fresized%2Furu-key-w600.jpg 600w, https://en.ovcharov.me/uploads%2Fresized%2Furu-key-w800.jpg 800w, https://en.ovcharov.me/uploads%2F2020%2F06%2F29%2Furu-key.jpg 1200w"><figcaption>URU Key with the battery</figcaption></figure>
<p>However, my wallet is always with me. The PCB sized as a standard credit card should perfectly fit there. The power source becomes a problem, that’s true. But, wait, is it difficult to find a charger or power bank with Micro USB nowadays?</p>
<figure>
<img src="https://d33wubrfki0l68.cloudfront.net/1218f869b13f89ff97edd4cf56c554a12caa4e77/36a93/uploads/resized/uru-card-wallet-w800.jpg" data-src="https://d33wubrfki0l68.cloudfront.net/1218f869b13f89ff97edd4cf56c554a12caa4e77/36a93/uploads/resized/uru-card-wallet-w800.jpg" data-srcset="/uploads%2Fresized%2Furu-card-wallet-w150.jpg 150w, /uploads%2Fresized%2Furu-card-wallet-w300.jpg 300w, /uploads%2Fresized%2Furu-card-wallet-w600.jpg 600w, /uploads%2Fresized%2Furu-card-wallet-w800.jpg 800w, /uploads%2F2020%2F06%2F29%2Furu-card-wallet.jpg 1200w" sizes="(max-width: 230px) 150px, (max-width: 380px) 300px, (max-width: 680px) 600px, (max-width: 880px) 800px, 1200px" alt="URU Card in the wallet" title="URU Card in the wallet" srcset="https://en.ovcharov.me/uploads%2Fresized%2Furu-card-wallet-w150.jpg 150w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-wallet-w300.jpg 300w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-wallet-w600.jpg 600w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-wallet-w800.jpg 800w, https://en.ovcharov.me/uploads%2F2020%2F06%2F29%2Furu-card-wallet.jpg 1200w"><figcaption>URU Card in the wallet</figcaption></figure>
<p>The name <strong>URU Card</strong> makes a lot of sense for this project, isn’t it?</p>
<h2 id="user-interface">User interface</h2>
<p>As on one hand, I can not use biometrics for open source project and on other hand, I do not want to omit the authentication completely leaving the device insecure there is a need for some form of user verification. Simple touch keyboard and OLED screen should allow people to enter pin code or password.</p>
<figure>
<img src="https://d33wubrfki0l68.cloudfront.net/421c686c17665f43cfbb25fd3059190a055e4ae5/b0f7c/uploads/resized/uru-card-2-w800.jpg" data-src="https://d33wubrfki0l68.cloudfront.net/421c686c17665f43cfbb25fd3059190a055e4ae5/b0f7c/uploads/resized/uru-card-2-w800.jpg" data-srcset="/uploads%2Fresized%2Furu-card-2-w150.jpg 150w, /uploads%2Fresized%2Furu-card-2-w300.jpg 300w, /uploads%2Fresized%2Furu-card-2-w600.jpg 600w, /uploads%2Fresized%2Furu-card-2-w800.jpg 800w, /uploads%2F2020%2F06%2F29%2Furu-card-2.jpg 1200w" sizes="(max-width: 230px) 150px, (max-width: 380px) 300px, (max-width: 680px) 600px, (max-width: 880px) 800px, 1200px" alt="URU Card - touch keyboard and OLED screen" title="URU Card - touch keyboard and OLED screen" srcset="https://en.ovcharov.me/uploads%2Fresized%2Furu-card-2-w150.jpg 150w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-2-w300.jpg 300w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-2-w600.jpg 600w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-2-w800.jpg 800w, https://en.ovcharov.me/uploads%2F2020%2F06%2F29%2Furu-card-2.jpg 1200w"><figcaption>URU Card - touch keyboard and OLED screen</figcaption></figure>
<p>The keyboard should be implemented with the <strong>MPR121</strong> I2C touch-sensor controller, and the screen is a widely available <strong>OLED</strong> screen with the <strong>SSD1306</strong> controller. The screen is placed in the special cut in the PCB keeping the device thickness below 2 millimetres.</p>
<h2 id="framework-for-the-development">Framework for the development</h2>
<p>Fortunately, the <strong>Arduino</strong> framework is ported to the <strong>ESP32</strong> platform. There are hundreds of libraries for almost every use case and this factor should significantly simplify the project. There are libraries for <strong>ATECC508A</strong>, <strong>MPR121</strong> and <strong>SSD1306</strong> already. All that is needed is to wire everything together.</p>
<p>However, the Arduino IDE will be hardly usable for a project complex like this one. I am going to use Visual Studio Code + <strong>PlatformIO</strong> for the development and recommend others to do the same.</p>
<h2 id="the-current-state-of-the-project">The current state of the project</h2>
<p>At the moment the working <strong>BLE server</strong> with <strong>FIDO2</strong> endpoints is implemented. The device is “visible” and the computer connects to it in order to perform an authentication procedure. However, the commands are not implemented yet - it’s going to be the next step.</p>
<figure>
<img src="https://d33wubrfki0l68.cloudfront.net/42cf03f71cfe1940cec7bbce89f7b7689bcdc96f/5b9eb/uploads/resized/uru-card-3-w800.jpg" data-src="https://d33wubrfki0l68.cloudfront.net/42cf03f71cfe1940cec7bbce89f7b7689bcdc96f/5b9eb/uploads/resized/uru-card-3-w800.jpg" data-srcset="/uploads%2Fresized%2Furu-card-3-w150.jpg 150w, /uploads%2Fresized%2Furu-card-3-w300.jpg 300w, /uploads%2Fresized%2Furu-card-3-w600.jpg 600w, /uploads%2Fresized%2Furu-card-3-w800.jpg 800w, /uploads%2F2020%2F06%2F29%2Furu-card-3.jpg 1200w" sizes="(max-width: 230px) 150px, (max-width: 380px) 300px, (max-width: 680px) 600px, (max-width: 880px) 800px, 1200px" alt="URU Card - the components" title="URU Card - the components" srcset="https://en.ovcharov.me/uploads%2Fresized%2Furu-card-3-w150.jpg 150w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-3-w300.jpg 300w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-3-w600.jpg 600w, https://en.ovcharov.me/uploads%2Fresized%2Furu-card-3-w800.jpg 800w, https://en.ovcharov.me/uploads%2F2020%2F06%2F29%2Furu-card-3.jpg 1200w"><figcaption>URU Card - the components</figcaption></figure>
<p>There is a PCB design as well and you can try to build the device, but do it on your own risk - it’s in a very early stage now.</p>
<h2 id="joining-the-project">Joining the project</h2>
<p>Sweetest part. The project is free to join for everyone. The minimal requirement is just an ESP32 development board like the one below.</p>
<figure>
<img src="https://d33wubrfki0l68.cloudfront.net/69b040b8cb4968124bd224f9590788e74ac50f1d/f2b5c/uploads/resized/esp32-dev-board-w800.jpg" data-src="https://d33wubrfki0l68.cloudfront.net/69b040b8cb4968124bd224f9590788e74ac50f1d/f2b5c/uploads/resized/esp32-dev-board-w800.jpg" data-srcset="/uploads%2Fresized%2Fesp32-dev-board-w150.jpg 150w, /uploads%2Fresized%2Fesp32-dev-board-w300.jpg 300w, /uploads%2Fresized%2Fesp32-dev-board-w600.jpg 600w, /uploads%2Fresized%2Fesp32-dev-board-w800.jpg 800w, /uploads%2F2020%2F06%2F29%2Fesp32-dev-board.jpg 1200w" sizes="(max-width: 230px) 150px, (max-width: 380px) 300px, (max-width: 680px) 600px, (max-width: 880px) 800px, 1200px" alt="ESP32 development board" title="ESP32 development board" srcset="https://en.ovcharov.me/uploads%2Fresized%2Fesp32-dev-board-w150.jpg 150w, https://en.ovcharov.me/uploads%2Fresized%2Fesp32-dev-board-w300.jpg 300w, https://en.ovcharov.me/uploads%2Fresized%2Fesp32-dev-board-w600.jpg 600w, https://en.ovcharov.me/uploads%2Fresized%2Fesp32-dev-board-w800.jpg 800w, https://en.ovcharov.me/uploads%2F2020%2F06%2F29%2Fesp32-dev-board.jpg 1200w"><figcaption>ESP32 development board</figcaption></figure>
<p>The security element, screen and keyboard can be purchased separately and attached as external modules.</p>
<p>The links to the GitHub repository and other useful resources are given below.</p>
<p>I will be really thankful if consider sharing the project and leave comments with your thoughts and suggestions.</p>
<h2 id="references">References</h2>
<ul>
<li><a rel="nofollow noopener noreferrer" target="_blank" href="https://github.com/uru-card/uru-card">GitHub repository</a></li>
<li><a rel="nofollow noopener noreferrer" target="_blank" href="https://hackaday.io/project/173443-uru-card">Project on Hackaday.io</a></li>
<li><a rel="nofollow noopener noreferrer" target="_blank" href="https://platformio.org/">PlatformIO</a></li>
</ul>
</article>
</section>
</div>
</div>
</section>
</div></div>]]>
            </description>
            <link>https://en.ovcharov.me/2020/06/29/uru-card-arduino-fido2-authenticator/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23676006</guid>
            <pubDate>Mon, 29 Jun 2020 09:57:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Do call yourself a programmer, and other career advice (2013)]]>
            </title>
            <description>
<![CDATA[
Score 115 | Comments 48 (<a href="https://news.ycombinator.com/item?id=23675363">thread link</a>) | @luu
<br/>
June 29, 2020 | http://yosefk.com/blog/do-call-yourself-a-programmer-and-other-career-advice.html | <a href="https://web.archive.org/web/*/http://yosefk.com/blog/do-call-yourself-a-programmer-and-other-career-advice.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>This is a (very late) reply to Patrick McKenzie's "<a href="http://www.kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/">Don't Call Yourself A Programmer, And Other Career Advice</a>". I find much of his advice very sensible, and it might be very helpful to someone in the beginning of their career – assuming they can act upon it (and I really don't know whether my 20-year-old self could actually use the advice to improve his negotiation skills, for example).</p>
<p>A few things in the article I disagree with, however. Here I'll mostly focus on those few things, recommending you to read the original article so that you don't miss the rest of it.</p>
<p>"Disagree" is not necessarily the right word – a more precise way to put it would be "it's different in my experience". Which is to be expected because both of us are speaking based on our own careers, which have been rather different. Patrick McKenzie is a small business owner running <a href="http://www.bingocardcreator.com/">Bingo Card Creator</a> and a successful consultant. I'm a lead chip architect at a billion-dollar company. Both of us have thus traveled some distance away from "purely programming" (whatever that means), but in rather different directions.</p>
<p><strong>What company are you going to work for?<br>
</strong></p>
<p>Patrick McKenzie says 90% of the jobs involve things like implementing an internal travel expense reporting form, rather than a product shipped to external customers. He advises you to get used to the idea, even though such software is "soul-crushingly boring" as he puts it.</p>
<p>How bad is it, and is it really 90% of the jobs? Spolsky <a href="http://www.joelonsoftware.com/items/2007/12/04.html">thinks</a> it's maybe 80% – and that it's bad enough to "drain the life out of you". He goes on to elaborate why it "sucks to be an in-house programmer":</p>
<ul>
<li>There's rarely a business reason to improve in-house software past the point of "barely good enough". "Forget any pride of craftsmanship – you're going to churn out embarrassing junk".</li>
<li>At software companies, what you do is more directly related to the way the company makes money, so you're more likely to be respected. "A programmer is never going to rise to become CEO of Viacom, but you might well rise to become CEO of a tech company." "…no matter how critical it was for Viacom to get this internet thing  right, when it came time to assign people to desks, the in-house  programmers were stuck with 3 people per cubicle in a dark part of the  office".</li>
</ul>
<p>Note that McKenzie and Spolsky are in almost complete agreement over these points. But then Spolsky says you should be gunning for a position in a software company – the environment where creatures of your kind naturally thrive. Conversely, McKenzie explains how to prosper as a programmer outside software companies – <em>moving in the opposite direction of where things go by default</em> (being stuck in a dark part of the office while they're trying to outsource your job.)</p>
<p>So the question is which path you prefer. "Not so fast", you say: one of these jobs is way easier to land – 80-90% of the chances are you're not getting inside a software company – so it's not just a question of preference.</p>
<p>Here I disagree: even if only 10-20% of programmers work in software companies (where are the stats?..), and even if they're "the best" (according to what metric?), McKenzie himself says in that same article:</p>
<blockquote><p>You radically overestimate the average skill of the competition because of the crowd you hang around with:&nbsp; Many people already successfully employed as senior engineers cannot actually implement FizzBuzz.</p></blockquote>
<p>But if competition is relatively unskilled on average, you probably can land a job in the 10-20% of the sector that you want – as did most people who graduated around the time I did. So I rather firmly believe that it's a matter of choice: do you <em>want</em> to work on in-house software or one-off businessy projects of that kind, or do you prefer a software company?</p>
<p>Let's proceed to McKenzie's advice to in-house programmers – which should in itself help one make that choice.</p>
<p><strong>How to call yourself<br>
</strong></p>
<p>One such advice is:</p>
<blockquote><p>Don't call yourself a programmer. “Programmer” sounds like “anomalously high-cost peon who types some mumbo-jumbo into some other mumbo-jumbo.” Instead, describe yourself by what you have accomplished for previous employers vis-a-vis increasing revenues or reducing costs.</p></blockquote>
<p>Sure – an in-house programmer is likely doing some type of expensive mumbo-jumbo in the eyes of his non-technical MBA-wielding manager.</p>
<p>To me, however, a programmer is who I'm looking for, while a resume full of revenue increases and cost reductions sounds like an "anomalously high-cost parasite who types some mumbo-jumbo into Excel and PowerPoint, claiming credit for others' work".</p>
<p>McKenzie says a software company looks at this just like a company hiring internal programmers, essentially. His example is "the guy who wrote the backend billing code that 97% of Google’s revenue passes through – he’s now an angel investor". The guy apparently got rich by being near a "profit center" rather than through his unusual skills.</p>
<p>The thing is, in this case I believe he's talking about <a href="http://www.flownet.com/ron/">Ron Garret</a>, the PhD from NASA's Jet Propulsion Laboratory. Do you think they hired him because he described his work at the JPL in terms of revenues and costs? (BTW he didn't like working on the billing code, bought his stock options and quit, instead of choosing a career at the company's biggest "profit center".)</p>
<p>Did any unusual skills go into the billing code? Ron Garret <a href="http://www.flownet.com/ron/xooglers.html">says</a>:</p>
<blockquote><p>I did end up writing the credit card billing and accounting system, which is a nontrivial thing to get right. Fortunately for me, just before coming to Google I had taken some time to study computer security and cryptography, so I was actually well prepared for that particular task. …I designed the billing system to be secure against even a dishonest employee with root access (which is not such an easy thing to do). I have no idea if they are still using my system, but if they are then I'd feel pretty confident that my credit card number was not going to get stolen.</p></blockquote>
<p>Sounds to me that his technical knowledge and programming ability was the bulk of his contribution, whereas deep thoughts such as realizing that there will be some "cost reduction" due to not having credit card numbers stolen is not something an employer needs to hire anyone for.</p>
<p>So if I ever send out a resume as a chip architect, I will focus on my technical role in transitioning from fixed-function hardware accelerators to programmable processors, more than the manpower this saved and the business we won as a result (which I think were real outcomes of our work, but which is rather hard to quantify – as these things often are unless you're a business-friendly-sounding liar.)</p>
<p>Incidentally, I'm not sure <em>when </em>I'll send out that resume, which brings us to the next point.</p>
<p><strong>On job hopping, backstabbing, and the lack thereof </strong></p>
<blockquote><p>Co-workers and bosses are not usually your friends: You will spend a lot of time with co-workers.&nbsp; You may eventually become close friends with some of them, but in general, you will move on in three years…</p>
<p>&lt;your boss will&gt; attempt to do things that none of your actual friends would ever do,  like try to talk you down several thousand dollars in salary or  guilt-trip you into spending more time with the company when you could  be spending time with your actual friends. &nbsp;You will have other  coworkers who — affably and ethically — will suggest things which go  against your interests…</p></blockquote>
<p>There is a certain internal consistency to a view that your coworkers are not your friends, because you will move on in 3 years. In fact, it's a bit circular. They aren't your friends – because you'll move on. And why will you move on? Well, I dunno, maybe for a 10% salary increase. What's there to lose? Relationships with coworkers? But coworkers aren't your friends!</p>
<p>Again, I don't disagree, but rather offer an alternative view, equally internally consistent. I have stayed at one job for more than a decade, in large part because I'm rather attached to the people I work with. To be sure, I got raises, and I was ready to quit over employment terms – but it'd take much more than 10%.</p>
<p>Isn't it just a quantitative difference in preferences – a 10% raise not being fundamentally different than, say, 100%? Well, sufficiently large quantitative changes add up to qualitative changes, as Marxian dialectics or some other Soviet philosophy thingie that my parents sometimes quote taught us. What's going on is that both approaches can lead to career advancement, but they do so very differently.</p>
<p>If you're willing to change jobs over a small raise, you'll be changing them frequently. You won't get attached to people, or to the work you're doing together. You will be very good at finding jobs and you will know what's generally going on in the industry and what's in demand. You will <em>not </em>know that many things specific to any of your employers. <em>You and your employer will become very useful to each other fairly quickly, but you'll also be somewhat expendable for each other.</em></p>
<p>Alternatively, you can keep a job as long as it's a fun environment, requiring a significant raise once in a while. Your relationships with people combined with your long-term outlook can let you do things together that you otherwise couldn't plan or execute, and learn things you wouldn't have learned.</p>
<p>Much of my knowledge about chip design comes from ASIC hackers I worked with, and their willingness to develop their biggest ideas together with me came from trust that necessarily took time to build. It takes time to learn that none of you is in the habit of "suggesting things going against the other's interest", or pulling other unfriendly shenanigans.</p>
<p>Incidentally, if you stay at one place for a long while, then your worth to the employer grows to the point where you can get the significant raise that you'd quit over without actually quitting. Your worth can also grow well above what employers are willing to pay to experienced new hires, so there's no longer a point in switching jobs. This is somewhat analogous to becoming a consultant after having switched a …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://yosefk.com/blog/do-call-yourself-a-programmer-and-other-career-advice.html">http://yosefk.com/blog/do-call-yourself-a-programmer-and-other-career-advice.html</a></em></p>]]>
            </description>
            <link>http://yosefk.com/blog/do-call-yourself-a-programmer-and-other-career-advice.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23675363</guid>
            <pubDate>Mon, 29 Jun 2020 07:37:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Use Unix Pipes to Improve Chromecast Playback]]>
            </title>
            <description>
<![CDATA[
Score 173 | Comments 50 (<a href="https://news.ycombinator.com/item?id=23673825">thread link</a>) | @lowmemcpu
<br/>
June 28, 2020 | https://alexdelorenzo.dev/linux/2020/03/14/pipes | <a href="https://web.archive.org/web/*/https://alexdelorenzo.dev/linux/2020/03/14/pipes">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">  <p>When casting from YouTube to a Chromecast, sometimes the audio playback will <a href="https://support.google.com/chromecast/thread/13807879?hl=en">skip</a> and <a href="https://support.google.com/youtube/thread/17251626?hl=en">stutter</a>. This issue is independent of the quality of the video, the FPS, or the internet connection. Trying to watch certain videos will reliably cause playback issues on the Chromecast.</p> <p>According to <a href="https://support.google.com/chromecast/thread/13807879">this thread</a>, the playback issue only appeared in the latest and final firmware update for the Chromecast. Successful playback of the videos was possible in the past, and casting the downloaded YouTube videos eliminates the problem entirely. The problem exists solely when casting certain videos using the YouTube app.</p>  <p>Given that the problem only occurs with the YouTube app, you can download a video and cast it via <a href="https://github.com/xat/castnow"><code>castnow</code></a> or <a href="https://github.com/skorokithakis/catt"><code>catt</code></a>, skipping the YouTube app entirely.</p> <p>You could do something like:</p> <div><div><pre><code><span>#!/usr/bin/env bash</span>

<span>export </span><span>url</span><span>=</span><span>"https://youtu.be/Kas0tIxDvrg"</span>

<span>function </span>cast<span>()</span> <span>{</span>
    <span>url</span><span>=</span><span>"</span><span>$1</span><span>"</span>
    
    <span>filename</span><span>=</span><span>$(</span>youtube-dl <span>--get-filename</span> <span>"</span><span>$url</span><span>"</span><span>)</span>
    youtube-dl <span>"</span><span>$url</span><span>"</span>
    
    <span># wait a bit to download the file</span>
    castnow <span>"</span><span>$filename</span><span>"</span>
    <span>rm</span> <span>"</span><span>$filename</span><span>"</span>
<span>}</span>

cast <span>"</span><span>$url</span><span>"</span>
</code></pre></div></div> <p>But having to skip using the YouTube app to cast is already a clunky solution, and downloading every video before playing them is an even worse user experience. Instant playback and ephemeral streams are what make for a pleasant video streaming experience in 2020, and this solution implements neither of them</p>  <p><a href="https://github.com/ytdl-org/youtube-dl/blob/master/README.md">Since <code>youtube-dl</code> allows us to output to <code>stdout</code></a>, if we can hook its <code>stdout</code> to a casting app, we could emulate the instant playback and ephemeral videos we expect because we don’t have to wait for an entire file to download.</p> <p>Unfortunately, <code>castnow</code> and <code>catt</code> won’t cast from <code>stdin</code>. You’re expected to pass it file locations to cast from.</p> <p>This is where one of my favorite shell features really shines: <a href="https://tldp.org/LDP/abs/html/process-sub.html">process substitution</a>.</p> <p>With process substitution, Bash gives us a convenient way to make ephemeral <a href="https://en.wikipedia.org/wiki/Anonymous_pipe">anonymous pipes</a>. This method is both efficient and concurrent, making this primitive an apt choice to build a solution to the problem at hand. A process reading the <a href="https://en.wikipedia.org/wiki/Pipeline_(Unix)">pipe</a> blocks until the pipe is opened by another process for writing. A process writing to the pipe will suspend until the pipe’s buffer is read by another process. The anonymous pipe will automatically remove itself, and when it is manually removed, its dependent processes will be terminated.</p> <p>When using process substitution, a process’ <code>stdout</code> is hooked up to an anonymous pipe. That pipe can be accessed from a file descriptor, and the location of the pipe is given to the calling process.</p> <div><div><pre><code><span>$ </span><span>echo</span> &lt;<span>(</span><span>echo</span> <span>"Content sent to pipe"</span><span>)</span>
/dev/fd/63

<span>$ </span><span>cat</span> &lt;<span>(</span><span>echo</span> <span>"Content sent to pipe"</span><span>)</span>
Content sent to pipe
</code></pre></div></div> <p>In the example above, the <code>&lt;(command)</code> syntax is how we invoke process substitution in Bash. The output of <code>command</code> is written to an anonymous pipe, and the calling process is given the location of the file descriptor to access that pipe.</p> <p>Using that example, we can take advantage of process substitution:</p> <div><div><pre><code>vlc &lt;<span>(</span>youtube-dl <span>-q</span> <span>-o</span> - <span>"</span><span>$url</span><span>"</span><span>)</span>
</code></pre></div></div> <p>The command above will play the YouTube video locally with <a href="https://www.videolan.org/vlc/index.html">VLC</a>, and illustrates that process substitution can work for our use case.</p> <p>However, when we try to use <code>castnow</code>, we can’t cast from the pipe:</p> <div><div><pre><code><span>$ </span>castnow &lt;<span>(</span>youtube-dl <span>-q</span> <span>-o</span> - <span>"</span><span>$url</span><span>"</span><span>)</span>
Error: Load failed
</code></pre></div></div> <p>Nor can we cast with <code>catt</code>:</p> <div><div><pre><code><span>$ </span>catt cast &lt;<span>(</span>youtube-dl <span>-q</span> <span>-o</span> - <span>"</span><span>$url</span><span>"</span><span>)</span>
Error: The chosen file does not exist.
</code></pre></div></div> <p>We know we can use VLC locally, and VLC also lets you cast to a Chromecast using its IP address.</p> <p>Let’s try that again using VLC:</p> <div><div><pre><code><span>function </span>cast_vlc<span>()</span> <span>{</span>
    <span>path</span><span>=</span><span>"</span><span>$1</span><span>"</span>

    <span># get the ip address for chromecast.lan host</span>
    <span>ip</span><span>=</span><span>$(</span>dig +short chromecast.lan | <span>tail</span> <span>-n</span> 1<span>)</span>

    vlc <span>-I</span> ncurses <span>\</span>
      <span>--sout</span> <span>'#chromecast'</span> <span>\</span>
      <span>--sout-chromecast-ip</span><span>=</span><span>"</span><span>$ip</span><span>"</span> <span>\</span>
      <span>--demux-filter</span><span>=</span>demux_chromecast <span>\</span>
      <span>"</span><span>$path</span><span>"</span> &lt; /dev/tty
<span>}</span>

cast_vlc &lt;<span>(</span>youtube-dl <span>-q</span> <span>-o</span> - <span>"</span><span>$url</span><span>"</span><span>)</span>
</code></pre></div></div> <p>That works.</p> <p>As an aside, we hook VLC’s <code>stdin</code> to <code>/dev/tty</code> so that we can use the ncurses interface even if we invoke the function from a script.</p> <p>Let’s look at the <a href="https://wiki.videolan.org/Documentation:Modules/ncurses/">ncurses interface</a>.</p> <div> <p><img src="https://d33wubrfki0l68.cloudfront.net/d0822203c304db88f3cbec618e94a3d9f55cd269/c5939/assets/imgs/converted/vlc-fs8.webp" loading="lazy"> </p> </div> <p>It only displays the file descriptor, and very little about the video itself. I’m not a fan of that.</p>  <p>Instead of using anonymous pipes, we can use <a href="https://en.wikipedia.org/wiki/Named_pipe">named pipes</a>. Named pipes are like anonymous pipes, except they are not anonymous (they have a name) nor are they ephemeral. Named pipes still give us the efficiency and concurrency benefits that anonymous pipes give us, but Bash lacks the syntactic sugar it has for process substitution when it comes to named pipes.</p> <p>This is how we create named pipes, write to them, read from them and remove them.</p> <div><div><pre><code><span>$ </span><span>mkfifo </span>ourpipe
<span>$ </span><span>echo</span> <span>"Content in pipe"</span> <span>&gt;</span> ourpipe &amp;
<span>$ </span><span>cat </span>ourpipe
Content <span>in </span>pipe
<span>$ </span><span>rm </span>ourpipe
</code></pre></div></div> <p>Not as pretty as <code>&lt;(command)</code>, but it gets the job done.</p> <p>We can give a named pipe the same name as our YouTube video, and that way, the VLC interface will show the name of what we’re watching.</p> <div><div><pre><code><span>function </span>cast_ytdl<span>()</span> <span>{</span>
  <span>url</span><span>=</span><span>"</span><span>$1</span><span>"</span>

  <span># create a temporary named pipe</span>
  <span># why? because vlc will show the file descriptor path if we just use process substitution</span>
  <span>filename</span><span>=</span><span>$(</span>youtube-dl <span>--get-filename</span> <span>"</span><span>$url</span><span>"</span><span>)</span>
  <span>path</span><span>=</span><span>"/tmp/</span><span>$filename</span><span>"</span>
  <span>mkfifo</span> <span>"</span><span>$path</span><span>"</span>

  <span># download in background, push to named pipe</span>
  youtube-dl <span>-q</span> <span>-o</span> - <span>"</span><span>$url</span><span>"</span> <span>&gt;</span> <span>"</span><span>$path</span><span>"</span> &amp;
  <span>pid</span><span>=</span><span>"</span><span>$!</span><span>"</span>
  <span>disown</span> <span>$pid</span>

  <span># cast from named pipe</span>
  cast_vlc <span>"</span><span>$path</span><span>"</span>

  <span># cleanup process and named pipe</span>
  <span>kill</span> <span>-9</span> <span>"</span><span>$pid</span><span>"</span> &amp;&gt; /dev/null
<span>}</span>

cast_ytdl <span>"</span><span>$url</span><span>"</span>
</code></pre></div></div> <p>This works, too.</p> <p>We need to manually create a named pipe with <a href="https://linux.die.net/man/1/mkfifo"><code>mkfifo</code></a>, redirect <code>youtube-dl</code>’s <code>stdout</code> to the named pipe while running the process in the background, and then cleanup the process after casting from it via VLC, otherwise it might linger in the background. Each of those tasks would have been handled for us automatically using process substitution.</p> <p>But it does look a bit better:</p> <div> <p><img src="https://d33wubrfki0l68.cloudfront.net/25d68fb2085191adb26bb931e9bb31c638e5a634/f664c/assets/imgs/converted/vlc3-fs8.webp" loading="lazy"> </p> </div>  <p>Pipes, anonymous pipes and named pipes are also known by another name because of the way they behave: <a href="https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics)#Pipes">FIFOs</a>, or <strong>f</strong>irst <strong>i</strong>n, <strong>f</strong>irst <strong>o</strong>ut. What’s written to the pipe is read from the pipe in first in, first out order. This behavior maps well to video streaming.</p> <p>While you can interact with anonymous and named pipes like you would a file, the interface isn’t 1:1 with a <a href="https://en.wikipedia.org/wiki/Unix_file_types#Regular_file">standard file</a>. You cannot <code>seek()</code> forward or backward in pipe, you can just read the next forward values. For our use case, that means we cannot skip forward or backward in our streaming videos. We can only play, pause or stop the video.</p> <p>That’s not a problem for me, however it’s something that can be mitigated if it’s a problem for you. The first solution I can think of would be to write to a temporary file via <code>youtube-dl</code> and read from it. Or <a href="https://alexdelorenzo.dev/programming/2019/04/14/buffer">perhaps a temporary spooled file can act as a buffer for the pipe</a>, such that you can <code>seek()</code> through the buffer, but the buffer itself is ephemeral unlike a normal file.</p> </div></div>]]>
            </description>
            <link>https://alexdelorenzo.dev/linux/2020/03/14/pipes</link>
            <guid isPermaLink="false">hacker-news-small-sites-23673825</guid>
            <pubDate>Mon, 29 Jun 2020 02:27:39 GMT</pubDate>
        </item>
    </channel>
</rss>
