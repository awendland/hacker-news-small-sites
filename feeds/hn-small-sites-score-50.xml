<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 23 Sep 2020 04:24:42 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 23 Sep 2020 04:24:42 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[We need young programmers; We need old programmers]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 72 (<a href="https://news.ycombinator.com/item?id=24540919">thread link</a>) | @mrcsharp
<br/>
September 21, 2020 | https://blog.ploeh.dk/2020/09/14/we-need-young-programmers-we-need-old-programmers/ | <a href="https://web.archive.org/web/*/https://blog.ploeh.dk/2020/09/14/we-need-young-programmers-we-need-old-programmers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post">
	<p>
		<em>The software industry loves young people, but old-timers serve an important purpose, too.</em>
	</p>
	<p>
		Our culture idolises youth. There's several reasons for this, I believe. Youth seems synonymous with vigour, strength, beauty, and many other desirable qualities. The cynical perspective is that young people, while rebellious, also tend to be easy to manipulate, if you know which buttons to push. A middle-aged man like me isn't susceptible to the argument that I should buy a particular pair of Nike shoes because they're named after Michael Jordan, but for a while, one pair wasn't enough for my teenage daughter.
	</p>
	<p>
		In intellectual pursuits (like software development), youth is often extolled as the source of innovation. You're often confronted with examples like that of <a href="https://en.wikipedia.org/wiki/%C3%89variste_Galois">Évariste Galois</a>, who made all his discoveries before turning 21. <a href="https://en.wikipedia.org/wiki/Ada_Lovelace">Ada Lovelace</a> was around 28 years when she produced what is considered the 'first computer program'. <a href="https://en.wikipedia.org/wiki/Alan_Turing">Alan Turing</a> was 24 when he wrote <a href="https://en.wikipedia.org/wiki/Turing%27s_proof">On Computable Numbers, with an Application to the Entscheidungsproblem</a>.
	</p>
	<p>
		Clearly, young age is no detriment to making ground-breaking contributions. It has even become folklore that everyone past the age of 35 is a has-been whose only chance at academic influence is to write a textbook.
	</p>
	<h3 id="800321a74c054ea0b75815c86f4ce18d">
		The story of the five monkeys <a href="#800321a74c054ea0b75815c86f4ce18d" title="permalink">#</a>
	</h3>
	<p>
		You may have seen a story called <em>the five monkeys experiment</em>. It's most likely a fabrication, but it goes like this:
	</p>
	<p>
		A group of scientists placed five monkeys in a cage, and in the middle, a ladder with bananas on the top. Every time a monkey went up the ladder, the scientists soaked the rest of the monkeys with cold water. After a while, every time a monkey went up the ladder, the others would beat it up.
	</p>
	<p>
		After some time, none of the monkeys dared go up the ladder regardless of the temptation. The scientists then substituted one of the monkeys with a new one, who'd immediately  go for the bananas, only to be beaten up by the others. After several beatings, the new member learned not to climb the ladder even though it never knew why.
	</p>
	<p>
		A second monkey was substituted and the same occurred. The first monkey participated in beating the second. A third monkey was exchanged and the story repeated. The fourth was substituted and the beating was repeated. Finally the fifth monkey was replaced.
	</p>
	<p>
		Left was a group of five monkeys who, even though they never received a cold shower, continued to beat up any monkey who attempted to climb the ladder. If it was possible to ask the monkeys why they would beat up all who attempted to go up the ladder, the answer would probably be:
	</p>
	<p>
		"That's how we do things here."
	</p>
	<p>
		While the story is probably just that: a story, it tells us something about the drag induced by age and experience. If you've been in the business for decades, you've seen numerous failed attempts at something you yourself tried when you were young. You know that it can't be done.
	</p>
	<p>
		Young people don't know that a thing can't be done. If they can avoid the monkey-beating, they'll attempt the impossible.
	</p>
	<h3 id="4add8a9af0424d7e889d3125837ed611">
		Changing circumstances <a href="#4add8a9af0424d7e889d3125837ed611" title="permalink">#</a>
	</h3>
	<p>
		Is attempting the impossible a good idea?
	</p>
	<p>
		In general, no, because it's... impossible. There's a reason older people tell young people that a thing can't be done. It's not just because they're stodgy conservatives who abhor change. It's because they see the effort as wasteful. Perhaps they're even trying to be kind, guiding young people off a path where only toil and disappointment is to be found.
	</p>
	<p>
		What old people don't realise is that sometimes, circumstances change.
	</p>
	<p>
		What was impossible twenty years ago may not be impossible today. We see this happening in many fields. Producing a commercially viable electric car was impossible for decades, until, with the advances made in battery technology, it became possible.
	</p>
	<p>
		Technology changes rapidly in software development. People trying something previously impossible may find that it's possible today. Once, if you had lots of data, you had to store it in fully normalised form, because storage was expensive. For a decade, relational databases were the only game in town. Then circumstances changed. Storage became cheaper, and a new movement of NoSQL storage emerged. What was before impossible became possible.
	</p>
	<p>
		Older people often don't see the new opportunities, because they 'know' that some things are impossible. Young people push the envelope driven by a combination of zest and ignorance. Most fail, but a few succeed.
	</p>
	<h3 id="4272a069588e47f796646bd282b9de02">
		Lottery of the impossible <a href="#4272a069588e47f796646bd282b9de02" title="permalink">#</a>
	</h3>
	<p>
		I think of this process as a lottery. Imagine that every impossible thing is a red ball in an urn. Every young person who tries the impossible draws a random ball from the urn.
	</p>
	<p>
		The urn contains millions of red balls, but every now and then, one of them turns green. You don't know which one, but if you draw it, it represents something that was previously impossible which has now become possible.
	</p>
	<p>
		This process produces growth, because once discovered, the new and better way of doing things can improve society in general. Occasionally, the young discoverer may even gain some fame and fortune.
	</p>
	<p>
		It seems wasteful, though. Most people who attempt the impossible will reach the predictable conclusion. What was deemed impossible was, indeed, impossible.
	</p>
	<p>
		When I'm in a cynical mood, I don't think that it's youth in itself that is the source of progress. It's just the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a> applied. If there's a one in million chance that something will succeed, but ten million people attempt it, it's only a matter of time before one succeeds.
	</p>
	<p>
		Society at large can benefit from the success of the few, but ten million people still wasted their efforts.
	</p>
	<h3 id="016744f0ea77495c958a7914f08187db">
		We need the old, too <a href="#016744f0ea77495c958a7914f08187db" title="permalink">#</a>
	</h3>
	<p>
		If you accept the argument that young people are more likely to try the impossible, we need the young people. Do we need the old people?
	</p>
	<p>
		I'm turning fifty in 2020. You may consider that old, but I expect to work for many more years. I don't know if the software industry needs fifty-year-olds, but that's not the kind of old I have in mind. I'm thinking of people who have retired, or are close to retirement.
	</p>
	<p>
		In our youth-glorifying culture, we tend to dismiss the opinion and experiences of old people. <em>Oh, well, it's just a codgy old man</em> (or woman), we'll say.
	</p>
	<p>
		We ignore the experience of the old, because we believe that they haven't been keeping up with times. Their experiences don't apply to us, because we live under new circumstance. Well, see above.
	</p>
	<p>
		I'm not advocating that we turn into a gerontocracy that venerates our elders solely because of their age. Again, according to the law of large numbers, some people live to old age. There need not be any correlation between survivors and wisdom.
	</p>
	<p>
		We need the old to tell us the truth, because they have little to lose.
	</p>
	<h3 id="8b5c613ba6c44bb4b4e6dbba7ae7d19a">
		Nothing to lose <a href="#8b5c613ba6c44bb4b4e6dbba7ae7d19a" title="permalink">#</a>
	</h3>
	<p>
		In the last couple of years, I've noticed a trend. A book comes out, exposing the sad state of affairs in some organisation. This has happened regularly in Denmark, where I live. One book may expose the deplorable conditions of the Danish tax authorities, one may describe the situation in the ministry of defence, one criticises the groupthink associated with the climate crisis, and so on.
	</p>
	<p>
		Invariably, it turns out that the book is written by a professor emeritus or a retired department head.
	</p>
	<p>
		I don't think that these people, all of a sudden, had an epiphany after they retired. They knew all about the rot in the system they were part of, while they were part of it, but they've had too much to lose. You could argue that they should have said something before they retired, but that requires a moral backbone we can't expect most people to have.
	</p>
	<p>
		When people retire, the threat of getting fired disappears. Old people can speak freely to a degree most other people can't.
	</p>
	<p>
		Granted, many may simply use that freedom to spew bile or shout <em>Get off my lawn!</em>, but many are in the unique position to reveal truths no-one else dare speak. Many are, perhaps, just bitter, but some may possess knowledge that they are in a unique position to reveal.
	</p>
	<p>
		When that grumpy old guy on Twitter writes something that makes you uncomfortable, consider this: he may still be right.
	</p>
	<h3 id="2d64bd2c7ccb4b7ca2418802ed82689e">
		Being unreasonable <a href="#2d64bd2c7ccb4b7ca2418802ed82689e" title="permalink">#</a>
	</h3>
	<p>
		In a way, you could say that we need young and old people for the same fundamental reason. Not all of them, but enough of them, are in a position to be unreasonable.
		</p><blockquote>
			<p>
				"The reasonable man adapts himself to the world: the unreasonable one persists in trying to adapt the world to himself. Therefore all progress depends on the unreasonable man."
			</p>
			
		</blockquote><p>
		Young people and old people are unreasonable in each their own way, and we need both.
	</p>
	<h3 id="df88f595ec814e2bafcbd018ff5f5ad2">
		Conclusion <a href="#df88f595ec814e2bafcbd018ff5f5ad2" title="permalink">#</a>
	</h3>
	<p>
		We need young people in the software development industry. Because of their vigour and inexperience, they'll push the envelope. Most will fail to do the impossible, but a few succeed.
	</p>
	<p>
		This may seem like a cynical view, but we've all been young, and most of us have been through such a phase. It's like a rite of passage, and even if you fail to make your mark on the world, you're still likely to have learned a lot.
	</p>
	<p>
		We need old people because they're in a position to speak truth to the world. Notice that I didn't make my argument about the <em>experience</em> of old-timers. Actually, I find that valuable as well, but that's the ordinary argument: <em>Listen to old people, because they have experience and wisdom.</em>
	</p>
	<p>
		Some of them do, at least.
	</p>
	<p>
		I didn't make much out of that argument, because you already know it. There'd be no reason to write this essay if that was all I had to say. Old people have less on the line, so they can speak more freely. If someone you used to admire retires and all of a sudden starts saying or writing unpleasant and surprising things, there might be a good explanation, and it might be a good idea to pay attention.
	</p>
	<p>
		Or …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.ploeh.dk/2020/09/14/we-need-young-programmers-we-need-old-programmers/">https://blog.ploeh.dk/2020/09/14/we-need-young-programmers-we-need-old-programmers/</a></em></p>]]>
            </description>
            <link>https://blog.ploeh.dk/2020/09/14/we-need-young-programmers-we-need-old-programmers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24540919</guid>
            <pubDate>Mon, 21 Sep 2020 08:05:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Little Things: Speeding up C++ compilation]]>
            </title>
            <description>
<![CDATA[
Score 118 | Comments 61 (<a href="https://news.ycombinator.com/item?id=24537231">thread link</a>) | @ingve
<br/>
September 20, 2020 | https://codingnest.com/the-little-things-speeding-up-c-compilation/ | <a href="https://web.archive.org/web/*/https://codingnest.com/the-little-things-speeding-up-c-compilation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    <article>

        

        <section>
            <p><em>The Little Things</em> is a new series of posts based on Locksley's internal training sessions. Often the contents are either proprietary (e.g. the inner workings of specific master key platforms) or not generally interesting (e.g. our internal libraries and tooling), but sometimes the contents are suitable for a wider audience, in which case I want to share them.</p>
<hr>
<p>This post will be about some source-level techniques for speeding up C++ compilation, and their (dis)advantages. It will <strong>not</strong> talk about things external to C++, such as buying better hardware, using a better build system, or using smarter linker<sup><a href="#fn1" id="fnref1">[1]</a></sup>. It will also not talk about the tooling that can find compilation bottlenecks, as that will be a subject of a later post.</p>
<h2 id="overviewofccompilationmodel">Overview of C++ compilation model</h2>
<p>I will start with a quick overview of the C++ compilation model, to provide context for some of the tricks I will show later. Note that this overview will be very coarse, if you want a detailed look at the subtleties of the <em>9</em> phase compilation model defined in the C++ standard, look elsewhere.</p>
<p>We will consider the compilation of C++ binary to happen in 3 steps:</p>
<ol>
<li>Preprocessing</li>
<li>Compilation</li>
<li>Linking</li>
</ol>
<h3 id="preprocessing">Preprocessing</h3>
<p>The first step is preprocessing. During it, the preprocessor takes a .cpp file and parses it, looking for <em>preprocessor directives</em>, such as <code>#include</code>, <code>#define</code>, <code>#ifdef</code>, etc.</p>
<p>Let's take this super simple file as an example</p>
<pre><code>// tiny.cpp
#define KONSTANTA 123

int main() {
    return KONSTANTA;
}
</code></pre>
<p>It contains one preprocessor directive, <code>#define</code>. It says that any following occurence of <code>KONSTANTA</code> should be replaced with <code>123</code>. Running the file through a preprocessor leads to output like this one:</p>
<pre><code>$ clang++ -E tiny.cpp
# 1 "tiny.cpp"
# 1 "&lt;built-in&gt;" 1
# 1 "&lt;built-in&gt;" 3
# 383 "&lt;built-in&gt;" 3
# 1 "&lt;command line&gt;" 1
# 1 "&lt;built-in&gt;" 2
# 1 "tiny.cpp" 2


int main() {
    return 123;
}
</code></pre>
<p>We can see that in <code>return KONSTANTA</code> the <code>KONSTANTA</code> part was replaced with <code>123</code>, as it should be. We also see that the compiler left itself a bunch of other notes, that we do not care about that much<sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p>
<p>The big problem with the preprocessor model is that the <code>#include</code> directive literally means "copy-paste all of this file's contents here". Of course, if that file's contents contain further <code>#include</code> directives, then more files will be opened, their contents copied out, and in turn, the compiler will have more code to deal with. In other words, preprocessing increases the size of the input, usually significantly so.</p>
<p>The following is a simple "Hello World" in C++, using streams.</p>
<pre><code>// hello-world.cpp
#include &lt;iostream&gt;

int main() {
    std::cout &lt;&lt; "Hello World\n";
}
</code></pre>
<p>After preprocessing, the file will have <strong>28115</strong><sup><a href="#fn3" id="fnref3">[3]</a></sup> lines for the next step, compilation, to deal with.</p>
<pre><code>$ clang++ -E hello-world.cpp | wc -l
28115
</code></pre>
<h3 id="compilation">Compilation</h3>
<p>After a file is preprocessed, it is compiled into an <em>object file</em>. Object files contain the actual code to run, but cannot be run without linking. One of the reasons for this is that object files can refer to symbols (usually functions) that they do not have the definition (code) for. This happens, e.g. if a .cpp file uses a function that has been declared, but not defined, like so:</p>
<pre><code>// unlinked.cpp
void bar(); // defined elsewhere (hopefully)

void foo() {
    bar();
}
</code></pre>
<p>You can look inside a compiled object file to see what symbols it provides and what symbols it needs, using <code>nm</code> (Linux) or <code>dumpbin</code> (Windows). If we look at the output for the <code>unlinked.cpp</code> file, we get this:</p>
<pre><code>$ clang++ -c unlinked.cpp &amp;&amp; nm -C unlinked.o
                 U bar()
0000000000000000 T foo()
</code></pre>
<p><code>U</code> means that the symbol is not defined in this object file. <code>T</code> means that the symbol is in the text/code section and that it is exported, which means that other object files can get <code>foo</code> from this <code>unlinked.o</code>. It is important to know that symbols might also be present in an object file, but not be available to other object files. Such symbols are marked with <code>t</code>.</p>
<h3 id="linking">Linking</h3>
<p>After all the files have been compiled into object files, they have to be <em>linked</em> into the final binary artefact. During linking, all the various object files are smashed together in a specific format, e.g. ELF, and the various references to undefined symbols in object files are resolved with the address of the symbol, as provided by a different object file (or library).</p>
<p>With this overview done, we can start tackling the different ways to speed up the compilation of your code. Let's start simple.</p>
<h2 id="includeless"><code>#include</code> less</h2>
<p>Including a file usually brings in a <em>lot</em> of extra code, which the compiler then needs to parse and check. Thus the simplest, and usually also the biggest, way to speed up the compilation of your code, is to just <code>#include</code> fewer files. Reducing the include set is especially beneficial in header files, as they are likely to be included from other files, thus amplifying the impact of your improvements.</p>
<p>The easiest way to do this is to remove any unused includes. Unused includes shouldn't happen often, but sometimes they are left behind during refactoring, and using a tool like <a href="https://include-what-you-use.org/">IWYU</a> <em>can</em><sup><a href="#fn4" id="fnref4">[4]</a></sup> make it simple to do. However, just cleaning up unused includes is unlikely to provide many benefits, and so you will have to reach for bigger guns, forward declarations and manual outlining.</p>
<p>But before explaining forward declarations and manual outlining, I want to go over the costs of header inclusion quickly, so we can build up intuition on what sort of speed-ups we can expect from pruning down include graphs.</p>

<p>The table below shows the time required by Clang<sup><a href="#fn5" id="fnref5">[5]</a></sup> to compile a file that <em>only</em> includes some stdlib headers.</p>

<table>
<thead>
<tr>
<th>header(s) included</th>
<th>time to compile (ms)</th>
<th>difference from baseline (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>none</td>
<td>11.3  ± 0.2</td>
<td>-</td>
</tr>
<tr>
<td><code>&lt;vector&gt;</code></td>
<td>68.8  ± 0.3</td>
<td>57.5 ±  0.36</td>
</tr>
<tr>
<td><code>&lt;string&gt;</code></td>
<td>136.3  ± 0.8</td>
<td>125.0 ±  0.82</td>
</tr>
<tr>
<td><code>&lt;stdexcept&gt;</code></td>
<td>137.0  ± 0.8</td>
<td>125.7 ±  0.82</td>
</tr>
<tr>
<td><code>&lt;vector&gt;</code>, <code>&lt;string&gt;</code></td>
<td>155.3  ± 0.9</td>
<td>144.0 ±  0.92</td>
</tr>
<tr>
<td><code>&lt;string&gt;</code>, <code>&lt;stdexcept&gt;</code></td>
<td>136.7  ± 0.7</td>
<td>125.4 ±  0.73</td>
</tr>
<tr>
<td><code>&lt;vector&gt;</code>, <code>&lt;string&gt;</code>, <code>&lt;stdexcept&gt;</code></td>
<td>156.1  ± 0.8</td>
<td>144.8 ±  0.82</td>
</tr>
</tbody>
</table>
<p>The first row shows the time needed to compile a completely empty file, to provide a baseline time required by the compiler to start, read the file, and do nothing. The other lines are more interesting. As the second line says, just including <code>&lt;vector&gt;</code> adds 57 ms to compilation times, even though there will be no actual line emitted. As we can see, the cost to include <code>&lt;string&gt;</code> is more than double of <code>&lt;vector&gt;</code>, and the cost to include <code>&lt;stdexcept&gt;</code> is about the same as for <code>&lt;string&gt;</code>.</p>
<p>More interesting are the rows for combinations of headers, because no combination of headers is as expensive as compiling each of them on its own. The reason is quite simple: their internal include overlap. The most extreme case is <code>&lt;string&gt;</code> + <code>&lt;stdexcept&gt;</code>, because <code>&lt;stdexcept&gt;</code> is basically <code>&lt;string&gt;</code> + couple of types deriving from <code>std::exception</code>.</p>
<p>What you should take away from this are two things:</p>
<ul>
<li>Even if you do not use anything from a header, you still have to pay for it.</li>
<li>Include costs do not neatly sum, nor subtract.</li>
</ul>
<p>Now let's go through techniques we can use to include fewer files.</p>
<h3 id="forwarddeclarations">Forward declarations</h3>
<p>Quite often, when we mention a type, we only need to know that it exists but do not need to know its definition. The common case is creating a pointer or a reference to a type, in which case you need a knowledge that the type exists (a <em>forward declaration</em>), but not what it looks like (a <em>definition</em>).</p>
<p>As an example, this header is valid:</p>
<pre><code>class KeyShape; // forward declaration

size_t count_differences(KeyShape const&amp; lhs, KeyShape const&amp; rhs);
</code></pre>
<p>as long as the implementation file includes the appropriate headers:</p>
<pre><code>#include "key-shape.hpp" // provides the full definition of KeyShape

size_t count_differences(KeyShape const&amp; lhs, KeyShape const&amp; rhs) {
    assert(lhs.positions() == rhs.positions());
    ...
}
</code></pre>
<p>You can also use forward declaration together with some templated classes, whose size does not change depending on the template argument, e.g. <code>std::unique_ptr</code> and <code>std::vector</code><sup><a href="#fn6" id="fnref6">[6]</a></sup>. However, doing so can force you to outline your constructors, destructors and other special member functions (<em>SMFs</em>), as those usually need to see the full definition of the type. Your code then ends up looking like this:</p>
<pre><code>// foo.hpp
#include &lt;memory&gt;

class Bar;

class Foo {
    std::unique_ptr&lt;Bar&gt; m_ptr;
public:
    Foo(); // = default;
    ~Foo(); // = default;
};
</code></pre>
<pre><code>// foo.cpp
#include "bar.hpp"

Foo::Foo() = default;
Foo::~Foo() = default;
</code></pre>
<p>Notice that we still use the compiler-generated default constructor and destructor, but do so in the <code>.cpp</code> file, where we see the full definition of <code>Bar</code>. I also like to use the <code>// = default;</code> comment to signal to other programmers reading the code that the SMF is explicitly declared but will be defaulted, and thus there won't be any special logic in it.</p>
<p>When using this technique, please remember that the outlined functions cannot be inlined without LTO. In other words, you probably do not want to outline <em>every</em> function just because you can, because calling trivial functions can be much more expensive than inlining their code directly.</p>
<h3 id="explicitoutlining">Explicit outlining</h3>
<p>The idea underlying explicit outlining is quite simple: sometimes we get better results if a piece of code is explicitly split away from a function. One of the most common reasons is, perhaps ironically, improving inlining by making the common path of a function small. However, in our case, the reason for doing this is to improve the compilation times.</p>
<p>If a piece of code is expensive to compile, and inlining it is not crucial for performance, only one TU has to pay for compiling it. The canonical example of this is throwing an exception in general, and exceptions from <code>&lt;stdexcept&gt;</code> in particular. Throwing an exception generates quite a lot of code, and throwing more complex standard exception types, such as <code>std::runtime_error</code>, also requires an expensive<sup><a href="#fn7" id="fnref7">[7]</a></sup> header, <code>&lt;stdexcept&gt;</code> to be included.</p>
<p>By instead replacing all <code>throw foo;</code> statements with calls to a helper function along the lines of <code>[[noreturn]] void throw_foo(char const* msg)</code>, the call sites become …</p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://codingnest.com/the-little-things-speeding-up-c-compilation/">https://codingnest.com/the-little-things-speeding-up-c-compilation/</a></em></p>]]>
            </description>
            <link>https://codingnest.com/the-little-things-speeding-up-c-compilation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24537231</guid>
            <pubDate>Sun, 20 Sep 2020 20:41:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Not Rust?]]>
            </title>
            <description>
<![CDATA[
Score 291 | Comments 271 (<a href="https://news.ycombinator.com/item?id=24536645">thread link</a>) | @dochtman
<br/>
September 20, 2020 | https://matklad.github.io/2020/09/20/why-not-rust.html | <a href="https://web.archive.org/web/*/https://matklad.github.io/2020/09/20/why-not-rust.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <article>
  
  <p>Sep 20, 2020</p>
  <p>I’ve recently read an article criticizing Rust, and, while it made a bunch of good points, I didn’t enjoy it — it was an easy to argue with piece.
In general, I feel that I can’t recommend an article criticizing Rust.
This is a shame — confronting drawbacks is important, and debunking low effort/miss informed attempts at critique sadly inoculates against actually good arguments.</p>
<p>So, here’s my attempt to argue <em>against</em> Rust:</p>
<div>
<dl>
<dt>Not All Programming is Systems Programming</dt>
<dd>
<div>
<div>
<p>Rust is a systems programming language.
It offers precise control over data layout and runtime behavior of the code, granting  you maximal performance and flexibility.
Unlike other systems programming languages, it also provides memory safety — buggy programs terminate in a well-defined manner, instead of unleashing (potentially security-sensitive) undefined behavior.</p>
<p>However, in many (most) cases, one doesn’t need ultimate performance or control over hardware resources.
For these situations, modern managed languages like Kotlin or Go offer decent speed, enviable
<a href="https://qconlondon.com/london-2017/system/files/presentation-slides/highperformancemanagedlanguages.pdf">time to performance</a>, and are memory safe by virtue of using a garbage collector for dynamic memory management.</p>
</div>
</div>
</dd>
<dt>Complexity</dt>
<dd>
<div>
<div>
<p>Programmer’s time is valuable, and, if you pick Rust, expect to spend some of it on learning the ropes.
Rust community poured a lot of time into creating high-quality teaching materials, but the Rust language <em>is</em> big.
Even if a Rust implementation would provide value for you, you might not have resources to invest into growing the language expertise.</p>
<p>Rust’s price for improved control is the curse of choice:</p>
<div>
<div>
<pre><code data-lang="rust"><table><tbody><tr><td><pre>1
2
3
4
5
6
</pre></td><td><pre><span>struct</span> <span>Foo</span>     <span>{</span> <span>bar</span><span>:</span> <span>Bar</span>         <span>}</span>
<span>struct</span> <span>Foo</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span> <span>bar</span><span>:</span> <span>&amp;</span><span>'a</span> <span>Bar</span>     <span>}</span>
<span>struct</span> <span>Foo</span><span>&lt;</span><span>'a</span><span>&gt;</span> <span>{</span> <span>bar</span><span>:</span> <span>&amp;</span><span>'a</span> <span>mut</span> <span>Bar</span> <span>}</span>
<span>struct</span> <span>Foo</span>     <span>{</span> <span>bar</span><span>:</span> <span>Box</span><span>&lt;</span><span>Bar</span><span>&gt;</span>    <span>}</span>
<span>struct</span> <span>Foo</span>     <span>{</span> <span>bar</span><span>:</span> <span>Rc</span><span>&lt;</span><span>Bar</span><span>&gt;</span>     <span>}</span>
<span>struct</span> <span>Foo</span>     <span>{</span> <span>bar</span><span>:</span> <span>Arc</span><span>&lt;</span><span>Bar</span><span>&gt;</span>    <span>}</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<p>In Kotlin, you write <code>class Foo(val bar: Bar)</code>, and proceed with solving your business problem.
In Rust, there are choices to be made, some important enough to have dedicated syntax.</p>
<p>All this complexity is there for a reason — we don’t know how to create a simpler memory safe low-level language.
But not every task requires a low-level language to solve it.</p>

</div>
</div>
</dd>
<dt>Compile Times</dt>
<dd>
<div>
<div>
<p>Compile times are a multiplier for everything.
A program written in a slower to run but faster to compile programming language can be <em>faster</em> to run because the programmer will have more time to optimize!</p>
<p>Rust intentionally picked slow compilers in the <a href="https://research.swtch.com/generic">generics dilemma</a>.
This is not necessary the end of the world (the resulting runtime performance improvements are real), but it does mean that you’ll have to fight tooth and nail for reasonable build times in larger projects.</p>
<p><code>rustc</code> implements what is probably the most advanced <a href="https://rustc-dev-guide.rust-lang.org/queries/incremental-compilation.html">incremental compilation</a> algorithm in production compilers, but this feels a bit like fighting with language compilation model.</p>
<p>Unlike C++, Rust build is not embarrassingly parallel; the amount of parallelism is limited by length of the critical path in the dependency graph.
If you have 40+ cores to compile, this shows.</p>
<p>Rust also lacks an analog for the <a href="https://en.cppreference.com/w/cpp/language/pimpl">pimpl</a> idiom, which means that changing a crate requires recompiling (and not just relinking) all of its reverse dependencies.</p>
</div>
</div>
</dd>
<dt>Maturity</dt>
<dd>
<div>
<div>
<p>Five years old, Rust is definitely a young language.
Even though its future looks bright, I will bet more money on “C will be around in ten years” than on “Rust will be around in ten years”
(See <a href="https://en.wikipedia.org/wiki/Lindy_effect">Lindy Effect</a>).
If you are writing software to last decades, you should seriously consider risks associated with picking new technologies.
(But keep in mind that picking Java over Cobol for banking software in 90s retrospectively turned out to be the right choice).</p>
<p>There’s only one complete implementation of Rust — the <a href="https://github.com/rust-lang/rust/"><code>rustc</code></a> compiler.
The most advanced alternative implementation, <a href="https://github.com/thepowersgang/mrustc"><code>mrustc</code></a>, purposefully omits many static safety checks.
<code>rustc</code> at the moment supports only a single production-ready backend — LLVM.
Hence, its support for CPU architectures is narrower than that of C, which has GCC implementation as well as a number of vendor specific proprietary compilers.</p>
<p>Finally, Rust lacks an official specification.
<a href="https://doc.rust-lang.org/reference/">The reference</a> is a work in progress, and does not yet document all the fine implementation details.</p>
</div>
</div>
</dd>
<dt>Alternatives</dt>
<dd>
<div>
<div>
<p>There are other languages besides Rust in systems programming space, notably, C, C++, and Ada.</p>
<p>Modern C++ provides <a href="https://www.viva64.com/en/pvs-studio/">tools</a> and <a href="https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines">guidelines</a> for improving safety.
There’s even a proposal for a Rust-like <a href="https://github.com/isocpp/CppCoreGuidelines/blob/master/docs/Lifetime.pdf">lifetimes</a> mechanism!
Unlike Rust, using these tools does not <em>guarantee</em> the absence of memory safety issues.
However, if you already maintain a large body of C++ code, it makes sense to check if following best practices and using <a href="https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html">sanitizers</a> helps with security issues.
This is hard, but clearly is easier than rewriting in another language!</p>
<p>If you use C, you can use formal methods to <a href="https://sel4.systems/Info/FAQ/proof.pml">prove</a> the absence of undefined behaviors, or just <a href="https://sqlite.org/testing.html">exhaustively test</a> everything.</p>
<p>Ada is memory safe if you don’t use dynamic memory (never call <code>free</code>).</p>
<p>Rust is an interesting point on the cost/safety curve, but is far from the only one!</p>
</div>
</div>
</dd>
<dt>Tooling</dt>
<dd>
<div>
<div>
<p>Rust tooling is a bit of a hit and miss.
The baseline tooling, the compiler and the build system
(<a href="https://doc.rust-lang.org/cargo/index.html">cargo</a>), are often cited as best in class.</p>
<p>But, for example, some runtime-related tools (most notably, heap profiling) are just absent — it’s hard to reflect on the runtime of the program if there’s no runtime!
Additionally, while IDE support is decent, it is nowhere near the Java-level of reliability.
Automated complex refactors of multi-million line programs are not possible in Rust today.</p>
</div>
</div>
</dd>
<dt>Integration</dt>
<dd>
<div>
<div>
<p>Whatever the Rust promise is, it’s a fact of life that today’s systems programming world speaks C, and is inhabited by C and C++.
Rust intentionally doesn’t try to mimic these languages — it doesn’t use C++-style classes or C ABI.</p>
<p>That means that integration between the worlds needs explicit bridges.
These are not seamless.
They are <code>unsafe</code>, not always completely zero-cost and need to be synchronized between the languages.
While the general promise of <a href="http://adventures.michaelfbryan.com/posts/how-to-riir/">piece-wise integration</a> holds up and the <a href="https://github.com/dtolnay/cxx">tooling</a> catches up, there is accidental complexity along the way.</p>
<p>One specific gotcha is that Cargo’s opinionated world view (which <em>is</em> a blessing for pure Rust projects) might make it harder to integrate with a bigger build system.</p>
</div>
</div>
</dd>
<dt>Performance</dt>
<dd>
<div>
<div>
<p>“Using LLVM” is not a universal solution to all performance problems.
While I am not aware of benchmarks comparing performance of C++ and Rust at scale, it’s not to hard to come up with a list of cases where Rust leaves some performance on the table relative to C++.</p>
<p>The biggest one is probably the fact that Rust’s move semantics is based on values (<code>memcpy</code> at the machine code level).
In contrast, C++ semantics uses special references you can steal data from (pointers at the machine code level).
In theory, compiler should be able to see through chain of copies; in practice it often doesn’t: <a href="https://github.com/rust-lang/rust/issues/57077">#57077</a>.
A related problem is the absence of placement new — Rust sometimes need to copy bytes to/from the stack, while C++ can construct the thing in place.</p>
<p>Somewhat amusingly, Rust’s default ABI (which is not stable, to make it as efficient as possible) is sometimes worse than that of C: <a href="https://github.com/rust-lang/rust/issues/26494#issuecomment-619506345">#26494</a>.</p>
<p>Finally, while in theory Rust code should be more efficient due to the significantly richer aliasing information, enabling aliasing-related optimizations triggers LLVM bugs and miscompilations: <a href="https://github.com/rust-lang/rust/issues/54878">#54878</a>.</p>
<p>But, to reiterate, these are cherry-picked examples, sometimes the field is tilted the other way.
For example, <code>std::unique_ptr</code> <a href="https://www.youtube.com/watch?v=rHIkrotSwcc&amp;feature=youtu.be&amp;t=1261">has a performance problem</a> which Rust’s <code>Box</code> lacks.</p>
<p>A potentially bigger issue is that Rust, with its definition time checked generics, is less expressive than C++.
So, some C++ <a href="http://eigen.tuxfamily.org/index.php?title=Expression_templates">template tricks</a> for high performance are not expressible in Rust using a nice syntax.</p>
</div>
</div>
</dd>
<dt>Meaning of Unsafe</dt>
<dd>
<div>
<div>
<p>An idea which is even more core to Rust than ownership &amp; borrowing is perhaps that of <code>unsafe</code> boundary.
That, by delineating all dangerous operations behind <code>unsafe</code> blocks and functions and insisting on providing a safe higher-level interface to them, it is possible to create a system which is both</p>
<div>
<ol>
<li>
<p>sound (non-<code>unsafe</code> code can’t cause undefined behavior),</p>
</li>
<li>
<p>and modular (different <code>unsafe</code> blocks can be checked separately).</p>
</li>
</ol>
</div>
<p>It’s pretty clear that the promise works out in practice: <a href="https://github.com/rust-fuzz/trophy-case">fuzzing Rust code</a> unearths panics, not buffer overruns.</p>
<p>But the theoretical outlook is not as rosy.</p>
<p><em>First</em>, there’s no definition of Rust memory model, so it is impossible to formally check if a given unsafe block is valid or not.
There’s informal definition of “things rustc does or might rely on” and in in-progress <a href="https://github.com/rust-lang/miri">runtime verifier</a>, but the actual model is in flux.
So there might be some <code>unsafe</code> code somewhere which works OK in practice today, might be declared invalid tomorrow, and broken by a new compiler optimization next year.</p>
<p><em>Second</em>, there’s also an observation that <code>unsafe</code> blocks are not, in fact, modular.
Sufficiently powerful <code>unsafe</code> blocks can, in effect, extend the language.
Two such extensions might be fine in isolation, but lead to undefined behavior if used simultaneously:
<a href="https://smallcultfollowing.com/babysteps/blog/2016/10/02/observational-equivalence-and-unsafe-code/">Observational equivalence and unsafe code</a>.</p>

</div>
</div>
</dd>
</dl>
</div>
<p>Here are some thing I’ve deliberately omitted from the list:</p>
<div>
<ul>
<li>
<p>Economics (“it’s harder to hire Rust programmers”) — I feel that the “maturity” section captures the essence of it which is not reducible to chicken and egg problem.</p>
</li>
<li>
<p>Dependencies (“stdlib is too small / everything has too many deps”) — given how good Cargo and the relevant parts of the language are, I personally don’t see this as a problem.</p>
</li>
<li>
<p>Dynamic linking (“Rust should have stable ABI”) — I don’t think this is a strong argument. Monomorphization is pretty fundamentally incompatible with dynamic linking and there’s C ABI if you really need to. I do think that the situation here can be improved, <a href="https://internals.rust-lang.org/t/a-stable-modular-abi-for-rust/12347/10?u=matklad">but I don’t think that improvement needs to be Rust-specific</a>.</p>
</li>
</ul>
</div>

</article>

  </div></div>]]>
            </description>
            <link>https://matklad.github.io/2020/09/20/why-not-rust.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24536645</guid>
            <pubDate>Sun, 20 Sep 2020 19:27:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We need physical audio kill switches]]>
            </title>
            <description>
<![CDATA[
Score 418 | Comments 401 (<a href="https://news.ycombinator.com/item?id=24535408">thread link</a>) | @stargrave
<br/>
September 20, 2020 | https://rubenerd.com/we-need-physical-audio-kill-switches/ | <a href="https://web.archive.org/web/*/https://rubenerd.com/we-need-physical-audio-kill-switches/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div property="articleBody">
<p>(Update: I didn’t mention this concerned <strong>wired</strong> headphones).</p>
<p>I aggressively disagree with any computer design decisions that detract from ergonomics or health, and nowhere does this continue to remain bafflingly true than audio output. Strap in, I’m about to get a bit ranty!</p>
<p>If we encounter an unwanted audio signal emanating from our computers, especially an uncomfortably-loud one over headphones, we should <em>immediately</em> be able to terminate it. No exceptions. If there is any latency <em>whatsoever</em> between us hitting a mute button and the audio not cutting out, the hardware or software has failed. Crypton Future Media’s Hatsune Miku wouldn’t tolerate latency with her headphones, and neither should we.</p>
<p><img src="https://rubenerd.com/files/2020/miku-headphones@1x.jpg" srcset="https://rubenerd.com/files/2020/miku-headphones@1x.jpg 1x, https://rubenerd.com/files/2020/miku-headphones@2x.jpg 2x" alt=""></p>
<p>I was in a conference call last Friday where I’d adjusted the volume up to compensate for the client’s quiet microphone, only to be audibly shot in the ears by an auto-playing video on a website. There is a <em>lot</em> of problematic stuff to unpack there, much of which is not the fault of the audio hardware or OS. But shocked in the moment, I hit the mute button on my MacBook Pro Touchbar, and it took a solid two seconds for it to register. My ears were ringing throughout the whole call. <em>This is unacceptable.</em></p>
<p>Well-engineered mute buttons on keyboards shouldn’t need to go to software, they should immediately send a signal to the motherboard’s DAC—ideally on a separate wire or connection—to say <em>terminate this signal</em>. Then it’s less of a concern if it takes the OS a few seconds to react to the change, because our ears have been spared.</p>
<p id="just-ackchyually">The <em>just ackchyually</em> crowd would don their Captain Obvious capes and brightly-coloured underwear to proclaim that people could <em>just</em> unplug their headphones, or rip them off ones head when suddenly inundated with loud audio. Sure, and if you start getting electric shocks from your keyboard you could <em>just</em> use an external one, bro. Or if you get your hand caught in a mixer, <em>just</em> use your other hand, that’s why you have two of them. There are so many reasons why this dismissive attitude is specious, but even if it weren’t, it would still take more physical effort <em>than a button</em>. And if a mute button doesn’t fulfill the function for which it’s labelled and designed, what’s the point of it? But then, these people know all that, they’re just being obtuse.</p>
<p>We have valid privacy arguments advocating for physical Wi-Fi, camera, and microphone buttons; I’d say audio should be voiced in these discussions too. They should be heard. Sound ideas should be reverberated. Miku.</p>
</div></div>]]>
            </description>
            <link>https://rubenerd.com/we-need-physical-audio-kill-switches/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24535408</guid>
            <pubDate>Sun, 20 Sep 2020 17:19:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Basic Printing on OpenBSD]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24535357">thread link</a>) | @paedubucher
<br/>
September 20, 2020 | https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html | <a href="https://web.archive.org/web/*/https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>I have a roughly ten year old Brother HL-5370DW printer on the shelf next to me.
This printer is mostly used by my wife to print sewing patterns. When I was
studying computer science, I sometimes printed documents I've written for
proofreading. I often was able to find typos that I didn't see on the screen
even after proofreading the document two or three times. However, I didn't
bother to print out my bachelor thesis. Printing 120 pages just for proofreading
just seemed a waste to me. I did my proofreading on the screen extra carefully,
and nobody complained about typos. (Which doesn't mean that there were none.)</p>
<p>Having finished my studies, I hardly ever print out documents. However, I still
prefer to read long texts on paper rather than on the screen. Therefore I often
buy technical books as paperbacks or hardcovers rather than ebooks. And if I buy
an ebook with demanding content, I print out those sections for offline reading.</p>
<p>Having switched to OpenBSD for my private computing shifted my reading habits
more towards manpages. When I need to figure out how something works on
OpenBSD, <code>apropos(1)</code> beats Google as a starting point in many cases. Some
manpages are really long, for example <code>ksh(1)</code>. I have a book on the Korn Shell
in my basement, which covers <code>ksh93</code>.  However, there are some differences
between <code>ksh93</code> and OpenBSD's <code>pdksh</code>. So reading the manpage not only gives me
more accurate information, but also <em>less</em> to read.</p>
<p>So why not printing out the manpage <code>ksh(1)</code>? I can do so even nicely formatted
using PostScript:</p>
<pre><code>$ man -T ps -O paper=a4 ksh &gt;ksh.1.ps
</code></pre>
<p>Now <code>ksh.1.ps</code> can be read with <code>zathura(1)</code>, given that the package
<code>zathura-ps</code> is installed:</p>
<pre><code># pkg_add zathura zathura-ps
$ zathura ksh.1.ps
</code></pre>
<p>But why using PostScript and not PDF like anybody else for the last twenty five
years? Because PostScript is the least common denominator and, thus, supported
out of the box by OpenBSD. (For fancier printing options, check out <code>cups</code>, but
I'd like to keep it minimalistic for the moment.)</p>

<p>I figured out how to configure my printer by reading the section <em>The lpd
Printing Daemon</em> in the 16th chapter of <a href="https://nostarch.com/obenbsd2e">Absolute OpenBSD (2nd
Edition)</a> (p. 306-307) by <a href="https://mwl.io/">Michael W
Lucas</a>. This is how I applied the configuration to my local
setup.</p>
<p>First, I created the file <code>/etc/printcap</code> with the following content:</p>
<pre><code>lp|brother:\
    :sh=:\
    :rm=192.168.178.52:\
    :sd=/var/spool/output/brother:\
    :lf=/var/log/lpd-errs:\
    :rp=brother
</code></pre>
<p>There must be a newline at the end of the file. The line breaks are escaped
using backslashes, except for the last line. The options are defined as follows:</p>
<ul>
<li>The first line defines two names for my printer: <code>lp</code>, which should always be
  there, and <code>brother</code>, which is my arbitrary name for the printer.</li>
<li>The second line (<code>sh</code>) defines that no <em>burst page</em> (summarizing the last
  print job on a special page) should be printed.</li>
<li>The third line (<code>rm</code>) refers to the printer on the network. My FritzBox always
  gives the same IP to my printer. It's also possible to use the printer's
  hostname.</li>
<li>The fourth line (<code>sd</code>) defines the spooler directory for this printer. Print
  jobs are written into that directory.</li>
<li>The fifth line (<code>lf</code>) defines a log file for error messages, which you hopefully
  never need to check.</li>
<li>The sixth line (<code>rp</code>) defines the remote printer name.</li>
</ul>
<p>Next, the spooler directory needs to be created. It must be owned by the user
<code>root</code> and the group <code>daemon</code>. Regular users need write access to this directory
in order to print documents:</p>
<pre><code># mkdir /var/spool/output/brother
# chown -R root:daemon /var/spool/output/brother
# chmod 770 /var/spool/output/brother
</code></pre>
<p>Now the printer daemon <code>lpd</code> needs to be activated. To do so on system startup,
add the following line to <code>/etc/rc.conf/local</code>:</p>
<pre><code>lpd_flags=""
</code></pre>
<p>Then start the service:</p>
<pre><code># /etc/rc.d/lpd restart
</code></pre>
<p><strong>Update (2020-09-21)</strong>: As one reader on
<a href="https://news.ycombinator.com/item?id=24535357#24538879">Hacker News</a> pointed
out, the last two steps can be performed using <code>rcctl(8)</code>:</p>
<pre><code># rcctl enable lpd
# rcctl restart lpd
</code></pre>
<p>The manpage says that <code>rcctl(8)</code> was introduced in OpenBSD 5.7 back in 2015.
<em>Absolute OpenBSD (2nd Edition)</em> is from 2013 and, thus, older than that. (At
the time of this writing, I'm using Version 6.7.)</p>
<p>Another reader pointed out that setting the access rights to <code>777</code> is a bad
practice. That's true, and I actually got the reasoning behind this wrong: I
thought any user must be able to write to the spooler, because any user is
supposed to print. However, it's <code>lpd</code> that is writing to the spooler, which of
course runs under the <code>daemon</code> group. Therefore, the access rights for
<code>/var/spool/output/brother</code> should be set to <code>770</code>, not to <code>777</code> (as corrected
above).</p>

<p>Now the printer is ready to accept jobs. In order to print the PostScript file
generated before, just run <code>lpr</code> on the file:</p>
<pre><code>$ lpr ksh.1.ps
</code></pre>
<p>It's also possible to send the PostScript output directly to the printer (this
is Unix, after all), if no preview is needed:</p>
<pre><code>$ man -T ps -O paper=a4 ksh | lpr
</code></pre>
<p>Printing plain text files behaved strange on my setup, but could to using the
<code>pr</code> formatter with <code>lpr</code> as follows:</p>
<pre><code>$ lpr -p plain.txt
</code></pre>
<p>Instead, I also convert plain text files to PostScript, which looks quite nice
on paper. I use <code>enscript(1)</code> for this task:</p>
<pre><code># pkg_add enscript
$ enscript plain.txt -o plain.ps
$ lpr plain.ps
</code></pre>
<p>PDFs can also be converted to PostScript using <code>pdf2ps(1)</code>, which comes with
GhostScript, i.e. the <code>ghostscript</code> package:</p>
<pre><code>$ pdf2ps document.pdf document.ps
</code></pre>
<p>Unfortunately, this doesn't work with all PDFs. But for the time being, I have
enough manpages to read. Printing PostScript works extremely fast, by the way.
When I press return at the end of a <code>lpr</code> command, I can see the status LED on
my printer start blinking almost immediately.</p></div></div>]]>
            </description>
            <link>https://paedubucher.ch/articles/2020-09-20-basic-printing-on-openbsd.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24535357</guid>
            <pubDate>Sun, 20 Sep 2020 17:15:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tarsnap – cleaning up old backups]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 23 (<a href="https://news.ycombinator.com/item?id=24535046">thread link</a>) | @tosh
<br/>
September 20, 2020 | https://dan.langille.org/2020/09/10/tarsnap-cleaning-up-old-backups/ | <a href="https://web.archive.org/web/*/https://dan.langille.org/2020/09/10/tarsnap-cleaning-up-old-backups/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://dan.langille.org/2020/09/10/tarsnap-cleaning-up-old-backups/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24535046</guid>
            <pubDate>Sun, 20 Sep 2020 16:43:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Laid Off, Now What?]]>
            </title>
            <description>
<![CDATA[
Score 374 | Comments 334 (<a href="https://news.ycombinator.com/item?id=24534685">thread link</a>) | @bbhat
<br/>
September 20, 2020 | https://bharathpbhat.github.io/2020/09/19/laid-off-now-what.html | <a href="https://web.archive.org/web/*/https://bharathpbhat.github.io/2020/09/19/laid-off-now-what.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>As an immigrant on an H1B, you have exactly 60 days to find a new job when you are laid-off. This is a very short window of time to explore and land any job, let alone a job that matches your skills and interests. I found myself in this situation along with many others when Uber announced <a href="https://www.theverge.com/2020/5/18/21262337/uber-layoff-3000-employees-covid-19-coronavirus">layoffs</a> earlier this year. The following is a recollection of some things that worked well for me during my eventually successful job hunt.</p>
<ul>
<li><a href="#always-be-prepping">Always be Prepping</a></li>
<li><a href="#reach-out-to-everyone">Reaching Out</a></li>
<li><a href="#interview-preparation">Interview Preparation</a></li>
<li><a href="#closing-thoughts">Closing Thoughts</a></li>
</ul>
<h3 id="always-be-prepping">Always be prepping</h3>
<p>Coding interviews are hard to crack if you haven't been prepping, so I was lucky that I had been spending roughly 3-4 hours every week on <a href="http://leetcode.com/">leetcode</a>, from about 2 months before the layoff rumours broke. I was lucky that</p>
<ul>
<li>I knew that I wanted to change jobs in any case and</li>
<li>Rumours of layoffs broke approximately a month before the actual layoffs happened, giving me more lead time to prepare and send out emails to recruiters and friends.</li>
</ul>
<p>Whenever there's an economic downturn, I think it's critical to be acutely aware of what's happening at the company and start preparing for job interviews right away.</p>
<h3 id="reach-out-to-everyone">Reach out to everyone</h3>
<p>One of the hardest things to do when you are laid off is to write to your friends and family seeking help. But if there's ever a time to swallow your pride, then this is it. I reached out to everyone I knew, and told them plainly about my situation, and asked to be recommended to specific roles at their companies, or to tell their friends who may be hiring. I am extremely grateful to the help I got from my network, and the kind messages that I received. So many friends wrote to make sure I was okay, and kept checking in throughout the interview process, and they all have my immense gratitude.</p>
<p>It is tempting to just apply on the careers page when you find relevant roles at a company instead of spending time on finding connections and reaching out to them, but in my experience, it was very much worth it. Response times from recruiters was roughly 1-2 days when I was referred by an employee, whereas applying on the careers page was a hit or miss. One BigCo. took 40 days to respond, while some smaller companies were much quicker (3-4 days).</p>
<h4 id="the-process">The Process</h4>
<p>In terms of companies, cast a wide net because you absolutely need <em>a</em> job before a deadline. The steps are the obvious ones:</p>
<ol>
<li>Make a list of companies</li>
<li>For each company, compile a list of open job profiles that are relevant.</li>
<li>Email/Text a connection at the company, or apply on the careers page if all else fails.</li>
</ol>
<p>I think I reached out to an initial list of 10 companies or so on the day news of the layoffs broke. This worked well because there's at least a week's time before you speak to a hiring manager or interviewer from when you reach out, so there's ample time to prepare.</p>
<p>What companies to reach out to? In my case, it was the usual suspects (FAANG), and then some domain specific ones such as autonomous vehicle companies. The two most common roles that I applied to were:</p>
<ul>
<li><strong>Machine Learning Engineer</strong> - This is a hybrid role with ML + Software Engineering skills needed, and job roles usually talk about some specific domain such as recommendation systems, or in the case of autonomous vehicles, things such as perception or object detection. I typically looked for some mention of Computer Vision, NLP and deep learning.</li>
<li><strong>Machine Learning Infra Engineer</strong> - This role tends to be more on the software systems side, and deals with the infra for training and serving ML models for production workloads.</li>
</ul>
<h3 id="interview-preparation">Interview Preparation</h3>
<ul>
<li><a href="#an-initial-screen-with-the-hiring-manager">Hiring Manager Screen</a></li>
<li><a href="#coding-interviews-phone--onsite">Coding Interviews</a></li>
<li><a href="#machine-learning-interviews">Machine Learning Interviews</a></li>
<li><a href="#behavioral-interviews">Behavioral Interviews</a></li>
</ul>
<p>Interviewing for ML specific roles typically involves a few different kinds of interviews, each of which needs specific preparation. I'm outlining the most common ones I saw below:</p>
<h3 id="an-initial-screen-with-the-hiring-manager">An initial screen with the hiring manager</h3>
<p>Companies that do general interviews (Google / Facebook) don't have this step, but most others do. I personally like this, because it means that you are interviewing for a specific position in a specific team, and there's a high level of engagement from the beginning. Most of these calls were about getting to know me, and making sure I have relevant work experience, while some of them also were rapid fire technical questions. The latter ones were rare, and I encountered them when the manager wasn't certain that I was the right person for the job. In my experience, the introduction is the most important part of this interview (<strong>Tell me about yourself</strong>), and it helps to have prepared intros for each type of role that you are applying to. The idea is to tailor your story to highlight aspects of your work experience that are relevant to the job role. The next most important question is "<strong>What would you like to do in your next role?</strong>". Again, it helps immensely to be prepared to answer this question, and ideally, in a way so that there's reasonable overlap between your answer and what the role offers. Being able to answer this question also provides clarity to the job search process. For example, a consistent theme for me was to be (a) in an impactful / critical role for the company and (b) continue to work with the latest in ML.</p>
<p>Writing and rehearsing your stories often seems unimportant when compared to more tangible preparation steps such as spending time on leetcode, but I believe that it was critical, because it sets the tone and gives you confidence that you have done this in the past, and done it well, and there's no reason for the interviewer to doubt your abilities.</p>
<h3 id="coding-interviews-phone--onsite">Coding Interviews (Phone / Onsite)</h3>
<p>These are the standard leetcode style coding interviews, done using coderpad, or some similar service. The template for these is consistent across all companies, and involves 1 or 2 coding questions (or 1 question with follow-ups) that you are expected to implement and test. Some tips that were helpful for me preparation:</p>
<ol>
<li><strong>Get a premium subscription with leetcode</strong> - It is nice to be able to filter by companies and have access to the entire question bank, and it is good karma. The service is valuable and the creators should be compensated.</li>
<li><strong>Simulate the interview setting as much as possible</strong> - For example, I would set aside a 3 hour block of time for leetcode, shut myself in a room, and do 4 questions, 45 minutes each. If you are unable to solve a question in 45 minutes, you still move on to the next one. No extensions or looking at the solution. Think of it like moving on to the next interviewer. After the 3 hour session is done, go back to the questions as needed, either to look at solutions or to understand them better. A question is <code>Done</code> when your solution passes all the tests on leetcode and is <code>Accepted</code>.</li>
<li><strong>Talk out loud</strong> - This is big. Again, assuming that you are in an actual interview, talk out loud about the process you are using during these practice sessions. Talking out loud helps massively because you are forced to put your current train of thought into words, and it is often evident when a solution isn't justifiable.</li>
<li><strong>How to pick questions?</strong> - I filtered for questions that were tagged <code>Hard</code>, and then picked at random. No filter for company, or problem type. I went from doing all Mediums to a mix of Mediums and Hard to all Hards over a span of 4-5 weeks.</li>
<li><strong>How many questions to do?</strong> - In the first 2-3 weeks of my prep, I was doing 4 questions on one of the weekend days, and once I had more time post the lay-off news, it was 4 questions every 3 days or so. Overall, my stats look so:</li>
</ol>
<p><img src="https://bharathpbhat.github.io/assets/images/leetcode_xp.png" alt="Leetcode stats"></p>
<h3 id="machine-learning-interviews">Machine Learning Interviews</h3>
<p>These typically come in two flavors:</p>
<h4 id="concepts--basics">Concepts / Basics</h4>
<p>These are kind of like rapid fire questions where the interviewer will quiz you about ML basics. Some questions that I recall right now, to give a flavor of things:</p>
<pre><code>- What are some unsupervised learning methods?
- What is underfitting / overfitting?
- What is batch normalization? What's the motivation behind it?
- What is dropout? 
- What optimizers have you used? And typically some follow-up like, why does momentum make sense?
- What are some object detection techniques / papers that you are familiar with? (Computer Vision specific)
- What are decision trees? 
- How does logistic regression work?
- How do you train a linear regression model?
- What are some loss functions that you are familiar with?
- Why does Cross Entropy loss make sense?
- What are residual networks?
</code></pre>
<p>These are usually follow up questions where the interviewer will try to dig deeper into these concepts, often picking on some portion of the initial answer.</p>
<p>I did a lot of reading, and then some writing with a pen and paper for this part of the interview. If I am already somewhat/fairly familiar with a topic, like say, object detection, then my process was:</p>
<ol>
<li><strong>Write</strong> from memory a summary of what I remember about the topic</li>
<li>Note down <strong>questions</strong> for the parts that I am not clear about</li>
<li><strong>Read</strong> about the topic, and fill in whatever I missed on first go.</li>
</ol>
<p>If I don't remember much about a topic at all, like say, multi-armed bandits, then I would do step (3) first, and then do steps (1) and (2) a few days later, and eventually repeating step (3) as needed.</p>
<p>It helps to start with a list of topics that you want do this for. This list will grow as you remember more topics or expand the list of companies you are interviewing at. For reference, the list of topics I looked at is <a href="https://bharathpbhat.github.io/assets/files/index_card.pdf">here</a>, and a sample of the handwritten notes I made is here, for <a href="https://bharathpbhat.github.io/assets/files/rec_sys.pdf">recommendation systems</a>.</p>
<h4 id="ml-system-design">ML System Design</h4>
<p>This is my favorite interview, and corresponds neatly to skills used day to day as a ML practitioner. These are typically open ended interviews where the candidate is expected to design a product with some ML at its core. For example, things like:</p>
<pre><code>- Let's build a model that ranks photos in your photo library based on quality.
- How would you build a model that identifies pedestrians from drone imagery?
- Let's build a model that can does face detection for a user's photo library.
- How do you build a model that automatically picks out …</code></pre></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bharathpbhat.github.io/2020/09/19/laid-off-now-what.html">https://bharathpbhat.github.io/2020/09/19/laid-off-now-what.html</a></em></p>]]>
            </description>
            <link>https://bharathpbhat.github.io/2020/09/19/laid-off-now-what.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24534685</guid>
            <pubDate>Sun, 20 Sep 2020 15:54:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is unauthenticated encryption insecure?]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 34 (<a href="https://news.ycombinator.com/item?id=24534619">thread link</a>) | @todsacerdoti
<br/>
September 20, 2020 | https://cybergibbons.com/reverse-engineering-2/why-is-unauthenticated-encryption-insecure/ | <a href="https://web.archive.org/web/*/https://cybergibbons.com/reverse-engineering-2/why-is-unauthenticated-encryption-insecure/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Cryptography is a complex subject. There are many subtle issues that can be introduced if you don’t know what you are doing.</p>



<p>There is a common mantra: “don’t roll your own crypto”. This is because both inexperienced and experienced developers frequently build cryptographic systems that are insecure.</p>



<p>However, there has to be a line – when does it start becoming “rolling your own”? Particularly in embedded systems, there are times when custom protocols need to be used, and developers stray into the dangerous area of cryptography.</p>



<p>One of the most common mistakes we have seen is the use of unauthenticated encryption.</p>



<h3>What is encryption?</h3>



<p>Encryption is encoding a plaintext into a ciphertext using a key, with the goal of keeping the plaintext confidential.</p>



<p>Only someone with the correct key should be able to decrypt the ciphertext and turn it back into plaintext.</p>



<p>Encryption provides confidentiality. It stops someone working out what the message is.</p>



<h3>So what’s the issue?</h3>



<p>An attacker can modify the ciphertext and cause the plaintext to change. There is no inherent means in encryption to detect this change.</p>



<p>Encryption does not provide authenticity. You cannot check that the message is genuine and has not been tampered with.</p>



<h3>What can an attacker do with this?</h3>



<p>I’m going to describe one attack against unauthenticated encryption.</p>



<p>Many encryption algorithms only operate on fixed-size blocks of data – they are called <a href="https://en.wikipedia.org/wiki/Block_cipher">block ciphers</a>. To encrypt longer lengths of data, a <a href="https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation">mode of operation</a> is used to apply the block cipher repeatedly.</p>



<p>One mode of operation is called <a href="https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Cipher_block_chaining_(CBC)">CBC</a> (Cipher Block Chaining). When encrypting the data, the previous ciphertext block is mixed into the current plaintext block using an operation called “<a href="https://en.wikipedia.org/wiki/Exclusive_or">exclusive OR</a>“. This is denoted with the + in a circle in diagrams.</p>



<figure><img src="https://upload.wikimedia.org/wikipedia/commons/8/80/CBC_encryption.svg" alt=""></figure>



<p>There is also an input called the initialisation vector, or IV. This is a random input to the algorithm, and is intended to ensure that the ciphertext is different, even if the same plaintext is encrypted. This prevents leaking information about the content.</p>



<p>The initialisation vector is transmitted alongside the ciphertext.</p>



<p>Decryption is similar. The previous ciphertext block is exclusive ORed with the output of the block cipher to obtain the plaintext.</p>



<figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/CBC_decryption.svg/1200px-CBC_decryption.svg.png" alt=""></figure>



<p>Exclusive OR is a deterministic operation. If we look at a single bit, then it operates as follows:</p>



<figure><table><tbody><tr><td>A</td><td>B</td><td>Output</td></tr><tr><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>1</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td></tr><tr><td>1</td><td>1</td><td>0</td></tr></tbody></table></figure>



<p>I always think of this as “if one input is high, invert the other input, otherwise leave it alone”.</p>



<p>The operation is carried out for each bit in a byte.</p>



<pre><code>A: 0 1 0 1 1 0 0 1 (0x59)
B: 1 1 1 1 0 0 0 0 (0xF0)
O: 1 0 1 0 1 0 0 1 (0xA9)</code></pre>



<p>What this means is that modifying one of the inputs to exclusive OR results in a predictable change to the output. And the operation can be easily reversed.</p>



<pre><code>A: 0123456789ABCDEF
B: FFFF00FFF00F0FF0
O: FEDC459879A4C21F</code></pre>



<p>If we now exclusive OR the output with one of the inputs:</p>



<pre><code>A: FEDC459879A4C21F
B: FFFF00FFF00F0FF0
O: 0123456789ABCDEF</code></pre>



<p>Hopefully that explains exclusive OR.</p>



<p>Let’s look back to how CBC uses this in decryption. In the first block, the IV is exclusive ORed with the output of the block cipher. The IV is transmitted alongside the ciphertext and an attacker can modify both at at will.</p>



<figure><img loading="lazy" width="922" height="786" src="https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-14.55.37.png" alt="" srcset="https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-14.55.37.png 922w, https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-14.55.37-300x256.png 300w, https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-14.55.37-768x655.png 768w" sizes="(max-width: 922px) 100vw, 922px"></figure>



<p>We can encrypt the string “A dog’s breakfast” using a key and the initialisation vector of all 0x00 (<a href="https://gchq.github.io/CyberChef/#recipe=AES_Encrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'0000000000000000000000000000000'%7D,'CBC','Raw','Hex')&amp;input=QSBkb2cncyBicmVha2Zhc3Q">here</a> on CyberChef).</p>



<pre><code>Key: 0123456789ABCDEF0123456789ABCDEF
IV:  0000000000000000000000000000000
Plaintext: A dog's breakfast
Ciphertext: c7b1d96f0f520f33faaccfdc107f718aafe8892c3a29c76b0732a760a0f54f50</code></pre>



<p>Of course, this can be decrypted (<a href="https://gchq.github.io/CyberChef/#recipe=AES_Decrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'0000000000000000000000000000000'%7D,'CBC','Hex','Raw',%7B'option':'Hex','string':''%7D)&amp;input=YzdiMWQ5NmYwZjUyMGYzM2ZhYWNjZmRjMTA3ZjcxOGFhZmU4ODkyYzNhMjljNzZiMDczMmE3NjBhMGY1NGY1MA">here</a> on CyberChef).</p>



<p>If I change just one byte in the ciphertext, the entire message is corrupted (<a href="https://gchq.github.io/CyberChef/#recipe=AES_Decrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'0000000000000000000000000000000'%7D,'CBC','Hex','Raw',%7B'option':'Hex','string':''%7D)&amp;input=YzdiMmQ5NmYwZjUyMGYzM2ZhYWNjZmRjMTA3ZjcxOGFhZmU4ODkyYzNhMjljNzZiMDczMmE3NjBhMGY1NGY1MA">here</a> on Cyberchef). There’s no way for me to predictably modify this plaintext by changing the ciphertext.</p>



<pre><code>Key: 0123456789ABCDEF0123456789ABCDEF
IV:  0000000000000000000000000000000
Ciphertext: c7b2d96f0f520f33faaccfdc107f718aafe8892c3a29c76b0732a760a0f54f50
Plaintext: .L...Q½êU...ì7Ò.t</code></pre>



<p>But the attacker also has control over the IV. Let’s set the first byte of the IV to 0xFF (<a href="https://gchq.github.io/CyberChef/#recipe=AES_Decrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'FF00000000000000000000000000000'%7D,'CBC','Hex','Raw',%7B'option':'Hex','string':''%7D)&amp;input=YzdiMWQ5NmYwZjUyMGYzM2ZhYWNjZmRjMTA3ZjcxOGFhZmU4ODkyYzNhMjljNzZiMDczMmE3NjBhMGY1NGY1MA">here</a> on CyberChef). Only the first byte of the plaintext has changed!</p>



<pre><code>Key: 0123456789ABCDEF0123456789ABCDEF
IV:  FF00000000000000000000000000000
Ciphertext: c7b1d96f0f520f33faaccfdc107f718aafe8892c3a29c76b0732a760a0f54f50
Plaintext: ¾ dog's breakfast</code></pre>



<p>And it has changed predictably. The capital A (ASCII 0x41) has been exclusive ORed with 0xFF to become 0xBE (which decodes as ¾ although it’s above the normal ASCII range).</p>



<pre><code>A: 0 1 0 0 0 0 0 1 (0x41)
B: 1 1 1 1 1 1 1 1 (0xFF)
O: 1 0 1 1 1 1 1 0 (0xBE)</code></pre>



<p>This is a very high level of control! The attacker can now modify the plaintext without detection. Let’s try and significantly change the meaning of it.</p>



<p>The original message contained “A dog’s breakfast”. Can we change this canine feast into a feline one?</p>



<p>We exclusive OR the original plaintext with the desired one (<a href="https://gchq.github.io/CyberChef/#recipe=XOR(%7B'option':'UTF8','string':'The%20cat%5C's%20breakfast'%7D,'Standard',false)To_Hex('Space',0)&amp;input=VGhlIGRvZydzIGJyZWFrZmFzdA">here</a> on CyberChef). Notice how the output only has value for the characters we have changed.</p>



<pre><code>Original: A. .d.o.g.'.s. .b.r.e.a.k.f.a.s.t.
Original: 4120646f67277320627265616b66617374
Desired:  A. .c.a.t.'.s. .b.r.e.a.k.f.a.s.t.
Desired:  4120636174277320627265616b66617374
Output:   0000070e13000000000000000000000000</code></pre>



<p>Pop that output in as the IV to the decryption, and we’ve successfully changed the message (here on <a href="https://gchq.github.io/CyberChef/#recipe=AES_Decrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'0000070e13000000000000000000000000'%7D,'CBC','Hex','Raw',%7B'option':'Hex','string':''%7D)&amp;input=YzdiMWQ5NmYwZjUyMGYzM2ZhYWNjZmRjMTA3ZjcxOGFhZmU4ODkyYzNhMjljNzZiMDczMmE3NjBhMGY1NGY1MA">CyberChef</a>). All of this without even knowing the key.</p>



<pre><code>Key: 0123456789ABCDEF0123456789ABCDEF
IV:  0000070e130000000000000000000000
Ciphertext: c7b1d96f0f520f33faaccfdc107f718aafe8892c3a29c76b0732a760a0f54f50
Plaintext: A cat's breakfast</code></pre>



<p>Of course, the attacker needs to have knowledge of the plaintext to make use of this attack. However, it’s extremely common for some or all of the message to be known. For example, when we visit most websites, the first part of the response will be “HTTP/1.1 200 OK”. If this was only protected by CBC encryption, we could change that to “HTTP/1.1 404 No”, changing the behaviour of the browser (here on <a href="https://gchq.github.io/CyberChef/#recipe=AES_Decrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'00000000000000000006000000012400'%7D,'CBC','Hex','Raw',%7B'option':'Hex','string':''%7D)&amp;input=ZGJkY2FkYWZjYjQ5NTJiNDE0OTBhODM4NDFhYzgxZGE">CyberChef</a>).</p>



<p>This doesn’t just impact the first block of data either. After the first block, instead of the IV, the previous ciphertext block is used in the exclusive OR operation. The attacker can modify the ciphertext and end up controlling the plaintext.</p>



<figure><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/CBC_decryption.svg/2880px-CBC_decryption.svg.png" alt=""></figure>



<p>This comes at a cost though – the previous plaintext block will be totally corrupted as a result.</p>



<p>To illustrate this, we can encrypt a longer block of text (<a href="https://gchq.github.io/CyberChef/#recipe=AES_Encrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'0000000000000000000000000000000'%7D,'CBC','Raw','Hex')&amp;input=VGhpcyBpcyBvdXIgd29ybGQgbm93Li4uIHRoZSB3b3JsZCBvZiB0aGUgZWxlY3Ryb24gYW5kIHRoZSBzd2l0Y2gsIHRoZQpiZWF1dHkgb2YgdGhlIGJhdWQuICBXZSBtYWtlIHVzZSBvZiBhIHNlcnZpY2UgYWxyZWFkeSBleGlzdGluZyB3aXRob3V0IHBheWluZwpmb3Igd2hhdCBjb3VsZCBiZSBkaXJ0LWNoZWFwIGlmIGl0IHdhc24ndCBydW4gYnkgcHJvZml0ZWVyaW5nIGdsdXR0b25zLCBhbmQKeW91IGNhbGwgdXMgY3JpbWluYWxzLiAgV2UgZXhwbG9yZS4uLiBhbmQgeW91IGNhbGwgdXMgY3JpbWluYWxzLiAgV2Ugc2VlawphZnRlciBrbm93bGVkZ2UuLi4gYW5kIHlvdSBjYWxsIHVzIGNyaW1pbmFscy4gIFdlIGV4aXN0IHdpdGhvdXQgc2tpbiBjb2xvciwKd2l0aG91dCBuYXRpb25hbGl0eSwgd2l0aG91dCByZWxpZ2lvdXMgYmlhcy4uLiBhbmQgeW91IGNhbGwgdXMgY3JpbWluYWxzLgpZb3UgYnVpbGQgYXRvbWljIGJvbWJzLCB5b3Ugd2FnZSB3YXJzLCB5b3UgbXVyZGVyLCBjaGVhdCwgYW5kIGxpZSB0byB1cwphbmQgdHJ5IHRvIG1ha2UgdXMgYmVsaWV2ZSBpdCdzIGZvciBvdXIgb3duIGdvb2QsIHlldCB3ZSdyZSB0aGUgY3JpbWluYWxzLg">here</a> on CyberChef).</p>



<p>Let’s change “baud” to “cats”. We need to locate the correct place in the ciphertext. AES (the encryption algorithm we are using) works in 16 byte blocks. The word “baud” is 85 characters in, so in the 6th block. We therefore want to modify the 5th block of ciphertext.</p>



<p>The exclusive OR is a bit more complex than last time – we now need to exclusive OR the ciphertext, the original text, and the desired text (<a href="https://gchq.github.io/CyberChef/#recipe=AES_Decrypt(%7B'option':'Hex','string':'0123456789ABCDEF0123456789ABCDEF'%7D,%7B'option':'Hex','string':'0000000000000000000000000000000'%7D,'CBC','Hex','Raw',%7B'option':'Hex','string':''%7D)&amp;input=MTkxNWRkOGU4ODBhN2JlZjgzN2Y1NDRlMzBlZGI5YmQxMjA3ZjMwMmRjM2NlZGQwY2I2NGJkY2JiOTk3YjVkYmM4M2RhNjU3MmZkNmMyMDVmOGQ4ZDI2NjQ5MmQyMDY3M2U0NGZhNzUwNGU2YzY0ZTI4M2E2NzI2MmIyYzMwNjM4OGI4ZjQyZTBmYjMxNzdmNmFmMTNlMWE0OGUxNDBmYTFhNDhmMThmZGYyNTc3MzgwMTUwYzM5ZDIwZTYyY2QzMzQ1ZDVmNTFiYzU4NDU2NGUwMzc5MTFkYTM1MTc3YjVkY2ZmOTkxZTRmYzg3NDFlYmJjMmRmM2I2YTc3OGViOTU3MTI1MmQxYTY0Yjk5NmRhOWFkMzFmNGE5MTI3NjM0M2FhMmU1ODQ1NjEyOTM1MDg0Zjc3Y2FhMmRiNDRiYTM5OTA5NzFkOTcwZWVlMjFlZDc3MjRiOWU3MDMwODEyZWI4N2U1ZDVmYmI3Y2M1MGE1NzYxNDBiN2I0NzhiYmZiNzU1MGU1MWU3ZmM0ZTg5ODExY2Y4MTg1OTJjNGY4ZWU3NGIyNTQ0Y2VhMGE4ZDdkZjM0OTE2YjIzYmMwOWIxYWJhN2IwN2ZlNDM0YWRjNjY5MzhhNzczMDU4MjNhYzdkMWJjZmEwOGNlOTRhYzc0MjUzNjdiODQwMGE5NGFlMDc0ZTFhY2NmZDkwYThjNDllYmYzNDNkMmU4YWQ5MmI2NDZlZDM0OTM4Yzg3NTI2MDUyYjA4ZjQ1MzgxMWQ4YTYyZjE2MzczMzkxZmE4YTBlZGIwZDJlZDBhYWQyNDViY2RlZmI1YTk0ZmRmZTBkNzA4YTMwYTVjZDVlZmI5ZjExNTk3MDU2NWFiMjg1ZGUyY2FlYWNkMTI3YzBhNzhkZDRjNmE4Y2U2NjRjYTFiOWI0YjI1ODk0MTYxMmUzMjgwOWEwNGRhYzIxODlkNGVkN2Q2ZDU4ZDcwMGNlODM5NTIzYzlmNTZiOGU2YWY1NGIzYjMxYjAxM2E4ODM4MjljM2Y0YTJhZmI3Mzc3OTFjNjBiN2E1N2I4NGNhOTgxYjFiM2E3N2M2YmI5ZWNiMzIwNzk3YmVhNzAyMDk5NGUwNzRmYmQ1NzM0MWQwMmVjYTY3ZWM1NWU5YzA1MmFkODA3NjUzMmUxZTI4MDJjMzc2YmRhMzg1NWIxYzYzY2FhNzRhZmI0YTRjNTFkMDNlNGZiMjEzY2ZiMTM4YjcxMTc1NzFhNTIzOTQzZGU1MWJiNzZiYTgwMzY2MDNkNDI2NmFmMzI3MGMyYjBhOTNjZDdlYzkyZmVjMjA0MTAyYjJkYWZlNDliMzUwZDFhNDk2NjVhYjE0MTFiMjhkZWQ1MmE5ZWE5NTA3ZWU5ZDljM2M0NzI4ZDBlNTk0YjEzM2VkMmRiOGUwYWQxZjBjZWM0NWRhYjJlN2Y1ODE5YTQyNWQ4NTY2ZWQ5MGQwYzI4MTMzZjlkZTM4ODQ4OTE3NjJhYTcxMzc2MjZmNmM2MTEzMDY4M2NkNWEzYmFjN2EzNTFkZDY0MjZjYzI2NzdjOGRjYWI0ZDMwZjg0OGNiZjYwOTBmMjM4MDM2ZTFlMzczMGZmODc4MTk2YWYyMjg4YWY5MTU5ZThkZA">here</a> on CyberChef). But change those 4 bytes, and we change the word “baud” to “cats”.</p>



<figure><img loading="lazy" width="1024" height="272" src="https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-16.03.54-1024x272.png" alt="" srcset="https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-16.03.54-1024x272.png 1024w, https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-16.03.54-300x80.png 300w, https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-16.03.54-768x204.png 768w, https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-16.03.54-1536x407.png 1536w, https://cybergibbons.com/wp-content/uploads/2020/09/Screenshot-2020-09-20-at-16.03.54.png 1682w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The only issue is, as expected, the previous block has been entirely corrupted. Whilst in this case, it’s made part of the message nonsensical, it frequently has no impact when carrying out attacks.</p>



<h3>But there are worse problems?</h3>



<p>The above issue allows an attacker to modify the plaintext without detection. This would be an issue in certain situations, such as lock/unlock messages to a door.</p>



<p>But not authenticating your encryption can lead to worse issues. A type of attack called <a href="https://en.wikipedia.org/wiki/Padding_oracle_attack">padding oracle attacks</a> can let an attacker obtain the plaintext by sending a large number of specially crafted packets.</p>



<p>Block ciphers only operated on fixed blocks. If the data is shorter than a block, it must be padded. There are a number of ways of doing this, such as appending the number of padding bytes (e.g. 0x02 0x02 or 0x05 0x05 0x05 0x05 0x05). The process of decryption may check this padding is correct or not, and respond differently in each case. </p>



<p>An attacker can exploit these differential responses to leak the plaintext. This can break the confidentiality of messages.</p>



<h3>What’s the solution to this?</h3>



<p>Encryption should always be authenticated. There are two common solutions to this:</p>



<ul><li>Add a <a href="https://en.wikipedia.org/wiki/Message_authentication_code">Message Authentication Code</a> (MAC). This is a keyed cryptographic checksum that provides authenticity and integrity.</li><li>Use an authenticated mode of operation such as <a href="https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Galois/Counter_(GCM)">GCM</a>.  </li></ul>



<p>Even with this advice, there are many pitfalls. Applying the authentication and encryption in the wrong order can lead to weaknesses; this is so common that it has been deemed the <a href="https://moxie.org/2011/12/13/the-cryptographic-doom-principle.html">Cryptographic Doom Principle</a>.</p>



<p>Generally, developers shouldn’t be working with cryptography at this level unless they are suitably skilled. That’s easy to say, harder to put into action. There is a big movement to make use of secure-by-default cryptographic libraries and APIs that provide developers with useful functions without giving them so much rope they can hang themselves.</p>



<p>There are scant few reasons for not authenticating encryption.</p>
			</div></div>]]>
            </description>
            <link>https://cybergibbons.com/reverse-engineering-2/why-is-unauthenticated-encryption-insecure/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24534619</guid>
            <pubDate>Sun, 20 Sep 2020 15:42:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apache Arrow and MinIO]]>
            </title>
            <description>
<![CDATA[
Score 65 | Comments 32 (<a href="https://news.ycombinator.com/item?id=24534274">thread link</a>) | @jtsymonds
<br/>
September 20, 2020 | https://blog.min.io/turbocharging-minio-datalakes-with-arrowrdd/ | <a href="https://web.archive.org/web/*/https://blog.min.io/turbocharging-minio-datalakes-with-arrowrdd/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                    <p>More and more enterprises have begun or have already implemented a data lake strategy based on some of the work we did a couple of years ago. If you want to take a moment to review - you can find those posts below <a href="https://blog.min.io/modern-data-lake-with-minio-part-1/">here</a> and <a href="https://blog.min.io/modern-data-lake-with-minio-part-2/">here</a>. </p><h2 id="objective">Objective</h2><p>In this article, I am going to explain a mechanism to turbocharge the use of MinIO. Nothing changes as far as MinIO is concerned, the optimization will be on the underlying storage of our data. We are going to choose one of the latest formats to improve agility manifoldly. We are going to show the ways by which your data lake data can travel across systems without experiencing any "conversion" time. </p><h2 id="apache-arrow">Apache Arrow</h2><p>I believe understanding this article needs some basic concepts of<a href="https://arrow.apache.org/"> </a>how applications like Spark works. Let me explain it in simple terms.</p><p>Imagine you got a nice job at a location different from where you live currently and you want to relocate, as the new company demands it and pays for it. You have got the most modern televisions, refrigerators, super soft leather sofas, bed and so on. You engage a moving company, who comes, disassembles everything, packs it conveniently. They also make sure to pack as much possible in containers to fill the truck such that they can do it in a single trip. Once they reach the destination, they unpack, assembles and restore everything as it was.</p><p>The same applies to data. When I store some data in MinIO , and I need to feed it to, say, another application, say Spark, the consuming application needs to disassemble the data from MinIO data lake, pack it and transport it through the wire (or wireless), receive, unpack and re-assemble. </p><p>Let's use more technical terms for this disassembly and assembly - serialization and de-serialization of data. The unfortunate part is, both these processes are complex and time consuming. Here is a brief diagram illustrating what happens in Apache Spark when it reads data</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-1.54.30-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-1.54.30-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-1.54.30-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-1.54.30-AM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-1.54.30-AM.png 1944w" sizes="(min-width: 720px) 720px"><figcaption>Experimental setup. Courtesy: <a href="https://databricks.com/session/running-apache-spark-on-a-high-performance-cluster-using-rdma-and-nvme-flash">Spark Summit</a></figcaption></figure><p>You may not have noticed this problem before. Assume that MinIO is on a machine(s) on the network. We write a Spark Map-Reduce application. Eventhough the network limit is 100 GbE, we are almost getting less that 10 GbE speed. What's the use of this high speed network then? What is the potential problem which is not allowing us to utilize the full potential of the network, or at least 70-80% of it?</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-2.00.34-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-2.00.34-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-2.00.34-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-2.00.34-AM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-2.00.34-AM.png 1948w" sizes="(min-width: 720px) 720px"><figcaption>Additional layers, buffers and serializers</figcaption></figure><p>The issues are with the way in which Spark is retrieving the data. Look at the number of layers the data has to pass though. This creates a limit on the throughput that we can achieve. There are projects like <a href="https://crail.apache.org/">Apache Crail,</a> which are designed to address these issues.</p><h2 id="optimization-columnar-data-format">Optimization : Columnar Data Format </h2><div><p>If we think about the relocation example mentioned above, we see that the logistic company will never take the sofa as it is, they will break it down to make it easy to transport. Note that this is for transportation purposes only - if that objective is different, then disassembling the sofa might not be the right approach. </p><p>Given that the objective for a data lake is analytics - rather than transactional needs we must take that under consideration. For transactions, we often use OLTP systems like Oracle or PostGres - given that they are particularly well suited for the job. A quick review of OLAP's analytics requirements is probably in order. </p></div><figure><img src="https://blog.min.io/content/images/2020/09/Picture1.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Picture1.png 600w, https://blog.min.io/content/images/2020/09/Picture1.png 966w" sizes="(min-width: 720px) 720px"><figcaption>Introducing columnar format</figcaption></figure><p>Let's start with one of the most famous RDMBS table - the "emp" table of Oracle. The top part shows how the data is stored in RDBMS as a "relation" or "tuple". We call it a table. I am providing you two queries</p><!--kg-card-begin: markdown--><ol>
<li>select ename from emp where job = 'CLERK'</li>
<li>select sum(sal) from emp</li>
</ol>
<!--kg-card-end: markdown--><p>The first is a transactional query. It has to scan every row on the table and find out the name of the employee wherever the job is clerk. The second is an analytical query - rather than an atomic result, the goal is a general result. Unfortunately, the first and second query has to scan through all the rows, if we use RDBMS way of representation of data. If the size of data is 20 GB, all the 20 GB more or less will be scanned. This is the top part of above figure.</p><p>Let's make some changes - taking all of our columns and make them into rows. Like a transpose of a matrix - and see the bottom part of above figure, how your data will look like. Following this transposition, an entire block is just representing one column. How many blocks need to be scanned for the second analytical query? Just one block, probably around 2 GB of size. </p><p>The difference is significant? Columnar representation is what is being used in ORC (Optimized Row Columnar) and Parquet files - with the goal of making the analytics faster.</p><p>Columnar formats are easier to read, however, they pose another problem - they are usually stored in compressed format. As a result, the consuming application will need to uncompress it while reading and compress it back while writing. </p><p>Note this, as we will revisit the point later.</p><h2 id="the-science-of-reading-writing-data">The Science of Reading/Writing Data</h2><p>Let me explain briefly how reading/writing happens in a software system and what role is played by the hardware.</p><p>Microprocessors normally use two methods to connect external devices: <strong>memory mapped</strong> or <strong>port mapped</strong> I/O. </p><p>Memory mapped I/O is mapped into the same address space as program memory and/or user memory, and is accessed in the same way.</p><p>Port mapped I/O uses a separate, dedicated address space and is accessed via a dedicated set of microprocessor instructions.</p><p>In memory mapped approach, I/O devices are mapped into the system memory map along with RAM and ROM. To access a hardware device, simply read or write to those 'special' addresses using the normal memory access instructions.The advantage to this method is that every instruction which can access memory can be used to manipulate an I/O device.</p><p>Usually applications use Port mapped I/O. If we are using memory mapped I/O for a particular format, it will be faster, especially for analytical needs. When combined with our columnar data format, then it becomes even more advantageous.</p><p>Welcome to <a href="https://arrow.apache.org/">Apache Arrow</a>. </p><p>Arrow uses memory mapped I/O and avoids serialization/deserialization overheads when you convert between most of the formats while leveraging the columnar data format. </p><p>Thanks to <a href="https://wesmckinney.com/">Wes McKinney</a> for this brilliant innovation, its not a surprise that such an idea came from him and team, as he is well known as the creator of Pandas in Python. He calls Arrow as the future of data transfer.</p><h2 id="store-data-in-minio-in-arrow-format">Store Data in MinIO in Arrow Format</h2><p>This is how we are going to make MinIO even more powerful. </p><p>We are going to store that data in Arrow and then let the consuming applications read it - resulting in dramatically increased speeds. Step one has us putting the data into MinIO in Arrow format. I was using my own approach until I saw a much better implementation from <a href="https://github.com/BryanCutler">Bryan Cutler</a>, whose contributions include integrating Arrow formats to Spark as well.</p><p>We will start with a a .csv file, in this case movie ratings downloaded from the <a href="https://movielens.org/">movielens</a> site. For illustration purposes, I took about 100K rows. First, let's write a Spark program to read this CSV file and write it into Arrow format using Arrow RDD. You can get the full code from the link given towards the bottom of this article.</p><p>Step 1: build.sbt , please note the arrow dependencies</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-4.38.17-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-4.38.17-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-4.38.17-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-4.38.17-AM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-4.38.17-AM.png 1990w" sizes="(min-width: 720px) 720px"><figcaption>See lines 18 and 19, we have Arrow related dependencies with Spark</figcaption></figure><p>We will use Spark 3.0, with Apache Arrow 0.17.1</p><p>The ArrowRDD class has an iterator and RDD itself. For creating a custom RDD, essentially you must override mapPartitions method. You can browse the code for details. </p><p>Next, start MinIO and create a bucket named "arrowbucket". </p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-4.26.37-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-4.26.37-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-4.26.37-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-4.26.37-AM.png 1600w, https://blog.min.io/content/images/size/w2400/2020/09/Screen-Shot-2020-09-06-at-4.26.37-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Create a bucket named "arrowbucket" in MinIO</figcaption></figure><p>Let's use ArrowRDD and create an ArrowFile in local. Here is the code:</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-4.48.45-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-4.48.45-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-4.48.45-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-4.48.45-AM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-4.48.45-AM.png 2372w" sizes="(min-width: 720px) 720px"><figcaption>Writing Arrow file with ArrowRDD</figcaption></figure><p>Lines 22 to 34 do the main part. Compile and execute the code:</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-4.50.42-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-4.50.42-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-4.50.42-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-4.50.42-AM.png 1600w, https://blog.min.io/content/images/size/w2400/2020/09/Screen-Shot-2020-09-06-at-4.50.42-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Execute the code</figcaption></figure><p>As you see from code, the Arrow format file is is generated in data directory. Let's copy it to the MinIO bucket we created earlier (bucket name is arrowbucket)</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-5.04.23-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-5.04.23-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-5.04.23-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-5.04.23-AM.png 1600w, https://blog.min.io/content/images/size/w2400/2020/09/Screen-Shot-2020-09-06-at-5.04.23-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Copy the arrow file we generated to MinIO bucket</figcaption></figure><p>Let's have some fun now. </p><p>Use your favorite Python editor, and write some code. First, let us start with Spark reading the file and converting it to a dataframe, with and without Arrow enabled options.</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.52.01-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-11.52.01-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-11.52.01-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-11.52.01-AM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.52.01-AM.png 1698w" sizes="(min-width: 720px) 720px"><figcaption>Initializing Spark context and connection parameters to Minio</figcaption></figure><p>Start your Spark cluster. Complete the code with all settings and check whether we created the Spark context successfully. To ensure that our app (named Minio-Arrow-Spark at line 8) is connected, just check the Spark UI. You should see something like this:</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.46.01-AM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-11.46.01-AM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-11.46.01-AM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-11.46.01-AM.png 1600w, https://blog.min.io/content/images/size/w2400/2020/09/Screen-Shot-2020-09-06-at-11.46.01-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Spark UI (default localhost:8080) is showing our app is connected</figcaption></figure><p>Run the below code now:</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.09.47-PM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-11.09.47-PM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-11.09.47-PM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-11.09.47-PM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.09.47-PM.png 1708w" sizes="(min-width: 720px) 720px"><figcaption>Reading from MinIO with Arrow format "not enabled" (top) and "enabled"(bottom)</figcaption></figure><p>The output which displays the time, shows the power of this approach. The performance boost is tremendous, almost 50%.</p><p>Recall that we created an ArrowRDD earlier and used it to write to MinIO. Let us test the memory consumption in reading it. We will use different methods.</p><figure><img src="https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.17.22-PM.png" alt="" srcset="https://blog.min.io/content/images/size/w600/2020/09/Screen-Shot-2020-09-06-at-11.17.22-PM.png 600w, https://blog.min.io/content/images/size/w1000/2020/09/Screen-Shot-2020-09-06-at-11.17.22-PM.png 1000w, https://blog.min.io/content/images/size/w1600/2020/09/Screen-Shot-2020-09-06-at-11.17.22-PM.png 1600w, https://blog.min.io/content/images/2020/09/Screen-Shot-2020-09-06-at-11.17.22-PM.png 1882w" sizes="(min-width: 720px) 720px"><figcaption>See the results - Arrow is zero copy memory format</figcaption></figure><p>We are reading different file formats and seeing the memory consumption for each. As it is evident, Arrow format based files are zero copy - almost no memory consumed at all.</p><p>By combining MinIO with the Arrow Format, you can enhance your analytics ecosystem and virtually eliminating the friction associated with converting between different formats. This is primarily due to the reduction of serialization overhead.</p><h2 id="code">Code </h2><p>You can see the<a href="https://github.com/passionbytes/ArrowRDD"> Jupyter notebook and ArrowRDD code here</a>.</p><p>Ravishankar Nair is a technology evangelist, a consultant and an inspiring speaker. He is the CTO of PassionBytes, based in Florida. With his vast expertise in data engineering, Ravi provides consultancy in machine learning, modern data lakes and distributed computing technology. You can refer to his other articles related to MinIO here:</p><p>1) <a href="https://blog.min.io/modern-data-lake-with-minio-part-1/">Modern Data Lakes with Minio - Part 1</a></p><p>2) <a href="https://blog.min.io/modern-data-lake-with-minio-part-2/">Modern Data Lakes with MinIO - Part 2</a></p><p>3) <a href="https://blog.min.io/building-an-on-premise-ml-ecosystem-with-minio-powered-by-presto-r-and-s3select-feature/">Building an …</a></p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.min.io/turbocharging-minio-datalakes-with-arrowrdd/">https://blog.min.io/turbocharging-minio-datalakes-with-arrowrdd/</a></em></p>]]>
            </description>
            <link>https://blog.min.io/turbocharging-minio-datalakes-with-arrowrdd/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24534274</guid>
            <pubDate>Sun, 20 Sep 2020 14:32:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Yelp: Local Economic Impact Report]]>
            </title>
            <description>
<![CDATA[
Score 137 | Comments 170 (<a href="https://news.ycombinator.com/item?id=24534186">thread link</a>) | @bookofjoe
<br/>
September 20, 2020 | https://www.yelpeconomicaverage.com/business-closures-update-sep-2020.html | <a href="https://web.archive.org/web/*/https://www.yelpeconomicaverage.com/business-closures-update-sep-2020.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
      
    
    <section>
      <!--<p class='prose flag'><i>If you'd like additional detail on how the economy is shifting, please contact us at <a href='mailto:press@yelp.com'>press@yelp.com</a> or <a class=underline href=http://eepurl.com/cMFvGL target=_blank>join our mailing list</a> to receive an email when new reports are released.</i></p>-->
    
      
        <p>Since the first fears of the pandemic emerged in the U.S. in early March, businesses across the nation have endured six months of uncertainty. Yet, businesses are adapting and proving their resilience through lockdowns, reopenings, a <a href="https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html?name=styln-coronavirus-markets&amp;region=TOP_BANNER&amp;variant=1_Show&amp;block=storyline_menu_recirc&amp;action=click&amp;pgtype=Article&amp;impression_id=76e345d2-e946-11ea-8ee3-5b12c6c78bc9" target="_blank">summer surge in virus cases</a>, new ways of doing business such as <a href="https://www.nytimes.com/2020/08/23/nyregion/outdoor-dining-new-york.html" target="_blank">outdoor dining</a>, new mask wearing rules and backlash from <a href="https://www.washingtonpost.com/nation/2020/07/18/covid-pandemic-store-clerk-north-carolina/?arc404=true" target="_blank">anti-mask patrons</a>, as well as milestones such as the <a href="https://www.yelpeconomicaverage.com/back-to-school-2020.html" target="_blank">return to school</a>. Even in the wake of increased closures we’re seeing businesses effectively transition to new operating models while keeping their employees and consumers safe.</p>
        <p>Yelp closure data shows that businesses providing home, local and professional services have been able to withstand the effects of the pandemic particularly well. But despite bright spots in some sectors, restaurants and retail continue to struggle and total closures nationwide have started to increase.</p>
        <p>The <a href="https://www.yelpeconomicaverage.com/yea-q2-2020.html" target="_blank">last Yelp Economic Average</a> showed a decreasing number of overall closures, 132,580 in total. As of August 31, 163,735 total U.S. businesses on Yelp have closed since the beginning of the pandemic (observed as March 1), a 23% increase since July 10. In the wake of COVID-19 cases increasing and local restrictions continuing to change in many states we’re seeing both permanent and temporary closures rise across the nation, with 60% of those closed businesses not reopening (97,966 permanently closed).</p>
    </section>
    
    <section>
      <h3>Business Closures Continue to Increase Nationally</h3>
      <h4>Number of businesses marked closed on Yelp that were open March 1</h4>
      <p><i></i>Hover over a circle to see closures</p>
    	
    </section>
    
    <!--
    <section class='report-wrapper'>
    	<h3 class='centered-title'>Business Closures Continue to Increase Nationally</h3>
      <h4 class='centered-title'>Number of businesses marked closed on Yelp that were open March 1</h4>
    
    	<div class='static-image-container'>
    		<img class='static-image-desktop' src='./assets/img/closures092020/Closures_Rate_Desktop-2a05305cb0.png'/>
    		<img class='static-image-tablet' src='./assets/img/closures092020/Closures_Rate_Desktop-2a05305cb0.png'/>
    		<img class='static-image-mobile' src='./assets/img/closures092020/Closures_Rate_Mobile-f0f8a42c99.png'/>
    	</div>
    </section>
    -->    <section>
    	<h2>Resilient Businesses Operating in an Unpredictable Economy</h2>
        <p>Some business sectors have been able to weather the COVID-19 storm particularly well. In general, professional services and solo proprietors as a whole have been able to maintain a relatively low fraction of closures since March 1. This group includes lawyers, real estate agents, architects, and accountants – all with only two to three out of every thousand businesses closed, as of August 31. Health related businesses in particular have been able to maintain a low rate of closures – orthopedists, internal medicine, hospitals, physicians, family doctors and OB/GYNs all have less than three closures out of every thousand businesses.</p>
        <p>Yelp’s closure data also shows that demand for <a href="https://blog.yelp.com/2020/08/yelp-reinvents-the-hiring-experience-for-home-and-local-services" target="_blank">home, local</a> and automotive services has remained robust with a far lower rate of closures compared to restaurants and retail. Towing companies, plumbers and contractors in particular have maintained a low rate of closures, with only six to seven out of every thousand businesses closed. In fact, the share of consumer interest in home and local services is up 24% between March 1 and August 31, relative to all categories on Yelp, compared to the same time last year.</p>
    </section>
    
    <section>
    	<h3>Home, Local, Professional, and Auto Services Prove Their Strength Amid the Pandemic</h3>
    
    	<p><img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Sector_Desktop-b84f739960.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Sector_Desktop-b84f739960.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Sector_Mobile-069e01b319.png">
    	</p>
    </section>    <section>
      <h2>Restaurants Remain Hardest Hit, Permanent and Temporary Closures Increase</h2>
        <p>The restaurant industry continues to be among the most impacted with an increasing number of closures – totalling 32,109 closures as of August 31, with 19,590 of these business closures indicated to be permanent (61%). Breakfast and brunch restaurants, burger joints, sandwich shops, dessert places and Mexican restaurants are among the types of restaurants with the highest rate of business closures. Foods that work well for delivery and takeout have been able to keep their closure rates lower than others, including pizza places, delis, food trucks, bakeries and coffee shops.</p>
        <p>Meanwhile, bars and nightlife, an industry 6X smaller than restaurants, has endured an especially high closure rate, with an increasing percentage of closures being permanent. As of the end of August there were 6,451 total business closures, of which 3,499 were permanently closed (54%). The share of permanent closures within bars and nightlife have increased by 10% since our <a href="https://www.yelpeconomicaverage.com/yea-q2-2020.html" target="_blank">Economic Average Report</a> in July.</p>
        <p>Retail and shopping follows closely behind restaurants with 30,374 total business closures, 17,503 of which are permanent (58%). Similar to bars and nightlife, the share of permanent closures increased by 10% since July. Both men and women’s clothing, as well as home decor, have the highest rate of business closures.</p>
        <p>The beauty industry has seen a 22% increase in closures since July, totalling 16,585 closures. Of all closed businesses in the beauty industry 7,002 won’t reopen (42%), a significant 43% increase since July when we reported that 4,897 of all closures in the beauty industry were permanent. Similarly the fitness industry has endured a 23% increase in closures since July, with 6,024 total closures, 2,616 of which are permanently closed.</p>
    </section>
    
    <section>
    	<h3>Restaurants and Retail Continue to Struggle</h3>
      <h4>Number of businesses marked closed on Yelp that were open March 1</h4>
    
    	<p><img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Category_Desktop-91d671567c.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Category_Desktop-91d671567c.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Category_Mobile-529b856e87.png">
    	</p>
    </section>    <section>
      <h2>Larger States and Metros See a Greater COVID-19 Impact on Local Businesses</h2>
        <p>Even as the pandemic spreads nationally, geographically Yelp data shows business closure rates vary across the country. Bigger states and metros with higher rents and more stringent local operations for small businesses throughout the last six months have felt a greater toll. So have businesses more closely linked to physical locations that require crowds of consumers to attain profitability. Meanwhile, smaller cities and solo operations that can do their work one-on-one or virtually have proven better positioned to stay in business.</p>
        <p>For the states with widespread business closures, the economic struggle appears to be closely coupled with unemployment rates. Hawaii, California, and Nevada have the highest rate of total closures and permanent closures – they’re also the three states with the <a href="https://finance.yahoo.com/news/these-states-are-suffering-from-the-worst-unemployment-rates-144451899.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAABND8yZDak2Xg6GnNC9LukDHqDayj3GYnFbfZn_9NctEnowVC1JMpg9oZFKixnWrRLGLPortUEEaymyEAYmZ0jMN8vOuriLR1N7S0Roqv9OT99H-WdN5XH_sd_I_r0-EgEJSDExY9yTtI7Xduv3Q-Agxb55dUepi3-k8T1fGZ153" target="_blank">highest unemployment rates</a>, and among the <a href="https://www.worldatlas.com/articles/the-most-visited-states-in-the-us.html" target="_blank">biggest states for tourism</a>. Meanwhile, West Virginia and the Dakotas have the lowest closure rates.</p>
        <p>The states with the most closures are home to the hardest-hit metros: Las Vegas in Nevada, Honolulu in Hawaii, and several of the largest California urban areas all are among the metro areas with the highest total closure and permanent closure rates (San Diego, San Francisco, San Jose, Los Angeles and others), with roughly 20 businesses per thousand temporarily or permanently closing their doors since March 1. Larger metros with far fewer closures tend to be in the East, including Pittsburgh, Philadelphia, and Baltimore, all with closure rates below 10 per thousand.</p>
    </section>
    
    <section>
    	<h3>Where are the Most Businesses Closed?</h3>
      <h4>Geographic areas with the largest number of business closures since March 1</h4>
    	<div>
    		<p>Total Closures</p>
    		<p>Closures per 1,000</p>
    	</div>
    
    	<p><img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Geography_Total_Desktop-dce0442da9.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Geography_Total_Desktop-dce0442da9.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Geography_Total_Mobile-98f53c8115.png">
    	</p>
    
    	<p><img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Geography_Capita_Desktop-846de29ad1.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Geography_Capita_Desktop-846de29ad1.png">
    		<img src="https://www.yelpeconomicaverage.com/assets/img/closures092020/Closures_Geography_Capita_Mobile-1878bbd645.png">
    	</p>
    </section>
    <section>
        <p>Keep an eye out for our next update in our Q3 <a href="https://www.yelpeconomicaverage.com/index.html" target="_blank">Yelp Economic Average</a>.</p>
        <p>—Carl Bialik and Daniel Gole contributed to this report</p>
    </section>
    
    <section>
        <p><em>If you'd like additional detail on how the economy is shifting, please contact us at <a href="https://www.yelpeconomicaverage.com/cdn-cgi/l/email-protection#2d5d5f485e5e6d5448415d034e4240"><span data-cfemail="671715021414271e020b174904080a">[email&nbsp;protected]</span></a> or <a href="http://eepurl.com/cMFvGL" target="_blank">join our mailing list</a> to receive an email when new reports are released.</em></p>
        <p><em>Interested in learning how Yelp data can assist you in developing market insights for your business? Yelp Knowledge can help, learn more <a href="https://www.yelp.com/knowledge" target="_blank">here</a>.</em></p>
    </section>
    
    
    <!--
    <section class='report-wrapper'>
    </section>
    -->
    
    <section>
    	
    	<h2>Methodology</h2>
    
    		<p><em>Business Closures</em></p>
    		<p>On each date, starting with March 1, we count U.S. businesses that were open on March 1 and were closed on that day. Closure can be permanent or temporary, and is signaled by a business owner marking the business as closed, including by changing its hours or through a COVID-19 banner on its Yelp page. Closure counts are likely an estimate of the businesses most impacted, with many others not counted because they remain open with curtailed hours and staffing, or because they have not yet updated their Yelp business pages to reflect closures. Additionally, we only count closures that have been vetted by our User Ops team or have been updated directly by a business owner. Closures are counted by state, metro area, and category; some businesses are in more than one category. Businesses can also set automatic reopening dates on Yelp, which are counted as reopenings unless the business updates their information.</p>
    		<p><em>Downloadable static graphics can be found <a href="https://drive.google.com/drive/folders/1kSIOmVz_06NEP37NRfkODkzrpE0lAl3X?usp=sharing" target="_blank">here</a>.</em></p>
    		<p><em>See Yelp's previous Local Economic Impact Reports at our Data Science Medium, <a href="https://medium.com/tag/yelp-coronavirus-report/archive" target="_blank">Locally Optimal</a>.</em></p>
    </section>
    <section>
      <!-- <p class='prose'><strong>Interested in the numbers behind YEA? Check out the <a class='underline' href='./methodology.html'>methodology</a>.</strong></p> -->
      
    </section>    
  </div></div>]]>
            </description>
            <link>https://www.yelpeconomicaverage.com/business-closures-update-sep-2020.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24534186</guid>
            <pubDate>Sun, 20 Sep 2020 14:17:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Generative Bad Handwriting]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24532352">thread link</a>) | @atulvi
<br/>
September 19, 2020 | https://avinayak.github.io/programming/art/2020/09/18/p5-strokes.html | <a href="https://web.archive.org/web/*/https://avinayak.github.io/programming/art/2020/09/18/p5-strokes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>So.. I made a popular tweet last week in the <a href="https://twitter.com/hashtag/%E3%81%A4%E3%81%B6%E3%82%84%E3%81%8DProcessing?src=hashtag_click">#つぶやきProcessing</a> circles.</p>

<blockquote><p lang="cy" dir="ltr">j=24,m=0,draw=(a=&gt;{for(v=(i=&gt;w/3*(n=noise)(i)-k),createCanvas(w=1e3,w),noFill(),background('<a href="https://twitter.com/hashtag/fd7?src=hash&amp;ref_src=twsrc%5Etfw">#fd7</a>'),translate(0,m--),i=0,y=0;y&lt;w-m;y+=j)for(x=k=90;x&lt;w-k;x+=9)if(y+k&gt;-m?curve(v(i++)+x,v(i++)+y,x,j+y,x+9,j+y,v(i++)+x,v(i++)+y):i+=4,x+=v(i++)%9,n(x*y)&lt;.13)y+=j});//<a href="https://twitter.com/hashtag/%E3%81%A4%E3%81%B6%E3%82%84%E3%81%8DProcessing?src=hash&amp;ref_src=twsrc%5Etfw">#つぶやきProcessing</a> <a href="https://t.co/WNIwAAAXjQ">pic.twitter.com/WNIwAAAXjQ</a></p>— atulvinayak (@atulvinayak) <a href="https://twitter.com/atulvinayak/status/1305116417419653120?ref_src=twsrc%5Etfw">September 13, 2020</a></blockquote>


<p>I’ll try to explain how this script worked and how I was able to fit the whole thing into 280 characters.</p>



<p>All of this started when last week when I was experimenting with the p5js <code>curve()</code> function. Internally this is an implementation of the <a href="https://en.wikipedia.org/wiki/Centripetal_Catmull%E2%80%93Rom_spline" title="Centripetal Catmull–Rom spline">Centripetal Catmull–Rom spline</a>. I tried generating a bunch of 8 legged water spiders for fun :)</p>

<video controls="" muted="" src="https://video.twimg.com/ext_tw_video/1303620577589039104/pu/vid/720x720/8iYWjReFxe-9kWVl.mp4?tag=10">
</video>

<p>I admit this looks pretty stupid. The aim was to generate an animation a whole bunch of water spiders with the camera panning around. But then, for me to understand how exactly the Catmull-Rom spline worked, I decided to randomly plot a bunch of curves on a 2D canvas and it somehow resembled handwriting from my native language (<a href="https://en.wikipedia.org/wiki/Malayalam">Malayalam</a>).</p>

<p><img src="https://avinayak.github.io/uploads/download-12.png" alt=""></p>

<p>Actual Malayalam handwriting sample.<br>
<img src="https://avinayak.github.io/uploads/4f31bc2ce9a02537444fc6eeea276dc5.jpg" alt=""></p>

<p>Reducing character spacing.. Do you see the similarity now?<br>
<img src="https://avinayak.github.io/uploads/download-10.png" alt=""></p>

<p>Also, at this time I was playing the PC remaster of <a href="https://en.wikipedia.org/wiki/Journey_(2012_video_game)">Journey (2012)</a>. Journey has a very beautiful blocky scriptures all over the temples in the game.</p>

<p><img src="https://avinayak.github.io/uploads/eayhyxhueaagmqu.jpg" alt=""></p>

<p>I guessed this is pretty easy generate. I made a few attempts to reproduce the approximate style using p5.js</p>

<p><img src="https://avinayak.github.io/uploads/download-8.png" alt=""></p>

<p>and even an infinite scrolling version</p>

<video controls="" muted="" src="https://video.twimg.com/ext_tw_video/1304311867284664323/pu/vid/720x720/zNWZ-LQrIl0KrnCU.mp4?tag=10">
</video>

<p>This script had some serious performance issues(you can see it slowing down towards the end). Later I learned that this style of meaningless writing is a thing in the art community known as Generative <a href="https://en.wikipedia.org/wiki/Asemic_writing">Asemic Writing</a>. According to Wikipedia:</p>

<blockquote>
  <p><strong>Asemic writing</strong> is a wordless open semantic form of writing. The word asemic means “having no specific semantic content”. With the nonspecificity of asemic writing there comes a vacuum of meaning which is left for the reader to fill in and interpret.</p>
</blockquote>

<p>I decided to combine the two and make an infinite generator of malayalam-esque asemic writing. I’ve seen curve generated asemic <a href="https://www.reddit.com/r/asemic/comments/dw5ze3/generative_script/?ref=share&amp;ref_source=link">before</a>. So, What I did is not something new.. however, maybe the way I made it infinite scrolling was something new(?). I’ll try to explain how the code works.</p>



<p>I lost the original script in the minifying process, but I managed to unminify the tweet somehow.</p>

<div><div><pre><code>var yOffset = 24;
var scrollPosition = 0;
var canvasWidth = 800;
var margin = 90

function setup() {
    createCanvas(canvasWidth, canvasWidth)
    noFill();
}

function deterministicRandom(index) {
  return 1000 / 3 * noise(index) - 90
}

function draw() {
    background('#fd7');
    translate(0, scrollPosition--);

    for (i = 0, y = 0; y &lt; canvasWidth - scrollPosition; y += yOffset)
        for (x = 90; x &lt; canvasWidth - margin;) {
            if (y + margin &gt; -scrollPosition) {
                curve(
                  deterministicRandom(i++) + x, 
                  deterministicRandom(i++) + y, 
                  x, 
                  y + yOffset, 
                  x + 9, 
                  y + yOffset, 
                  deterministicRandom(i++) + x, 
                  deterministicRandom(i++) + y
                )
            } else {
                i += 4
            }
            x += (9 + deterministicRandom(i++) % 9)
            if (noise(x * y) &lt; .13)
            {
              y += 2*yOffset
              x = margin
            }
        }

}
</code></pre></div></div>

<p>The most important part of the code is the function <code>deterministicRandom()</code> which is used a lot of times in the sketch. It’s basically <code>noise()</code> but mapped to range <code>[243, -90]</code>. p5 js <code>curve()</code> takes in 2 control point and 2 physical point coordinate to determine the location and shape of the curve. Each character is is thus a set of 4 deterministically random numbers for control points + 4 constants for physical points. All of these points are offset by a base <code>&lt;x,y&gt;</code> coordinate to place the curve in a line. <em>Because it’s deterministically random, the shapes and location of the curves are preserved in every frame</em>.. making the infinite scroll effect work.</p>

<p>The 2 loops iterate over x and y, at a constant rate. x by 9 pixels and y by 24 pixels. But, inside the loop, based on deterministic random, x is randomly incremented by up to 9 pixels to simulate the randomness in spaces between characters. Also, if for a random condition with somewhat low probability (<code>noise(x * y) &lt; .13</code>), a line-break is added. Which means, y is incremented thrice in that loop and x is reset to a margin value (90).</p>





<p>The infinite scroll effect is basically done using <code>translate(0, scrollPosition--)</code>. The loop termination clause is adjusted such that only lines within the frame are rendered (between <code>y = scrollPosition to scrollPosition+canvasHeight</code>). The condition <code>y + margin &gt; -scrollPosition</code> directly inside the loop checks for this. This also offsets the random number index to the one needed by the lines being rendered in the else case. Here’s a version of the script that shows lines being rendered as the script runs:</p>



<p>And that’s basically it. The initial version I designed rendered every line from the first scroll position to the last in every frame, even if those lines were not visible. This is terrible for performance and the if condition inside the loop fixed this.</p>



<p>Step one of minifying was converting all the functions to <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions">arrow functions</a>. This took up way less space. Then I moved all the <code>setup()</code> stuff to <code>draw()</code>. p5 does not re-execute <code>createCanvas</code> even if you place it in <code>draw()</code>. Then I had to cut down number of variables as much as I can. 2 of them were reused: <code>canvasWidth(w)</code> and <code>margin(k)</code> were also used as a coefficient in <code>deterministicRandom()</code>. Finally spaces were removed and long names were truncated to single characters.</p>



<p>This script was written in about 2-3 hours. Looking back, I can see a lot of places where I’d try to reduce repeated code and make it smoother. I never thought this would go so popular, so I never really cared to optimize so much. But there you go.. a simple way to generate bad handwriting :)</p>

</div></div>]]>
            </description>
            <link>https://avinayak.github.io/programming/art/2020/09/18/p5-strokes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24532352</guid>
            <pubDate>Sun, 20 Sep 2020 06:53:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bevy 0.2]]>
            </title>
            <description>
<![CDATA[
Score 252 | Comments 42 (<a href="https://news.ycombinator.com/item?id=24530698">thread link</a>) | @_cart
<br/>
September 19, 2020 | https://bevyengine.org/news/bevy-0-2/ | <a href="https://web.archive.org/web/*/https://bevyengine.org/news/bevy-0-2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><img src="https://bevyengine.org/news/bevy-0-2/matching_squares.png"></p>
      
    
  </div><div><p>A month after the initial Bevy release, and thanks to <strong>87</strong> contributors, <strong>174</strong> pull requests, and our <a href="https://github.com/sponsors/cart"><strong>generous sponsors</strong></a>, I'm happy to announce the <strong>Bevy 0.2</strong> release on <a href="https://crates.io/crates/bevy">crates.io</a>!</p>
<p>For those who don't know, Bevy is a refreshingly simple data-driven game engine built in Rust. You can check out <a href="https://bevyengine.org/learn/book/getting-started/">Quick Start Guide</a> to get started. Bevy is also free and open source forever! You can grab the full <a href="https://github.com/bevyengine/bevy">source code</a> on GitHub.</p>
<p>Here are some of the highlights from this release:</p>
<h2 id="async-task-system">Async Task System</h2>

<p>Bevy uses multi-threading throughout the engine: ECS scheduling, asset loading, rendering, etc. Before this release it used <a href="https://github.com/rayon-rs/rayon">Rayon</a> for almost all of these tasks. Rayon is nice because it is generally as simple as calling <code>some_list.par_iter().for_each(|x| do_something(x))</code>. Rayon then automatically breaks the <code>for_each</code> into tasks and runs them on as many cores as it can. Rayon is a great choice if you want to easily parallelize code, but it has the downside of being pretty cpu-hungry.</p>
<p>Bevy (and a number of other rust game engines and ecs frameworks using rayon) have received feedback that they were overly cpu hungry / usage was not proportional to "real" work done.</p>
<p>We decided to resolve this problem by building a custom async-friendly task system, which enables the creation of context-specific task pools. For example, you might have separate pools for compute, IO, networking, etc. This also gives us the flexibility to load balance work appropriately according to work type and/or priority. The cpu usage wins have been huge:</p>
<h3 id="total-combined-percent-cpu-usage-8-core-machine-smaller-is-better">Total Combined Percent CPU Usage - 8 Core Machine (smaller is better)</h3>
<p><img src="https://bevyengine.org/news/bevy-0-2/bevy_tasks_1.svg" alt="threading cpu usage 8 core"></p>
<h3 id="total-combined-percent-cpu-usage-32-core-machine-smaller-is-better">Total Combined Percent CPU Usage - 32 Core Machine (smaller is better)</h3>
<p><img src="https://bevyengine.org/news/bevy-0-2/bevy_tasks_2.svg" alt="threading cpu usage 32 core"></p>
<h2 id="initial-web-platform-support">Initial Web Platform Support</h2>
<p>authors: @smokku</p>
<p>(A subset of) Bevy now runs on the web using WebAssembly/WASM! Specifically, Bevy apps can run Bevy ECS schedules, react to input events, create an empty canvas (using winit), and a few other things. This is a huge first step, but it is important to call out that there are still a number of missing pieces, such as 2D/3D rendering, multi-threading, and sound. </p>
<p>Those limitations haven't stopped @mrk-its from building the first WASM Bevy game!</p>
<h3 id="bevy-robbo-playable-here"><a href="https://github.com/mrk-its/bevy-robbo">bevy-robbo</a> (<a href="https://s3.eu-central-1.amazonaws.com/mrk-public/bevy-robbo/index.html">playable here</a>)</h3>
<p><img src="https://bevyengine.org/news/bevy-0-2/bevy-robbo.png" alt="bevy-robbo"></p>
<p>They use Bevy for game logic and cleverly work around the render limitations by passing ASCII art game state from <a href="https://github.com/mrk-its/bevy-robbo/blob/master/src/systems/js_render.rs">this Bevy system</a> to <a href="https://github.com/mrk-its/bevy-robbo/blob/master/assets/render.js">this JavaScript function</a>. </p>
<p>You can play around with some Bevy WASM examples by <a href="https://github.com/bevyengine/bevy/tree/master/examples#wasm">following the instructions here</a>.</p>
<h2 id="parallel-queries">Parallel Queries</h2>
<p>authors: @GrantMoyer</p>
<p>Bevy ECS Queries are a flexible way to retrieve data from the Entity Component System. Systems that <em>use</em> queries already run in parallel, but before this change the queries themselves could not be <em>iterated</em> in parallel. <strong>Bevy 0.2</strong> adds the ability to easily iterate queries in parallel:</p>
<pre><code><span>fn </span><span>system</span><span>(</span><span>pool</span><span>: </span><span>Res</span><span>&lt;</span><span>ComputeTaskPool</span><span>&gt;</span><span>, </span><span>mut </span><span>query</span><span>: </span><span>Query</span><span>&lt;&amp;</span><span>mut</span><span> Transform</span><span>&gt;) {</span><span>
    query</span><span>.</span><span>iter</span><span>().</span><span>par_iter</span><span>(</span><span>32</span><span>).</span><span>for_each</span><span>(&amp;</span><span>pool</span><span>, |</span><span>mut </span><span>transform</span><span>| {</span><span>
      transform</span><span>.</span><span>translate</span><span>(</span><span>Vec3</span><span>::</span><span>new</span><span>(</span><span>1.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>));
    });
}
</span></code></pre>
<p>This provides a nice functional api (similar to Rayon) that runs on top of the new <code>bevy_tasks</code> system. It breaks the query up into 32 "batches" and runs each batch as a different task in the bevy task system. </p>
<h2 id="transform-system-rewrite">Transform System Rewrite</h2>
<p>authors: @MarekLg</p>
<pre><code><span>// old
</span><span>fn </span><span>system</span><span>(</span><span>translation</span><span>: &amp;</span><span>Translation, </span><span>rotation</span><span>: &amp;</span><span>Rotation, </span><span>scale</span><span>: &amp;</span><span>Scale</span><span>) {
  </span><span>println!</span><span>("</span><span>{} {} {}</span><span>",</span><span> translation</span><span>.</span><span>0</span><span>,</span><span> rotation</span><span>.</span><span>0</span><span>,</span><span> scale</span><span>.</span><span>0</span><span>);
}

</span><span>// new
</span><span>fn </span><span>system</span><span>(</span><span>transform</span><span>: &amp;</span><span>Transform</span><span>) {
  </span><span>println!</span><span>("</span><span>{} {} {}</span><span>",</span><span> transform</span><span>.</span><span>translation</span><span>(),</span><span> transform</span><span>.</span><span>rotation</span><span>(),</span><span> transform</span><span>.</span><span>scale</span><span>());
}
</span></code></pre>
<p>Bevy's old transform system used separate <code>Translation</code>, <code>Rotation</code>, and <code>Scale</code> components as the "source of truth". Users modified with these components in their systems, after which they were synced to a <code>LocalTransform</code> component, which was in turn synced to a global <code>Transform</code> component, taking hierarchy into account. This was nice for a couple of reasons:</p>
<ul>
<li>Slightly more cache efficient to retrieve individual components like <code>Translation</code> (because less data needs to be accessed)</li>
<li>Theoretically more parallel-friendly. Systems that only access <code>Translation</code> won't block systems accessing <code>Rotation</code>.</li>
</ul>
<p>However this approach also has some pretty serious downsides:</p>
<ul>
<li>The "individual components" are the source of truth, so <code>LocalTransform</code> is out of date when user systems are running. If an up to date "full transform" is needed, it must be manually constructed by accessing all three components.</li>
<li>Very hard to reason about. There are 5 components users need to think about and they all interact with each other differently.</li>
<li>Setting a Transform to a specific matrix value (ex: <code>Mat4::look_at()</code>) was extremely cumbersome, and the value would be immediately overwritten unless the user explicitly disabled component syncing.</li>
</ul>
<p>Given these issues, we decided to move to a single unified local-to-parent <code>Transform</code> component as the source of truth, and a computed <code>GlobalTransform</code> component for world-space transforms. We think this api will be much easier to use and to reason about. <a href="https://gist.github.com/joeante/79d25ec3a0e86436e53eb74f3ac82c0c">Unity is also considering a similar Transform rework for their ECS</a> and a lot of discussion on this topic happened in this <a href="https://community.amethyst.rs/t/legion-transform-design-discussion">Amethyst Forum Thread</a>.</p>
<h2 id="joystick-gamepad-input">Joystick/Gamepad Input</h2>
<p>authors: @simpuid</p>
<p>The Bevy Input plugin now has cross-platform support for most controllers thanks to the <a href="https://gitlab.com/gilrs-project/gilrs">gilrs</a> library!</p>
<pre><code><span>fn </span><span>button_system</span><span>(</span><span>gamepads</span><span>: </span><span>Res</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>Gamepad</span><span>&gt;&gt;</span><span>, </span><span>button_input</span><span>: </span><span>Res</span><span>&lt;</span><span>Input</span><span>&lt;</span><span>GamepadButton</span><span>&gt;&gt;) {
    </span><span>for</span><span> gamepad </span><span>in</span><span> gamepads</span><span>.</span><span>iter</span><span>() {
        </span><span>if</span><span> button_input</span><span>.</span><span>just_pressed</span><span>(</span><span>GamepadButton</span><span>(*</span><span>gamepad</span><span>, </span><span>GamepadButtonType</span><span>::</span><span>RightTrigger</span><span>)) {
            </span><span>println!</span><span>("</span><span>Pressed right trigger!</span><span>");
        }
    }
}
</span></code></pre><h2 id="bevy-ecs-performance-improvements">Bevy ECS Performance Improvements</h2>
<p>authors: @cart</p>
<h3 id="generational-entity-ids">Generational Entity IDs</h3>
<p>We changed Entity IDs from being random UUIDs to incrementing generational indices. Random UUIDs were nice because they could be created anywhere, were unique across game runs, and could be safely persisted to files or reused across networks. I was really hoping we could make them work, but they ended up being too slow relative to the alternatives. The randomness had a measurable cost and entity locations had to be looked up using a hash map.</p>
<p>By moving to generational indices (we use the hecs implementation), we can directly use entity ids as array indices, which makes entity location lookups lightning fast.</p>
<h3 id="read-only-queries">Read Only Queries</h3>
<p>I implemented "read only" traits for queries that don't mutate anything. This allows us to guarantee that a query won't mutate anything.</p>
<h3 id="removed-locking-from-world-apis">Removed locking from World apis</h3>
<p>This gives us a really nice speed boost. We can do this safely due to a combination of the new "read only queries" and changing World mutation apis to be a mutable World borrow.</p>
<p>This is not yet enabled for <code>Queries</code> in systems because a system could have multiple <code>Queries</code>, which could be simultaneously accessed in a way that doesn't make mutable access unique. I think thats a solve-able problem, but it will take a bit more work. Fortunately "for-each" systems don't have any collision risk, so we now use lock-less queries there.</p>
<h3 id="direct-component-lookup-in-nanoseconds-smaller-is-better">Direct component lookup (in nanoseconds, smaller is better)</h3>
<p>As a result of these optimizations, direct component lookup is <em>much</em> faster than it used to be:</p>
<p><img src="https://bevyengine.org/news/bevy-0-2/get_component.svg" alt="get_component graph"></p>
<p>Note that this benchmark used <code>world.get::&lt;T&gt;(entity)</code>. <code>query.get::&lt;T&gt;(entity)</code> should have results similar to the <code>hecs</code> results because it still uses a lock. Eventually I'm hoping that we can remove locks from system queries too.</p>
<h2 id="change-log">Change Log</h2>
<h3 id="added">Added</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/384">Task System for Bevy</a>
<ul>
<li>Replaces rayon with a custom designed task system that consists of several "TaskPools".</li>
<li>Exports <code>IOTaskPool</code>, <code>ComputePool</code>, and <code>AsyncComputePool</code> in <code>bevy_tasks</code> crate.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/292">Parallel queries for distributing work over with the <code>ParallelIterator</code> trait.</a>
<ul>
<li>e.g. <code>query.iter().par_iter(batch_size).for_each(/* ... */)</code></li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/280">Added gamepad support using Gilrs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/503">Implement WASM support for bevy_winit</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/506">Create winit canvas under WebAssembly</a> </li>
<li><a href="https://github.com/bevyengine/bevy/pull/496">Implement single threaded task scheduler for WebAssembly</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/271">Support for binary glTF (.glb).</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/358">Support for <code>Or</code> in ECS queries.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/339">Added methods <code>unload()</code> and <code>unload_sync()</code> on <code>SceneSpawner</code> for unloading scenes.</a>.</li>
<li><a href="https://github.com/bevyengine/bevy/pull/145">Custom rodio source for audio.</a>
<ul>
<li><code>AudioOuput</code> is now able to play anything <code>Decodable</code>.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/362"><code>Color::hex</code></a> for creating <code>Color</code> from string hex values.
<ul>
<li>Accepts the forms RGB, RGBA, RRGGBB, and RRGGBBAA.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/381"><code>Color::rgb_u8</code> and <code>Color::rgba_u8</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/396">Added <code>bevy_render::pass::ClearColor</code> to prelude.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/430"><code>SpriteResizeMode</code> may choose how <code>Sprite</code> resizing should be handled. <code>Automatic</code> by default.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/428">Added methods on <code>Input&lt;T&gt;</code></a> for iterator access to keys.
<ul>
<li><code>get_pressed()</code>, <code>get_just_pressed()</code>, <code>get_just_released()</code></li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/270">Derived <code>Copy</code> for <code>MouseScrollUnit</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/390">Derived <code>Clone</code> for UI component bundles.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/332">Some examples of documentation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/451">Update docs for Updated, Changed and Mutated</a></li>
<li>Tips for faster builds on macOS: <a href="https://github.com/bevyengine/bevy/pull/312">#312</a>, <a href="https://github.com/bevyengine/bevy/pull/314">#314</a>, <a href="https://github.com/bevyengine/bevy/pull/433">#433</a></li>
<li>Added and documented cargo features
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/249">Created document <code>docs/cargo_features.md</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/249">Added features for x11 and wayland display servers.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/363">and added a feature to disable libloading.</a> (helpful for WASM support)</li>
</ul>
</li>
<li>Added more instructions for Linux dependencies
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/275">Arch / Manjaro</a>, <a href="https://github.com/bevyengine/bevy/pull/290">NixOS</a>, <a href="https://github.com/bevyengine/bevy/pull/463">Ubuntu</a> and <a href="https://github.com/bevyengine/bevy/pull/331">Solus</a></li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/491">Provide shell.nix for easier compiling with nix-shell</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/505">Add <code>AppBuilder::add_startup_stage_|before/after</code></a></li>
</ul>
<h3 id="changed">Changed</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/374">Transform rewrite</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/504">Use generational entity ids and other optimizations</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/417">Optimize transform systems to only run on changes.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/323">Send an AssetEvent when modifying using <code>get_id_mut</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/332">Rename <code>Assets::get_id_mut</code> -&gt; <code>Assets::get_with_id_mut</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/183">Support multiline text in <code>DrawableText</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/324">iOS: use shaderc-rs for glsl to spirv compilation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/304">Changed the default node size to Auto instead of Undefined to match the Stretch implementation.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/478">Load assets from root path when loading directly</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/485">Add <code>render</code> feature</a>, which makes the entire render pipeline optional.</li>
</ul>
<h3 id="fixed">Fixed</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/361">Properly track added and removed RenderResources in RenderResourcesNode.</a>
<ul>
<li>Fixes issues where entities vanished or changed color when new entities were spawned/despawned.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/385">Fixed sprite clipping at same depth</a>
<ul>
<li>Transparent sprites should no longer clip.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/345">Check asset path existence</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/376">Fixed deadlock in hot asset reloading</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/394">Fixed hot asset reloading on Windows</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/406">Allow glTFs to be loaded that don't have uvs and normals</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/383">Fixed archetypes_generation being …</a></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bevyengine.org/news/bevy-0-2/">https://bevyengine.org/news/bevy-0-2/</a></em></p>]]>
            </description>
            <link>https://bevyengine.org/news/bevy-0-2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530698</guid>
            <pubDate>Sat, 19 Sep 2020 22:21:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Meeting everyone on a new team]]>
            </title>
            <description>
<![CDATA[
Score 256 | Comments 83 (<a href="https://news.ycombinator.com/item?id=24529176">thread link</a>) | @craigkerstiens
<br/>
September 19, 2020 | https://www.annashipman.co.uk/jfdi/meeting-everyone.html | <a href="https://web.archive.org/web/*/https://www.annashipman.co.uk/jfdi/meeting-everyone.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="meeting-everyone">
    
    <date>17 September 2020</date>
      <p>When I joined the Financial Times as Technical Director for FT.com, I inherited a team of around 50 engineers. One of the first things I did was meet each of them for a one-to-one. I was initially resistant, but it was extremely valuable, I’m glad I did it, and I would definitely do it again in a future role.</p>

<h2 id="my-mentors-advice-about-the-content-of-the-meeting">My mentor’s advice about the content of the meeting</h2>

<p>The idea was suggested to me by a mentor, who’d been advised to do it by <em>his</em> mentor, a Rear Admiral, who said this was something you should do whenever you have a team of fewer than 150 people. My mentor gave me some tips:</p>

<ul>
  <li>Be clear about whether you will take action or whether this is for information only.</li>
  <li>This should mostly be about listening – you should talk for maybe 5 minutes and they should talk for 25.</li>
  <li>It’s to find out what’s going well and what’s not going well.</li>
  <li>It’s informal, but make sure it’s in an enclosed meeting room so that people feel they can speak freely.</li>
  <li>Sometimes it will be quite boring, sometimes you may just learn a lot about someone’s family or hobbies, but that is useful from a getting to know people/relationship-building perspective and it means that you know some things about that person.</li>
  <li>Aim is to get a bit about their background, their priorities and their pressures.</li>
</ul>

<h2 id="making-time-was-hard">Making time was hard</h2>

<p>I was initially resistant because of the time commmitment. With a team of ~50, that’s a lot of hours, and I was also working four days a week so each meeting takes up a greater proportion of time. However, once I’d made the decision to do this and announced my intention, it was important to me to follow through, so I made sure to make time.</p>

<p>I scheduled four of these 1:1s a week, starting with the people reporting directly to me and then on down the management chain.</p>

<h2 id="i-ran-each-meeting-in-the-same-way">I ran each meeting in the same way</h2>

<p>Firstly I ran through everything I planned to cover, and then stepped through it.</p>


<ul>
  <li>I am asking the same questions to everyone</li>
  <li>This is information for me only to get an idea of themes and how things are going; I’m not explicitly planning to take action on anything we discuss, so if something comes up that I need to take action on, let’s make sure we discuss that after this meeting</li>
  <li>This is confidential. If you say something about someone else I’m not going to go and tell them. I may report on ‘what people are saying’, but I’ll say ‘the engineers feel’ or ‘an engineer said’; I won’t say “[Your name] said…”</li>
</ul>

<h3 id="what-were-going-to-discuss">What we’re going to discuss</h3>

<ul>
  <li>First I’ll introduce myself, and tell you a bit about my background</li>
  <li>Then, if you like, I’d love you to tell me a bit about yourself – as much or as little as you feel like sharing</li>
  <li>Then we’ll discuss the following questions:</li>
</ul>

<ol>
  <li>What do you think the most important things we should be doing over the next year?</li>
  <li>What will get in the way of us doing that?</li>
  <li>What’s going well, i.e. what should we make sure we don’t change?</li>
  <li>Is there anything you think I should know about?</li>
</ol>

<h2 id="is-there-anything-i-should-know-about">Is there anything I should know about?</h2>

<p>When I asked this question I talked a bit about why I was asking. I explained that I might not necessarily see or know things that may seem apparent to them, and while they should always feel able to bring things to me, now was a good opportunity to do so. It was an opportunity to make sure I’ve heard what’s important to you, what things should change and what things should stay the same.</p>

<p>This question always elicited very interesting responses, from organisational issues, to personal information people felt it was valuable for me to know about them.</p>

<h2 id="i-told-them-what-i-was-planning-to-ask-in-advance">I told them what I was planning to ask in advance</h2>

<p>I put all the information in the meeting invite.</p>

<div>
<p>I mentioned that I wanted to have a chat with everyone on FT.com to understand how things are going, does this time suit you for this?</p>

<p>The meeting agenda is the same for everyone; a quick intro and then the following questions (I'll go through this in the meeting too):</p>

<ul>
<li>What do you think the most important things we should be doing over the next year?</li>
<li>What will get in the way of us doing that?</li>
<li>What’s going well, i.e. what should we make sure we don’t change?</li>
<li>Is there anything you think I should know about?</li>
</ul>

<p>Thanks,</p>
<p>Anna</p>
</div>

<p>Some people did not read the meeting invite and came with no idea what the meeting was about. Some people had fully prepared and written notes that they then read out to me. Actually people having prepared sometimes was less useful, because sometimes it led the conversation to solutions rather than problems. However it was great that people had really given it some thought.</p>

<h2 id="making-notes-felt-too-much-like-a-promise">Making notes felt too much like a promise</h2>

<p>Each meeting was half an hour. In the very first one, I made notes in a notebook, but I realised that created an implicit commitment that I was going to take action on everything that was said, even though I had said it was information only.</p>

<p>However, I do not have a very good memory, so for all the subsequent ones I made a few notes after each meeting of key themes. This meant I couldn’t do more than two in a row or go straight into another meeting, so it made scheduling slightly harder. These days, people are much more aware of the shorter meeting approach so if doing this again, I’d go for the ‘therapy hour’ – 25 minutes for conversation then 5 minutes for me to make the notes.</p>

<h2 id="introducing-myself">Introducing myself</h2>

<p>In my intro, I gave a potted career history. Starting from my degree in philosophy, and my first career in <a href="https://www.barringtonstoke.co.uk/">children’s book publishing</a>, through teaching myself to code, my <a href="https://www.hw.ac.uk/study/uk/postgraduate/information-technology-software-systems.htm">masters in Software Systems</a> and then my 15+ year career in programming, infrastructure and operations, technical architecture, and my previous role as <a href="https://www.annashipman.co.uk/jfdi/a-year-in-the-life-os-lead.html">Open Source lead</a>. I also talked about what appealed to me about the job as Technical Director at the FT.</p>

<p>I said roughly the same thing to everyone. I don’t normally introduce myself and give my background, but in this case I thought that as a new Tech Director most of them would not be working closely with me, and I would not be contributing code, so it was worth giving my credentials.</p>

<p>My mentor had suggested I also say something personal. I think he intended something like “married with two children” (or whatever), but instead, I tried to give a different kind of personal detail, something about my interests. I tried to come up with a different one for each conversation, for example something about my <a href="https://twitter.com/annashipman/status/1043917006477643777">cross-stitch hobby</a>.</p>

<p>This part was the hardest part for me, because prior to this I had generally enjoyed keeping a clear boundary between work stuff and personal stuff, so that definitely didn’t cover talking about cross-stitch, or my home life, on a first meeting. However, I had been trying to bring more of my personal self to work, and this part of the intro did lead to some really interesting conversations and I think helped make a better connection.</p>

<p>Of course, these days, when we are all at home, my personal life is in meetings with me, so it’s good I’d already started on that journey!</p>

<p>Giving so much information in my introduction also allowed the other person to introduce themselves how they wanted. Some talked career history, some focused on their hobbies, others were really open about their lives and aspirations.</p>

<h2 id="i-am-so-glad-i-did-this">I am so glad I did this</h2>

<p>My mentor was wrong about one thing –&nbsp;none of the conversations were boring.</p>

<p>In my first few months in the new job, I often felt really stretched for time, but I never regretted a single one of these meetings; it was always extremely interesting, my team are brilliant and it was great to meet them one on one, and each conversation always contained some valuable information.</p>

<p>There were two very valuable things about this for me.</p>

<p>The first was getting an idea of what change was needed. These meetings gave me a brilliant insight that wasn’t available elsewhere. Patterns started emerging very quickly, and formed the basis of our <a href="https://medium.com/ft-product-technology/the-difficult-teenage-years-setting-tech-strategy-after-a-launch-7f42eb94a424">tech strategy</a>.</p>

<p>The second was building relationships. A lot of the people I had 1:1s with I would not have come into contact with during the course of the ordinary working week. It would have taken time to meet everyone at socials, and it wouldn’t have been the same quality of conversation. I still feel, two years on, that I know a bit about all the people I had those conversations with, which has felt to me like a good foundation for our subsequent conversations.</p>

<p>It was also good, as someone who is a bit shy, to have names to faces quite quickly and people to say hello to when walking round the office.</p>

<h2 id="was-it-useful-for-my-team">Was it useful for my team?</h2>

<p>About a year later, I asked some of the people with whom I’d had these conversations whether they’d been useful (in an anonymous form).</p>

<p>All of the people who responded said they found the conversation valuable, and some of their comments were:</p>

<ul>
  <li>“It broke down barriers and helped me feel less intimidated about approaching you, whether to talk about work or just to have a general chit chat. You are a very busy person who I wouldn’t ever work with directly so it was good to feel that you knew I existed.”</li>
  <li>“There is hardly any opportunity for me to talk to people in higher position like you except when the team has a big problem. The 1:1 was really casual and I felt really comfortable talking to you. It was a good time to know what kind of person you are. If we didn’t do the 1:1, the answer of the question below “Do you feel able to raise issues with me?” would be “No”.”</li>
  <li>“We sat down when you first started and it was nice to get some one-to-one time because it’s not often you get to do that with a Technical Director. It was nice to raise issues but for me, it was more of an opportunity to understand if I could trust you in the future with raising issues. Raising issues can be difficult and scary so it’s important to know if the person you are raising them to is receptive.”</li>
  <li>“It really showed that you cared about us as humans, and how we fit with the rest of the team. It was also a great opportunity to get to know you”</li>
  <li>“I think often of that conversation”</li>
</ul>

<h2 id="did-it-make-them-feel-more-able-to-raise-issues-with-me">Did it make them feel more able to raise issues with me?</h2>
</div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.annashipman.co.uk/jfdi/meeting-everyone.html">https://www.annashipman.co.uk/jfdi/meeting-everyone.html</a></em></p>]]>
            </description>
            <link>https://www.annashipman.co.uk/jfdi/meeting-everyone.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529176</guid>
            <pubDate>Sat, 19 Sep 2020 17:45:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Twister OS: Make Raspberry Pi Look Like Windows or macOS]]>
            </title>
            <description>
<![CDATA[
Score 147 | Comments 62 (<a href="https://news.ycombinator.com/item?id=24528732">thread link</a>) | @yboris
<br/>
September 19, 2020 | https://twisteros.com/index.html | <a href="https://web.archive.org/web/*/https://twisteros.com/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Twister OS 2020 | <a href="https://discord.gg/Fh8sjmu" target="_blank">Join our Discord!</a></p></div></div>]]>
            </description>
            <link>https://twisteros.com/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24528732</guid>
            <pubDate>Sat, 19 Sep 2020 16:50:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Backing up data like the adult I supposedly am]]>
            </title>
            <description>
<![CDATA[
Score 182 | Comments 121 (<a href="https://news.ycombinator.com/item?id=24526706">thread link</a>) | @miked85
<br/>
September 19, 2020 | https://magnusson.io/post/backups/ | <a href="https://web.archive.org/web/*/https://magnusson.io/post/backups/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
      <div>
        

<article>
  <header>
  
  
  <time datetime="2020-09-18T10:47:47+02:00">
    18 September, 2020
  </time>
  
</header>

  <p>Like so many things I’m supposed to do but don’t — getting exercise, eating right, sleeping well, standing up for women and minorities in public spaces — backing up my data has always been something I’ve half-assed at best.</p>
<p>I’ve lugged around an external hard drive with a few hundred gigabytes of data for the last 10 years, and made backups to it once every three or four years or so. Every time I’ve tried restoring anything from those backups I’ve regretted it, because of course I just bought the drive, plugged it in and copied stuff to it, so it is a FAT32 drive while I have mostly had EXT4 filesystems, which means all my file permissions get lost during the process.</p>
<p>I’ve written shameful little shell scripts to set file permissions to 0644 and directory permissions to 0755, recursively, many many times.</p>
<p>Part of my problem was that I both know just enough rsync to be dangerous and have a credit card so I can provision cloud VMs, so forever just around the corner was my perfect backup solution that I’d write myself and maintain and actually do instead of dealing with whatever I had going on in my life. I’ve come to accept that this will never happen, or perhaps more definitively, that I’d rather cut myself than write and maintain another piece of ad-hoc software for myself.</p>
<p>Luckily I recently found two things that have solved this whole problem for me: <a href="https://borgbackup.readthedocs.io/en/stable/">borg</a> and <a href="https://www.rsync.net/">rsync.net</a>.</p>
<p>Borg is backup software. It compresses and deduplicates data at the block level, and strongly encourages (but does not force) you to encrypt data before backing it up. It is everything I’d want from my half-assed rsync and shell script abomination.</p>
<p>I read its documentation a couple of times and was impressed. I then set about comparing different VM hosts to see which one would give me the cheapest block storage option, when the result of some <a href="https://github.com/scotte/borgsnap">random google search</a> led me to rsync.net. They are a company that stores backups, pretty cheaply, and <a href="http://www.rsync.net/products/attic.html">even more cheaply</a> if you use borg to take them. I guess they just really love borg and want us to love it too.</p>
<p>I signed up for their cheapest plan, which starts at 100GB stored for $18 per year. They have no network in- or egress costs, and the storage amount can be adjusted at any time. Once my account had been activated, I did a little password reset dance, and uploaded a public SSH key.</p>
<p>I wanted to back up my <code>$HOME</code> directory, so after installing borg I ran:</p>
<div>
<div>
<pre>export BORG_REMOTE_PATH="borg1"
borg init --encryption repokey-blake2 UID@ch-s011.rsync.net:home</pre>
</div>
</div>
<p>This created a remote borg repository called "home" on rsync.net’s servers. The environment variable is so we use a more recent version of borg on the remote server (version 1.1.11 at the time of writing), as the default version is rather old (version 0.29.0).</p>
<p>When choosing what encryption method to use, one can choose between a "repokey" or a "keyfile". They both create a private key locked with a passphrase; the difference is that with "repokey" the key is stored in the borg repo, while with "keyfile" it is stored outside of it. This boils down to whether we think a passphrase is enough security for our data, or whether we think having a secret keyfile is necessary. I figured my password manager could create a strong enough passphrase for my needs, and I didn’t want to think about losing the keyfile, so I chose "repokey-blake2".</p>
<p>To create my first backup, I ran</p>
<div>
<div>
<pre>borg create --exclude "$HOME/.cache" UID@ch-s011.rsync.net:home::backup-1 "$HOME"</pre>
</div>
</div>
<p>which created the archive "backup-1" in my "home" borg repository. I didn’t change the compression algorithm from the default one.</p>
<p>By default borg compresses data with lz4. It can use other compression methods (xz, zlib, zstd). I compared their compression ratios on some binary files I had and found no difference between them. I think this is because the large binary files I have are mostly audio and video files in lossy formats, which don’t seem to benefit very much from further compression. I have a lot of text files as well, but text takes up so little relative space on today’s hardware that it makes no sense to spend CPU cycles on compressing it better than lz4 does.</p>
<p>This backup command hummed along for a good while, and through a couple of reboot cycles. Doing a second backup right after it finished (or the day after) took a lot less time because of the deduplication:</p>
<div>
<div>
<pre>borg create --exclude "$HOME/.cache" UID@ch-s011.rsync.net:home::backup-2 "$HOME"</pre>
</div>
</div>
<p>Restoring from backup is also easy:</p>
<div>
<div>
<pre>borg extract UID@ch-s011.rsync.net:home::backup-2</pre>
</div>
</div>
<p>I set this up to run as a daily timed systemd service at noon (very easy on NixOS, which every Linux user should be using unless they hate themselves), and will never, ever think about this again. For a handful of bucks a year, that is a good deal.</p>

  







  



</article>


      </div>
    </div>
  </div></div>]]>
            </description>
            <link>https://magnusson.io/post/backups/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526706</guid>
            <pubDate>Sat, 19 Sep 2020 11:32:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stanisław Leśniewski: rethinking the philosophy of mathematics [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24526676">thread link</a>) | @danielam
<br/>
September 19, 2020 | https://biblio.ugent.be/publication/4443772/file/4443780.pdf | <a href="https://web.archive.org/web/*/https://biblio.ugent.be/publication/4443772/file/4443780.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://biblio.ugent.be/publication/4443772/file/4443780.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526676</guid>
            <pubDate>Sat, 19 Sep 2020 11:27:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A New Back End for Cranelift: Instruction Selection]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24526088">thread link</a>) | @pcr910303
<br/>
September 19, 2020 | https://cfallin.org/blog/2020/09/18/cranelift-isel-1/ | <a href="https://web.archive.org/web/*/https://cfallin.org/blog/2020/09/18/cranelift-isel-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This post is the first in a three-part series about my recent work on
<a href="https://github.com/bytecodealliance/wasmtime/tree/main/cranelift">Cranelift</a>
as part of my day job at Mozilla. In this first post, I will set some context
and describe the instruction selection problem. In particular, I’ll talk about
a revamp to the instruction selector and backend framework in general that
we’ve been working on for the last nine months or so. This work has been
co-developed with my brilliant colleagues Julian Seward and <a href="https://benj.me/">Benjamin
Bouvier</a>, with significant early input from <a href="https://github.com/sunfishcode">Dan
Gohman</a> as well, and help from all of the
wonderful Cranelift hackers.</p>

<h2 id="background-cranelift">Background: Cranelift</h2>

<p>So what is Cranelift? The project is a compiler framework written in
<a href="https://www.rust-lang.org/">Rust</a> that is designed especially (but not
exclusively) for <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">just-in-time
compilation</a>. It’s a
general-purpose compiler: its most popular use-case is to compile
<a href="https://www.webassembly.org/">WebAssembly</a>, though several other frontends
exist, for example,
<a href="https://github.com/bjorn3/rustc_codegen_cranelift">cg_clif</a>, which adapts the
Rust compiler itself to use Cranelift. Folks at Mozilla and several other
places have been developing the compiler for a few years now.  It is the
default compiler backend for
<a href="https://github.com/bytecodealliance/wasmtime">wasmtime</a>, a runtime for
WebAssembly outside the browser, and is used in production in several other
places as well. We recently flipped the switch to turn on Cranelift-based
WebAssembly support in nightly Firefox on <a href="https://en.wikipedia.org/wiki/AArch64">ARM64
(AArch64)</a> machines, including most
smartphones, and if all goes well, it will eventually go out in a stable
Firefox release. Cranelift is developed under the umbrella of the <a href="https://bytecodealliance.org/">Bytecode
Alliance</a>.</p>

<p>In the past nine months, we have built a new framework in Cranelift for the
“machine backends”, or the parts of the compiler that support particular CPU
instruction sets. We also added a new backend for AArch64, mentioned above, and
filled out features as needed until Cranelift was ready for production use in
Firefox. This blog post sets some context and describes the design process that
went into the backend-framework revamp.</p>

<p>It can be a bit confusing to keep all of the moving parts straight. Here’s a
visual overview of Cranelift’s place among various other components, focusing
on two of the major Rust crates (the Wasm frontend and the codegen backend) and
several of the other programs that make use of Cranelift:</p>

<p><img src="https://cfallin.org/assets/2020-09-10-cranelift-components.svg" alt="Figure: Cranelift and other components"></p>

<h2 id="old-backend-design-instruction-legalizations">Old Backend Design: Instruction Legalizations</h2>

<p>To understand the work that we’ve done recently on Cranelift, we’ll need to
zoom into the <code>cranelift_codegen</code> crate above and talk about how it <em>used to</em>
work. What is this “CLIF” input, and how does the compiler translate it to
machine code that the CPU can execute?</p>

<p>Cranelift makes use of
<a href="https://github.com/bytecodealliance/wasmtime/blob/main/cranelift/docs/ir.md">CLIF</a>,
or the Cranelift IR (Intermediate Representation) Format, to represent the code
that it is compiling. Every compiler that performs program optimizations uses
some form of an <a href="https://en.wikipedia.org/wiki/Intermediate_representation">Intermediate Representation
(IR)</a>: you can think
of this like a virtual instruction set that can represent all the operations a
program is allowed to do. The IR is typically simpler than real instruction
sets, designed to use a small set of well-defined instructions so that the
compiler can easily reason about what a program means. The IR is also
independent of the CPU architecture that the compiler eventually targets; this
lets much of the compiler (such as the part that generates IR from the input
programming language, and the parts that optimize the IR) be reused whenever
the compiler is adapted to target a new CPU architecture.  CLIF is in <a href="https://en.wikipedia.org/wiki/Static_single_assignment_form">Static
Single Assignment
(SSA)</a> form, and
uses a conventional <a href="https://en.wikipedia.org/wiki/Control-flow_graph">control-flow
graph</a> with basic blocks
(though it previously allowed extended basic blocks, these have been phased
out). Unlike many SSA IRs, it represents φ-nodes with block parameters
rather than explicit φ-instructions.</p>

<p>Within <code>cranelift_codegen</code>, before we revamped the backend design, the program
remained in CLIF throughout compilation and up until the compiler emitted the
final machine code. This might seem to contradict what we just said: how can
the IR be machine-independent, but also be the final form from which we emit
machine code?</p>

<p>The answer is that the old backends were built around the concept of
“legalization” and “encodings”. At a high level, the idea is that every
<em>Cranelift</em> instruction either corresponds to one <em>machine</em> instruction, or can
be replaced by a sequence of other <em>Cranelift</em> instructions. Given such a
mapping, we can refine the CLIF in steps, starting from arbitrary
machine-independent instructions from earlier compiler stages, performing edits
until the CLIF corresponds 1-to-1 with machine code. Let’s visualize this
process:</p>

<p><img src="https://cfallin.org/assets/2020-09-10-cranelift-legalization.svg" alt="Figure: legalization by repeated instruction expansion"></p>

<p>A very simple example of a CLIF instruction that has a direct “encoding” to a
machine instruction is <code>iadd</code>, which just adds two integers. On essentially any
modern architecture, this should map to a simple ALU instruction that adds two
registers.</p>

<p>On the other hand, many CLIF instructions do not map cleanly. Some arithmetic
instructions fall into this category: for example, there is a CLIF instruction
to count the number of set bits in an integer’s binary representation
(<code>popcount</code>); not every CPU has a single instruction for this, so it might be
expanded into a longer series of bit manipulations. There are operations that
are defined at a higher semantic level, as well, that will necessarily be
lowered with expansions: for example, accesses to Wasm memories are lowered
into operations that fetch the linear memory base and its size, bounds-check
the Wasm address against the limit, compute the real address for the Wasm
address, and perform the access.</p>

<p>To compile a function, then, we iterate over the CLIF and find instructions
with no direct machine encodings; for each, we simply expand into the legalized
sequence, and then recursively consider the instructions in that sequence. We
loop until all instructions have machine encodings. At that point, we can emit
the bytes corresponding to each instruction’s encoding<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>.</p>

<h2 id="growing-pains-and-a-new-backend-framework">Growing Pains, and a New Backend Framework?</h2>

<p>There are a number of advantages to the legacy Cranelift backend design, which
performs expansion-based legalization with a single IR throughout. As one might
expect, though, there are also a number of drawbacks. Let’s discuss a few of
each.</p>

<h3 id="single-ir-and-legalization-pros">Single IR and Legalization: Pros</h3>

<ol>
  <li>
    <p>By operating on a single IR all the way to machine-code emission, the same
optimizations can be applied at multiple stages. For example, consider a
legalization expansion that turns a high-level “access Wasm memory”
instruction into a sequence of loads, adds and bounds-checks. If many such
sequences occur in one function, we might be able to factor out common
portions (e.g.: computing the base of the Wasm memory).  Thus the
legalization scheme exposes as much code as possible, at as many stages as
possible, to opportunities for optimization. The legacy Cranelift pipeline
in fact works in this way: it runs “pre-opt” and “post-opt” optimization
passes, before and after legalization respectively.</p>
  </li>
  <li>
    <p>If <em>most</em> of the Cranelift instructions become one machine instruction, and
few legalizations are necessary, then this scheme can be very fast: it
becomes simply a single traversal to fill in “encodings”, which were
represented by small indices into a table.</p>
  </li>
</ol>

<h3 id="single-ir-and-legalization-cons">Single IR and Legalization: Cons</h3>

<ol>
  <li>
    <p>Expansion-based legalization may not always result in
optimal code. So far we’ve seen that legalization can convert from CLIF to
machine instructions with one-to-one or one-to-many mappings. However, there
are sometimes also <em>single</em> machine instructions that implement the behavior of
<em>multiple</em> CLIF instructions, i.e. a many-to-one mapping. In order to generate
efficient code, we want to be able to make use of these instructions.</p>

    <p>For example, on x86, an instruction that references memory can compute an
address like <code>base + scale * index</code>, where <code>base</code> and <code>index</code> are registers
and <code>scale</code> is 1, 2, 4, or 8. There is no notion of such an address mode in
CLIF, so we would want to pattern-match the raw <code>iadd</code> (add) and <code>ishl</code>
(shift) or <code>imul</code> (multiply) operations when they occur in the address
computation. Then, we would want to somehow select the encoding on the
<code>load</code> instruction based on the fact that its input is some specific
combination of adds and shifts/multiplies.  This seems to break the
abstraction that the encoding represents only that instruction’s operation.</p>

    <p>In principle, we could implement more general pattern matching for legalization
rules to allow many-to-one mappings. However, this would be a significant
refactor; and as long as we were reconsidering the design in whole, there were
other reasons to avoid patching the problem in this way.</p>
  </li>
  <li>
    <p>There is a conceptual difficulty with the single-IR approach: there is
no static representation of which instructions are expanded into which others
and it is difficult to reason about the correctness and termination properties
of legalization as a whole.</p>

    <p>Specifically, the expansion-based legalization rules must obey a partial
order among instructions: if A expands into a sequence including B, then B
cannot later expand into A. In practice, mappings were mostly one-to-one,
and for those that weren’t, there was a clear domain separation between the
“input” high-level instructions and the “machine-level” instructions.
However, for more complex machines, or more complex matching schemes that
attempt to make better use of the target instruction set, this could become
a real difficulty for the machine-backend author to keep straight.</p>
  </li>
  <li>
    <p>There are efficiency concerns with expansion-based legalization. At
an algorithmic level, we prefer to avoid fixpoint loops (in this case,
“continue expanding until no more expansions exist”) whenever possible. The
runtime is bounded, but the bound is somewhat difficult to reason about,
because it depends on the maximum depth of chained expansions.</p>

    <p>The data structures that enable in-place editing are also much slower than
we would like. Typically, compilers store IR instructions in linked lists to
allow for in-place editing. …</p></li></ol></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cfallin.org/blog/2020/09/18/cranelift-isel-1/">https://cfallin.org/blog/2020/09/18/cranelift-isel-1/</a></em></p>]]>
            </description>
            <link>https://cfallin.org/blog/2020/09/18/cranelift-isel-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526088</guid>
            <pubDate>Sat, 19 Sep 2020 08:40:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: How attractive is your website? Check using Visual Mind AI]]>
            </title>
            <description>
<![CDATA[
Score 107 | Comments 169 (<a href="https://news.ycombinator.com/item?id=24525995">thread link</a>) | @myraahio
<br/>
September 19, 2020 | https://myraah.io/visualmind | <a href="https://web.archive.org/web/*/https://myraah.io/visualmind">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            <div>
              
              <h4>Visual rank of your website is as important as your SEO rank.</h4>
              <p>Users make lasting judgments about a website’s appeal within a split second.  This first impression is influential enough to later affect their opinions of a site’s usability and trustworthiness.</p>
            </div>
          </div> <!-- col -->
        </div><div>
          <div>
            <div>
              
              <h4>What is Visual Mind ?</h4>
              <p>Visual Mind is an AI engine specifically designed for understanding and scoring visual appearance of a website. Visual Mind has analyzed over a million websites to achieve an accuracy rate of over 97%.</p>
            </div>
          </div> <!-- col -->
        </div><div>
          <div>
            <div>
              
              <h4>What is Visual Mind Score and why it matters ?</h4>
              <p>For too long, aesthetics of a website has been dismissed as a superficial concern. That is a mistake. As latest research demonstrates ( See recommended ref) , the visual appeal of a website is tied up with far weightier issues, such as functionality and trustworthiness.</p>
              <p>Have you “fast-tested” your website? Remember, you have only fifty milliseconds to impress your visitors. Flash your website to people for a very short period of time and then ask for their opinion. That is the opinion that matters.</p>
              <p>Visual Mind score – provides you with a qualitative score about that first impression. It can help you evaluate your website aesthetics and make improvements.</p>
              <p><a href="https://myraah.io/index.php/visualmind">Check Your VM SCORE</a></p>
            </div>
          </div> <!-- col -->
        </div><div>
          <div>
            <div>
				<h4>Want to explore more – we recommend</h4>
              <p>A.  Bauerly, M., and Liu, Y. Effects of Symmetry and Number of Compositional Elements on Interface and Design Aesthetics. Int. Journal of Human-Computer Interaction 3 (2008).</p>
              <p>B. Cyr, D. Modeling Website Design across Cultures: Relationships to Trust, Satisfaction and E-loyalty. Journal of Management Information Systems 24, 4 (2008)</p>
              <p>C. Everard, A., and Galletta, D. How presentation flaws affect perceived site quality, trust, and intention to purchase from an online store. Journal of Management Information Systems 22, 3 (2006)</p>
              <p>D. Geissler, G., Zinkhan, G., and Watson, R. The Influence of Home Page Complexity on Consumer Attention, Attitudes, and Purchase Intent. Journal of Advertising 35, 2 (2006)</p>
              <p>E. Hall, R. H., and Hanna, P. The Impact of Web Page Text-background Colour Combinations on Readability,Retention, Aesthetics and Behavioural Intention. Behaviour &amp; Information Technology 23, 3 (2004)</p>
              <p>G. Lindgaard, G., Fernandes, G., Dudek, C., and Brown, J. Attention Web Designers: You Have 50 Milliseconds to Make a Good First Impression! Behaviour &amp; Information Technology 25, 2 (2006)</p>
              <p>H. Michailidou, E., Harper, S., and Bechhofer, S. Visual Complexity and Aesthetic Perception of Web Pages. Proc. Design of Communication (2008)</p>
              <p>I. Tuch, A. N., Bargas-Avila, J. A., and Opwis, K. Symmetry and Aesthetics in Website Design: It’s a Man’s Business. Computers in Human Behavior 26, 6 (2010)</p>
              
			</div>
          </div> <!-- col -->
        </div></div>]]>
            </description>
            <link>https://myraah.io/visualmind</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525995</guid>
            <pubDate>Sat, 19 Sep 2020 08:13:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hand-Optimizing VLIW Assembly Language as a Game]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24525372">thread link</a>) | @luu
<br/>
September 18, 2020 | http://silverspaceship.com/hovalaag/ | <a href="https://web.archive.org/web/*/http://silverspaceship.com/hovalaag/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://silverspaceship.com/hovalaag/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525372</guid>
            <pubDate>Sat, 19 Sep 2020 05:27:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[U.S. bans WeChat, TikTok, citing national security reasons]]>
            </title>
            <description>
<![CDATA[
Score 171 | Comments 183 (<a href="https://news.ycombinator.com/item?id=24524662">thread link</a>) | @empressplay
<br/>
September 18, 2020 | https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The U.S. Commerce Department has issued an order that will bar people in the United States from downloading Chinese-owned messaging app WeChat and video-sharing app TikTok, starting Sunday.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5729631.1600444028!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/1263681818.jpg"></p></div><figcaption>U.S. business transactions with the Chinese-owned social apps WeChat and TikTok are to be banned, starting Sunday.<!-- --> <!-- -->(Cindy Ord/Getty Images)</figcaption></figure><p><span><p>The U.S. Commerce Department has issued an order that will bar people in the United States from downloading Chinese-owned messaging app WeChat and video-sharing app TikTok, starting Sunday.</p>  <p>Commerce officials said the ban on new U.S. downloads of TikTok could be still rescinded by President Donald Trump before it takes effect late Sunday as TikTok owner ByteDance races to clinch an agreement over the fate of its U.S. operations.</p>  <p>ByteDance has been in talks with Oracle Corp and others to create a new company, TikTok Global, which&nbsp;aims to address U.S. concerns about the security of its users' data. ByteDance still needs Trump's approval to stave off a U.S. ban.</p>  <p>Commerce officials said they will not bar additional technical transactions for TikTok until Nov. 12, which gives the company additional time to see if ByteDance can reach a deal for its U.S. operations. "The basic TikTok will stay intact until Nov. 12," Commerce Secretary Wilbur Ross told Fox Business Network.</p>  <p>The department said the actions will "protect users in the U.S. by eliminating access to these applications and significantly reducing their functionality."</p>  <p>U.S. Commerce Department officials said they were taking the extraordinary step because of the risks the apps' data collection poses. China and the companies have denied U.S. user data is collected for spying.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1205295609.jpg 300w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1205295609.jpg 460w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1205295609.jpg 620w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1205295609.jpg 780w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1205295609.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1205295609.jpg"></p></div><figcaption>U.S. Secretary of Commerce Wilbur Ross said the ban on Tik Tok and WeChat will combat China's 'malicious collection of American citizens' personal data.'<!-- --> <!-- -->(Saul Loeb/AFP/Getty Images)</figcaption></figure></span></p>  <p>Ross said in a written statement "we have taken significant action to combat China's malicious collection of American citizens' personal data, while promoting our national values, democratic rules-based norms, and aggressive enforcement of U.S. laws and regulations."</p>  <p>"We disagree with the decision from the Commerce Department, and are disappointed that it stands to block new app downloads from Sunday and ban use of the TikTok app in the U.S. from Nov. 12," the company said in a statement. "We will continue to challenge the unjust executive order, which was enacted without due process and threatens to deprive the American people and small businesses across the U.S. of a significant platform for both a voice and livelihoods."</p>  <p>The Commerce Department order will "de-platform" the two apps in the U.S. and bar Apple Inc's app store, Alphabet Inc's Google Play and others from offering the apps on any platform "that can be reached from within the United States," a senior Commerce official told Reuters.</p>  <p>The order will not ban U.S. companies from doing business&nbsp;on WeChat outside the United States, which will be welcome news to U.S. firms like Walmart and Starbucks that use WeChat's embedded "mini-app"&nbsp;programs to facilitate transactions and engage consumers in China, officials said.</p>    <p>The order will not bar transactions with WeChat-owner Tencent Holdings' other businesses, including its online gaming operations, and will not prohibit Apple, Google or others from offering TikTok or WeChat apps anywhere outside the United States.</p>  <p>The bans are in response to a pair of executive orders issued by Trump on Aug.&nbsp;6 that gave the Commerce Department 45 days to determine what transactions to block from the apps he deemed pose a national security threat. That deadline expires on Sunday.</p>  <h2>'Untrusted'&nbsp;Chinese apps</h2>  <p>The Trump administration has ramped up efforts to purge "untrusted" Chinese apps from U.S. digital networks and has called TikTok and WeChat&nbsp;"significant threats."</p>  <p>TikTok has 100 million users in the United States and is especially popular among younger Americans.</p>  <p>WeChat has had an average of 19 million daily active users in the United States, analytics firm&nbsp;Apptopia said in early August. It is popular among Chinese students, ex-pats and some Americans who have personal or business relationships in China.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1228542119.jpg 300w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1228542119.jpg 460w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1228542119.jpg 620w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1228542119.jpg 780w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1228542119.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1228542119.jpg"></p></div><figcaption>People walk past the headquarters of ByteDance, the parent company of TikTok, in Beijing.<!-- --> <!-- -->(Greg Baker/AFP/Getty Images)</figcaption></figure></span></p>  <p>WeChat is an all-in-one mobile app that combines services similar to Facebook, WhatsApp, Instagram and Venmo. The app is an essential part of daily life for many in China and boasts more than 1 billion users.</p>  <p>The Commerce Department will not seek to compel people in the United States to remove the apps or stop using them but will not allow updates or new downloads. "We are aiming at a top corporate level. We're not going to go out after the individual users," one Commerce official said.</p>  <p>Over time, officials said, the lack of updates will degrade the apps' usability.</p>  <p>"The expectation is that people will find alternative ways to do these actions," a senior official said. "We expect the market to act and there will be more secure apps that will fill in these gaps that Americans can trust and that the United States government won't have to take similar actions against."</p>    <p>The Commerce Department is also barring additional technical transactions with WeChat starting Sunday that will significantly reduce the usability and functionality of the app in the United States.</p>  <p>The order bars data hosting within the United States for WeChat, content delivery services and networks that can increase functionality and internet transit or peering services.</p>  <p>"What immediately is going to happen is users are going to experience a lag or lack of functionality," a senior Commerce official said of WeChat users. "It may still be usable but it is not going to be as functional as it was." There may be sporadic outages as well, the official said.</p>  <p>Commerce will bar the same set of technical transactions for TikTok, but that will not take effect until Nov. 12 to give the company additional time to see if ByteDance can reach a deal for its U.S. operations. The official said TikTok U.S. users would not see "a major difference" in the app's performance until Nov. 12.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1273236956.jpg 300w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1273236956.jpg 460w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1273236956.jpg 620w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1273236956.jpg 780w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1273236956.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1273236956.jpg"></p></div><figcaption>U.S. President Donald Trump could still rescind the download ban before it comes into effect Sunday. <!-- --> <!-- -->(Scott Olson/Getty Images)</figcaption></figure></span></p>  <p>Commerce will not penalize people who use TikTok or WeChat in the United States.</p>  <p>The order does not bar data storage within the United States for WeChat or TikTok.</p>  <p>Some Americans may find workarounds. There is nothing that would bar an American from travelling to a foreign country and downloading either app, or potentially using a virtual private network and a desktop client, officials conceded.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249</link>
            <guid isPermaLink="false">hacker-news-small-sites-24524662</guid>
            <pubDate>Sat, 19 Sep 2020 03:15:20 GMT</pubDate>
        </item>
    </channel>
</rss>
