<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Fri, 25 Sep 2020 20:24:59 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Fri, 25 Sep 2020 20:24:59 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Show HN: Open-Source Memex – Alternative Approach to Roam/Obsidian]]>
            </title>
            <description>
<![CDATA[
Score 144 | Comments 41 (<a href="https://news.ycombinator.com/item?id=24572449">thread link</a>) | @steve1820
<br/>
September 23, 2020 | https://www.steveliu.co/memex | <a href="https://web.archive.org/web/*/https://www.steveliu.co/memex">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-block-type="2" id="block-33de0799eab39cfadbe1"><div><p>I’ve never been a huge online note taker. From high school to university, I’ve always relied on pen and paper as my weapon of choice. At the time and even to some extent now, I’ve felt like this was a good enough solution to my problems.</p><p>I’ve always felt the simpler the solution the better. Why complicate things?</p><p>This changed however after working as a software engineer in industry. As I worked on a software product that derived its core functionality from machine learning, it seemed that I was constantly drowning in a sea of information.&nbsp;</p><p>It was a constant repetition of learning something, forgetting about it 5 months later and then having to recycle through my notes and reread the article/paper/blog.</p><p>My brain was a leaky bucket. Every time I poured something in, something else would leak out.</p><p>It was during those dark times of desperation that I stumbled upon the “niche” industry of Knowledge Management Systems (KMS) and as an extension, the Memex.</p><p> I was fascinated with all the innovation coming from up and coming open source projects and companies in this space. Software like Athens (https://github.com/athensresearch/athens), Roam (https://roamresearch.com/), Obsidian (https://obsidian.md/) all seemed so promising. </p><p>I was particularly inspired by reading karlicoss’s blog (https://beepb00p.xyz/promnesia.html). He outlines so many good and intuitive reasons why the current solutions are broken (although in this particular post he focuses on browser history).</p></div></div></div></div>]]>
            </description>
            <link>https://www.steveliu.co/memex</link>
            <guid isPermaLink="false">hacker-news-small-sites-24572449</guid>
            <pubDate>Wed, 23 Sep 2020 21:44:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bleeding-edge tech will kill your startup]]>
            </title>
            <description>
<![CDATA[
Score 166 | Comments 118 (<a href="https://news.ycombinator.com/item?id=24571216">thread link</a>) | @bmaho
<br/>
September 23, 2020 | https://www.contrast.app/posts/bleeding-edge-tech-means-youll-bleed-to-death | <a href="https://web.archive.org/web/*/https://www.contrast.app/posts/bleeding-edge-tech-means-youll-bleed-to-death">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Remember those old don't-do-drug ads? A shaky camera pans to a narc wearing a wrinkled button-down:</p><p><strong>Egg is lifted:</strong></p><p>"This is your brain"</p><p><strong>Points to hot skillet:</strong></p><p>"This is drugs"</p><p><strong>Cracks egg into skillet. Egg starts frying:</strong></p><p>"This is your brain on drugs"</p><figure id="w-node-8595f4c9f275-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8cd757b4ac440ca61db3_Frame%202.png" loading="lazy" alt=""></p></figure><p>Firstly, makes no sense seeing everyone loves fried eggs. Secondly, this is how my brain looked when trying to start a "bleeding edge" tech company.</p><p>2018. Winter. NYC.</p><p>Lying on the floor, looking like the beached whale I am, I stared at the ceiling. For months, we had been trying to start a food delivery business. Nothing bleeding-edge, just pain.</p><p>It was in that moment, with my shirt gathering fuzz from the un-vacuumed carpet below, that we changed everything. New market, new users, new product, new tech. We just wanted to do something new—something no one else was doing.</p><p>As designers and engineers, we knew of a rather niche problem: if your company has a design system, it's practically impossible to know the adoption rate of those components in your product. Huzzah! we thought. We should build a tool that tells you the adoption rate of every component in your design system—across your product. Chefs kiss.</p><figure id="w-node-f806252cfa53-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8ce4411317e302386081_Frame%208.png" loading="lazy" alt=""></p></figure><p>Now, no worries if you don't know what this means. For the sake of the article, this was "bleeding-edge tech," and this simple fact got us psyched. We would be the first, the trailblazers. We felt like how I imagine all the kids who grow up watching "Baby Einstein" feel—pompous, cute, and little geniuses.</p><p>Little did I know, this mentality would fuck us...</p><p>Before continuing with my failures, a history lesson. After a quick Wikipedia search, the first time we see the term "bleeding edge technology" being used was during the early 80s—right when the drug-ad above aired (clearly some weird shit happening then). Bleeding edge was an iteration on the classic "cutting-edge" or "leading-edge" phrases—altered to show an even higher level of risk for both the company and the customer.</p><p>So how did this mentality fuck us? Well, I've come up with a rather clever framework I'm calling:</p><h5>"The Bloody Edged Circle of Death"</h5><p>‍</p><p>This loop consists of <strong>three parts:</strong></p><ol role="list"><li>Sales</li><li>Product</li><li>Mental game</li></ol><figure id="w-node-c5258bced305-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8bf55a4723fd0e275db4_Frame%2012-min.png" loading="lazy" alt=""></p></figure><h5>Sales</h5><p>Sales is hard and if you want to make it 10X harder, try selling something nobody knows anything about. Like literally nothing. When we started, prospects didn't know our name, the problem we were solving, our solution, how valuable it would be, what it should be priced at, and the list went on and on. And here's the kicker, because they knew nothing, they didn't trust us. We weren't proven and thus we were a huge risk to adopt.</p><p>With this bleeding edge product in hand, we were banging our heads against the wall, trying to sell to early users who'd be willing to try something incredibly new and incredibly risky. Not fun. Very difficult.</p><p>‍</p><figure id="w-node-b112f59a5983-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8bfe1409e87b269d51cd_Frame%2013-min.png" loading="lazy" alt=""></p></figure><h5>Product</h5><p>The brutal sales cycles then bled into our product. After a few months, we had convinced a handful of companies to test our tech. The day would arrive and we'd finally onboard them. But guess what, they had no idea how to use our tool or integrate it within their workflow. And to be honest, nor did we. Adoption was bleak and feedback was dry—silence in startups is not golden.</p><p>What I realized is when you're the first, there is very little to look at for inspiration, ideas, or as a benchmark. It's up to you as the trailblazer to build the path and hope it will lead somewhere. This is something I think many of wish we could do—but I think very few can.</p><p>‍</p><figure id="w-node-97e798a4e2ae-ccb706c6"><p><img src="https://uploads-ssl.webflow.com/5f08abd1a8581a1027bfc65a/5f6b8c07e074c53b56bc6989_Frame%2014-min.png" loading="lazy" alt=""></p></figure><h5>Mental game</h5><p>I think the worst stage is the mental-game. When you're trying to drag your zombie-death startup from the crypts of hell and into the light, there are so many things to stress about. Your messaging, positioning, pricing, product, customers, sales, employees—literally everything. But when you're building something truly new, there is an additional poisonous leach that creeps into your brain and plants its ugly seed: doubt.</p><p>Bleeding edge tech means you'll constantly be doubting whether what you've created will ever be something or not—there is no market data to tell you otherwise. You don't know if customers are already using a solution like yours. You don't know if anyone is willing to pay for your offering. You don't know how big the market is or will ever become. You don't know anything.</p><p>This was the hardest thing for me to deal with personally. The daily doubt. Constantly wondering if we needed to just tweak a few of the 4 P's or much worse—that this wasn't a real problem needed to be solved.</p><p>"The Bloody Edged Circle of Death" continued its cycle until one day, we decided no more. We didn't have the conviction in ourselves and the product to keep pushing through the long sales cycles, silence from inactive users, and the crippling doubt. We were done <strong>bleeding out from bleeding edge tech.</strong></p><p>Since then, I've been thinking a lot about why we were so keen on building tech no one had ever seen before. When I started to look for answers, I noticed something peculiar. All of the "hottest" tech companies: Airtable, Notion, Slack, Zoom—none of them are "bleeding edge" per se. Sure, they have incrementally innovated on existing products in existing categories—but they're not doing anything truly <em>new.</em></p><p>They identified problems with the status quo, in massive markets, with huge amounts of users and money. They then developed a better product by improving the UX, performance, and adding new features. Don't get me wrong, this is an insanely difficult feat to ever pull off. Yet, before I gave this any thought, I assumed they were successful because they were doing something new—because they were bleeding edge.</p><p>So why was I fed this lie and believed it to be true?</p><p>I think the answer is pretty simple—marketing and ego. If you're a founder, you want your company to be seen as revolutionary. Even if your startup just slapped lipstick on a pig, you want the world to believe you invented a super hot and sexy pig, one that sprouts wings and poops pearls.</p><p>And same thing goes for VCs. Investors want to be seen as picking the best and brightest—finding the diamonds in the rough. Their aim is to say: "I backed a ground breaking technology that changed the world," and not: "I invested in 10 slightly-better-than-Google-Docs competitors." No one on Twitter is getting jacked on that.</p><p>If you're anything like me, you've been reading, listening, and following the big players in Silicon-Valley for years. We've been told a narrative that sounds great and inspires, but it's not based in facts. Bleeding edge is a myth that fucks up your chances of success.</p><p>Since this realization, we've purposefully taken our company in a different direction. A new strategy, one where we purposefully have entered an existing market—a market where we have expertise. Yes, there is competition, but we've been using these tools for years, we know the problems they have, and we want to make something better—both for our team and customers.</p><p>Although this path doesn't have the same sparkle as a bleeding-edge tech company might, I can sleep easier knowing we have a higher chance of getting somewhere with it. I can finally focus on the problems every startup deals with without that doubt—that nose-crinkling, water-trash stench of doubt. </p><p>There is a market. There are users who pay. There is value to what we're doing. Now it's just up to us to make our product, positioning, and team the best it can be.</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.contrast.app/posts/bleeding-edge-tech-means-youll-bleed-to-death</link>
            <guid isPermaLink="false">hacker-news-small-sites-24571216</guid>
            <pubDate>Wed, 23 Sep 2020 19:56:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ray Marching Soft Shadows in 2D]]>
            </title>
            <description>
<![CDATA[
Score 160 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24569542">thread link</a>) | @rjkaplan
<br/>
September 23, 2020 | https://www.rykap.com/2020/09/23/distance-fields/ | <a href="https://web.archive.org/web/*/https://www.rykap.com/2020/09/23/distance-fields/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    


    <div role="main">
      <div>
        




<article>
  

<p><em>Disclaimer: the demos on this page use WebGL features that aren’t available on some mobile devices.</em></p>

<p>A couple of weeks ago I tweeted a video of a toy graphics project (below). It’s not done, but a lot of people liked it which was surprising and fun! A few people asked how it works, so that’s what this post is about.</p>



<p>Under the hood it uses something called a distance field. A distance field is an image like the one below that tells you how far each pixel is from your shape. Light grey pixels are close to the shape and dark grey pixels are far from it.</p>

<img src="https://www.rykap.com/images/ray-marching/distance-field.png">

<p>When the demo starts up, it draws some text on a 2D canvas and generates a distance field of it. It uses <a href="https://github.com/ryankaplan/gpu-distance-field">a library I wrote</a> that generates distance fields really quickly. If you’re curious how the library works, I wrote about that <a href="http://rykap.com/graphics/skew/2016/02/25/voronoi-diagrams/">here</a>.</p>

<p>Our lighting scheme works like this: when processing a particular pixel we consider a ray from it to the light, like so…</p>

<img src="https://www.rykap.com/images/ray-marching/ray-march-1.png">

<p>If the ray intersects a glyph, the pixel we’re shading must be in shadow because there’s something between it and the light.</p>

<p>The simplest way to check this would be to move along the ray in 1px increments, starting from the pixel we’re shading and ending at the light, repeatedly asking the distance field if we’re distance 0 from a shape. This would work, but it’d be really slow.</p>

<p>We could pick some specific length like 30px and move in increments of that size, but then we risk jumping over glyphs that are smaller than 30px. We might think we’re not in shadow when we should be.</p>

<p><strong>Ray marching’s core idea is this: the distance field tells you how far you are from the closest glyph. You can safely advance along your ray by that distance without skipping over any glyphs.</strong></p>

<p>Let’s walk through an example. We start as pictured above and ask the distance field how far we are from any glyph. Turns out in this case that the answer is 95px (pictured left). This means that we can move 95px along our ray without skipping over anything!</p>

<img src="https://www.rykap.com/images/ray-marching/ray-march-2.png">

<p>Now we’re a little closer to the light. We repeat the process until we hit the ascender of the b! If the b glyph weren’t there, we’d have kept going until we hit the light.</p>

<p>Below is a demo that shows the ray marching steps for a given pixel. The red box is the pixel we’re shading, and each circle along the ray represents a ray marching step and the distance from the scene at that step.</p>

<p>Try dragging the light and the pixel around to build an intuition for it.</p>



<p>Below is GLSL to implement this technique. It assumes you’ve defined a function <code>getDistance</code> that samples the distance field.</p>

<div><div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>

<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>;</span>
<span>while</span> <span>(</span><span>true</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>distance</span><span>(</span><span>rayOrigin</span><span>,</span> <span>lightPosition</span><span>))</span> <span>{</span>
    <span>// We hit the light! This pixel is not in shadow.</span>
    <span>return</span> <span>1</span><span>.;</span>
  <span>}</span>

  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span>
    <span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>It turns out that some pixels are really expensive to process. So in practice we use a for-loop instead of a while loop – that way we bail out if we’ve done too many steps. A common “slow case” in ray marching is when a ray is parallel to the edge of a shape in the scene…</p>



<p>The approach I’ve described so far will get you a scene that looks like the one below.</p>



<p>It’s cool, but the shadows are sharp which doesn’t look very good. The shadows in the demo look more like this…</p>

<img src="https://www.rykap.com/images/ray-marching/desired-shadows.png">

<p>One big disclaimer is that they’re not physically realistic! Real shadows look like hard shadows where the edges have been fuzzed. This approach does something slightly different: all pixels that were previously in shadow are still fully in shadow. We’ve just added a penumbra of partially shaded pixels around them.</p>

<p>The upside is that they’re pretty and fast to compute, and that’s what I care about! There are three “rules” involved in computing them.</p>

<p><strong>Rule 1:</strong> The closer a ray gets to intersecting a shape, the more its pixel should be shadowed. In the image below there are two similar rays (their distances to the shape pictured in yellow and green). We want the one that gets closer to touching the corner to be more shadowed.</p>

<img src="https://www.rykap.com/images/ray-marching/rule-1.png">

<p>This is cheap to compute because the variable <code>sceneDist</code> tells us how far we are from the closest shape at each ray marching step. So the smallest value of <code>sceneDist</code> across all steps is a good approximation for the yellow and green lines in the image above.</p>

<p><strong>Rule 2:</strong> if the pixel we’re shading is far from the point where it almost intersects a shape, we want the shadow to spread out more.</p>

<img src="https://www.rykap.com/images/ray-marching/rule-2.png">

<p>Consider two pixels along the ray above. One is closer to the almost-intersection and is lighter (its distance is the green line). The other is farther and darker (its distance is the yellow line). In general: the further a pixel is from its almost intersection, the more “in shadow” we should make it.</p>

<p>This is cheap to compute because the variable <code>rayProgress</code> is the length of the green and yellow lines in the image above.</p>

<p>So: we previously returned <code>1.0</code> for pixels that weren’t in shadow. To implement rules 1 and 2, we compute <code>sceneDist / rayProgress</code> on each ray marching step, keep track of its minimum value, and return that instead.</p>

<div><div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>
<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>.;</span>
<span>float</span> <span>stopAt</span> <span>=</span> <span>distance</span><span>(</span><span>samplePt</span><span>,</span> <span>lightPosition</span><span>);</span>
<span>float</span> <span>lightContribution</span> <span>=</span> <span>1</span><span>.;</span>
<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>64</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>stopAt</span><span>)</span> <span>{</span>
    <span>return</span> <span>lightContribution</span><span>;</span>
  <span>}</span>

  <span>// `getDistance` samples our distance field texture.</span>
  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span>
    <span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>lightContribution</span> <span>=</span> <span>min</span><span>(</span>
    <span>lightContribution</span><span>,</span>
    <span>sceneDist</span> <span>/</span> <span>rayProgress</span>
  <span>);</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>

<span>// Ray-marching took more than 64 steps!</span>
<span>return</span> <span>0</span><span>.;</span>
</code></pre></div></div>

<p>This ratio feels kind of magical to me because it doesn’t correspond to any physical value. So let’s build some intuition for it by thinking through why it might take on particular values…</p>

<ul>
  <li>
    <p>If <code>sceneDist / rayProgress &gt;= 1</code>, then either <code>sceneDist</code> is big or <code>rayProgress</code> is small (relative to each other). In the former case we’re far from any shapes and we shouldn’t be in shadow, so a light value of <code>1</code> makes sense. In the latter case, the pixel we’re shadowing is really close to an object casting a shadow and the shadow isn’t fuzzy yet, so a light value of <code>1</code> makes sense.</p>
  </li>
  <li>
    <p>The ratio is <code>0</code> only when <code>sceneDist</code> is <code>0</code>. This corresponds to rays that intersect an object and whose pixels are in shadow.</p>
  </li>
</ul>

<p>And here’s a demo of what we have so far…</p>



<p><strong>Rule #3</strong> is the most straightforward one: light gets weaker the further you get from it.</p>

<p>Instead of returning the minimum value of <code>sceneDist / rayProgress</code> verbatim, we multiply it by a <code>distanceFactor</code> which is <code>1</code> right next to the light, <code>0</code> far away from it, and gets quadratically smaller as you move away from it.</p>

<p>All together, the code for the approach so far looks like this…</p>

<div><div><pre><code><span>vec2</span> <span>rayOrigin</span> <span>=</span> <span>...;</span>
<span>vec2</span> <span>rayDirection</span> <span>=</span> <span>...;</span>
<span>float</span> <span>rayProgress</span> <span>=</span> <span>0</span><span>.;</span>
<span>float</span> <span>stopAt</span> <span>=</span> <span>distance</span><span>(</span><span>samplePt</span><span>,</span> <span>lightPosition</span><span>);</span>
<span>float</span> <span>lightContribution</span> <span>=</span> <span>1</span><span>.;</span>
<span>for</span> <span>(</span><span>int</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>64</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>rayProgress</span> <span>&gt;</span> <span>stopAt</span><span>)</span> <span>{</span>
    <span>// We hit the light!</span>
    <span>float</span> <span>LIGHT_RADIUS_PX</span> <span>=</span> <span>800</span><span>.;</span>

    <span>// fadeRatio is 1.0 next to the light and 0. at</span>
    <span>// LIGHT_RADIUS_PX away.</span>
    <span>float</span> <span>fadeRatio</span> <span>=</span>
      <span>1</span><span>.</span><span>0</span> <span>-</span> <span>clamp</span><span>(</span><span>stopAt</span> <span>/</span> <span>LIGHT_RADIUS_PX</span><span>,</span> <span>0</span><span>.,</span> <span>1</span><span>.);</span>

    <span>// We'd like the light to fade off quadratically instead of</span>
    <span>// linearly.</span>
    <span>float</span> <span>distanceFactor</span> <span>=</span> <span>pow</span><span>(</span><span>fadeRatio</span><span>,</span> <span>2</span><span>.);</span>
    <span>return</span> <span>lightContribution</span> <span>*</span> <span>distanceFactor</span><span>;</span>
  <span>}</span>

  <span>// `getDistance` samples our distance field texture.</span>
  <span>float</span> <span>sceneDist</span> <span>=</span> <span>getDistance</span><span>(</span><span>rayOrigin</span> <span>+</span> <span>rayProgress</span> <span>*</span> <span>rayDirection</span><span>);</span>
  <span>if</span> <span>(</span><span>sceneDist</span> <span>&lt;=</span> <span>0</span><span>.)</span> <span>{</span>
    <span>// We hit a shape! This pixel is in shadow.</span>
    <span>return</span> <span>0</span><span>.;</span>
  <span>}</span>

  <span>lightContribution</span> <span>=</span> <span>min</span><span>(</span>
    <span>lightContribution</span><span>,</span>
    <span>sceneDist</span> <span>/</span> <span>rayProgress</span>
  <span>);</span>

  <span>rayProgress</span> <span>+=</span> <span>sceneDist</span><span>;</span>
<span>}</span>

<span>// Ray-marching took more than 64 steps!</span>
<span>return</span> <span>0</span><span>.;</span>
</code></pre></div></div>

<p>I forget where I found this soft-shadow technique, but I definitely didn’t invent it. Inigo Quilez <a href="https://www.iquilezles.org/www/articles/rmshadows/rmshadows.htm">has a great post on it</a> where he talks about using it in 3D.</p>

<p>Inigo’s post also talks about a gotcha with this approach that you might have noticed in the demos above: it causes banding artifacts. This is because Rule 1 assumes that the smallest value of <code>sceneDist</code> across all steps is a good approximation for the distance from a ray to the scene. This is not always true because we sometimes take very few ray marching steps.</p>

<p>So in my demo I use an improved approximation that Inigo writes about in his post. I also use another trick that is more effective but less performant: instead of advancing by <code>sceneDist</code> on each ray marching step, I advance by something like <code>sceneDist * randomJitter</code> where <code>randomJitter</code> is between <code>0</code> and <code>1</code>.</p>

<p>This improves the approximation because we’re adding more steps to our ray march. But we could do that by advancing by <code>sceneDist * .3</code>. The random jitter ensures that pixels next to each other don’t end up in the same band. This makes the result a little grainy which isn’t great. But I think looks better than banding… This is an aspect of the demo that I’m still not satisfied with, so if you have ideas for how to improve it please tell me!</p>

<p>Overall my demo has a few extra tweaks that I might write about in future but this is the core of it. Thanks for reading! If you have questions or comments, let me know <a href="https://twitter.com/ryanjkaplan">on Twitter</a>.</p>

<p><em>Thank you to Jessica Liu, Susan Wang, Matt Nichols and Kenrick Rilee for giving feedback on early drafts of this post! Also, if you enjoyed this post you might enjoy working with me at <a href="https://www.figma.com/careers/">Figma</a>!</em></p>

</article>






  
  
  






      </div>
    </div>
  </div></div>]]>
            </description>
            <link>https://www.rykap.com/2020/09/23/distance-fields/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24569542</guid>
            <pubDate>Wed, 23 Sep 2020 17:25:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Almost Realtime Live Data Visualization in QGIS – Air Traffic Use Case]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24565250">thread link</a>) | @geomatics99
<br/>
September 23, 2020 | https://www.geodose.com/2020/09/realtime%20live%20data%20visualization%20qgis.html | <a href="https://web.archive.org/web/*/https://www.geodose.com/2020/09/realtime%20live%20data%20visualization%20qgis.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-3280341045593062951" itemprop="articleBody">
<p>
  When working in a GIS software like QGIS, mostly we are working with static
  data like street, building, land cover, etc. Or might be a data which has time
  information so we can visualize the temporal change. What about visualize live
  data in almost real time? Do we need a GIS server, cloud or map service? I
  think this is an interesting topic, and I will discuss about it in this post
  with live air traffic data use case.
</p>
<p>
  In the previous post, I made a tutorial
  <a href="https://www.geodose.com/2020/08/create-flight-tracking-apps-using-python-open-data.html">how to build a flight tracking application with open air traffic data in
    Python</a>. The application is running in a web browser and the flight data will be
  updated in a specified time interval. In this tutorial we will do the same
  thing in QGIS. We will visualize the air traffic live data on QGIS map canvas
  and get the update data in every five or ten seconds. At the end of this
  tutorial we will get an almost realtime airplanes' location within an area as
  in figure 1 below.
</p>

<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-Uk3IIrnlhrg/X1ulZ9x2OlI/AAAAAAAACRA/E1JjjYxKqBYgOCpc2ConnZDGREkWEuNAwCNcBGAsYHQ/s997/live-data-sfo-airport3.gif"><img alt="Air Traffic Live Data in QGIS" data-original-height="602" data-original-width="997" height="386" src="https://1.bp.blogspot.com/-Uk3IIrnlhrg/X1ulZ9x2OlI/AAAAAAAACRA/E1JjjYxKqBYgOCpc2ConnZDGREkWEuNAwCNcBGAsYHQ/w640-h386/live-data-sfo-airport3.gif" title="Air Traffic Live Data in QGIS" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 1. Air Traffic Live Data in QGIS. Airplanes are queueing for
        landing at San Fransisco International Airport <br>
      </td>
    </tr>
  </tbody>
</table>

<p>
  Figure 2 is the schema that shows how the system works. Can be seen from the
  schema that Python plays role for requesting the data, get the response,
  process it and store the data into a CSV plain text. On the other side, QGIS
  will render the airplanes position based on the data and refresh it with in an
  interval to get the latest information. To make it happen, we don't need any
  GIS server, cloud or map service. Anyone could do this as long as there is
  internet connection available with Python and QGIS installed in a machine.<br>
</p>
<br>
<div>
  <table>
    <tbody>
      <tr>
        <td>
          <img alt="Schema how the system works (QGIS Live Data)" data-original-height="173" data-original-width="1053" height="106" src="https://1.bp.blogspot.com/-JaG5RhN6xJA/X19BMD_ygnI/AAAAAAAACRg/2u5J81mQquAFKSYqE6eBa_PMtrKRVuX_gCNcBGAsYHQ/w640-h106/schema%2Blive%2Bdata%2Bqgis.png" title="Schema how the system works (QGIS Live Data)" width="640">
        </td>
      </tr>
      <tr>
        <td>
          &nbsp;Figure2 . Schema how the system works<br>
        </td>
      </tr>
    </tbody>
  </table>
  
  
</div>






<p>
  Based on the schema, this tutorial consist of several sub-topics, such as:
  getting the data (send the request and process the response), Plotting the
  data on QGIS map canvas and render it within a time interval. Let's get
  started!<br>
</p>

<p><h3>Getting Air Traffic Live Data</h3></p>

<p>
  The data for this tutorial is coming from
  <a href="https://opensky-network.org/" rel="nofollow" target="_blank">OpenSky Network</a>
  which is an association that provides air traffic data around the globe. There
  are some APIs that can be used to retrieve data from OpenskyNetwork such as:
  Python API, Java API and REST API. In this tutorial we will use REST API to
  retrieve data within a specified boundary area.
</p>
<p>
  To retrieve air traffic data within an area we need to define minimum and
  maximum coordinate in geographic coordinate system. For example I want to
  fetch all planes over United States with minimum and maximum coordinate
  respectively -125.974,30.038 and -68.748,52.214. The REST API query to request
  the data anonymously will be as follow:
</p>
<p>
  <i>https://opensky-network.org/api/states/all?lamin=30.038&amp;lomin=-125.974&amp;
    <br>lamax=52.214&amp;lomax=-68.748</i><br>
</p>
<p>
  The anonymous request has resolution 10 seconds, it means we can send the
  request in every 10 seconds. On the other hand if you are a registered user,
  the resolution will be faster, about 5 seconds. To make a request as
  registered user, the username and password must be include in the query. Then
  the query will be:
</p>
<p>
  https://<span>username:password</span>@opensky-network.org/api/states/all?lamin=30.038lomin=-125.974&amp;
  lamax=52.214&amp;lomax=-68.748
</p>
<p>
  To try the query, simply copy it and paste into a browser. If you get a
  response like figure 3, means it works and we are ready to continue to the
  next step. Furthermore if you want to know in more detail about air traffic
  data response from OpenSky Network please visit
  <a href="https://opensky-network.org/apidoc/rest.html">REST API Documentation</a>.<br>
</p>

<table>
  <tbody>
    <tr>
      <td>
        <img alt="Live Air Traffic Data Response" data-original-height="452" data-original-width="793" height="364" src="https://1.bp.blogspot.com/-Y8dig7kzJZY/X2Dzni4ErgI/AAAAAAAACRs/AngxaoNxE94l0AU3j2rVDHTzH3I8FhFYgCNcBGAsYHQ/w640-h364/air-traffic-data-response.png" title="Live Air Traffic Data Response" width="640">
      </td>
    </tr>
    <tr>
      <td>
        Figure 3. Live Air Traffic Data Response<br>
      </td>
    </tr>
  </tbody>
</table>



<h3>
  Sending Request and Process The Live Data Response
</h3>
<p>
  In this step we will write Python code to request the live air traffic data
  and process the response. The complete code can be found at the end of this
  section.
</p>
<p>
  We are starting with importing some libraries namely: requests, json, csv and
  time. Then define the coordinate extent with minimum and maximum coordinate.
  Next at line 16 an output path where the response data will be stored is
  specified, so make sure to change with yours. If you are a registered OpenSky
  Network user, giver your username and also the password in
  <i>user_name</i> and <i>password</i> variable at line 19-20. The last part of
  the code is used to send the query using requests, get response in JSON format
  and save it into a CSV file. This process will be done in a loop within
  interval 10 seconds for anonymous request or 5 seconds for a registered user.
  &nbsp; <br>
</p>

<div>
  <table>
    <tbody>
      <tr>
        <td>
          <pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47</pre>
        </td>
        <td>
          <pre><span>'''</span>
<span>LIVE AIR DATA TRAFFIC REQUEST</span>
<span>by ideagora geomatics | www.geodose.com | @ideageo</span>
<span>'''</span>
<span>#IMPORTING LIBRARIES</span>
<span>import</span> <span>requests</span>
<span>import</span> <span>json</span>
<span>import</span> <span>csv</span>
<span>import</span> <span>time</span>

<span>#AREA EXTENT COORDINATE GCS WGS84</span>
<span>lon_min,lat_min</span><span>=-</span><span>125.974</span><span>,</span><span>30.038</span>
<span>lon_max,lat_max</span><span>=-</span><span>68.748</span><span>,</span><span>52.214</span>

<span>#CSV OUPUT PATH</span>
<span>csv_data</span><span>=</span><span>'/home/data.csv'</span>

<span>#REST API QUERY</span>
<span>user_name</span><span>=</span><span>''</span>
<span>password</span><span>=</span><span>''</span>
<span>url_data</span><span>=</span><span>'https://'</span><span>+</span><span>user_name</span><span>+</span><span>':'</span><span>+</span><span>password</span><span>+</span><span>'@opensky-network.org/api/states/all?'</span><span>+</span><span>'lamin='</span><span>+</span><span>str(lat_min)</span><span>+</span><span>'&amp;lomin='</span><span>+</span><span>str(lon_min)</span><span>+</span><span>'&amp;lamax='</span><span>+</span><span>str(lat_max)</span><span>+</span><span>'&amp;lomax='</span><span>+</span><span>str(lon_max)</span>
<span>col_name</span><span>=</span><span>[</span><span>'icao24'</span><span>,</span><span>'callsign'</span><span>,</span><span>'origin_country'</span><span>,</span><span>'time_position'</span><span>,</span><span>'last_contact'</span><span>,</span><span>'long'</span><span>,</span><span>'lat'</span><span>,</span><span>'baro_altitude'</span><span>,</span><span>'on_ground'</span><span>,</span><span>'velocity'</span><span>,</span>       
<span>'true_track'</span><span>,</span><span>'vertical_rate'</span><span>,</span><span>'sensors'</span><span>,</span><span>'geo_altitude'</span><span>,</span><span>'squawk'</span><span>,</span><span>'spi'</span><span>,</span><span>'position_source'</span><span>]</span>

<span>#REQUEST INTERVAL</span>
<span>if</span> <span>user_name</span> <span>!=</span><span>''</span> <span>and</span> <span>password</span> <span>!=</span><span>''</span><span>:</span>
    <span>sleep_time</span><span>=</span><span>5</span>
<span>else</span><span>:</span>
    <span>sleep_time</span><span>=</span><span>10</span>

<span>#GET DATA AND STORE INTO CSV</span>
<span>while</span> <span>col_name</span> <span>!=</span><span>''</span><span>:</span>
    <span>with</span> <span>open(csv_data,</span><span>'w'</span><span>)</span> <span>as</span> <span>csv_file:</span>
        <span>csv_writer</span><span>=</span><span>csv</span><span>.</span><span>writer(csv_file,delimiter</span><span>=</span><span>','</span><span>,quotechar</span><span>=</span><span>'"'</span><span>,quoting</span><span>=</span><span>csv</span><span>.</span><span>QUOTE_ALL)</span>
        <span>csv_writer</span><span>.</span><span>writerow(col_name)</span>
        <span>response</span><span>=</span><span>requests</span><span>.</span><span>get(url_data)</span><span>.</span><span>json()</span>
        
        <span>try</span><span>:</span>
            <span>n_response</span><span>=</span><span>len(response[</span><span>'states'</span><span>])</span>
        <span>except</span> <span>Exception</span><span>:</span>
            <span>pass</span>
        <span>else</span><span>:</span>
            <span>for</span> <span>i</span> <span>in</span> <span>range(n_response):</span>
                <span>info</span><span>=</span><span>response[</span><span>'states'</span><span>][i]</span>
                <span>csv_writer</span><span>.</span><span>writerow(info)</span>
    <span>time</span><span>.</span><span>sleep(sleep_time)</span>
    <span>print(</span><span>'Get'</span><span>,len(response[</span><span>'states'</span><span>]),</span><span>'data'</span><span>)</span>
</pre>
        </td>
      </tr>
    </tbody>
  </table>
</div>

<p>
  Save the code with Python extension (.py) and run it from a command prompt or
  terminal. Type <i>python</i> or <i>python3</i> if you use python 3 followed by
  the file name. The code will be running as in figure 4 below.&nbsp;
</p>
<p>
  Don't close the terminal, because it will work continuously to get the latest
  air traffic data from OpenSky Network, and we will use it in QGIS. <br>
</p>
<table>
  <tbody>
    <tr>
      <td>
        <img alt="Flight Data Request" data-original-height="309" data-original-width="480" height="258" src="https://1.bp.blogspot.com/-RAkHV_x_kTE/X2Fl4gX-HsI/AAAAAAAACSA/IAMcBX7xGTMkROtvJ6GQp_FoQn2yRHMqQCNcBGAsYHQ/w400-h258/flight-request-code-running.png" title="Flight Data Request" width="400">
      </td>
    </tr>
    <tr>
      <td>
        Figure 4. Flight Data Request<br>
      </td>
    </tr>
  </tbody>
</table>

<h3>Visualize Live Air Traffic Data in QGIS</h3>
<p>
  We already get the live data streaming, now let's visualize it in QGIS with
  the following steps.
</p>
<p>
  Add the CSV data into QGIS. From <i>Data Source Manager</i>, select
  <i>Delimited Text</i> in the left menu. Then in the right side, select&nbsp;
  the <i>File Name. </i>In <i>Geometry Definition</i> section select
  <i>long</i> column for <i>X field</i> and <i>lat </i>column for
  <i>Y field</i>. Make sure to get the <i>Sample Data</i> correctly. If not try
  to change the delimiter properties in <i>File Format</i> section with
  <i>Custom delimiters</i> option.&nbsp; &nbsp; &nbsp;
</p>

<table>
  <tbody>
    <tr>
      <td>
        <img alt="Add flight data QGIS" data-original-height="601" data-original-width="856" height="450" src="https://1.bp.blogspot.com/-nTLyM8xeWeE/X2JDVOIugiI/AAAAAAAACSY/tUnwZ1Z0Gxg2-FMWbU_Db3QcWIAMH0bowCNcBGAsYHQ/w640-h450/qgis-add-flight-data.png" title="Add flight data QGIS" width="640">
      </td>
    </tr>
    <tr>
      <td>
        Figure 5. Add Air Traffic Data to QGIS<br>
      </td>
    </tr>
  </tbody>
</table>
<p>
  After pushing the <i>Add</i> button in the <i>Data Source Manager</i>, the air
  plane's position within the requested area will be plotted on QGIS map canvas.
  To make it more meaningful in a geospatial extent, add a basemap. To add a
  basemap I used
  <a href="https://www.geodose.com/2018/11/qgis3-basemap-plugin-tile-plus.html">Tile+</a>
  plugin which provides some popular basemaps. For this case I used STAMEN
  TERRAIN basemap.&nbsp; Figure 6 shows all aircraft's position over the US with
  STAMEN TERRAIN basemap.<br>
</p>
<table>
  <tbody>
    <tr>
      <td>
        <img alt="Aircraft Position in QGIS Map Canvas" data-original-height="599" data-original-width="1091" height="352" src="https://1.bp.blogspot.com/-6FDJtf32igE/X2JFxFj06TI/AAAAAAAACSs/2VvbdqRN8WA5YRMYS7hv0RueuEWFBGviACNcBGAsYHQ/w640-h352/flight-data-plot-qgis-basemap.png" title="Aircraft Position in QGIS Map Canvas" width="640">
      </td>
    </tr>
    <tr>
      <td>
        Figure 6. Aircraft Position in QGIS Map Canvas<br>
      </td>
    </tr>
  </tbody>
</table>
<p>
  So far we already get airplane position in a static way. The position of
  airplanes will not change because it doesn't fetch any updated data. Therefore
  in this last step, we will make it dynamic. The position of aircraft will be
  updated every 5 or 10 seconds. Then we will change the dot marker with
  airplane icon and also rotate it respectively with the track direction.
</p>
<p>
  Firstly let's change the dot marker into airplane icon. Right click on data
  layer and then select <i>Properties</i>. On the left menu select
  <i>Symbology</i> and chose <i>topo airport</i> icon as in figure 7.<br>
</p>

<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-LKKiEhhdoSQ/X2JKDHl_NkI/AAAAAAAACS4/pjayRfO1zGo4kYT062mVtN7zqkaFxiA0gCNcBGAsYHQ/s823/change-symbology-qgis.png"><img alt="Change Symbology" data-original-height="566" data-original-width="823" height="440" src="https://1.bp.blogspot.com/-LKKiEhhdoSQ/X2JKDHl_NkI/AAAAAAAACS4/pjayRfO1zGo4kYT062mVtN7zqkaFxiA0gCNcBGAsYHQ/w640-h440/change-symbology-qgis.png" title="Change Symbology" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 7. Change Symbology<br>
      </td>
    </tr>
  </tbody>
</table>
<p>To set rotation angle of the marker, select the menu at the right of
<i>Rotation</i> parameter then&nbsp; select <i>Field type:....</i> and then
select <i>true_track</i> column (see figure 8).&nbsp; <br>

<ins data-ad-client="ca-pub-5632482621101280" data-ad-format="fluid" data-ad-layout="in-article" data-ad-slot="7655392618"></ins></p>
<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-w2UafXoJYDg/X2JLHf8QBuI/AAAAAAAACTA/z3V9zCZTI6s9hb4iwPLifcEcTp-UXvBuwCNcBGAsYHQ/s513/select-rotation-angle.png"><img alt="Set Rotation Angle" data-original-height="294" data-original-width="513" height="229" src="https://1.bp.blogspot.com/-w2UafXoJYDg/X2JLHf8QBuI/AAAAAAAACTA/z3V9zCZTI6s9hb4iwPLifcEcTp-UXvBuwCNcBGAsYHQ/w400-h229/select-rotation-angle.png" title="Set Rotation Angle" width="400"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 8. Set Rotation Angle<br>
      </td>
    </tr>
  </tbody>
</table>
&nbsp;

<p>
  Before proceeding to the next step, click <i>Apply </i>or <i>OK </i>button.
  You should see the airplane marker and&nbsp; it rotates in flight direction
  angle as shown in figure 9.
</p>
<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-T9sCVld99NY/X2JMuwFYwWI/AAAAAAAACTM/RAB0B78Cihw7oel_TPmZmigrsxAOoAAuACNcBGAsYHQ/s1089/airplanes-marker-rotate.png"><img alt="Airplane marker with it's rotation angle" data-original-height="600" data-original-width="1089" height="352" src="https://1.bp.blogspot.com/-T9sCVld99NY/X2JMuwFYwWI/AAAAAAAACTM/RAB0B78Cihw7oel_TPmZmigrsxAOoAAuACNcBGAsYHQ/w640-h352/airplanes-marker-rotate.png" title="Airplane marker with it's rotation angle" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 9. Airplane marker with it's rotation angle<br>
      </td>
    </tr>
  </tbody>
</table>

<p>
  Finally let's update the data within a time interval. Select data layer and
  choose Properties again. On the left menu select <i>Rendering</i>. In the
  right window check <i>Refresh layer at interval (seconds)</i> option, and set
  it to 5 or 10 as in figure 10. It means the layer will refreshed every 5 or 10
  seconds. If there is any data change after refreshing is taking place, the
  position of airplanes will be updated and we get an almost realtime air
  traffic live data that visualized in QGIS as seen before in figure 1.<br>
</p>
<table>
  <tbody>
    <tr>
      <td>
        <a href="https://1.bp.blogspot.com/-xXMt1izln4g/X2JN5Hnp72I/AAAAAAAACTY/UKczXaiKqGovSKw8gQrWjM9uuEEUW-ekQCNcBGAsYHQ/s824/refreshed-layer.png"><img alt="Set refresh layer interval time" data-original-height="616" data-original-width="824" height="478" src="https://1.bp.blogspot.com/-xXMt1izln4g/X2JN5Hnp72I/AAAAAAAACTY/UKczXaiKqGovSKw8gQrWjM9uuEEUW-ekQCNcBGAsYHQ/w640-h478/refreshed-layer.png" title="Set refresh layer interval time" width="640"></a>
      </td>
    </tr>
    <tr>
      <td>
        Figure 10. Set refresh layer interval time<br>
      </td>
    </tr>
  </tbody>
</table>

<p>
  That's all this tutorial how to visualize an almost realtime live data in
  QGIS. I think&nbsp; this approach can be applied for other cases like
  visualize data from a sensor that taking measurement in a field like water level, temperature, humidity, and many more. Hope this post could
  inspire you and thanks for reading!<br>
</p>

<!--large_rectangle_336x280-->

<p><i></i>
<a href="https://www.geodose.com/search/label/Live%20Data?&amp;max-results=8" rel="tag" title="Live Data">Live Data</a>
<a href="https://www.geodose.com/search/label/QGIS?&amp;max-results=8" rel="tag" title="QGIS">QGIS</a>
<a href="https://www.geodose.com/search/label/Tutorial?&amp;max-results=8" rel="tag" title="Tutorial">Tutorial</a></p>

</div></div>]]>
            </description>
            <link>https://www.geodose.com/2020/09/realtime%20live%20data%20visualization%20qgis.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24565250</guid>
            <pubDate>Wed, 23 Sep 2020 10:23:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Is a Minimally Good Life?]]>
            </title>
            <description>
<![CDATA[
Score 92 | Comments 140 (<a href="https://news.ycombinator.com/item?id=24565154">thread link</a>) | @bertdc
<br/>
September 23, 2020 | https://psyche.co/ideas/what-is-a-minimally-good-life-and-are-you-prepared-to-live-it | <a href="https://web.archive.org/web/*/https://psyche.co/ideas/what-is-a-minimally-good-life-and-are-you-prepared-to-live-it">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><strong>As a basic minimum</strong>, what do members of a society owe to one another? How we answer this question determines what safety nets societies provide for their members, and so helps to shape the structure of the society at large. It is crucial, then, that we formulate a method in which to figure out what, at a minimum, we owe to others. The way to do that is simple: we should consider whether <em>we</em> would be content to live the lives that the least fortunate in our society actually live. We should put ourselves into each otherâ€™s shoes â€“ and then consider what each person needs to live well<em>.</em></p>
<p>People need many of the same things, by virtue of being human. Everyone must be able to meet their basic needs for things such as food, water and shelter. But thatâ€™s not all. To live at least minimally well, the good things in each personâ€™s life (such as relationships, pleasures, knowledge, appreciation, worthwhile activities) must compensate for their difficulties, pains, losses and frustrations. Everyone also needs decent opportunities and the capabilities to realise them. Or, at least, each person should get as close as possible to meeting this standard.</p>
<p>However, the differences between people also matter immensely, and our differences as people explain why it is not enough if everyone has exactly the same things. Pregnant women, for instance, need more food than those who arenâ€™t pregnant. Those who canâ€™t walk might need help getting around. And, in some cases, we have to consider cultural differences just to ensure that everyone can eat.</p>
<p>We should ask whether <em>we</em> would really be content to live each other personâ€™s life in our society. To be clear, this is different from asking each person directly what they need. The danger here is that people can be mistaken about their needs. Some get so used to poor conditions that they no longer strive to improve them. Others are so poorly off that they simply donâ€™t understand that their conditions are poor in the first place. The thought is that having some distance from each personâ€™s experience will help us see whether that person really needs all the things they think they need. We might likewise consider whether the person needs resources, opportunities, capabilities and so forth that they think they donâ€™t need, but in fact do.</p>
<p>Of course, not everyone will agree on what we all need in order to live a minimally good life. But it is my contention that free, reasonable and caring people <em>should</em>. To see why, it is important to understand what it actually is that makes people reasonable, caring and free. People are reasonable when they are appropriately impartial; they donâ€™t privilege the greater needs of some over others. People are caring when they empathise with others: understanding their circumstances, their history, their perspectives. Caring people want to promote othersâ€™ interests in proportion to their weight. And people are free when they can reason about, make and carry out plans for themselves. Free people also have decent options and bargaining power.</p>
<p>I believe that no one really deserves to be born with what they have â€“ their natural resources, institutions or tools</p>
<p><strong>Now, consider why</strong> reasonable, caring, free people â€“ who have all the relevant information â€“ will agree that everyone should have adequate resources, opportunities, capabilities and so forth to live a minimally good life. If weâ€™re appropriately impartial, weâ€™ll set for others only that standard under which weâ€™re content to live as others do. If weâ€™re caring, weâ€™ll set a standard that we believe is sufficient for others with their particular interests. If weâ€™re free and caring, and have all the relevant information, we wonâ€™t make a mistake about whether the standard is sufficient for others with those interests.</p>
<p>There is a sense in which even some of the most impoverished, oppressed and disadvantaged people can live excellent, never mind minimally good, lives. As the philosopher Dan Haybron suggests, it is often reasonable to affirm lives, even when they lack many of the things that people can justifiably aspire to as a matter of basic rights<em>.</em> Still, I am interested here in the latter sense of what makes lives minimally good â€“ I am concerned with what people can justifiably aspire to as a matter of basic right.</p>
<p>My proposal is this: in order to figure out what this kind of minimally good life requires, we should attempt to avail ourselves of anotherâ€™s perspective on their own life, and consider what weâ€™d need to live such a life. And when we are reasonable, caring and free, weâ€™ll set a standard that is sufficient for others given their particular interests. Moreover, if we put ourselves in othersâ€™ shoes in trying to figure out what a minimally good life requires, we wonâ€™t set the threshold too high. The question is not whether a fortunate individual would be willing to trade places with someone who is able to live only a minimally good life. Rather, the question is only whether the free, reasonable and caring person would be content if they had to live as that person does.</p>
<p>Since people have different backgrounds, goals, tools and resources, one might argue that different standards are appropriate for those who grow up in different circumstances (whether it be the cornfields of Nebraska or the slums of New York City). Furthermore, itâ€™s commonly held that people deserve the advantages they have: since everyone has grown up in the â€˜real worldâ€™, they should know what to expect for their efforts. I believe that no one really deserves to be born with what they have â€“ their natural resources, institutions or tools. Everyone will try hard enough to live minimally well if they can. So, while some might need more than others, we should help everyone live at least minimally well. This doesnâ€™t mean we have to give everyone exactly the same things â€“ still, if we are, or consider ourselves to be, reasonable, caring and free, then we must help everyone secure the things they need to live minimally well.</p></div></div></div>]]>
            </description>
            <link>https://psyche.co/ideas/what-is-a-minimally-good-life-and-are-you-prepared-to-live-it</link>
            <guid isPermaLink="false">hacker-news-small-sites-24565154</guid>
            <pubDate>Wed, 23 Sep 2020 10:08:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Haskell's Children]]>
            </title>
            <description>
<![CDATA[
Score 170 | Comments 180 (<a href="https://news.ycombinator.com/item?id=24565019">thread link</a>) | @xiaodai
<br/>
September 23, 2020 | https://owenlynch.org/posts/2020-09-16-haskells-children/ | <a href="https://web.archive.org/web/*/https://owenlynch.org/posts/2020-09-16-haskells-children/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
          
          <p>
    Posted on September 16, 2020
    
</p>

<p>If I were to travel back in time 4 years ago, and tell my old self that Haskell was starting to lose its shine, I wouldn’t believe it. I grew up on Haskell, my appetite for category theory was whetted by Haskell, my biggest programming projects have been in Haskell, and my dream job was to work at a company that used Haskell.</p>
<p>But now, I find myself simply not as excited about Haskell as I used to be. What changed?</p>
<p>I think there are a couple things. I think one primary factor is that the kind of programming that Haskell really excells in; i.e.&nbsp;creating abstract, correct interfaces for things, is just not a type of programming that’s interesting to me anymore. When I wanted to work on software as a career, a language that allowed incredible facilities in not repeating yourself was very useful. Types that ensure correctness of data interchange, or lenses that allow access to complicated data structures are all very well for implementing, say, a compiler, or complicated business logic in a web backend. However, my interests in software are now primarily as a scientific/mathematical tool. Numerical algorithms can be done in Haskell, but they don’t really gain much benefit from the type system, and they also don’t have great library support.</p>
<p>No doubt Haskell could be made into the kind of language to use for the problems that I am interested in, but given the choice between working on the problems that interest me, and working on infrastructure for the problems that interest me, I would rather work on the problems that interest me. The general feeling that I have is that Haskell is a great tool for a software engineer, but I don’t want to be a software engineer, I want to be a mathematician that sometimes uses computers.</p>
<p>But there is another reason too. While I think that Haskell is still a great language, it is close to 30 years old at this point. It manages to stay fresh and relevant with an ever-growing list of extensions, and constantly changing best practices and libraries (which is itself a problem…), but it would be very sad if we as a society of programmers had failed to surpass it in any respect with any of the programming languages that have had the advantage of starting from a clean slate. In this post, I want to talk about these successor languages, and what I think about them.</p>
<h2 id="rust">Rust</h2>
<p>Surprisingly, one of Haskell’s great strengths is as a systems language. It manages to be much faster than most dynamic languages, while allowing a much higher-level interface than traditional systems languages like C (obviously). One great example of a “systems” program written in Haskell is git-annex. It is a git addition that adds tracking of large files, and was my primary backup system for a long time (I eventually decided that I didn’t need the additional power from it, and was better served by a more seamless solution).</p>
<p>However, in 2020, the premier system’s language is surely Rust. It would be unfair to compare the performance of Rust and Haskell, because Haskell is optimized for things other than performance. That being said, Rust is <em>faster</em> and <em>lower-latency</em> than Haskell, both of which are important. However, it also has a great type system, unlike C or C++ (we don’t talk about Go…). The type system in Rust is obviously very influenced by the type system of Haskell, but they also implemented “ownership” which allows for the killer feature garbage-collection free automatic memory management.</p>
<p>When I first started using Rust, I really missed monads. But here’s the thing. Having used lots of monads in Haskell, and read lots of blog posts about monads, I’ve learned that in systems contexts, it’s often best to just have a simple monad stack that just consists of Reader + IO (and Maybe’s and Option’s sprinkled about occasionally). Huge monad transformer stacks often raise more problems than they solve. But Reader + IO is <em>essentially</em> the “default monad stack” of Rust.</p>
<p>Rust also has some other killer features, like the ability to compile to webassembly (yes there is ghcjs, but, really, do you want to use ghcjs?) It also from the beginning was targetted towards industry, and consequentially has a much more vibrant ecosystem.</p>
<p>This all being said, I think it is worth looking at the features that are prominent in Haskell that ended up going to Rust</p>
<ul>
<li>Typeclasses (in Rust they are Traits)</li>
<li>Sum types (you may take this for granted, but a lot of languages don’t have them….)</li>
<li>Pervasive pattern matching</li>
<li>Hindley-Mindler type inference (automatic type inference for variables)</li>
<li>Pervasiveness of things being <em>expressions</em> rather than statements</li>
<li>Parametric Polymorphism</li>
<li>The feeling that once your program compiles, it will run</li>
</ul>
<p>I think that we should recognize Rust for what it is, a child of Haskell and the Haskell community, and like all good parents, we should want it to do better than the previous generation. In as much as Haskell is the ideas that form Haskell, the success of Rust is the success of Haskell.</p>
<h2 id="idris">Idris</h2>
<p>OK, mainstream programming languages are great, but sometimes you just want to make the perfect type-based interface to your stuff and show the imperative scrubs what a wiz-kid you are. Or alternatively, sometimes you really care that your software is correct. Or you want to concretize a new category-theory inspired design for a part of a compiler. Nowadays, the language for that is not Haskell, it is Idris.</p>
<p>There are about six different ways to sort of have dependent types in Haskell (types that depend on values, like a length-<span>n</span>) array. I don’t really fully understand any of them, and it is totally unclear to me how they work together. Presumably, there are blogs which outline the One True Way, but… it’s tough. In Idris, it just seems perfectly natural to use dependent types, like, why wouldn’t you able to have a type parameter which was a value? In many ways other than dependent types, Idris is a much cleaner language than Haskell too. And with Idris2, it has support for <em>linear types</em>, which allow mutability in a functional context via guarantees that nobody is going to try and use the old value. If I want to play around with a cool type system in a language that can also actually do things with the real world (i.e., unlike Agda or Coq), I would go to Idris rather than Haskell.</p>
<p>But Idris is undeniably Haskell’s child. The first version was written in Haskell (it is now self-hosting). They are similar in more ways than it is worth counting. Enough said.</p>
<h2 id="julia">Julia</h2>
<p>Unlike the first two, Julia doesn’t really muscle into Haskell’s territory. Scientific computing was never really Haskell’s forte, despite there being some very cool libraries written in it, like <code>ad</code> for autodifferentiation, or various array-handling packages that automatically fused consecutive array operations.</p>
<p>Also, Julia is a dynamically typed language. How could a filthy dynamically typed language ever claim to be Haskell’s child??</p>
<p>Well, for one it steals some of those cool libraries, and makes them much better! Flux is a neural networks library which essentially is just autodifferentiation + some nice utilities, and it is already competitive in my mind with TensorFlow. Julia also has StaticArrays, which integrates the size of the array into the type, and Julia has some neat fusion abilities too for making array operations really fast.</p>
<p>But wait, you ask, how can it do this if it’s not a statically typed language? Well, Julia is not your average dynamically typed language. It actually has a very interesting type system, a full discussion of which is beyond the scope of this post, and the focus on types as the unit of programming is (somewhat?) similar to Haskell (now I’m stretching it a little though).</p>
<p>The real reason I include Julia, however, is because for me personally, it has replaced Haskell as the place to do category theory. This is because of a shift of viewpoint: rather than providing a type system into which category theory can be embedded in to guide typical software engineering tasks, Julia provides a system in which <em>computations</em> in category theory can be carried out in an efficient way. Specifically, I’m talking about <a href="https://github.com/olynch/Catlab.jl">Catlab.jl</a>. A discussion of Catlab.jl is also beyond the scope of this post, but I encourage you to check it out.</p>
<p>Therefore, I count Julia as a child of Haskell (or maybe, I count Catlab.jl as a child of Haskell) because the idea of organizing computation with category theory would not exist in the same way if it weren’t for Haskell.</p>
<h2 id="conclusion">Conclusion</h2>
<p>If I could talk to the Haskell-obsessed teenager that was me four years ago, I would tell him to keep his mind open. Haskell is still great for a lot of things (compilers come to mind), but if Haskell couldn’t inspire superior successors, there wouldn’t be worthwhile ideas in Haskell. There are those on the internet who are talking about how Haskell is dying, and they may or may not be wrong. Stephen Diehl, one of my main Haskell idols, is distancing himself from the Haskell community because of Haskell’s use as intellectual eye-candy on scam cryptocurrencies, and I think that there may be a tipping point where Haskell loses the zeitgeist of being exciting, and because it never had much of a foothold to begin with in industry, slips into irrelevance. But Haskell will always live on; it had a huge impact on many programmers and many languages disproportionate to its actual use, and it will always have a special place in my heart.</p>





        </div>
      </div></div>]]>
            </description>
            <link>https://owenlynch.org/posts/2020-09-16-haskells-children/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24565019</guid>
            <pubDate>Wed, 23 Sep 2020 09:47:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Studio Ghibli releases 400 free-to-use images]]>
            </title>
            <description>
<![CDATA[
Score 627 | Comments 119 (<a href="https://news.ycombinator.com/item?id=24564775">thread link</a>) | @DyslexicAtheist
<br/>
September 23, 2020 | http://www.ghibli.jp/info/013344/ | <a href="https://web.archive.org/web/*/http://www.ghibli.jp/info/013344/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>今月からスタジオジブリ全作品の場面写真を順次提供することになりました。今月は、新しい作品を中心に 8作品、合計400枚提供します。</p>
<p>常識の範囲でご自由にお使いください。</p>
</div></div>]]>
            </description>
            <link>http://www.ghibli.jp/info/013344/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24564775</guid>
            <pubDate>Wed, 23 Sep 2020 09:12:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Frugality Is Non-Linear (2019)]]>
            </title>
            <description>
<![CDATA[
Score 105 | Comments 156 (<a href="https://news.ycombinator.com/item?id=24564669">thread link</a>) | @luu
<br/>
September 23, 2020 | https://scattered-thoughts.net/writing/frugality-is-non-linear/ | <a href="https://web.archive.org/web/*/https://scattered-thoughts.net/writing/frugality-is-non-linear/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>Most people have a mental model of budgeting which is roughly linear. If you spend half as much money, your money will last twice as long. As you approach zero spending, your runway goes up to infinity.</p>
<p>In this model, the space of options looks like this:</p>
<div>
<p>[I am an interactive graph made of javascript!]</p>
</div>
<p><label for="growth">Return on investment =&nbsp;</label>
  
</p>
<p>This model is wrong.</p>
<p>It's wrong because your savings grow over time. If you change the return rate above to 5%, you can see that someone who has 500k in savings and spends 75k per year has a runway of 7 years. At 50k per year that extends to 13 years. But if they can cut their spending to 25k per year they have a runway of 62 years!</p>
<p>Effectively, including growth in the model moves the asymptote to the right - your runway goes up to infinity as your spending approaches some percentage of your total savings, rather than as it approaches zero.</p>
<p>So halving your expenses can much more than double your runway. Or to put it another way - halving your expenses can much more than halve the number of years of your life you need to spend working.</p>
<hr>
<p>I picked the examples above with a particular motive in mind. According to <a href="https://danluu.com/startup-tradeoffs/">Dan Luu's conservative estimates</a> a fresh grad at a big tech company can safely earn ~$500k post-tax in 5 years. The US median income post-tax is ~$25k, and investing in an index fund has historically earned ~5% average returns in the long run. So as a tech worker, if you can manage to live as 'frugally' as the average American, you can <a href="https://networthify.com/calculator/earlyretirement?income=120000&amp;initialBalance=0&amp;expenses=25000&amp;annualPct=5&amp;withdrawalRate=4">comfortably retire</a> before 30.</p>
<p>In the tech industry we have some very loud voices arguing that if you desire autonomy or leverage, the best path forwards is to start a VC-backed startup. But reducing spending and saving towards early retirement has some compelling advantages:</p>
<ul>
<li>It's much more reliable - most startups fail, but most people who work at a large tech company make sufficient money to be able to retire early.</li>
<li>Financial independence is a huge safety net - reducing stress and lowering the risk of later projects. If you still want to run a startup, doing it from a position of infinite personal runway will be a lot less stressful.</li>
<li>By separating the means of earning money from the freedom you are pursuing, it enables pursuing goals in that under-served intersection of valuable but not profitable. Whether that's supporting free software, producing art or home-schooling your children, trying to fit such activities into a profitable enterprise inevitably produces uncomfortable compromises which can be avoided by removing the need to earn money.</li>
</ul>
<p>The last point is particularly compelling if you have strong ethical/political/economic beliefs that would benefit from the leverage of financial independence.</p>
<hr>
<h3 id="faq">FAQ</h3>
<p><strong>What about inflation?</strong> Inflation is essentially negative growth, so you can subtract it from the return rate and then keep the rest of the calculations in today-dollars. 5% seems to be a reasonable estimate of average inflation-adjusted returns on stocks based on recent decades, but see below for better models.</p>
<p><strong>What about volatility?</strong> I used a fixed average return rate above, which doesn't tell you odds of running out of money early due to a string of bad years. <a href="https://retirementplans.vanguard.com/VGApp/pe/pubeducation/calculators/RetirementNestEggCalc.jsf">But simulations based on historical data</a> produce similar results to those above, and <a href="https://www.kitces.com/wp-content/uploads/2014/11/Kitces-Report-March-2012-20-Years-Of-Safe-Withdrawal-Rate-Research.pdf">retirement planning literature</a> tends to put the asymptote at around 4-5% which is consistent with the numbers above. You should definitely use a more detailed model than this if you are seriously considering this path, but I think the simple model accurately conveys the underlying intuition - that the returns to reducing spending are non-linear.</p>
<p><strong>What about crashes?</strong> The simulation linked above uses data that covers existing crashes, including the Great Depression. But in the event that they are overly optimistic, I think there is a strong argument that having large savings and cheap habits are as useful for weathering a crash as having a filled-in employment history. Especially if you used the additional free time to build useful non-tech skills or strong communities.</p>
<p><strong>What about other countries?</strong> Dan Luu's article suggests that similar salaries are available in many major hubs. I've built a reasonably detailed model for my own situation in the UK and arrived at similar numbers. (Salaries are lower, unless you can land a remote job, but free healthcare and lower cost of living make up a lot of the difference.) It's worth at least running the numbers for your own country, just so you know what your options are.</p>
<p><strong>Hasn't the <a href="https://en.wikipedia.org/wiki/FIRE_movement">FIRE community</a> already said all of this?</strong> Yes, but I very rarely see it discussed in tech circles, so it seems worth repeating. Also I haven't seen the calculation in terms of runway before, and the graph above improved my intuition on the subject.</p>



</article></div>]]>
            </description>
            <link>https://scattered-thoughts.net/writing/frugality-is-non-linear/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24564669</guid>
            <pubDate>Wed, 23 Sep 2020 08:57:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Drive on Sand Without Getting Stuck (2017)]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 19 (<a href="https://news.ycombinator.com/item?id=24564542">thread link</a>) | @luu
<br/>
September 23, 2020 | https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach | <a href="https://web.archive.org/web/*/https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>Do you have the pleasure to live or travel in a sandy location? Tropical beach, rolling sand hills or maybe it’s the desert for work. No matter where your sandy destination is be alert of the challenges of driving in sand. Too many people join the ‘digging club’ and it’s one club you’d rather avoid.</p><p>Please note this is general Sand not Desert Driving, although many of the basic principles are the same, Desert Driving is a “long” discussion in itself.</p><p><img src="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/58e720ad6dba09dcc4206c19856bec9f/SandOffRoading_640.JPG" width="640" height="532" srcset="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/58e720ad6dba09dcc4206c19856bec9f/SandOffRoading_360.JPG 360w, https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/58e720ad6dba09dcc4206c19856bec9f/SandOffRoading_640.JPG 640w" sizes="(min-width: 768px) 720px, 100vw" alt="How to Drive Through Sand without Getting Stuck"></p><p><em>This content was developed by and owned by Paul Sinkinson, Xplorability owner. Paul is a 4wd Defensive Driver Training Consultant/Trainer and Programme Developer.</em></p><h2>Basic Driving in Sand Tips</h2><ul><li>A variety of conditions may co-exist - Learn to recognise surface conditions</li><li>Beware of “wet sand”. These areas can seem bottomless and usually require assistance from other vehicles with a winch to extricate</li><li>If unsure carry out reconnaissance on foot</li><li>Engage high ratio 4wd for long beach runs on hard sand. Engage low ratio 4wd for soft sand and dunes to avoid overheating transmission</li><li>Avoid sharp turns and wheel spin</li><li>Utilise speed for controlled momentum</li><li>Keep gear changes to a minimum – Normally choose a gear and stay in it to avoid baulking</li><li>Low tyre pressures are highly recommended</li><li>If it becomes necessary to stop on soft sand, try to choose an area that allows a down hill restart</li><li>To get re-started on flat, soft, dry sand. Reverse 1-2 metres and form hard sand ramps so as to get a good starting speed before hitting soft sand again</li><li>To ascend a sand hill, utilise controlled momentum. If you fail to ascend, back down the same wheel tracks far enough to allow a faster approach up the same wheel tracks</li><li>To descend keep vertical to the sand hill/descent and avoid brakes, accelerate gently if necessary to aid descent.</li></ul><h2>There is Nothing Quite Like Driving on Sand</h2><p>Most off-roaders may never come across sand unless they happen to make it to coastal regions and hit the beach. However, if they do start to undertake longer distance adventure treks, they may at some time
make it to the more arid regions and the odd few may even make it to the “Real” Desert. Perhaps like myself, when I first flew in on a light aircraft to a desert strip in the deep Sahara to run driver training, their answer to the question from colleagues as to what they thought of the place will be the same as mine. <strong>“I think I’m going to need a bigger bucket and spade!”</strong></p><h3>Beach Sand</h3><p>Beach Sand is totally different to the sand you find in the desert. Much of it tends to be much coarser in grain texture and therefore can at time be more forgiving that the desert sand that runs like water. Of course, being close to the sea it can also be damp, wet, extremely wet or waterlogged. It can also be dry, gravelly and have
the odd rock thrown in for good measure. Away from the water’s edge you will often find dunes, some with vegetation, some without and in certain areas around the World you will find larger dunes, similar to the ones you find in the desert. While those dunes look similar and do have some of the same features they are very different. We’ll come to that later but only touch on them briefly as “real” desert driving needs lots of space for discussion on techniques.</p><p>So, we’ve hit the beach, some great scenery, lots of fresh air, all your mates are with you looking at those long, sometimes wide strips of sand with the waves lapping at the edge. This is usually where common sense and safety goes out of the window! Competitive, idiotic behavior, spurred on usually by male testosterone reacting with the ozone fresh sea breeze leads to frantic and erratic driving at high speed along the sand and often in and out of the water. <strong>Beaches are great BUT they are not playgrounds with your truck. In a 4wd Vehicle
they can be one of the most dangerous places you will ever drive and can be like playing Russian Roulette, with a Round in every chamber.</strong> The normal <a href="https://www.offroaddiscovery.com/off-road-driver-training/hazard-identification-safety-environment">Hazard Identifications</a> when off-roading apply, so read the earlier article on this on the site.</p><h3>Three Common Sense Driving Tips for Beaches</h3><ol><li><p><strong>Keep out of the Sea</strong>: Salt Water is not good for 4wd vehicle chassis and body components. Driving in the Sea as many will do despite this article, will throw salt water everywhere and it will get into the chassis, it will in time cause rust erosion and pressure washing after may nod get rid of it. Vehicle electrics
and water do not go well together at any time. Hot radiator fins are thin, if salt water passes through the fins the heat dries it and the salt deposit remains to eat its way into the core. I’ve seen many a vehicle being taken home on a recovery truck from the beach due to water getting into badly position ECU and ignition units in the engine bay. It may look fantastic, cause lots of spray and a splash – BUT KEEP OUT OF THE SALT WATER!</p></li><li><p>Ok, hopefully you’ve got the message on point one and you’re just going to drive on the beach sand. It looks nice and flat; it may even look fairly dry. However, it’s a beach, normally the tide comes in and out twice a day that means, although it may look dry on the surface because of all that “Sunshine” in some Countries, it could well be soggy under the surface. You can <strong>drive it in either high or low range but KEEP THE SPEED DOWN</strong>.</p></li><li><p>As mentioned, it can be soggy below the surface or it can also be dry with patches of deeper sand. <strong>Keep driving in straight lines on it and NEVER turn sharply</strong> as if a front wheel happens to dig in the softer areas it will immediately act as a brake, weight will transfer on to it and you’ll become a statistic in the rollover records. These beach rollovers happen every day around the globe. IF you are going to turn, firstly slow down
and secondly turn in a wide arc. It doesn’t take much for a loaded 4wd, especially if you have equipment high up on a roof rack, for the dynamics to transfer weight. Even experienced and regular beach drivers forget this simple message. I’ve been on beaches around the world and viewed many who thought they were “experts,” being scraped up by the medics or recovery vehicles. Don’t join their gang!</p></li></ol><p><strong>NOTE: In an effort to try and keep you SAFE watch some videos</strong>
There are plenty of good and instructional videos covering
Sand on the Internet - (YouTube etc.) One of the best is <strong>“Guide to Off-Roading – Driving on Sand –
by Land Rover Experience</strong>. There are also lots of videos that highlight the art of BAD
Driving on both Beach and Desert Sand. They cover the accidents! Watch those ones several times and then look around you at the kids and the rest of your family and consider the implications should you drive in Sand (or anywhere else for that matter) like some of the idiots involved in these incidents.
Keep everyone’s Seat Belts on – Don’t let the kids hang out of the Sunroof or Side Windows. If you see some of the more serious rollover accidents you will understand why.</p><p><iframe width="640" height="360" src="https://www.youtube.com/embed/0-1__frSTno" frameborder="0" allowfullscreen=""></iframe></p><h3>Driving on Sand</h3><p>It is reasonable to assume that you can drive on firm sand if you take notice of the previous three tips but there will come a time when the surface will not be firm and supple sand is a different thing altogether.
Once you start driving in the softer sand, traction can be at a premium, any increase in speed may be and usually is, difficult if not impossible, so to make headway you need to maintain momentum.
Should you lose that, it is unlikely you will regain it and you will likely become bogged down.</p><p><em>Once you lose traction in sand and have wheel-spin, abort immediately or you
will just dig yourself in further. Engage reverse gear and “Gently” try to drive out
along your tracks.</em></p><p><img src="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/f56ccdf56a9223c94760e725fd691e55/4x4inSand_640.png" width="640" height="536" srcset="https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/f56ccdf56a9223c94760e725fd691e55/4x4inSand_360.png 360w, https://storage.googleapis.com/images.blogserve.co/offroaddiscovery/f56ccdf56a9223c94760e725fd691e55/4x4inSand_640.png 640w" sizes="(min-width: 768px) 720px, 100vw" alt="Tips for Driving on Sand and not getting Stuck"></p><h3>Tyre Pressures</h3><p>Assuming you are going to be on the beach or coastal sand for any length of time rather than just sticking your nose in for a few minutes to take in the sea air, it is advisable to reduce your tyre pressures.
The reason for this is that it provides what is termed as a better footprint reducing the ground pressure and allowing better “flotation” which in turn improves traction. (Think of the “Snow Shoe” effect on
Snow)</p><p>Most people think that reducing the tyre pressure makes the contact with the ground wider because they see the tyre “bulge” out and become fatter. In reality, this is not the case, the tyre footprint actually
becomes longer although there may be a slight increase in width. If you have ever seen a vehicle with a punctured tyre you will remember it was flat at the bottom along the ground for maybe 18 to
20 inches whereas when it was inflated, only 8 inches were in contact with the ground. When it comes to low ground pressure, “Size Matters.” With lower ground pressure there is less strain on the
vehicles steering components as well as the power unit and transmission as they don’t have to work as hard.
Before you consider lowering the pressures you must of course ensure that you have a suitable pump to re-inflate them when you come back to the normal roads and tracks. Assuming you have the
pump you can now reduce the pressure but to what level? Often you have the pump but no pressure gauge. Oops! I forgot to pack it in the truck. Now what do I do?</p><p><strong>For general sand driving a tyre pressure of circa 15 to 16 psi is the norm</strong>. If you don’t have a gauge, you can use the old explorer’s trick of using a stick or a small rock placed an inch away from the edge of
the tyre sidewall. On the average 4wd tyre of say 235x85x16, if you now let the air out of the tyre valve until the sidewall touches the stick on each tyre, you should have all the tyres down to the same level
and roughly with a suitable amount of deflation. You can use the same method the other way around when you re-inflate so you have them back to the same level for normal use until you can check and adjust with a gauge.</p><p>You must remember, now that you have the lower pressure and perhaps with the bulging side-walls, that the tyres are more susceptible to wear or damage. You need to balance the risk when lowering the pressure between the increased traction it delivers and the possible tyre damage and wear. The correct balance will certainly make the …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach">https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach</a></em></p>]]>
            </description>
            <link>https://www.offroaddiscovery.com/2017/10/16/taking-a-4x4-trip-down-the-beach</link>
            <guid isPermaLink="false">hacker-news-small-sites-24564542</guid>
            <pubDate>Wed, 23 Sep 2020 08:38:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Firefox usage is down despite Mozilla's top exec pay going up]]>
            </title>
            <description>
<![CDATA[
Score 1568 | Comments 1211 (<a href="https://news.ycombinator.com/item?id=24563698">thread link</a>) | @todsacerdoti
<br/>
September 22, 2020 | http://calpaterson.com/mozilla.html | <a href="https://web.archive.org/web/*/http://calpaterson.com/mozilla.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <article>
            
            <p>September 2020</p>
            <p id="article-description">Mozilla is in an absolute state: high
            overheads, falling usage of Firefox, questionable sources of revenue and
            now making big cuts to engineering as their income falls.</p>
            <hr>
            <figure>
                <img src="http://calpaterson.com/assets/mozilla-boss-pay.svg" alt="a graph showing that executive pay has grown fast while Firefox's market share has fallen">
                <figcaption>
                    Mozilla's top exec pay has gone up hugely even as usage has
                    crashed.
                </figcaption>
            </figure>
            <p>Mozilla recently announced that they would be dismissing 250 people.
            That's a quarter of their workforce so there are some deep cuts to their
            work too. The victims include: the MDN docs (those are the web standards
            docs everyone likes better than w3schools), the Rust compiler and even some
            cuts to Firefox development. Like most people I want to see Mozilla do well
            but those three projects comprise pretty much what I think of as the whole
            point of Mozilla, so this news is a big let down.</p>
            <p>The stated reason for the cuts is falling income. Mozilla largely relies
            on "royalties" for funding. In return for payment, Mozilla allows big
            technology companies to choose the default search engine in Firefox - the
            technology companies are ultimately paying to increase the number of
            searches Firefox users make with them. Mozilla haven't been particularly
            transparent about why these royalties are being reduced, except to blame
            the coronavirus.</p>
            <p>I'm sure the coronavirus is not a great help but I suspect the bigger
            problem is that Firefox's market share is now a tiny fraction of its
            previous size and so the royalties will be smaller too - fewer users, so
            fewer searches and therefore less money for Mozilla.</p>
            <p>The real problem is not the royalty cuts, though. Mozilla has already
            received more than enough money to set themselves up for financial
            independence. Mozilla received up to half a billion dollars a year (each
            year!) for many years. The <em>real problem</em> is that Mozilla didn't use
            that money to achieve financial independence and instead just spent it each
            year, doing the organisational equivalent of living hand-to-mouth.</p>
            <p>Despite their slightly contrived legal structure as a non-profit that
            owns a for-profit, Mozilla are an NGO just like any other. In this article
            I want to apply the traditional measures that are applied to other NGOs to
            Mozilla in order to show what's wrong.</p>
            <p>These three measures are: overheads, ethics and results.</p>
            <h2>Overheads</h2>
            <p>One of the most popular and most intuitive ways to evaluate an NGO is to
            judge how much of their spending is on their programme of works (or
            "mission") and how much is on other things, like administration and
            fundraising. If you give money to a charity for feeding people in the third
            world you hope that most of the money you give them goes on food - and not,
            for example, on company cars for head office staff.</p>
            <p>Mozilla looks bad when considered in this light. Fully 30% of all
            expenditure goes on administration. Charity Navigator, an organisation that
            measures NGO effectiveness, would give them <a href="https://www.charitynavigator.org/index.cfm?bay=content.view&amp;cpid=48#PerformanceMetricTwo">
            zero out of ten</a> on the relevant metric. For context, to achieve 5/10 on
            that measure Mozilla admin would need to be under 25% of spending and, for
            10/10, under 15%.</p>
            <p>Senior executives have also done very well for themselves. Mitchell
            Baker, Mozilla's top executive, was paid $2.4m in 2018, a sum I personally
            think of as instant inter-generational wealth. Payments to Baker have more
            than doubled in the last five years.</p>
            <p>As far as I can find, there is no UK-based NGO whose top executive makes
            more than £1m ($1.3m) a year. The UK certainly has its fair share of big
            international NGOs - many much bigger and more significant than
            Mozilla.</p>
            <p>I'm aware that <a href="https://concepts.effectivealtruism.org/concepts/relationship-between-overheads-and-effectiveness/">
            some people dislike overheads as a measure</a> and argue that it's possible
            for administration spending to increase effectiveness. I think it's hard to
            argue that Mozilla's overheads are correlated with any improvement in
            effectiveness.</p>
            <h2>Ethics</h2>
            <p>Mozilla now thinks of itself less as a custodian of the old Netscape
            suite and more as a 'privacy NGO'. One slogan inside Mozilla is: "Beyond
            the Browser".</p>
            <p>Regardless of how they view themselves, most of their income comes from
            helping to direct traffic to Google by making that search engine the
            default in Firefox. Google make money off that traffic via a big targeted
            advertising system that tracks people across the web and largely without
            their consent. Indeed, one of the reasons this income is falling is because
            as Firefox's usage falls less traffic is being directed Google's way and so
            Google will pay less.</p>
            <p>There is, as yet, no outbreak of agreement among the moral philosophers
            as to a universal code of ethics. However I think most people would
            recognise hypocrisy in Mozilla's relationship with Google. Beyond the
            ethical problems, the relationship certainly seems to create conflicts of
            interest. Anyone would think that a privacy NGO would build anti-tracking
            countermeasures into their browser right from the start. In fact, this was
            only added relatively recently (<a href="https://blog.mozilla.org/blog/2019/06/04/firefox-now-available-with-enhanced-tracking-protection-by-default/">in
            2019</a>), after both Apple (<a href="https://webkit.org/blog/7675/intelligent-tracking-prevention/">in
            2017</a>) and Brave (since release) paved the way. It certainly seems like
            Mozilla's status as a Google vassal has played a role in the absence of
            anti-tracking features in Firefox for so long.</p>
            <p>Another ethical issue is Mozilla's big new initiative to <a href="https://vpn.mozilla.org/">move into VPNs</a>. This doesn't make a lot of
            sense from a privacy point of view. Broadly speaking: VPNs are not a useful
            privacy tool for people browsing the web. A VPN lets you access the
            internet through a proxy - so your requests superficially appear to come
            from somewhere other than they really do. This does nothing to address the
            main privacy problem for web users: that they are being passively tracked
            and de-anonymised on a massive scale by the baddies at Google and
            elsewhere. This tracking happens regardless of IP address.</p>
            <p>When I tested Firefox through <a href="https://vpn.mozilla.org/">Mozilla
            VPN</a> (a rebrand of <a href="https://mullvad.net/">Mullvad VPN</a>) I
            found that I could be de-anonymised by browser fingerprinting - already a
            fairly widespread technique by which various elements of your browser are
            examined to create a "fingerprint" which can then be used to re-identify
            you later. Firefox does not include as many countermeasures against this as
            some other browsers (<strong>this is a correction</strong> - I previously
            said Firefox contained none but it's been pointed out to me that <a href="https://blog.mozilla.org/security/2020/01/07/firefox-72-fingerprinting/">since
            earlier this year</a> it does block some kinds of fingerprinting).</p>
            <figure>
                <img src="http://calpaterson.com/assets/panopticlick-firefox.png" alt="firefox's results on panopticlick - my browser has a unique fingerprint">
                <figcaption>
                    Even when using Mozilla's "secure and private" VPN, Firefox is
                    trackable by browser fingerprinting, as demonstrated by the
                    <a href="https://panopticlick.eff.org/">EFF's Panopticlick
                    tool</a>. Other browsers use randomised fingerprints as a
                    countermeasure against this tracking.
                </figcaption>
            </figure>
            <p>Another worry is that many of these privacy focused VPN services have a
            nasty habit of turning out to keep copious logs on user behaviour. A few
            months ago several "no log" VPN services inadvertently released terabytes
            of private user data that they had promised not to collect <a href="https://www.vpnmentor.com/blog/report-free-vpns-leak/">in a massive
            breach</a>. VPN services are in a great position to eavesdrop - and even if
            they promise not to, your only option is to take them at their word.</p>
            <h2>Results</h2>
            <p>I've discussed the Mozilla chair's impressive pay: $2.4m/year. Surely
            such impressive pay is justified by the equally impressive results Mozilla
            has achieved? Sadly on almost every measure of results both quantitative
            and qualitative, Mozilla is a dog.</p>
            <p>Firefox is now so niche it is in danger of garnering a cult following:
            it has just 4% market share, down from 30% a decade ago. Mobile browsing
            numbers are bleak: Firefox barely exists on phones, with a market share of
            less than half a percent. This is baffling given that mobile Firefox has a
            rare feature for a mobile browser: it's able to install extensions and so
            can block ads.</p>
            <p>Yet despite the problems within their core business, Mozilla, instead of
            retrenching, has diversified rapidly. In recent years Mozilla has
            created:</p>
            <ul>
                <li>a mobile app for making websites</li>
                <li>a federated identity system</li>
                <li>a large file transfer service</li>
                <li>a password manager</li>
                <li>an internet-of-things framework/standard</li>
                <li>an email relay service</li>
                <li>a completely new phone operating system</li>
                <li>an AI division (but of course)</li>
                <li>and spent $25 million buying the reading list management startup,
                Pocket</li>
            </ul>
            <p>Many of the above are now abandoned.</p>
            <p>Sadly <a href="https://www.mozilla.org/en-US/foundation/annualreport/2018/">Mozilla's
            annual report</a> doesn't break down expenses on a per-project basis so
            it's impossible to know how much of the spending that <em>is</em> on
            Mozilla's programme is being spent on Firefox and how …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://calpaterson.com/mozilla.html">http://calpaterson.com/mozilla.html</a></em></p>]]>
            </description>
            <link>http://calpaterson.com/mozilla.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24563698</guid>
            <pubDate>Wed, 23 Sep 2020 06:38:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A look at /e/OS on the FairPhone 3 – a FOSS OS for phones [video]]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24563576">thread link</a>) | @indidea
<br/>
September 22, 2020 | https://share.tube/videos/watch/ef3e5eec-27aa-45c8-b87b-64a57fc6e176 | <a href="https://web.archive.org/web/*/https://share.tube/videos/watch/ef3e5eec-27aa-45c8-b87b-64a57fc6e176">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://share.tube/videos/watch/ef3e5eec-27aa-45c8-b87b-64a57fc6e176</link>
            <guid isPermaLink="false">hacker-news-small-sites-24563576</guid>
            <pubDate>Wed, 23 Sep 2020 06:17:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Visualizing Gzip Compression with Python]]>
            </title>
            <description>
<![CDATA[
Score 74 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24563372">thread link</a>) | @brenns10
<br/>
September 22, 2020 | https://brennan.io/2020/09/22/compression-curves/ | <a href="https://web.archive.org/web/*/https://brennan.io/2020/09/22/compression-curves/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan • 22 September 2020</em></p><p>Not that long ago, I found myself wanting to understand gzip. I didn’t necessarily want to learn to implement the algorithm, but rather I just wanted to understand how it was performing on a particular file. Even more specifically, I wanted to understand which parts of a file compressed well, and which ones did not.</p>

<p>There may be readily available tools for visualizing this, but I didn’t find anything. Since I know gzip is implemented in the Python standard libraries, and I’m familiar with Python plotting libraries, I thought I would try to make my own visualization. This blog post (which is in fact just a Jupyter notebook) is the result.</p>

<h2 id="what-to-measure">What to measure</h2>

<p>Sometimes the hardest part of a data analysis problem is just figuring out what you want to measure. The data is all there, and you have a computer at your disposal, so the possibilities are endless. Knowing <em>what</em> to compute is tricky. In my case, I want to understand which parts of a file compress well. So it makes sense that whatever I visualize should include the position in the file along the X axis, and the compressed size along the Y axis. An uncompressed file would simply be a diagonal line. The better the compression, the more this line would stay <em>under</em> the diagonal line of an uncompressed file.</p>

<h2 id="how-to-measure-it">How to measure it?</h2>

<p>Since Python supports gzip in the standard library, let’s see how we can measure these X and Y coordinates. First, let’s create a file with some compressed data. My favorite to use in this instance is <a href="http://www.gutenberg.org/ebooks/11">Alice’s Adventures in Wonderland</a>, downloaded from Project Gutenberg. We’ll compress it on the command line for simplicity.</p>

<p>(Note that code prefixed by ‘!’ is executed via bash - everything else is executed in Python).</p>

<div><div><pre><code>!gzip -k alice.txt
!ls -lh alice*
</code></pre></div></div>

<div><div><pre><code>-rw-r--r-- 1 stephen stephen 171K Sep 22 20:25 alice.txt
-rw-r--r-- 1 stephen stephen  60K Sep 22 20:25 alice.txt.gz
</code></pre></div></div>

<p>gzip does a pretty decent job at compressing this, far better than I could do myself. Now, let’s use Python to decompress just a little bit of it, and how much of the original file is consumed as we go.</p>

<div><div><pre><code><span>import</span> <span>gzip</span>
<span>compressed</span> <span>=</span> <span>open</span><span>(</span><span>'alice.txt.gz'</span><span>,</span> <span>'rb'</span><span>)</span>
<span>gzip_file</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>compressed</span><span>)</span>
</code></pre></div></div>

<p>In the above code, we save the open file object as <code>compressed</code> before giving it over to the <code>GzipFile</code>. That way, as we read the decompressed data out of <code>gzip_file</code>, we’ll be able to use the <code>tell()</code> method to see how far we are through the compressed file.</p>

<div><div><pre><code><span>first_100_bytes</span> <span>=</span> <span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
<span>first_100_bytes</span>
</code></pre></div></div>

<div><div><pre><code>b'\xef\xbb\xbfThe Project Gutenberg EBook of Alice\xe2\x80\x99s Adventures in Wonderland, by Lewis Carroll\r\n\r\nThis eBook'
</code></pre></div></div>





<p>This feels disappointing. We only read 100 bytes and yet it took 8212 bytes of gzip to give us that data? Well, we have to consider that compression algorithms need to store some tables of data which help decompress the rest of the file, so we should cut gzip some slack. Let’s do this a few more times.</p>



<div><div><pre><code>b' is for the use of anyone anywhere at no cost and with\r\nalmost no restrictions whatsoever.  You may '
</code></pre></div></div>





<p>This feels wrong. After reading 8212 bytes of compressed data for the first 100 bytes, it takes zero bytes to get the next 100?</p>



<div><div><pre><code>b'copy it, give it away or\r\nre-use it under the terms of the Project Gutenberg License included\r\nwith '
</code></pre></div></div>





<p>Clearly there is some buffering going on here. 8212 is suspiciously close to 8192 (20 bytes away) which is a power of two, and thus likely to be a common buffer size. Python’s file I/O machinery is responsible for the buffering, but we can actually get rid of it by disabling buffering.</p>

<div><div><pre><code><span>gzip_file</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span> <span>=</span> <span>open</span><span>(</span><span>'alice.txt.gz'</span><span>,</span> <span>'rb'</span><span>,</span> <span>buffering</span><span>=</span><span>0</span><span>)</span>
<span>gzip_file</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>compressed</span><span>)</span>
<span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
<span>compressed</span><span>.</span><span>tell</span><span>()</span>
</code></pre></div></div>



<p>Hm. We made <code>compressed</code> an unbuffered file, but maybe <code>GzipFile</code> has its own internal buffering. To avoid this, let’s do a bad thing. We can actually set the buffer size for all I/O operations by modifying <code>io.DEFAULT_BUFFER_SIZE</code>. If we set it to a small value, then we can reduce the impact of buffering on our measurements. Just for fun, let’s try setting it to 1.</p>

<div><div><pre><code><span>import</span> <span>io</span>
<span>old_buffer_size</span> <span>=</span> <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span>
<span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>1</span>
<span>gzip_file</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span><span>.</span><span>close</span><span>()</span>

<span>compressed</span> <span>=</span> <span>open</span><span>(</span><span>'alice.txt.gz'</span><span>,</span> <span>'rb'</span><span>,</span> <span>buffering</span><span>=</span><span>0</span><span>)</span>
<span>gzip_file</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>compressed</span><span>)</span>
<span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
<span>compressed</span><span>.</span><span>tell</span><span>()</span>
</code></pre></div></div>



<p>This seems <em>much</em> more believable. To read 100 bytes of decompressed data, gzip had to read 205 bytes of compressed data (again, this is probably due to tables and other header information). Let’s continue for a bit:</p>

<div><div><pre><code><span>bytes_unc</span> <span>=</span> <span>100</span>
<span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>5</span><span>):</span>
    <span>gzip_file</span><span>.</span><span>read</span><span>(</span><span>100</span><span>)</span>
    <span>bytes_unc</span> <span>+=</span> <span>100</span>
    <span>bytes_cmp</span> <span>=</span> <span>compressed</span><span>.</span><span>tell</span><span>()</span>
    <span>print</span><span>(</span><span>f</span><span>'uncompressed: {bytes_unc} / compressed: {bytes_cmp}'</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code>uncompressed: 200 / compressed: 279
uncompressed: 300 / compressed: 335
uncompressed: 400 / compressed: 376
uncompressed: 500 / compressed: 449
uncompressed: 600 / compressed: 541
</code></pre></div></div>

<p>We can see that after reading 400 uncompressed bytes, the gzip compression has caught up! 376 compressed bytes needed to be read to give us those 400. The gap continues to widen as we go on.</p>

<p>Now that we’re confident that this approach is giving us interesting data, let’s make some functions to get all of this data for a particular file, so we can visualize it!</p>

<div><div><pre><code><span>gzip_file</span><span>.</span><span>close</span><span>()</span>
<span>compressed</span><span>.</span><span>close</span><span>()</span>
<span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>old_buffer_size</span>
</code></pre></div></div>

<p>That was just some cleanup. Since modifying the buffer size would likely impact other code we run, it’s best to only modify the buffer size when we need it, and reset it back to its original value when we’re done. This can be done with a context manager.</p>

<div><div><pre><code><span>import</span> <span>contextlib</span>
<span>@contextlib.contextmanager</span>
<span>def</span> <span>buffer_size</span><span>(</span><span>newsize</span><span>=</span><span>1</span><span>):</span>
    <span>old_buffer_size</span> <span>=</span> <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span>
    <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>newsize</span>
    <span>try</span><span>:</span>
        <span>yield</span>
    <span>finally</span><span>:</span>
        <span>io</span><span>.</span><span>DEFAULT_BUFFER_SIZE</span> <span>=</span> <span>old_buffer_size</span>
        
        
<span>with</span> <span>buffer_size</span><span>():</span>
    <span>print</span><span>(</span><span>f</span><span>'size: {io.DEFAULT_BUFFER_SIZE}'</span><span>)</span>
<span>print</span><span>(</span><span>f</span><span>'size: {io.DEFAULT_BUFFER_SIZE}'</span><span>)</span>
</code></pre></div></div>



<p>Now for a function to retrieve compressed and uncompressed sizes. We can do this with a “chunk size” as a parameter. The larger our chunk size, the fewer data points we will have, but the code will run faster. We used 100 as a chunk size above, which seems good enough, but I do prefer a good <a href="https://xkcd.com/1000/">round number</a>, so I’ll change it to 64.</p>

<div><div><pre><code><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span>def</span> <span>create_compression_curve</span><span>(</span><span>filename</span><span>,</span> <span>chunksize</span><span>=</span><span>64</span><span>):</span>
    <span>with</span> <span>buffer_size</span><span>(</span><span>1</span><span>),</span> <span>open</span><span>(</span><span>filename</span><span>,</span> <span>'rb'</span><span>)</span> <span>as</span> <span>fileobj</span><span>:</span>
        <span>gf</span> <span>=</span> <span>gzip</span><span>.</span><span>GzipFile</span><span>(</span><span>fileobj</span><span>=</span><span>fileobj</span><span>)</span>
        <span>records</span> <span>=</span> <span>[]</span>
        <span>read</span> <span>=</span> <span>0</span>
        <span>while</span> <span>True</span><span>:</span>
            <span>data</span> <span>=</span> <span>gf</span><span>.</span><span>read</span><span>(</span><span>chunksize</span><span>)</span>
            <span>if</span> <span>len</span><span>(</span><span>data</span><span>)</span> <span>==</span> <span>0</span><span>:</span>
                <span>break</span>  <span># end of file</span>
            <span>else</span><span>:</span>
                <span>read</span> <span>+=</span> <span>len</span><span>(</span><span>data</span><span>)</span>
                <span>records</span><span>.</span><span>append</span><span>((</span><span>read</span><span>,</span> <span>fileobj</span><span>.</span><span>tell</span><span>()))</span>
                
    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>records</span><span>,</span> <span>columns</span><span>=</span><span>[</span><span>'uncompressed'</span><span>,</span> <span>filename</span><span>])</span>
    <span>return</span> <span>df</span><span>.</span><span>set_index</span><span>(</span><span>'uncompressed'</span><span>)</span>


<span>ccurve</span> <span>=</span> <span>create_compression_curve</span><span>(</span><span>'alice.txt.gz'</span><span>)</span>
<span>ccurve</span>
</code></pre></div></div>

<div>

<table>
  <thead>
    <tr>
      <th></th>
      <th>alice.txt.gz</th>
    </tr>
    <tr>
      <th>uncompressed</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>64</th>
      <td>176</td>
    </tr>
    <tr>
      <th>128</th>
      <td>224</td>
    </tr>
    <tr>
      <th>192</th>
      <td>271</td>
    </tr>
    <tr>
      <th>256</th>
      <td>315</td>
    </tr>
    <tr>
      <th>320</th>
      <td>345</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>174272</th>
      <td>61351</td>
    </tr>
    <tr>
      <th>174336</th>
      <td>61362</td>
    </tr>
    <tr>
      <th>174400</th>
      <td>61379</td>
    </tr>
    <tr>
      <th>174464</th>
      <td>61404</td>
    </tr>
    <tr>
      <th>174484</th>
      <td>61417</td>
    </tr>
  </tbody>
</table>
<p>2727 rows × 1 columns</p>
</div>

<p>The above function simply reads the gzipped file in chunks, measuring the distance we’ve gone through the compressed file each time, and adding it to a list of “records”. This list is converted into a Pandas Dataframe, which is commonly used to hold tabular data like this. We set the “uncompressed” column to be the “index”, since that’s what we’d consider the X-axis.</p>

<p>The result looks exciting! We can even go right ahead and plot it from here.</p>

<div><div><pre><code><span># Some style changes to make the plots more pretty</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>matplotlib</span> <span>as</span> <span>mpl</span>
<span>plt</span><span>.</span><span>style</span><span>.</span><span>use</span><span>(</span><span>'ggplot'</span><span>)</span>
<span>mpl</span><span>.</span><span>rcParams</span><span>[</span><span>'figure.figsize'</span><span>]</span> <span>=</span> <span>[</span><span>16</span><span>,</span> <span>8</span><span>]</span>

<span>ccurve</span><span>.</span><span>plot</span><span>()</span>
</code></pre></div></div>

<p><img src="https://brennan.io/images/ccurves/output_28_1.png" alt="Plot 1"></p>

<p>Well, I gotta give it to gzip – it’s pretty consistent. I can’t really see anything interesting in the plot, except that the gzipped data is smaller than the uncompressed version (duh). We can add this in to make it more explicit:</p>

<div><div><pre><code><span>ccurve</span><span>[</span><span>'uncompressed'</span><span>]</span> <span>=</span> <span>ccurve</span><span>.</span><span>index</span>
<span>ccurve</span><span>.</span><span>plot</span><span>()</span>
</code></pre></div></div>

<p><img src="https://brennan.io/images/ccurves/output_30_1.png" alt="Plot 2"></p>

<h2 id="what-to-do-with-this-new-power">What to do with this new power?</h2>

<p>So, the result here seems to be blindingly mundane. gzip compresses reasonably well, it’s obviously better than uncompressed.</p>

<p>Well, let’s try to make things less mundane. First, a peek at the gzip(1) manual page indicates that it has different compression levels 1-9. I’ll let the manual do the explaining:</p>

<div><div><pre><code>   -# --fast --best
          Regulate the speed of compression using the specified digit #, where -1
          or  --fast  indicates the fastest compression method (less compression)
          and -9 or --best indicates the slowest compression  method  (best  com‐
          pression).   The  default  compression level is -6 (that is, biased to‐
          wards high compression at expense of speed).
</code></pre></div></div>

<p>What if we used this compression curve plot to compare the gzip compression levels?</p>

<div><div><pre><code><span>import</span> <span>os</span>
<span>files</span> <span>=</span> <span>[]</span>
<span>for</span> <span>level</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>10</span><span>):</span>
    <span>os</span><span>.</span><span>system</span><span>(</span><span>f</span><span>'gzip -k -S .gz.{level} -{level} alice.txt'</span><span>)</span>
    <span>files</span><span>.</span><span>append</span><span>(</span><span>f</span><span>'alice.txt.gz.{level}'</span><span>)</span>
    <span>print</span><span>(</span><span>f</span><span>'Created {files[-1]}'</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code>Created alice.txt.gz.1
Created alice.txt.gz.2
Created alice.txt.gz.3
Created alice.txt.gz.4
Created alice.txt.gz.5
Created alice.txt.gz.6
Created alice.txt.gz.7
Created alice.txt.gz.8
Created alice.txt.gz.9
</code></pre></div></div>

<p>Above I went ahead and created all the different compression levels. Now, we can get compression curves for all of them and plot them:</p>

<div><div><pre><code><span>ccurves</span> <span>=</span> <span>pd</span><span>.</span><span>concat</span><span>([</span>
    <span>create_compression_curve</span><span>(</span><span>fn</span><span>)</span> <span>for</span> <span>fn</span> <span>in</span> <span>files</span>
<span>],</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code><span>ccurves</span><span>.</span><span>plot</span><span>(</span><span>title</span><span>=</span><span>"gzip Compression Level Comparison (Alice's Adventures in Wonderland)"</span><span>)</span>
</code></pre></div></div>

<p><img src="https://brennan.io/images/ccurves/output_35_1.png" alt="Plot 3"></p>

<p>That seems slightly more interesting. The default compression level of 6 seems to be chosen well. Beyond level 6, the reduction in file size seems pretty difficult to notice. However, the difference between the compression ratios is rather small compared to the uncompressed line:</p>

<div><div><pre><code><span>ccurves</span><span>[</span><span>'uncompressed'</span><span>]</span> <span>=</span> <span>ccurves</span><span>.</span><span>index</span>
<span>ccurves</span><span>.</span><span>plot</span><span>()</span>
</code></pre></div></div>
<hr>
<p><img src="https://brennan.io/images/ccurves/output_37_1.png" alt="Plot 4"></p>

<h2 id="making-an-interesting-graph">Making an …</h2></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://brennan.io/2020/09/22/compression-curves/">https://brennan.io/2020/09/22/compression-curves/</a></em></p>]]>
            </description>
            <link>https://brennan.io/2020/09/22/compression-curves/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24563372</guid>
            <pubDate>Wed, 23 Sep 2020 05:41:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: I made a free-to-use GPT-2 API]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24561214">thread link</a>) | @edunteman
<br/>
September 22, 2020 | https://www.booste.io/pretrained-models | <a href="https://web.archive.org/web/*/https://www.booste.io/pretrained-models">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"><div><h2>Installation</h2><p>Download the Booste client for Python</p><div><div><p>If you have not already, download and install <a href="https://www.python.org/downloads/">python</a> and <a href="https://pip.pypa.io/en/stable/">pip</a></p></div>
<p>From your terminal, run:</p>
<pre><code>pip install booste</code></pre>
</div></div></div><div id="content"><div><h2>GPT-2</h2><p>Predict the next word(s) from a given sequence of words.</p><div><h3 id="add-this-to-your-python-code">Add this to your python code:</h3>
<pre><code>import booste

out_list = booste.gpt2(in_string, length)</code></pre>
<h4 id="arguments">Arguments:</h4>
<table>
<thead>
<tr>
<th>Arg</th>
<th>Description</th>
<th>Required</th>
<th>Type</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>in_string</td>
<td>The given sequence of words</td>
<td>True</td>
<td>string</td>
<td>"I went on a walk and suddenly, I"</td>
</tr>
<tr>
<td>length</td>
<td>The quantity of words to predict following the in_string</td>
<td>False (default=5)</td>
<td>int</td>
<td>10</td>
</tr>
</tbody></table>
<h4 id="return-type">Return Type:</h4>
<p>List - the predicted text, separated into list form. </p>
<p><em>Note: If a server error occurs, the return type will be None. This API is in beta and uptime is currently 95%.</em> </p>
<p>To convert to string, add:</p>
<pre><code>out_string = " ".join(out_list)</code></pre>
<h4 id="response-time">Response Time:</h4>
<p>GPT-2 is a large model. API calls can take 1s per word of length. <a href="https://forms.gle/4CWxhHptkT1Cx5dF7">Request faster response time here</a></p>
<h4 id="fine-tuning">Fine-Tuning:</h4>
<p>Adjusting these parameters from the default is not recommended.</p>
<table>
<thead>
<tr>
<th>Arg</th>
<th>Description</th>
<th>Required</th>
<th>Type</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>temperature</td>
<td>A value between 0.1 and 1, to adjust randomness.<p> Smaller values create seemingly random output. Larger values create repeating phrases in the output.</p></td>
<td>False (default=0.8)</td>
<td>float</td>
<td>0.8</td>
</tr>
<tr>
<td>batch_length</td>
<td>A value 1-50, to manage inference workload.<p> Booste splits the API call into batches to avoid server timeout. Adjusting this does not affect prediction quality.</p><p>Smaller values reduce total inference time, but require more API calls, so it is only suggested if you have high bandwidth. Larger values can cause server timeout.</p></td>
<td>False (default=20)</td>
<td>int</td>
<td>50</td>
</tr>
<tr>
<td>window_max</td>
<td>A value 1-200, to manage inference workload.<p>GPT-2 can only accept an input string less than 200 words long. Booste uses a "sliding window" approach to handle longer length requests, where the input string is trimmed to the n=window_max most recent words. </p><p>Smaller values reduce inference time dramatically, but output will drift into unrelated subjects due to lost context. Larger values increase output quality, but can 5x inference time per word and overload the model input limit.</p></td>
<td>False (default=50)</td>
<td>int</td>
<td>100</td>
</tr>
</tbody></table>
</div></div></div><div id="content"><div><h2>Want more pretrained models?</h2></div></div></div>]]>
            </description>
            <link>https://www.booste.io/pretrained-models</link>
            <guid isPermaLink="false">hacker-news-small-sites-24561214</guid>
            <pubDate>Tue, 22 Sep 2020 23:25:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Catch Breaking Changes by Diffing API Traffic]]>
            </title>
            <description>
<![CDATA[
Score 98 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24561119">thread link</a>) | @jeanyang
<br/>
September 22, 2020 | https://www.akitasoftware.com/blog/2020/9/22/faster-better-earlier-catch-breaking-changes-by-diffing-api-behavior | <a href="https://web.archive.org/web/*/https://www.akitasoftware.com/blog/2020/9/22/faster-better-earlier-catch-breaking-changes-by-diffing-api-behavior">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          

          <main>
            
              <section data-content-field="main-content">
                <article id="post-5f6a6536fa06431194e74792" data-item-id="5f6a6536fa06431194e74792">

    
      
    

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1600808326368" id="item-5f6a6536fa06431194e74792"><div><div><div data-block-type="2" id="block-b94a17a4246df542ccf7"><div><p>If you work on web apps, you may sometimes feel that your tools have forsaken you. And you’re not wrong. Because of the way modern tech stacks are set up, it’s getting harder and harder to make sure your code is doing what it’s supposed to.</p><p>In this blog post, we’ll show you a bug that slips through the cracks today, explain why it’s nobody’s fault, and show you how we diff API behaviors to catch bugs (while generating API specs along the way!!).</p><h2>😱 The Bug That (Almost) Ruined A Friendship</h2><p>You work on a web app. Your co-worker and #bff, Aki, opened a pull request to remove a property from a commonly used API. You two are tight, so you jump in to review the PR. Aki’s PR includes a link showing that this property’s usage has been removed from the website, once again proving why Aki is your #bff. You +1 the PR enthusiastically, with both of you believing that Aki has taken into account all of the dependencies.</p><p>The change gets merged. Then: BETRAYAL! There are now outages in the mobile clients, and it’s late nights for you and Aki. (Good thing you started out liking Aki so much.) It turns out that Aki’s API had been so good that the mobile APIs had started depending on the removed property, unbeknownst to both you and Aki.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_28487"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808423195-A8MEAE1ZSLUSRTJU27ZN/ke17ZwdGBToddI8pDm48kPpDoBIqNy1gGho9uyT02YZZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVFMU71HPrJlrka-k3KdzI_LQkclrYoo_JyQT2V0r-tdNKQvevUbj177dmcMs1F0H-0/been_here_all_along.gif" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808423195-A8MEAE1ZSLUSRTJU27ZN/ke17ZwdGBToddI8pDm48kPpDoBIqNy1gGho9uyT02YZZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVFMU71HPrJlrka-k3KdzI_LQkclrYoo_JyQT2V0r-tdNKQvevUbj177dmcMs1F0H-0/been_here_all_along.gif" data-image-dimensions="474x264" data-image-focal-point="0.5,0.5" alt="been_here_all_along.gif" data-load="false" data-image-id="5f6a65d7f2a96f4111ce87f9" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1600807765736_28787"><div><p>If this has happened to you, you are not to blame! These bugs are nasty little things: they have to do with dependencies on the code you wrote rather than the code itself, making it hard to catch them through source diffs. In fact, as soon as bugs cross API boundaries, they become extremely hard to catch because they elude common testing and software analysis (think static and dynamic analysis) solutions. Developers are left to rely on documentation, word of mouth, and crossing their fingers really hard.<br></p><h2>⚡️ Find Bugs by Diffing API Behavior</h2><p>To catch these kinds of bugs, you can simply install Akita to analyze API-impacting changes whenever you make a pull request. Here is an example of a comment that Akita leaves on your pull request:</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_41128"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808581307-IQU2P2L5CSD49CHWB6S5/ke17ZwdGBToddI8pDm48kBSisbFCmLfsWG8TQUrClwdZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw0ownQnmw6tXIbWXwOqlr6LyiEFX92T0WhozlucCNM3TxLzdvFHSK_CwcKuIV17xo/Screen+Shot+2020-09-21+at+9.38.25+PM.png" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808581307-IQU2P2L5CSD49CHWB6S5/ke17ZwdGBToddI8pDm48kBSisbFCmLfsWG8TQUrClwdZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw0ownQnmw6tXIbWXwOqlr6LyiEFX92T0WhozlucCNM3TxLzdvFHSK_CwcKuIV17xo/Screen+Shot+2020-09-21+at+9.38.25+PM.png" data-image-dimensions="720x395" data-image-focal-point="0.5,0.5" alt="Screen Shot 2020-09-21 at 9.38.25 PM.png" data-load="false" data-image-id="5f6a668349ec6e5af6d59390" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808581307-IQU2P2L5CSD49CHWB6S5/ke17ZwdGBToddI8pDm48kBSisbFCmLfsWG8TQUrClwdZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw0ownQnmw6tXIbWXwOqlr6LyiEFX92T0WhozlucCNM3TxLzdvFHSK_CwcKuIV17xo/Screen+Shot+2020-09-21+at+9.38.25+PM.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1600809689527_10094"><div><p>Akita enables you to compare API behavior across pull requests, multiple environments (ex production vs. test), and even pre-existing specs. If Aki had used Akita to compare the API behavior between their production and test environments, they surely would have caught the bug and saved you some late-night debugging!</p><p>By diffing on <em>behaviors</em> rather than code, Akita summarizes how a pull request changes your API, including: </p><ul data-rte-list="default"><li><p>Endpoints added that expose new functionality</p></li><li><p>Removed/changed parameters that may break existing clients</p></li><li><p>Simple type changes (ex string to int) that can pollute data pipelines</p></li><li><p>Complex type changes (ex phone number to datetime) that can break dependencies</p></li><li><p>👷🏻‍♀️ Coming soon: Impacted Clients, telling you exactly which mobile and web clients are affected</p></li></ul><p>Just this week, we released our GitHub integration to deliver insight on every pull request. You can now use Akita without changing any code or config files, without having to proxy—and as part of your normal developer workflow.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_62396"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808791165-T8ZNZ149RXNZXQIEMEZB/ke17ZwdGBToddI8pDm48kBPgYHUT9ThyHW4rLCUoJ0lZw-zPPgdn4jUwVcJE1ZvWhcwhEtWJXoshNdA9f1qD7Tj1Xq8fmJm7kR3YKhl46-gnroPAwW8vDSy_xFKJA22nuObuMlYXtIhhFNrGCmVLcw/taylor_swift_magic.gif" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808791165-T8ZNZ149RXNZXQIEMEZB/ke17ZwdGBToddI8pDm48kBPgYHUT9ThyHW4rLCUoJ0lZw-zPPgdn4jUwVcJE1ZvWhcwhEtWJXoshNdA9f1qD7Tj1Xq8fmJm7kR3YKhl46-gnroPAwW8vDSy_xFKJA22nuObuMlYXtIhhFNrGCmVLcw/taylor_swift_magic.gif" data-image-dimensions="268x300" data-image-focal-point="0.5,0.5" alt="taylor_swift_magic.gif" data-load="false" data-image-id="5f6a6753a4d7ab439775c1b5" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1600807765736_62695"><div><p>Akita does this by showing you <em>semantic diffs</em> between <em>implicit API contracts</em>. In other words, this means Akita does some magic to figure out what your API normally does and then diffs on that, instead of diffing syntactically on the source code. Diffing on observed behaviors, rather than the code itself or a static graph of what APIs could call each other, allows Akita’s reports to pinpoint potential issues much more precisely. Akita is flexible enough to run in both CI, as shown here, and in production environments!<br></p><h2>⚒ Under the Hood: Advanced API Analysis</h2><p>You may be wondering how we do this, since no existing linter, static analysis, or dynamic analysis gets anywhere close. Under the hood, Akita analyzes API traffic to build a model of your API from scratch. As a foundation, Akita builds an API spec for your API. Here we show an OpenAPI spec that Akita automatically generated:</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_66982"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808826794-Z8US2E3IAXUHH1IPLX75/ke17ZwdGBToddI8pDm48kEE5BRPVFdSFNJ9EWVjnQtJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzBAkhzHXcTUkbSFC51ULQzwiZaehtpr50pAWHTpVqK6r115xOpNXu01MbofqMIiwU/ezgif.com-gif-maker.gif" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808826794-Z8US2E3IAXUHH1IPLX75/ke17ZwdGBToddI8pDm48kEE5BRPVFdSFNJ9EWVjnQtJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzBAkhzHXcTUkbSFC51ULQzwiZaehtpr50pAWHTpVqK6r115xOpNXu01MbofqMIiwU/ezgif.com-gif-maker.gif" data-image-dimensions="600x393" data-image-focal-point="0.5,0.5" alt="ezgif.com-gif-maker.gif" data-load="false" data-image-id="5f6a6778d23bb34052efba66" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1600807765736_67281"><p>The next step is where the magic comes from. Akita uses advanced programming languages technology to detect not just the basic spec properties, but also <em>implicit API contracts</em>, for instance, specific types like datetime, email, phone number, and more:</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_71700"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808865413-FUBU45W5T149VXEPJIJN/ke17ZwdGBToddI8pDm48kBB-NYNLcJUiltQ1hCaI3rZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwA8pzCiywVt9LP9piA_ClOHaEOe2M-jmaR_vLVWBls2AqnUAfoiU4gMfB4Brxcm5Y/ezgif.com-video-to-gif+%283%29.gif" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600808865413-FUBU45W5T149VXEPJIJN/ke17ZwdGBToddI8pDm48kBB-NYNLcJUiltQ1hCaI3rZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwA8pzCiywVt9LP9piA_ClOHaEOe2M-jmaR_vLVWBls2AqnUAfoiU4gMfB4Brxcm5Y/ezgif.com-video-to-gif+%283%29.gif" data-image-dimensions="600x454" data-image-focal-point="0.5,0.5" alt="ezgif.com-video-to-gif (3).gif" data-load="false" data-image-id="5f6a679ca354713d8a568518" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1600807765736_71999"><p>All you have to do is set up Akita to watch your API traffic. No code changes and no proxying necessary:</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_76619"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600809028362-FJSJ12A8EEIKI2PM9EEB/ke17ZwdGBToddI8pDm48kMCbG34QsIB0ipg6r18MvYZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyi5tdW1COKT9Y4DK_rBTv9Q6FtHNTZjOfSYj3sGOmcYpFc0Nb-CneOPLcghlQYm6U/ezgif.com-video-to-gif+%281%29+%281%29.gif" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600809028362-FJSJ12A8EEIKI2PM9EEB/ke17ZwdGBToddI8pDm48kMCbG34QsIB0ipg6r18MvYZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyi5tdW1COKT9Y4DK_rBTv9Q6FtHNTZjOfSYj3sGOmcYpFc0Nb-CneOPLcghlQYm6U/ezgif.com-video-to-gif+%281%29+%281%29.gif" data-image-dimensions="600x476" data-image-focal-point="0.5,0.5" alt="ezgif.com-video-to-gif (1) (1).gif" data-load="false" data-image-id="5f6a684260fe9502d3149c18" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1600807765736_76918"><div><h2>👀 What’s Next?&nbsp;</h2><p>We’ve just released the spec viewer and GitHub integration in our private beta and would love to have you try out our spec generation and/or semantic API behavior diffs. Here’s how you can help:</p><ul data-rte-list="default"><li><p><a href="https://www.akitasoftware.com/get-invite?utm_campaign=2020_pre_launch&amp;utm_medium=blog&amp;utm_source=2020_09_22_product_update"><strong>Sign up for our private beta</strong></a><strong> if you’re interested in trying things out!</strong></p></li><li><p>You may also be interested in <a href="https://www.youtube.com/watch?utm_campaign=2020_pre_launch&amp;utm_medium=blog&amp;v=uYA4DsuMrg8%3Futm_source%3D2020_09_22_product_update">this talk and demo</a> we gave at the API Specs Conference last week.</p></li><li><p>We’re also constantly trying to make our tool better! If change analysis is an issue for you, please fill out <a href="https://akitasoftware.typeform.com/to/iAbs1tB5?utm_campaign=2020_pre_launch&amp;utm_medium=blog&amp;utm_source=2020_09_22_product_update">this survey</a>—with an opportunity to win a $50 Amazon gift card.</p></li><li><p>Spread the word about us! We’d love all the feedback we can get!</p></li></ul></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1600807765736_188572"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600809657904-GN3J9FTFCFFTRWQ9D4I2/ke17ZwdGBToddI8pDm48kEVUGKfLWnr3FrBNrBvkNl1Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHHv_BoFYeKfPNyfF1FiKR-vDFZ_sNHqsWdE5x3HFER1Bur-lC0WofN0YB1wFg-ZW0/next_chapter.gif" data-image="https://images.squarespace-cdn.com/content/v1/5b6f6c558ab722caa37858bf/1600809657904-GN3J9FTFCFFTRWQ9D4I2/ke17ZwdGBToddI8pDm48kEVUGKfLWnr3FrBNrBvkNl1Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHHv_BoFYeKfPNyfF1FiKR-vDFZ_sNHqsWdE5x3HFER1Bur-lC0WofN0YB1wFg-ZW0/next_chapter.gif" data-image-dimensions="478x266" data-image-focal-point="0.5,0.5" alt="next_chapter.gif" data-load="false" data-image-id="5f6a6aa4f2a96f4111cf7183" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div></div></div>

    

    

    <section id="comments-5f6a6536fa06431194e74792">
      
  


    </section>

  </article>





  <nav>

    

    
      <a href="https://www.akitasoftware.com/blog/2020/7/29/the-secret-to-building-better-software-get-in-control-of-your-apis">
        <div>
          <p>Next</p>
          <h4>The secret to building better software? Get in control of your APIs</h4>
          <div>
            <!--

            Categories

            --><p><span>Updates, Approach</span></p><!--

            Author

            --><p><span>Jean Yang</span></p><!--

            Date

            --><p><time datetime="2020-07-30">July 30, 2020</time>
          </p></div>
        </div><!--
        --><svg viewBox="0 0 23 48">
          <g>
            <polyline fill="none" stroke-miterlimit="10" points="1.5,45.7 20.4,23.5 1.5,1.3 "></polyline>
          </g>
        </svg>
      </a>
    

  </nav>
              </section>
            
          </main>

        </div></div>]]>
            </description>
            <link>https://www.akitasoftware.com/blog/2020/9/22/faster-better-earlier-catch-breaking-changes-by-diffing-api-behavior</link>
            <guid isPermaLink="false">hacker-news-small-sites-24561119</guid>
            <pubDate>Tue, 22 Sep 2020 23:13:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Older people have become younger]]>
            </title>
            <description>
<![CDATA[
Score 157 | Comments 126 (<a href="https://news.ycombinator.com/item?id=24560595">thread link</a>) | @betocmn
<br/>
September 22, 2020 | https://www.jyu.fi/en/current/archive/2020/09/older-people-have-become-younger-physical-and-cognitive-function-have-improved-meaningfully-in-30-years | <a href="https://web.archive.org/web/*/https://www.jyu.fi/en/current/archive/2020/09/older-people-have-become-younger-physical-and-cognitive-function-have-improved-meaningfully-in-30-years">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content-core">
  <div>

    <div id="parent-fieldname-text"><p><span lang="EN-GB" xml:lang="EN-GB">“Performance-based measurements describe how older people manage in their daily life, and at the same time, the measurements reflect one’s functional age,” says the principal investigator of the study, Professor <strong>Taina Rantanen.</strong></span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">Among men and women between the ages of 75 and 80, muscle strength, walking speed, reaction speed, verbal fluency, reasoning and working memory are nowadays significantly better than they were in people at the same age born earlier. In lung function tests, however, differences between cohorts were not observed. </span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">“Higher physical activity and increased body size explained the better walking speed and muscle strength among the later-born cohort,” says doctoral student <strong>Kaisa Koivunen</strong>, “whereas the most important underlying factor behind the cohort differences in cognitive performance was longer education.” </span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">Postdoctoral researcher <strong>Matti Munukka</strong> continues: “The cohort of 75- and 80-year-olds born later has grown up and lived in a different world than did their counterparts born three decades ago. There have been many favourable changes. These include better nutrition and hygiene, improvements in health care and the school system, better accessibility to education and improved working life.”</span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">The results suggest that increased life expectancy is accompanied by an increased number of years lived with good functional ability in later life. The observation can be explained by slower rate-of-change with increasing age, a higher lifetime maximum in physical performance, or a combination of the two.</span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">“This research is unique because there are only a few studies in the world that have compared performance-based maximum measures between people of the same age in different historical times,” says Rantanen.</span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">“The results suggest that our understanding of older age is old-fashioned. From an aging researcher’s point of view, more years are added to midlife, and not so much to the utmost end of life. Increased life expectancy provides us with more non-disabled years, but at the same time, the last years of life comes at higher and higher ages, increasing the need for care. Among the ageing population, two simultaneous changes are happening: continuation of healthy years to higher ages and an increased number of very old people who need external care.”</span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">The study was conducted at the Faculty of Sport and Health Sciences and Gerontology Research Center at University of Jyväskylä, Finland. The first cohort data were collected between 1989 and 1990 and consisted of 500 people born between 1910 and 1914. The second cohort data were collected from 2017 to 2018 and comprised 726 people born in 1938 or 1939 and 1942 or 1943. In both cohorts, the participants were assessed at the age 75 or 80 years. Participants were recruited from the Digital and Population Data Services Agency. The study was funded by the Academy of Finland and the European Research Council.</span></p>
<h4><span lang="EN-GB" xml:lang="EN-GB">Original publications:</span></h4>
<p><span lang="EN-GB" xml:lang="EN-GB">Koivunen, K, Sillanpää, E, Munukka, M, Portegijs, E, Rantanen, T. Cohort differences in maximal physical performance: a comparison of 75- and 80-year-old men and women born 28 years apart. J Gerontol A Biol Sci Med Sci. 2020. <a href="https://academic.oup.com/biomedgerontology/advance-article/doi/10.1093/gerona/glaa224/5901594" target="_blank" rel="noopener" data-linktype="external" data-val="https://academic.oup.com/biomedgerontology/advance-article/doi/10.1093/gerona/glaa224/5901594">https://doi.org/10.1093/gerona/glaa224.</a></span></p>
<p><span lang="EN-GB" xml:lang="EN-GB">Munukka M, Koivunen K, von Bonsdorff M, Sipilä S, Portegijs, E, Ruoppila I, Rantanen T. Birth cohort differences in cognitive performance in 75- and 80-Year-Olds - A comparison of two cohorts over 28 years. Aging Clin Exp Res. 2020. <a href="https://link.springer.com/article/10.1007/s40520-020-01702-0" target="_blank" rel="noopener" data-linktype="external" data-val="https://link.springer.com/article/10.1007/s40520-020-01702-0">https://doi.org/10.1007/s40520-020-01702-0</a>.</span></p>
<h4><span lang="EN-GB" xml:lang="EN-GB">Contact information: </span></h4>
<ul>
<li><span lang="EN-GB" xml:lang="EN-GB">Doctoral student Kaisa Koivunen, kaisa.m.koivunen@jyu.fi , +358503608818</span></li>
<li><span lang="EN-GB" xml:lang="EN-GB">Post doctoral researcher Matti Munukka, matti.munukka@jyu.fi, +358408053606</span></li>
<li>Professor Taina Rantanen, taina.rantanen@jyu.fi, +358408053590</li>
</ul></div>

    
  


  </div>
</div></div>]]>
            </description>
            <link>https://www.jyu.fi/en/current/archive/2020/09/older-people-have-become-younger-physical-and-cognitive-function-have-improved-meaningfully-in-30-years</link>
            <guid isPermaLink="false">hacker-news-small-sites-24560595</guid>
            <pubDate>Tue, 22 Sep 2020 22:12:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Absent aid, few US cities will be able to avoid austerity]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 59 (<a href="https://news.ycombinator.com/item?id=24559756">thread link</a>) | @walterbell
<br/>
September 22, 2020 | https://citymonitor.ai/government/absent-aid-few-us-cities-will-be-able-to-avoid-austerity | <a href="https://web.archive.org/web/*/https://citymonitor.ai/government/absent-aid-few-us-cities-will-be-able-to-avoid-austerity">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<p><span>Austin City Council member Jimmy Flannigan knows his city is lucky.&nbsp;</span>
</p><p><span>The fourth-largest city in Texas suffered a nasty shock at the beginning of the Covid-19 pandemic when Mayor Steve Adler canceled its signature event, South by Southwest.&nbsp; </span><span>The annual ten-day extravaganza draws luminaries from the entertainment, tech and media industries and </span><a href="https://explore.sxsw.com/hubfs/2019%20SXSW%20Economic%20Impact%20Analysis%20-%2011.18.19%20OPT.pdf"><span>brings over $350m</span></a><span> to Austin’s economy.</span>
</p><figure id="attachment_638" aria-describedby="caption-attachment-638"><img src="https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-1226093815-1.jpg" alt="A view of downtown Austin, Texas. " width="1024" height="683" srcset="https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-1226093815-1.jpg 1024w, https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-1226093815-1-768x512.jpg 768w, https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-1226093815-1-600x400.jpg 600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-638"><span>Austin’s economy is doing better than most US cities since the outbreak of Covid-19. (Photo by Tom Pennington/Getty Images)</span></figcaption></figure>
<p><span>Flannigan is the former head of the city’s LGBT Chamber of Commerce, and he immediately rallied venue and bar owners to discuss the economic fallout. But the loss of the conference, and its tens of thousands of attendees, was quickly eclipsed by, well, everything else.&nbsp;</span>											</p>
									 
			
<p><span>“There was the cancel South by stage, where we all thought, ‘This is going to suck, but it’s not coming here,’” says Flannigan. “Then very quickly, you realize that this is a globally interconnected city and Southy by is not the problem. In a short period of time, it became clear that this was going to be shutting down a lot more than music venues.”</span>
</p><p><span>The cancellation of South By Southwest was one of those moments, like the National Basketball Association’s decision to pause its season, when Americans started realizing the pandemic wasn’t just happening somewhere else. But in the months since, Austin is looking like one of the luckiest urban areas in the United States. Unemployment in the city <a href="https://www.statesman.com/business/20200918/austin-jobless-rate-keeps-improving-down-to-55">fell to 5.5% in August</a> – far lower even than other wealthy US cities such as San Francisco or New York. After a 20% year-over-year dive, sales tax revenues are recovering (although they have not yet returned to pre-pandemic levels). Austin is one of the only major cities in Texas that hasn’t furloughed or laid off public-sector workers thanks, in part, to the city’s approach to addressing another crisis roiling the US: Black Lives Matter and the demand for police reform.&nbsp;</span>
</p><p><span>But despite the relatively sunny outlook, Austin faces the same daunting dilemmas as do other cities across the United States.</span>
</p><p><span>“I think most cities are basically going to be broke for the foreseeable future,” says Bill Fulton, director of the Kinder Institute for Urban Research at Rice University in Houston. “I’ve been the mayor of a city, and if you lose 10 to 15 percent of revenue, it is impossible to maneuver or do anything. In a situation like this, you see a rapid drop in revenue and very slow restoration. And it’s impossible to know what business travel and the tourist economy is going to do in the years ahead.”</span>
</p><p><span>To make matters worse, for many cities, the fallout from the Great Recession never ended. Although the Obama administration’s stimulus package in 2009 provided aid to state and local governments, it ran dry in 2011. Revenues continued falling for years, only bottoming out in 2014.</span>
</p><p><span>Economists such as Amanda Page-Hoongrajok, an assistant professor of economics and finance at Saint Peter’s University, argue that austerity at the local and state levels fully offset the federal stimulus during the Great Recession. That’s part of why the recovery took so long and was so uneven: in cities and regions that weren’t doing as well, those public-sector jobs never fully came back. That meant a lot of private-sector jobs that relied on them died too.&nbsp;</span>
</p><p><span>“Pretty much any spending cuts at the state and local level are going to be a huge drag on the overall economic recovery,” says Page-Hoongrajok. “If state and local governments are condensing their balance sheets, they’re laying people off. They’re cutting spending on equipment. Capital spending is cut early during downturns. All of that is sucking demand right out of the economy.”</span>
</p><figure id="attachment_642" aria-describedby="caption-attachment-642"><img src="https://ind-dev-newstatesman-b2b.pantheonsite.io/citymonitor/wp-content/uploads/sites/18/2020/09/GettyImages-112145276.jpg" alt="" width="1024" height="683" srcset="https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-112145276.jpg 1024w, https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-112145276-768x512.jpg 768w, https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-112145276-600x400.jpg 600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-642"><span>Thousands of union members from around Michigan rally at the State Capitol to protest Governor Rick Snyder’s proposed budget cuts on 13 April, 2011. (Photo by Bill Pugliano/Getty Images)</span></figcaption></figure>
<p><span>Compared with many other rich nations, America governs itself with a relatively messy form of federalism. The powers of federal, state and local government all overlap somewhat, which means the higher branches can sometimes leave the lower ones to fend for themselves. Unless Congress steps in, it’s up to state and local governments to balance their own ledgers. This time, despite the massive stimulus efforts put forward in Washington, the only lifeline was a $150bn cut of the $2.2trn CARES Act, which then had to be split between states, cities and tribal governments. The funds could only be spent on efforts to ameliorate the pandemic.&nbsp;</span>
</p><p><span>That did nothing for the Covid-size hole blown into municipal budgets by skyrocketing unemployment, declining sales taxes and the overnight evisceration of revenue derived from pandemic-vulnerable sources like hotel and liquor taxes. As the pandemic rages, some governmental duties require less input, but public responsibilities like social services grow more costly.</span>
</p><p><span>Without further federal aid, this will pose a problem for many localities. Left to their own devices, the layers of American government below the federal level have a limited toolkit. States cannot print their own money, and many have balanced-budget requirements. Even those that don’t have explicit strictures about the need for current revenues to cover expenditures are afraid to get too creative with their financing for fear of making the costs of borrowing more expensive.&nbsp;</span>
</p><p><span>City governments, meanwhile, live at the whim of their states and face even tighter restrictions. In cases like Austin, where state governments are of a different ideological persuasion than municipal politicians, the ability of liberal local leaders to raise taxes can be limited by conservative elected officials in the statehouse. Their borrowing capabilities are even more tightly constrained than those of the states because their tax bases are much smaller and predecessor city governments have </span><a href="https://www.citymetric.com/politics/pandemic-economy-could-revive-one-america-s-ugliest-fights-over-local-control-5147" target="_blank" rel="noopener noreferrer"><span>suffered fiscal crises</span></a><span> after being disciplined by the bond markets.&nbsp;</span>
</p><p><span>That just leaves current revenues on the table, which are looking precarious. The mainstay of municipal finance, property taxes, mostly haven’t been paid yet this year. But the National Association of Counties reports that 27% of respondents to their membership polling have already experienced a lag in collection or a decline in revenue from this usually reliable source. Experts fear that property taxes won’t bring in what they once did in the medium term, either. What owner of a mostly empty commercial office building won’t be challenging their assessment?&nbsp;</span>
</p><p><span>In </span><a href="https://www.nlc.org/sites/default/files/users/user57221/City_Fiscal_Conditions_2020_FINAL.pdf" target="_blank" rel="noopener noreferrer"><span>a mid-August report</span></a><span>, the National League of Cities outlined the dire reality facing its membership. Almost 90% of cities anticipated being less able to meet their needs in fiscal year 2021, while the hundreds surveyed anticipated an average 13% decline in general fund revenues. </span><a href="https://covid19.nlc.org/wp-content/uploads/2020/06/What-Covid-19-Means-For-City-Finances_Report-Final.pdf" target="_blank" rel="noopener noreferrer"><span>A third of the nation’s</span></a><span> 3 million city workers are expected to be furloughed, laid off or required to serve on reduced pay. In April alone, almost 800,000 jobs in local governments were lost. Employment in the sector has been growing again since June, but it lags overall job growth.&nbsp;</span>
</p><p><span>All of this has been a nasty shock for a sector that only just recovered from the last (supposedly) once-in-a-generation crisis a little over a decade ago. Page-Hoongrajok says inflation-adjusted state and local government spending did not return to pre-Great Recession levels until 2019. Public-sector employment has fallen back to the same level it was in the 1990s, when the US population was roughly 60 million people fewer than it is today. Many key services were never restored to pre-Great Recession levels: infrastructure spending is at </span><a href="https://www.cbpp.org/research/state-budget-and-tax/its-time-for-states-to-invest-in-infrastructure" target="_blank" rel="noopener noreferrer"><span>a 70-year low point</span></a><span>, while capital spending on schools is </span><a href="https://www.cbpp.org/blog/with-federal-funding-unreliable-states-must-invest-in-school-buildings"><span>below what it was in 2008.</span></a>
</p><figure id="attachment_644" aria-describedby="caption-attachment-644"><img src="https://ind-dev-newstatesman-b2b.pantheonsite.io/citymonitor/wp-content/uploads/sites/18/2020/09/GettyImages-80923121.jpg" alt="" width="1024" height="683" srcset="https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-80923121.jpg 1024w, https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-80923121-768x512.jpg 768w, https://citymonitor.ai/wp-content/uploads/sites/18/2020/09/GettyImages-80923121-600x400.jpg 600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption id="caption-attachment-644"><span>A foreclosure sign sits in front of a home for sale on 29 April, 2008, in Stockton, California. (Photo by Justin Sullivan/Getty Images)</span></figcaption></figure>
<p><span>Fulton remembers the pain of the Great Recession well. In the late 2000s, he served as mayor of Ventura, California, a city of just over 100,000 people midway between Santa Barbara and Los Angeles.</span>
</p><p><span>Ventura’s experience during the Great Recession was harrowing: Fulton recalls the city’s tax revenues plunging almost 15% overnight. It took years to restore, despite the city’s relative affluence – today Ventura’s median income is almost $13,000 higher than the national average.&nbsp;&nbsp;&nbsp;</span>
</p><p><span>The downturn turned the position of mayor into a depressing bookkeeping exercise. Instead of cutting ribbons at new parks and infrastructure projects, Fulton spent his days cutting budgets and, more painfully, staff. Austerity proved even more painful elsewhere in the state. Cities across California’s Central Valley, which has not prospered alongside the coastal regions, eliminated their planning departments and never replaced them.</span>
</p><p><span>In Fulton’s estimation, there is an order to how local politicians implement austerity measures. First, they put a halt to capital projects and, then, services. The </span><a href="https://www.naco.org/sites/default/files/documents/Analysis-of-COVID-19s-Impact-on-County-Finances-and-Implications-for-the-US-Economy.pdf" target="_blank" rel="noopener noreferrer"><span>National Association of Counties found</span></a><span> that 71% of its members have already cut back on capital spending and 68% on services. Then non-personnel spending goes. After that, staff is cut through attrition as job vacancies open up but go unfilled.&nbsp;</span>
</p><p><span>“The second-to-last resort is that they cut staff, they lay people off,” says Fulton, “and then the last resort, when everything else fails, they actually start to rethink what they do.”&nbsp;</span>
</p><p><span>Fulton says that an emergency of this magnitude will force cities to make hard decisions. But it can also provide an opportunity to change long-established ways of operating that have proved resistant to crusading councilmembers or activist campaigns.&nbsp;</span>
</p><p><span>In Austin, easily the most left-wing city in Texas, the city government patched holes in municipal finances and funded needed city services by redistributing some of the police department’s budget. In the midst of the Black Lives Matter protests this summer, activists demanded moving funds – and responsibilities – away from …</span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://citymonitor.ai/government/absent-aid-few-us-cities-will-be-able-to-avoid-austerity">https://citymonitor.ai/government/absent-aid-few-us-cities-will-be-able-to-avoid-austerity</a></em></p>]]>
            </description>
            <link>https://citymonitor.ai/government/absent-aid-few-us-cities-will-be-able-to-avoid-austerity</link>
            <guid isPermaLink="false">hacker-news-small-sites-24559756</guid>
            <pubDate>Tue, 22 Sep 2020 20:53:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crab – an interpreter for a tiny subset of Logo]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24559333">thread link</a>) | @beagle3
<br/>
September 22, 2020 | http://beyondloom.com/crab | <a href="https://web.archive.org/web/*/http://beyondloom.com/crab">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://beyondloom.com/crab</link>
            <guid isPermaLink="false">hacker-news-small-sites-24559333</guid>
            <pubDate>Tue, 22 Sep 2020 20:17:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Era of Visual Studio Code]]>
            </title>
            <description>
<![CDATA[
Score 603 | Comments 656 (<a href="https://news.ycombinator.com/item?id=24558788">thread link</a>) | @robenkleene
<br/>
September 22, 2020 | https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/ | <a href="https://web.archive.org/web/*/https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <section id="post">
          <article>
            <header>
              <time datetime="">Monday, Sep 21, 2020</time>
              
            </header>

<p><img src="https://blog.robenkleene.com/assets/2020-09-20-vs-code-hero.png" alt="VS Code Hero"></p>

<p>The most important thing I look for when choosing which tools to use is longevity. Learning software is an investment, and if you have to switch to another application later, you lose some of that investment.</p>

<p>In most software categories, choosing the software with longevity is easy, the most popular tools <a href="https://blog.robenkleene.com/2020/04/27/software-to-die-for/">are usually the ones that have been around the longest</a>. <a href="https://www.microsoft.com/en-us/microsoft-365/excel">Microsoft Excel</a> and <a href="https://www.adobe.com/products/illustrator.html">Adobe Illustrator</a> were both released in 1987 and, for the most part, they’ve remained the most popular software in their categories since then.</p>

<p>Text editors, on the other hand, are a software category where the most popular options are not the oldest. According to the <a href="https://insights.stackoverflow.com/survey/">Stack Overflow Annual Developer Survey</a>, <a href="https://www.sublimetext.com/">Sublime Text</a> was the most popular text editor available on the Mac from 2015–2017. Sublime Text was released in 2008, a sprightly youth compared to Excel and Illustrator. Text editors have been a category with a lot of movement: In the last 20 years, TextMate, Sublime Text, and Atom have all been the text editor with the most momentum<sup id="fnref:whytexteditorshavechurn" role="doc-noteref"><a href="#fn:whytexteditorshavechurn">1</a></sup>. For big complicated desktop software, has any other category ever had so much movement?</p>

<p>I believe the era of new text editors emerging and quickly becoming popular has now ended with <a href="https://code.visualstudio.com/">Visual Studio Code</a>. VS Code has reached unprecedented levels of popularity and refinement, laying a foundation that could mean decades of market dominance. If, like me, one of your priorities for your tools is longevity<sup id="fnref:whynotemacsandvim" role="doc-noteref"><a href="#fn:whynotemacsandvim">2</a></sup>, than that means VS Code might be a great text editor to invest in learning today.</p>

<p>The case for VS Code’s longevity comes from several points we’ll cover in this piece:</p>

<ol>
  <li><a href="#popularity"><strong>Popularity</strong></a>: It’s crossed a popularity threshold that no earlier text editor in recent history has crossed.</li>
  <li><a href="#the-text-editor-as-platform"><strong>The Text Editor as Platform</strong></a>: It’s the endgame of a revolution that saw text editors be remade around extensions.</li>
  <li><a href="https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/paradigm-transcendence"><strong>Paradigm Transcendence</strong></a>: It’s transcended its paradigm as a desktop app by becoming a hosted web app, and even a reference implementation.</li>
  <li><a href="https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/company-management"><strong>Company Management</strong></a>: It’s managed by a powerful tech company, and it’s being developed <em>aggressively</em>.</li>
</ol>

<h2 id="popularity">Popularity</h2>

<p>VS Code is the most popular text editor today. It’s so popular, that it could be the most popular <a href="https://en.wikipedia.org/wiki/Graphical_user_interface">GUI</a> programming tool of all time. <a href="https://insights.stackoverflow.com/survey/2015#tech-editor">Since 2015</a>, Stack Overflow has included questions about text editors in their survey<sup id="fnref:stackoverflowremovedthetexteditorquestions" role="doc-noteref"><a href="#fn:stackoverflowremovedthetexteditorquestions">3</a></sup>. Back then <a href="https://notepad-plus-plus.org/">Notepad++</a> was the most popular text editor, with 34.7% of respondents saying they were “likely to use it”. In the following years, the popularities of different text editors moved around a bit, but nothing ever broke the 40% mark. That is, until its <a href="https://insights.stackoverflow.com/survey/2019#development-environments-and-tools">most recent poll in 2019</a>, when VS Code jumped to 50.7%. This was the second year in a row that VS Code increased by ~45%, this time jumping from <a href="https://insights.stackoverflow.com/survey/2019#development-environments-and-tools">34.9% in 2018</a>, where <em>it had already been the most popular</em>.</p>

<h3 id="text-editor-popularity-20152019">Text Editor Popularity 2015–2019</h3>

<p><img src="https://blog.robenkleene.com/assets/2020-09-20-text-editor-popularity.png" alt="Text Editor Popularity"></p>

<p>(Note that Stackoverflow started allowing multiple answers between 2015 and 2016, so I’d take the changes between those two years in particular with a grain of salt.)</p>

<h2 id="the-text-editor-as-platform">The Text Editor as Platform</h2>

<p>So VS Code is objectively wildly popular; the next point we’re going to look at is more qualitative. For the past couple of decades, text editors have been on a trajectory that I believe VS Code is the final representation of. This is the progression of text editors becoming platforms in their own right by increasing the role and capabilities of extensions. What follows is the history of this progression<sup id="fnref:texteditorprogessionpreviously" role="doc-noteref"><a href="#fn:texteditorprogessionpreviously">4</a></sup>.</p>

<h3 id="pre-2004-bbedit-emacs-and-vim">Pre-2004: BBEdit, Emacs, and Vim</h3>

<p><a href="https://www.barebones.com/products/bbedit/index.html">BBEdit</a>, <a href="https://www.gnu.org/software/emacs/">Emacs</a>, and <a href="https://www.vim.org/">Vim</a> are all great text editors in their own right, but they all have idiosyncrasies that (while beloved by people like me) prevent them from ever being the <em>most popular</em> text editor.</p>

<p>Emacs, and Vim’s predecessor Vi, were both first released in 1976, before many of todays user-interface conventions were solidified. Predating conventions like using a modifier key with <code>Z</code>, <code>X</code>, <code>C</code>, and <code>V</code> for undo, cut, copy, and paste (keyboard shortcuts that were popularized by the <a href="https://en.wikipedia.org/wiki/Macintosh#1978%E2%80%9384:_Development_and_introduction">original Macintosh</a> and <a href="https://en.wikipedia.org/wiki/Windows_1.0">Windows 1.0</a>, released in 1984 and 1985 respectively). Neither Emacs<sup id="fnref:cuamodeinemacs" role="doc-noteref"><a href="#fn:cuamodeinemacs">5</a></sup> or Vim use these keys, and instead use their own terminology. They both use the term “yank” for example (although to mean different things, it’s copy in Vim, and paste in Emacs).</p>

<p>BBEdit was released in 1992, around the time that some of the first GUI tools emerged that would become dynasties. Note the proximity to Excel (1987), Illustrator (1987), and Photoshop (1990). And just like those apps, BBEdit is still relevant today. But unlike those apps, it’s not the most popular in its category, by a wide margin. The reason seems to be at least partially that it never fully adapted to a world where text editors put so much emphasis on package-driven ecosystems.</p>

<h3 id="2004-textmate">2004: TextMate</h3>

<p>TextMate, released in 2004, is arguably the most influential text editor ever. Among the numerous features it popularized are <a href="https://macromates.com/manual/en/snippets">abbreviation-based snippets</a>, <a href="https://macromates.com/manual/en/working_with_text#auto-paired_characters_quotes_etc">automatic paired characters</a>, and <a href="https://macromates.com/manual/en/working_with_multiple_files#moving_between_files_with_grace">fuzzy finding by file name</a>. All of these features became staples in every popular text editor that followed. The implementations of <a href="https://macromates.com/manual/en/scope_selectors">Scope Selectors</a> and <a href="https://macromates.com/manual/en/themes">theming</a> that TextMate pioneered have also formed the basis for themes and syntax highlighting in every subsequent popular text editor.</p>

<p>That’s already a lot to originate from a single app, but it still doesn’t even include TextMate’s most significant innovation; the one that would go on to re-shape text editors, solidify niche status for every text editor that came before it, and pave the way for VS Code to become the most popular text editor in history a decade later. TextMate’s most important innovation was that it was the first popular text editor that was primarily built around <a href="https://en.wikipedia.org/wiki/Plug-in_%28computing%29">extensions</a>.</p>

<p>While TextMate popularized the concept of a text editor built around extensions, in hindsight, it didn’t go far enough. TextMate’s extensions had limitations that later text editors would thrive by removing.</p>

<h3 id="2008-sublime-text">2008: Sublime Text</h3>

<p><a href="https://www.sublimetext.com/">Sublime Text</a>, released in 2008, popularized the minimap and multiple cursors. And unlike TextMate and BBEdit, it’s cross-platform, running on Linux, MacOS, and Windows, which helped it reach a wider audience than those editors. But Sublime Text’s biggest impact was greatly expanding the capabilities of extensions.</p>

<p>Sublime Text’s extensions run in an embedded <a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> runtime with an <a href="https://www.sublimetext.com/docs/3/api_reference.html">extensive API</a>, unlike TextMate which <a href="https://github.com/textmate/bundle-support.tmbundle/blob/13169c6209ebb02564009c4ce573d62452c9b3ae/Support/shared/bin/ruby18">uses the scripting languages built-in to macOS</a>, and <a href="https://macromates.com/manual/en/appendix#plug-in_api">rather than having a proper extension API</a>, mainly centers on processing <a href="https://en.wikipedia.org/wiki/Standard_streams">standard out</a>.</p>

<p>Sublime Text greatly expanded what extensions could do, allowing more sophisticated integrations such as <a href="https://en.wikipedia.org/wiki/Lint_(software)">linters</a> that included GUI components. And <a href="https://packagecontrol.io/news">Package Control</a>, the enormously popular package manager for Sublime Text built by <a href="https://twitter.com/wbond/">Will Bond</a><sup id="fnref:sublimehqhire" role="doc-noteref"><a href="#fn:sublimehqhire">6</a></sup>, features a centralized source for package management, reducing the friction to browse, install, and update packages; a model that all subsequent popular text editors would also adopt.</p>

<p>Even with Sublime Text’s expanded extensions, it still didn’t go far enough. Package Control wasn’t built-in, and, while Sublime Text does have an API, its use of Python with custom calls for GUI components still left room for future text editors to make extensions more accessible to build.</p>

<h3 id="2014-atom">2014: Atom</h3>

<p><a href="https://atom.io/">Atom</a>, released by <a href="https://github.com/">GitHub</a> in 2014, finally brings extensions to their final form. Atom’s package manager is built in<sup id="fnref:textmate2hasabuiltinbundlebrowser" role="doc-noteref"><a href="#fn:textmate2hasabuiltinbundlebrowser">7</a></sup>, displays extension <a href="https://en.wikipedia.org/wiki/README">READMEs</a> complete with inline images (and early extensions made by GitHub themselves popularized the convention of using <a href="https://en.wikipedia.org/wiki/GIF">animated GIFs</a> to illustrate functionality), creating an extension experience reminiscent of an <a href="https://www.apple.com/ios/app-store/">app store</a>.</p>

<p>Then there’s the matter of <a href="https://en.wikipedia.org/wiki/HTML">HTML</a> and <a href="https://en.wikipedia.org/wiki/Cascading_Style_Sheets">CSS</a>. Atom is built on <a href="https://www.electronjs.org/">Electron</a><sup id="fnref:electronwasmadeforatom" role="doc-noteref"><a href="#fn:electronwasmadeforatom">8</a></sup>, which means the editor itself is written in JavaScript and runs on <a href="https://en.wikipedia.org/wiki/Node.js">Node</a><sup id="fnref:atomwaswrittenincoffeescript" role="doc-noteref"><a href="#fn:atomwaswrittenincoffeescript">9</a></sup>. Compared to Sublime Text’s Python API; HTML, CSS, and JavaScript are some of most widely-known languages in existence, which greatly lowers the barrier of entry for creating extensions.</p>

<p>Atom had essentially perfected the extension-based editor, there was just one problem: It’s slow. Performance complaints have plagued Atom since its release, and market ended up split with Sublime Text, which is lightning fast by comparison.</p>

<h3 id="2015-visual-studio-code">2015: Visual Studio Code</h3>

<p>VS Code was released in 2015, based on the <a href="https://microsoft.github.io/monaco-editor/">Monaco Editor</a> that Microsoft had <a href="https://weblogs.asp.net/jongalloway/a-quick-look-at-the-new-visual-studio-online-quot-monaco-quot-code-editor">first released in 2013</a> that could be embedded into websites. When GitHub released Electron along with Atom. Microsoft used it to create a desktop version of the Monaco Editor called Visual Studio Code.</p>

<p>VS Code takes the same formula as Atom<sup id="fnref:vscodeislesshackable" role="doc-noteref"><a href="#fn:vscodeislesshackable">10</a></sup>—a local web-based text editor written in Electron with an emphasis on extensions—and makes it more performant. VS Code makes extensions even more visible, by putting them in the sidebar, raising to the same level as file browsing, searching, source control, and debugging. VS Code extensions can have rich user-interfaces, being written in HTML, CSS, and JavaScript, and with full-access to <a href="https://nodejs.org/en/">Node</a>, they can essentially do anything any other application can do. And indeed, <a href="https://marketplace.visualstudio.com/items?itemName=GrapeCity.gc-excelviewer">some</a> <a href="https://marketplace.visualstudio.com/items?itemName=hediet.vscode-drawio">extensions</a> start to look like apps in and of themselves.</p>

<p>With VS Code, the extension-based text editor has seemingly reached its final form. Ever since TextMate, extensions have increased in prominence and capabilities, and with VS Code, that progression appears to have culminated. There just isn’t anywhere else to go. Correspondingly, there isn’t a way a new text editor can leapfrog VS Code the same way previous text editors have been leapfrogging each other by improving extensions.</p>

<h2 id="paradigm-transcendence">Paradigm Transcendence</h2>

<p>So far we’ve looked at VS Code’s popularity, and its extensions implementation, as indicators of longevity. The third indicator we’ll look at is how VS Code has moved beyond the confines of the desktop. The <a href="https://github.com/cdr/code-server"><code>code-server</code></a> project runs VS Code as a regular web app, in other words, hosted on a server and accessed through the browser. GitHub’s <a href="https://github.com/features/codespaces/">Codespaces</a> also run VS Code as a web app, this time by spinning up an ad hoc development environment.</p>

<p>Transcending a …</p></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/">https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/</a></em></p>]]>
            </description>
            <link>https://blog.robenkleene.com/2020/09/21/the-era-of-visual-studio-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24558788</guid>
            <pubDate>Tue, 22 Sep 2020 19:30:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Porting EBU R128 audio loudness analysis from C to Rust]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24558276">thread link</a>) | @Narishma
<br/>
September 22, 2020 | https://coaxion.net/blog/2020/09/porting-ebu-r128-audio-loudness-analysis-from-c-to-rust/ | <a href="https://web.archive.org/web/*/https://coaxion.net/blog/2020/09/porting-ebu-r128-audio-loudness-analysis-from-c-to-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-760">
	
	<!-- .entry-header -->

	<div>
		<p>Over the last few weeks I ported the <a href="https://github.com/jiixyj/libebur128/"><code>libebur128</code></a> C library to <a href="https://www.rust-lang.org/">Rust</a>, both with a proper Rust API as well as a 100% compatible C API.</p>
<p>This blog post will be split into 4 parts that will be published over the next weeks</p>
<ol>
<li><strong>Overview and motivation</strong></li>
<li>Porting approach with various details, examples and problems I ran into along the way</li>
<li>Performance optimizations</li>
<li>Building Rust code into a C library as drop-in replacement</li>
</ol>
<p>If you’re only interested in the code, that can be found on <a href="https://github.com/sdroege/ebur128">GitHub</a> and in the <code data-enlighter-language="raw">ebur128</code> crate on <a href="https://crates.io/crates/ebur128">crates.io</a>.</p>
<p>The initial versions of the <code data-enlighter-language="raw">ebur128</code> crate was built around the <code data-enlighter-language="raw">libebur128</code> C library (and included its code for ease of building), version 0.1.2 and newer is the pure Rust implementation.</p>


<h2><span id="EBU_R128">EBU R128</span></h2>
<p><a href="https://github.com/jiixyj/libebur128/"><code>libebur128</code></a> implements the <a href="https://www.ebu.ch/">EBU</a> <a href="https://tech.ebu.ch/docs/r/r128.pdf">R128</a> loudness standard. The <a href="https://en.wikipedia.org/wiki/EBU_R_128">Wikipedia</a> page gives a good summary of the standard, but in short it describes how to measure loudness of an audio signal and how to use this for loudness normalization.</p>
<p>While this intuitively doesn’t sound very complicated, there are lots of little details (like how human ears are actually working) that make this not as easy as one might expect. This results in there being many different ways for measuring loudness and is one of the reasons why this standard was introduced. <a href="https://xkcd.com/927/">Of course</a> it is also not the only standard for this.</p>
<p><code data-enlighter-language="raw">libebur128</code> is also the library that I used in the <a href="https://gitlab.freedesktop.org/gstreamer/gst-plugins-rs/-/blob/dc8b722f72a82e5c0819f9f67cc59c7a491d4110/audio/audiofx/src/audioloudnorm.rs">GStreamer loudness normalization plugin</a>, about which I <a href="https://coaxion.net/blog/2020/07/live-loudness-normalization-in-gstreamer-experiences-with-porting-a-c-audio-filter-to-rust/">wrote</a> a few weeks ago already. By porting the underlying loudness measurement code to Rust, the only remaining C dependency of that plugin is <a href="https://gstreamer.freedesktop.org/">GStreamer</a> itself.</p>
<p>Apart from that it is used by <a href="https://ffmpeg.org/">FFmpeg</a>, but they include their own modified copy, as well as many other projects that need some kind of loudness measurement and don’t use <a href="https://en.wikipedia.org/wiki/ReplayGain">ReplayGain</a>, another older but widely used standard for the same problem.</p>
<h2><span id="Why">Why?</span></h2>
<p>Before going over the details of what I did, let me first explain why I did this work at all. <code data-enlighter-language="raw">libebur128</code> is a perfectly well working library, in wide use for a long time and probably rather bug-free at this point and it was already possible to use the C implementation from Rust just fine. That’s what the initial versions of the <code data-enlighter-language="raw">ebur128</code> crate were doing.</p>
<p>My main reason for doing this was simply because it seemed like a fun little project. It isn’t a lot of code that is changing often so once ported it should be more or less finished and it shouldn’t be much work to stay in sync with the C version. I started thinking about doing this <a href="https://github.com/sdroege/ebur128/issues/1">already</a> after the initial release of the C-based <code data-enlighter-language="raw">ebur128</code> release, but after reading <a href="https://jneem.github.io/nnnoiseless/">Joe Neeman’s</a> blog post about porting another C audio library (<a href="https://github.com/xiph/rnnoise">RNNoise</a>) to Rust this gave me the final push to actually start with porting the code and to follow through until it’s done.</p>
<p>However, don’t go around and ask other people to rewrite their projects in Rust (don’t be rude) or think that your own rewrite is magically going to be much faster and less buggy than the existing implementation. While Rust saves you from a big class of possible bugs, it doesn’t save you from yourself and usually rewrites contain bugs that didn’t exist in the original implementation. Also getting good performance in Rust requires, like in every other language, some effort. Before rewriting any software, think about the goals of this rewrite realistically as well as the effort required to actually get it finished.</p>
<p>Apart from fun there were also a few technical and non-technical reasons for me to look into this. I’m going to just list two here (curiosity and portability). I will skip the usual Rust memory-safety argument as that seems less important with this code: the C code is widely used for a long time, not changing a lot and has easy to follow memory access patterns. While it definitely had a memory safety bug (see above), it was rather difficult to trigger and it was fixed in the meantime.</p>
<h3><span id="Curiosity">Curiosity</span></h3>
<p>Personally and at my company <a href="https://www.centricular.com/">Centricular</a> we try to do any new projects where it makes sense in Rust. While this worked very well in the past and we got great results, there were some questions for future projects that I wanted to get some answers, hard data and personal experience for</p>
<ul>
<li>How difficult is it to port a C codebase function by function to Rust while keeping everything working along the way?</li>
<li>How difficult is it to get the same or better performance with idiomatic Rust code for low-level media processing code?</li>
<li>How much bigger or smaller is the resulting code and do Rust’s higher-level concepts like iterators help to keep code concise?</li>
<li>How difficult is it to create a C-compatible library in Rust with the same API and ABI?</li>
</ul>
<p>I have some answers to all these questions already but previous work on this was not well structured and the results were also not documented, which I’m trying to change here now. Both to have a reference for myself in the future as well as for convincing other people that Rust is a reasonable technology choice for such projects.</p>
<p>As you can see the general pattern of these questions are introducing Rust into an existing codebase, replacing existing components with Rust and writing new components in Rust, which is also relates to my work on the Rust <a href="https://gitlab.freedesktop.org/gstreamer/gstreamer-rs">GStreamer bindings</a>.</p>
<h3><span id="Portability">Portability</span></h3>
<p>C is a very old language and while there is a standard, each compiler has its own quirks and each platform different APIs on top of the bare minimum that the C standard defines. C itself is very portable, but it is not easy to write portable C code, especially when not using a library like <a href="https://developer.gnome.org/glib/">GLib</a> that hides these differences and provides basic data structures and algorithms.</p>
<p>This seems to be something that is often forgotten when the portability of C is given as an argument against Rust, and that’s the reason why I wanted to mention this here specifically. While you can get a C compiler basically everywhere, writing C code that also runs well everywhere is another story and C doesn’t make this easy by design. Rust on the other hand makes writing portable code quite easy in my experience.</p>
<p>In practice there were three specific issues I had for this codebase. Most of the advantages of Rust here are because it is a new language and doesn’t have to carry a lot of historical baggage.</p>
<h4>Mathematical Constants and Functions</h4>
<p>Mathematical constants are not actually part of any C standard. While most compilers just define <code data-enlighter-language="raw">M_PI</code> (for π), <code data-enlighter-language="raw">M_E</code> (for 𝖾) and others in <code data-enlighter-language="raw">math.h</code> nonetheless as they’re defined by <a href="https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/math.h.html">POSIX</a> and <a href="http://www.unix.org/version2/unix98.html">UNIX98</a>.</p>
<p><a href="https://docs.microsoft.com/en-us/cpp/c-runtime-library/math-constants">Microsoft’s MSVC doesn’t</a>, but instead you have to <code data-enlighter-language="raw">#define _USE_MATH_DEFINES</code> before including <code>math.h</code>.</p>
<p>While not a big problem per-se, it is annoying and indeed caused the initial version of the <a href="https://github.com/sdroege/ebur128/commit/4b9b83f04a74bad6e148ec2548a97fec8c63ab9e"><code>ebur128</code></a> Rust crate to not compile with MSVC because I forgot about it.</p>
<p>Similarly, which mathematical functions are available depends a lot on the target platform and which version of the C standard is supported. An example of this is the <a href="https://pubs.opengroup.org/onlinepubs/009695399/functions/log10.html"><code>log10</code></a> function to calculate the base-10 logarithm. For portability reasons, <code data-enlighter-language="raw">libebur128</code> didn’t use it but instead calculated it via the natural logarithm (<code>ln(x) / ln(10) = log10(x)</code>) because it’s only available in POSIX and since C99. While C99 is from 1999, there are still many compilers out there that don’t fully support it, again most prominently MSVC until very recently.</p>
<p>Using <code data-enlighter-language="raw">log10</code> instead of going via the natural logarithm is faster and more precise due to floating point number reasons, which is why the Rust implementation uses it but in C it would be required to check at build-time if the function is available or not, which complicates the build process and can easily be forgotten. <code data-enlighter-language="raw">libebur128</code> decided to not bother with these complications and simply not use it. Because of that, some <a href="https://github.com/sdroege/ebur128/blob/a72f2aed6bf12da2fd9bac10c9478a5b13495c09/src/utils.rs#L23-L35">conditional code</a> in the Rust implementation is necessary for ensuring that both implementations return the same results in the tests.</p>
<h4>Data Structures</h4>
<p><code data-enlighter-language="raw">libebur128</code> uses a linked-list-based <a href="https://en.wikipedia.org/wiki/Queue_(abstract_data_type)">queue</a> data structure. As the C standard library is very minimal, no collection data structures are included. However on the BSDs and also on Linux with the GNU C library there is one available in <a href="https://man7.org/linux/man-pages/man3/queue.3.html"><code>sys/queue.h</code></a>.</p>
<p>Of course MSVC does not have this and other compilers/platforms probably won’t have it either, so <code data-enlighter-language="raw">libebur128</code> included a local copy of that queue implementation. Now when building, one has to decide whether there is a system implementation available or otherwise use the internal version. Or simply always use the internal version.</p>
<p>Copying implementations of basic data structures and algorithms into every single project is ugly and error-prone, so let’s maybe not do that. C not having a standardized mechanism for dependency handling doesn’t help with this, which is unfortunately why this is very common in C projects.</p>
<h4>One-time Initialization</h4>
<p>Thread-safe one-time initialization is another thing that is not defined by the C standard, and depending on your platform there are different APIs available for it or none at all. <a href="https://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_once.html">POSIX</a> again defines one that is widely available, but you can’t really depend on it unconditionally.</p>
<p>This complicates the code and build procedure, so libebur128 simply did not do that and did its one-time initializations of some global arrays every time a new instance was created. Which is probably fine, but a bit wasteful and probably strictly-speaking according to the C standard not actually thread-safe.</p>
<p>The initial version of the <a href="https://github.com/sdroege/ebur128/commit/3a329b96fad613434a001dce01d159e141aa3382"><code>ebur128</code></a> Rust crate side-stepped this problem by simply doing this initialization once with the API <a href="https://doc.rust-lang.org/std/sync/struct.Once.html">provided</a> by the Rust standard library. See part 2 and part 3 of this blog post for some more details about this.</p>
<h4>Easier to Compile and Integrate</h4>
<p>A Rust port only requires a Rust compiler, a mixed C/Rust codebase requires at least a C compiler in addition and some kind of build system for the C code.</p>
<p><code data-enlighter-language="raw">libebur128</code> uses <a href="https://cmake.org/"><code>CMake</code></a>, which would be an additional dependency so in the initial version of the <code data-enlighter-language="raw">ebur128</code> crate I went via <a href="https://doc.rust-lang.org/cargo/">cargo</a>‘s <a href="https://doc.rust-lang.org/cargo/reference/build-scripts.html"><code>build.rs</code></a> build scripts and the <a href="https://crates.io/crates/cc"><code>cc</code></a> crate as building <code data-enlighter-language="raw">libebur128</code> is easy enough. This works but build scripts are problematic for integration of the Rust code into other build …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://coaxion.net/blog/2020/09/porting-ebu-r128-audio-loudness-analysis-from-c-to-rust/">https://coaxion.net/blog/2020/09/porting-ebu-r128-audio-loudness-analysis-from-c-to-rust/</a></em></p>]]>
            </description>
            <link>https://coaxion.net/blog/2020/09/porting-ebu-r128-audio-loudness-analysis-from-c-to-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24558276</guid>
            <pubDate>Tue, 22 Sep 2020 18:42:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learning How to Learn Japanese]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24557961">thread link</a>) | @falava
<br/>
September 22, 2020 | https://zachdaniel.dev/learning-how-to-learn-japanese/ | <a href="https://web.archive.org/web/*/https://zachdaniel.dev/learning-how-to-learn-japanese/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


            <section>
                <div>
                    <p>tl;dr - This essay is broken into four sections: 1) why I’m learning Japanese, 2) some basics about Japanese alphabets, 3) what makes it a hard language to learn, and 4) the tools I’m using to learn it. If you just want to know about that last bit feel free to scroll down to the bottom.</p><p>I've always wanted to learn another language. For years, I would pick up Duolingo and mess around with it for a few weeks before moving on to something else. Then, a little over a year ago, my best friend decided that he wanted to learn Japanese. While I’d love to say that being able to speak Japanese has been a lifelong dream, my motivation is slightly less pure. Specifically, I took the fact that Japanese is, by some accounts,<a href="https://effectivelanguagelearning.com/language-guide/language-difficulty/"> the most difficult language</a> an English speaker can attempt to learn, as a personal challenge.</p><p>This changed the shape of my learning journey. Now, instead of trying to optimize for short-term usage, I was headed down a path that would take years to complete. Knowing that was surprisingly <em>helpful</em> with my motivation, as it changed my whole perspective around comprehension, ensuring the time took to learn new things didn't get me down. I was finally equipped for my language journey: I had motivation, a partner, and time, and that is all you need. Or so I thought.</p><h6 id="if-you-re-already-familiar-with-the-japanese-you-can-probably-skip-this-section-"><strong>If you're already familiar with the Japanese, you can probably skip this section.</strong></h6><p>For readers who are not familiar with the Japanese language, I'll take a brief interlude to explain some foundations. There are three character sets, two of which are phonetic (like the English alphabet), meaning that they each have a specific sound. The first is called<a href="https://en.wikipedia.org/wiki/Hiragana"> hiragana</a>(ひらがな) and is the most basic/ubiquitous of the three. The second is called<a href="https://en.wikipedia.org/wiki/Katakana"> katakana</a>(カタカナ) and generally has more specialized usage. For every hiragana, there is a corresponding katakana. They are generally used to represent "loan words" like バス(basu) for "bus" or パン(pan) for "bread" (from Portuguese). Having multiple phonetic alphabets sounded very strange to me at first, until I realized that English has <em>also</em> has two (not counting cursive)! Look at each letter in "bard" compared to "BARD" to see what I mean.</p><p>The final character set,<a href="https://en.wikipedia.org/wiki/Kanji"> Kanji(漢字)</a>, is the one that most people have heard of. It is also one of the most daunting aspects of learning Japanese. There are roughly 50,000 Kanji, but the vast majority of them are no longer in use. The Japanese government maintains a set of<a href="https://en.wikipedia.org/wiki/List_of_j%C5%8Dy%C5%8D_kanji"> 2,136 Kanji</a> that must be taught at school but most educated Japanese adults know somewhere from 3-4,000 (I can't find any official numbers on this). If you work in a specialized field like medicine, or if you are an academic, that number can go as high as 6,000! I'll talk more about Kanji and their challenges a bit further on.</p><p>Grammatically, Japanese is about as different as you can get from English. The sentence structure is so different that you essentially have to think in reverse (some resources even<a href="http://www.textfugu.com/season-1/japanese-grammar-with-yoda/"> teach the basics with Yoda</a>). As you learn more advanced/casual grammar, you find out that things can go in pretty much whatever order you want. This does not help in the slightest. Prepositions (in, at, on) in English are post-positioned in Japanese and have little in common. Plurality is complicated; In English, "book" is unequivocally singular. In Japanese, 本(hon), isn't plural or singular, plurality is indicated by context, or by adding qualifying words. I could go on, but I say all this only to explain that learning Kanji isn't the only barrier involved in learning Japanese.</p><h6 id="back-to-the-topic-at-hand"><strong>Back to the topic at hand</strong></h6><p>It became clear to me pretty quickly that Duolingo just wasn't going to cut it. The concepts didn't map well to the Japanese language and, in all honesty, I think Duolingo's framework just isn't challenging enough in general. This realization started my search for better tools. I had learned that multiple-choice/drag and drop questions are not an effective way to study languages. You need to be "producing" and "translating". Additionally, to learn Japanese you need to use tools that are tailored to Asian languages, or better yet the Japanese language itself. These two conditions knock out a lot of the most popular contenders like Duolingo, and Memrise. I'm not suggesting that those tools don't have value, or that they aren't a good place to start, but that there are more effective tools out there.</p><p>I'll spare you the whirlwind tour of every other tool that I tried and ultimately cast aside and skip ahead to the good stuff. What I ultimately discovered is that, at least for Japanese, you need to front-load Kanji and vocabulary. I've heard many people talk about this, and it is clear that they know it is true, but have trouble articulating <em>why</em> it is true. Luckily, I think I'm at a point where I can explain the issue. Keep in mind, this is not meant to discourage, only to explain why trying to just dive into Japanese probably won't get you very far.</p><p>You can learn to "read" English, with its relatively simple phonetic alphabet in a matter of days. Your pronunciation will often be wrong, and you certainly won't know what all the words mean, but you can turn <em>symbols</em> into <em>sounds</em> from an early stage. What this means is that, generally speaking, trying to remember a word that you <em>see</em> is the same as remembering a word that you <em>hear</em>. This same parallel isn't true for Japanese, because of Kanji. It typically takes <em>at least</em> a year, and typically longer, for an adult to learn the Joyo Kanji (what Japanese children learn in school). If you're presented a word that contains a Kanji that you've never seen before, there is no good way to know how it sounds. There are some patterns/tricks you can use to make decent guesses but this method is not reliable. This prevents you from trying to remember if you've ever "heard" this new word before. Worst of all, if you can figure out the <em>meaning</em> of this new word from context, you still won't know how to <em>say</em> that word to someone else.</p><p>To put this in context, there is a site called NHK Easy News that presents a simplified version of Japanese news articles. I know 1000 kanji currently, and that represents ~90% of the Kanji that they use on NKH Easy News. That may sound like a lot but having to stop on every 10th word to look it up, and not being able to turn these unknown words into sounds to help you remember/let you access your auditory memory makes trying to read without a really good grasp of Kanji <em>extremely</em> tiring. The thing I didn't mention yet: "Knowing" a kanji doesn't mean you know how to say it in every context. For example, the Kanji "人" in "人類", "一人", "芸人" is read differently for each (jin, ri, nin)! So I know 90% of the Kanji, but do I know 90% of all of the ways they use Kanji? Probably not.</p><p>Again, this wasn't meant to discourage you, but to show the merits of this approach.</p><p>I'm still just barely intermediate at Japanese but I believe this "stack" is going to take me well into the realm of high-intermediate/advanced, and give me an excellent foundation to continue my learning from. The magic in all of these tools is that they use an SRS or "Spaced Repetition System", that is designed to quiz you on things that you've learned <em>just before</em> you forget them. You'll be amazed at how much you can stuff into your brain using an SRS. <em>All</em> of the tools I use are based around an SRS. Prioritize these tools in descending order. The workflow is simple: If you have <em>any</em> pending reviews or lessons in a higher priority one, do those first. Get back to the other tools if/when you have time.</p><h3 id="wanikani"><a href="https://www.wanikani.com/"><strong>WaniKani</strong></a></h3><figure><img src="https://lh3.googleusercontent.com/B3G8yObyz0PrZ5os9g4YTR5JdV-OsNPJaQJP4MEqaeuqELDsVOK9YA8VGeFoLYG1H6kveL7IdTwoDRRQmLQf7F9g_HnhX51LWBsDnMWwoOzJcsUI--3aIBwxhu3jGBSyuyxJpTml"></figure><figure><img src="https://lh4.googleusercontent.com/voMiV_sz5ydmSwC-JBBG5potzafcgR0Z7CYuP1Bttc9fRZ37zaRB9T888DQ1xnoytJjm6ZCfT4Ww1_wkbYIGsLoEwOQMXxyW7dcZ5d4cR3bmi_Lsmy8O-gUVUjciwDK29WSZq56Y"></figure><p>WaniKani is a system that teaches you most of the Joyo Kanji, and ~6000 vocabulary words in an order tailored to English speaking adults. Their website will explain much better than I could. Wanikani is very much a "point and shoot" solution and it works wonders. Do your lessons when you have time, and do your reviews when they come up, and your Kanji and vocabulary knowledge will skyrocket.</p><h3 id="kamesame"><a href="https://www.kamesame.com/"><strong>KameSame</strong></a></h3><figure><img src="https://lh4.googleusercontent.com/3PLjw0J7OAlXE0OeK6egSpssbSdNfbP-JLevLn0AQIWmvyBkysUlqombhqB8LIbSfJnnmwy1sq6G0zlrUY0smmkZxdY0RgEVQKvUEAIUKxNzYA2cP5iqClyuUhkxo9ckvnnFax3E"></figure><p>WaniKani doesn't do everything. Specifically, it doesn't present you with the English translation of a vocabulary word and have you provide the Japanese word with the same meaning. Luckily, a tool was developed that integrates with WaniKani to provide exactly that. This takes your vocabulary learning to the next level. Just using WaniKani will set you up well for reading/listening, but with KameSame, you'll find yourself able to locate Japanese words on demand, which is (naturally) crucial for speaking.</p><p>Since I started using it, the creator has added a whole host of tools that will make it an extremely useful tool to have in your arsenal, even after you've completed WaniKani. This is one to keep your eye on. Unlike WaniKani, you add lessons at your own pace here with no real cap, so be careful not to get overzealous.</p><h3 id="bunpro"><a href="https://bunpro.jp/"><strong>Bunpro</strong></a></h3><figure><img src="https://lh6.googleusercontent.com/AjdQ-zWy-Nj6ZaMlebYPjW__bDJkac-KZR4bQ_ONbLzrjmvakGFW_uoly16ctKGu_4tEXmpGUxv8eKsv1IGEI3YuwYKW2ySI3yoaM9gUvfe_VC15RVk1_QsC5DxO9LirCU6VHxph"></figure><p>I said Kanji/Vocab <em>first</em> but not <em>only</em>. Bunpro is an amazing tool for learning grammar via practice. I've never seen anything quite like it, but it embodies the SRS system even while presenting you with different variations of the grammar you are reviewing. In the same way that KameSame will amp up your word production, using Bunpro for a couple of months will have you expressing ideas and building sentences that aren't just "subject-object-verb".</p><p>Sources:</p><p><a href="https://www.fluentu.com/blog/japanese/japanese-sentence-structure-patterns/">https://www.fluentu.com/blog/japanese/japanese-sentence-structure-patterns/</a></p><p><a href="https://www.sljfaq.org/afaq/how-many-kanji.html">https://www.sljfaq.org/afaq/how-many-kanji.html</a></p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://zachdaniel.dev/learning-how-to-learn-japanese/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24557961</guid>
            <pubDate>Tue, 22 Sep 2020 18:15:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The case for comments in code]]>
            </title>
            <description>
<![CDATA[
Score 111 | Comments 74 (<a href="https://news.ycombinator.com/item?id=24556782">thread link</a>) | @pcr910303
<br/>
September 22, 2020 | https://notes.eatonphil.com/the-case-for-comments-in-code.html | <a href="https://web.archive.org/web/*/https://notes.eatonphil.com/the-case-for-comments-in-code.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://notes.eatonphil.com/the-case-for-comments-in-code.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24556782</guid>
            <pubDate>Tue, 22 Sep 2020 16:47:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[No script is too simple]]>
            </title>
            <description>
<![CDATA[
Score 264 | Comments 153 (<a href="https://news.ycombinator.com/item?id=24556022">thread link</a>) | @nicbou
<br/>
September 22, 2020 | https://nicolasbouliane.com/blog/no-script-is-too-simple | <a href="https://web.archive.org/web/*/https://nicolasbouliane.com/blog/no-script-is-too-simple">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                            <div>
        <article>
            <p>
                Posted on <time datetime="2020-09-18 13:43:00">September 18, 2020</time>
            </p>
                        <p>Most of the projects I work on have a <code>scripts</code> directory. For examples, here's are the scripts I use for <a href="https://allaboutberlin.com/">All About Berlin</a>:</p>

<pre>backup-db.sh
clear-static-cache.sh
copy-production-db.sh
restore-db.sh
save-composer-dependencies.sh
</pre>
<p>Those scripts are all very simple. They're under 10 lines long, and take no arguments. Some just call a single command, so why bother creating them?<br></p>
<p><em>What</em> must be done ("start the project", "run the tests" etc.) won't change. <em>How</em> it must be done ("yarn ...", "docker-compose ...") can change multiple time in a project's lifespan. <strong>It's a lot easier to update a single script than to propagate a new set of commands</strong>.</p><p>This script contains the only valid instructions for starting the project. <strong>It's the single source of truth for how to perform that task</strong>. Your colleagues, your CI/CD pipeline, your commit hooks, your cronjobs, your other scripts and your documentation can just refer to <code>start.sh</code>. If you change the script, the new instructions propagate instantly. You don't need to inform your colleagues, update a bunch of Jenkins jobs or update the README. All of it is still up to date. There will be no discrepancies.</p><p>With all those people and machines working off the same scripts, it pays to improve them. You can add usability improvements that will benefit the whole team. You can check the python/node/docker versions, add error handling, remove noise from the output, add colours and headers to build steps, and even interactivity.<br></p><p><strong>Script calls are also more explicit than commands</strong>. A Jenkins job that calls <code></code><code>lint-source.sh &amp;&amp; start.sh &amp;&amp; run-tests.sh &amp;&amp; ...</code> is pretty self-explanatory. A script called <code>create-user.sh</code> is easier to remember than <code>docker-compose exec backend python3 manage.py createsuperuser</code>. This is especially nice when you frequently switch between projects, and constantly need to recall how each one works.<br></p>
                    </article>

                            </div>
            
                        </div></div>]]>
            </description>
            <link>https://nicolasbouliane.com/blog/no-script-is-too-simple</link>
            <guid isPermaLink="false">hacker-news-small-sites-24556022</guid>
            <pubDate>Tue, 22 Sep 2020 15:55:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Americans don't feel they can get justice when wronged by their ISP – survey]]>
            </title>
            <description>
<![CDATA[
Score 261 | Comments 91 (<a href="https://news.ycombinator.com/item?id=24555525">thread link</a>) | @KaiserSanchez
<br/>
September 22, 2020 | https://fairshake.com/consumer-news/survey-americans-no-justice-against-isps/ | <a href="https://web.archive.org/web/*/https://fairshake.com/consumer-news/survey-americans-no-justice-against-isps/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
        <div>
            <section>
                <div>
                    <p><span><img src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1.jpg" alt="" width="2022" height="1155" srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-300x171.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-1024x585.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-768x439.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-1536x877.jpg 1536w" sizes="(max-width: 2022px) 100vw, 2022px" data-srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-300x171.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-1024x585.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-768x439.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1-1536x877.jpg 1536w" data-src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/00-Header-1.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">It’s no secret that internet service providers (ISPs) cause plenty of frustration for their customers.</span></p>
<p><span>We recently set out to discover </span><a href="https://fairshake.com/consumer-guides/mapped-the-most-hated-internet-service-providers-in-every-u-s-state/"><span>which ISP was the most-hated in every U.S. state</span></a><span>, and in doing so, we learned more about the types of complaints many people have about their ISPs: billing problems, deceptive pricing, misleading sales tactics, poor service, slow speeds, and more.</span></p>
<p><span>We also got a glimpse of how many people have struggled with these types of problems —&nbsp;our research led us to thousands and thousands of one-star reviews left for ISPs online. It made us start to wonder: Just how widespread </span><i><span>are</span></i><span> these problems? How many people have disputes against their ISP, how many of those disputes go unresolved, and what avenues do consumers pursue to find help?</span></p>
<p><span>And so, we surveyed consumers, and the results were eye-opening. From our survey, we found that it’s possible that more than 20 million American households have unresolved complaints against their ISPs, and that while many of them look to legal action for help, most ultimately don’t end up moving forward —&nbsp;because the process is too expensive, it’s too complicated, or they just don’t know how.</span></p>
<p><span>Our survey results paint a picture of a country where consumers are almost held hostage by their ISPs — many of them experience serious problems, but when they do, they struggle to find avenues to resolution. Here’s everything we found.</span></p>
<h2><span>More Than 1/3 of Households Had a Dispute Against Their ISP in the Last Year</span></h2>
<p><span><img src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2.jpg" alt="" width="2022" height="1519" srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-300x225.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-1024x769.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-768x577.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-1536x1154.jpg 1536w" sizes="(max-width: 2022px) 100vw, 2022px" data-srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-300x225.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-1024x769.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-768x577.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2-1536x1154.jpg 1536w" data-src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/01-ImageV2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">In our survey, 37 percent of respondents said that they had had an experience with an ISP in the last year in which they were unfairly charged for service, or otherwise treated unfairly.</span></p>
<p><span>That’s more than one-third of respondents. And when you extrapolate that finding to the 110.57 million U.S. households that </span><a href="https://www.statista.com/statistics/183614/us-households-with-broadband-internet-access-since-2009/#:~:text=This%20timeline%20shows%20the%20number,to%2080%20million%20in%202009."><span>had fixed broadband access in 2018</span></a><span>, that’s more than 40 million households with a complaint against their ISP in </span><i><span>just one year</span></i><span> — 40,446,500 of them, to be exact.</span></p>
<h3><span>More Than Half of Those Disputes Went Unresolved</span></h3>
<p><span><img src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2.jpg" alt="" width="2022" height="1411" srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-300x209.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-1024x715.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-768x536.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-1536x1072.jpg 1536w" sizes="(max-width: 2022px) 100vw, 2022px" data-srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-300x209.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-1024x715.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-768x536.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2-1536x1072.jpg 1536w" data-src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/02-ImageV2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">What’s even more shocking is what we found when we asked respondents how their disputes against their ISPs had been handled. More than half of them —&nbsp;50 percent —&nbsp;answered no when asked if the issue was “resolved to their satisfaction.”</span></p>
<p><span>Extrapolate </span><i><span>that</span></i><span> result to all the internet-connected households in the U.S., and this survey finding indicates that as many as 20,371,000 — more than 20 million — U.S. households had an unresolved complaint against their ISP in the last year alone.</span></p>
<p><span>That’s a shocking number, so we wanted more details. We wanted to know how people consider fighting against unfair charges and other complaints, so we dug a little deeper. Here’s what we found.</span></p>
<h3><span>Many People Consider Legal Action, but Most Don’t</span></h3>
<p><span><img src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2.jpg" alt="" width="2022" height="1005" srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-300x149.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-1024x509.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-768x382.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-1536x763.jpg 1536w" sizes="(max-width: 2022px) 100vw, 2022px" data-srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-300x149.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-1024x509.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-768x382.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2-1536x763.jpg 1536w" data-src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/03-ImageV2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">According to our survey, a large number of people consider legal action as an avenue for resolving their complaint with an ISP —&nbsp;35 percent of them, or around 7 million households last year.</span></p>
<p><span>But we were shocked to learn that most people just don’t consider taking legal action against their ISP in the event of an unresolved issue.</span></p>
<p><span>Of our survey respondents who said they had unresolved complaints, 65 percent said they didn’t feel the need to pursue legal action — including a lawsuit, a small claims court case, a class action suit, or arbitration —&nbsp;or monetary compensation. Only just over a third, 35 percent, said they did consider legal action.</span></p>
<h3><span>Most People Who Do Consider Legal Action Don’t Go Through With It</span></h3>
<p><span>But most people with complaints against their ISP don’t even reach that point. Of the respondents in our survey who considered legal action:</span></p>
<ul>
<li><span>67 percent decided not to move forward with legal action</span></li>
<li><span>21 percent attempted to pursue legal action but were unsuccessful</span></li>
<li><span>13 percent were successful in pursuing legal action against their ISP</span></li>
</ul>
<h3><span><img src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2.jpg" alt="" width="2022" height="1575" srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-300x234.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-1024x798.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-768x598.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-1536x1196.jpg 1536w" sizes="(max-width: 2022px) 100vw, 2022px" data-srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-300x234.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-1024x798.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-768x598.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2-1536x1196.jpg 1536w" data-src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/04-ImageV2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">Most People Find Legal Action Too Expensive to Pursue</span></h3>
<p><span>With so many people looking into legal action and then deciding not to pursue it, we wanted to know what made them stop. Here’s what we found.<img src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2.jpg" alt="" width="2022" height="1937" srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-300x287.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-1024x981.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-768x736.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-1536x1471.jpg 1536w" sizes="(max-width: 2022px) 100vw, 2022px" data-srcset="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2.jpg 2022w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-300x287.jpg 300w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-1024x981.jpg 1024w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-768x736.jpg 768w, https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2-1536x1471.jpg 1536w" data-src="https://4bvihq1vzfw92gpacq41usu0-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/05-ImageV2.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></span></p>
<ul>
<li><span>31 percent of respondents said they stopped pursuing legal action when they found the process to be too expensive.</span></li>
<li><span>31 percent stopped because they simply weren’t sure how to proceed.</span></li>
<li><span>22 percent of respondents said they wanted to pursue legal action, but the process was too complicated.</span></li>
<li><span>16 percent of respondents said they needed help pursuing legal action, but couldn’t find the help they needed.</span></li>
</ul>
<p><span>That last statistic really stood out for us. More than 16 percent of people were wronged by their ISP and wanted help getting justice, but they couldn’t find the help they needed. That help is out there —&nbsp;or, right here.</span></p>
<h2><span>FairShake Is the Best Way to Get the Justice Your Deserve</span></h2>
<p><span>Getting the justice you deserve against ISPs —&nbsp;and all kinds of other companies —&nbsp;doesn’t have to be long, drawn out, complicated, or expensive. And most importantly, you don’t have to do it alone.</span></p>
<p><span>At </span><a href="https://fairshake.com/"><span>FairShake</span></a><span>, we help consumers avoid expensive and time-consuming court cases, and instead help them pursue arbitration, a process similar to mediation that employs a professional, neutral third party to help reach resolution in a dispute. You just tell us about your complaint, and we’ll create an official legal demand on your behalf. Then, we’ll help guide you through the next legal steps so you’re never left alone or in the dark.</span></p>
<p><span>Ready to get your fair shake against an ISP that let you down? Check out FairShake today.</span></p>
<h2><span>Methodology</span></h2>
<p><span>We surveyed 1,200 American consumers over the age of 18 using PollFish.com between June 15-16, 2020. We asked six total questions about whether they’d had a dispute with their ISP in the last year, whether it was ever resolved, and what they did to try to get justice.</span></p>
<p><span>11 percent of respondents were ages 18 to 24. 17 percent were ages 25 to 34. 22 percent were ages 35 to 44. 19 percent were ages 45 to 54, and 32 percent were 55 and up.</span></p>
<p><span>58 percent identified as female, and 42 percent identified as male.</span></p>
<p><span>To get a bigger-picture view of the scope of some of our survey results, we extrapolated out sample size to the population of the entire U.S., and the number of U.S. households that have fixed broadband home internet access. Our sample size allowed us to apply results to a group the size of the U.S. population with 95 percent confidence and a 5 percent margin of error.</span></p>
                    
                </div>

                
                                
                            </section>

                                                        <section>
                    
                    
                </section>
                                    </div>
    </section></div>]]>
            </description>
            <link>https://fairshake.com/consumer-news/survey-americans-no-justice-against-isps/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24555525</guid>
            <pubDate>Tue, 22 Sep 2020 15:21:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[2020 General Election Early Vote Statistics]]>
            </title>
            <description>
<![CDATA[
Score 51 | Comments 47 (<a href="https://news.ycombinator.com/item?id=24553908">thread link</a>) | @blacksqr
<br/>
September 22, 2020 | https://electproject.github.io/Early-Vote-2020G/index.html | <a href="https://web.archive.org/web/*/https://electproject.github.io/Early-Vote-2020G/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>




<!--/.navbar -->




<p>Last updated: 09/25/2020 03:41 PM Eastern Time</p>
<p>Michael McDonald • Professor, University of Florida<br>
Web: <a href="http://www.electproject.org/">United States Elections Project</a> • Twitter: <a href="https://twitter.com/ElectProject">@ElectProject</a></p>
<p>Detailed state statistics and analyses for reporting states<br>
<a href="https://electproject.github.io/Early-Vote-2020G/CA.html">CA</a> | <a href="https://electproject.github.io/Early-Vote-2020G/CO.html">CO</a> | <a href="https://electproject.github.io/Early-Vote-2020G/DC.html">DC</a> | <a href="https://electproject.github.io/Early-Vote-2020G/FL.html">FL</a> | <a href="https://electproject.github.io/Early-Vote-2020G/GA.html">GA</a> | <a href="https://electproject.github.io/Early-Vote-2020G/HI.html">HI</a> | <a href="https://electproject.github.io/Early-Vote-2020G/IA.html">IA</a> | <a href="https://electproject.github.io/Early-Vote-2020G/IL.html">IL</a> | <a href="https://electproject.github.io/Early-Vote-2020G/ME.html">ME</a> | <a href="https://electproject.github.io/Early-Vote-2020G/MI.html">MI</a> | <a href="https://electproject.github.io/Early-Vote-2020G/MN.html">MN</a> | <a href="https://electproject.github.io/Early-Vote-2020G/MT.html">MT</a> | <a href="https://electproject.github.io/Early-Vote-2020G/NC.html">NC</a> | <a href="https://electproject.github.io/Early-Vote-2020G/NJ.html">NJ</a> | <a href="https://electproject.github.io/Early-Vote-2020G/NV.html">NV</a> | <a href="https://electproject.github.io/Early-Vote-2020G/OH.html">OH</a> | <a href="https://electproject.github.io/Early-Vote-2020G/OR.html">OR</a> | <a href="https://electproject.github.io/Early-Vote-2020G/PA.html">PA</a> | <a href="https://electproject.github.io/Early-Vote-2020G/SC.html">SC</a> | <a href="https://electproject.github.io/Early-Vote-2020G/UT.html">UT</a> | <a href="https://electproject.github.io/Early-Vote-2020G/VA.html">VA</a> | <a href="https://electproject.github.io/Early-Vote-2020G/VT.html">VT</a> | <a href="https://electproject.github.io/Early-Vote-2020G/WA.html">WA</a> | <a href="https://electproject.github.io/Early-Vote-2020G/WI.html">WI</a></p>
<p>These states are those where I have current data on mail ballot activity. More states will be added as state reports become available.</p>
<div id="summary-statistics-for-reporting-states">
<h2>Summary Statistics for Reporting States</h2>
<div id="mail-ballots-returned-or-in-person-early-votes">
<h3>Mail Ballots Returned or In-Person Early Votes</h3>
<p>Voters have cast a total of <strong>744,005</strong> ballots in the reporting states.</p>


<div id="voted-by-party-registration">
<h4>Voted by Party Registration</h4>
<p>Reporting states with party registration data: FL, IA, NC</p>
<table>
<thead>
<tr>
<th>
Party
</th>
<th>
Count
</th>
<th>
Percent
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Democrats
</td>
<td>
123,126
</td>
<td>
54.3
</td>
</tr>
<tr>
<td>
Republicans
</td>
<td>
37,014
</td>
<td>
16.3
</td>
</tr>
<tr>
<td>
Minor
</td>
<td>
708
</td>
<td>
0.3
</td>
</tr>
<tr>
<td>
No Party Affiliation
</td>
<td>
65,901
</td>
<td>
29.1
</td>
</tr>
<tr>
<td>
TOTAL
</td>
<td>
226,749
</td>
<td>
100.0
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="mail-ballots-requested">
<h3>Mail Ballots Requested</h3>
<p>Voters have requested a total of <strong>63,661,305</strong> ballots in the reporting states.</p>


<div id="mail-ballots-requested-by-party-registration">
<h4>Mail Ballots Requested by Party Registration</h4>
<p>Reporting states with party registration data: CA, CO, DC, FL, IA, ME, NC, NJ, NV, OR, PA, UT</p>
<table>
<thead>
<tr>
<th>
Party
</th>
<th>
Count
</th>
<th>
Percent
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Democrats
</td>
<td>
20,137,421
</td>
<td>
43.4
</td>
</tr>
<tr>
<td>
Republicans
</td>
<td>
12,109,176
</td>
<td>
26.1
</td>
</tr>
<tr>
<td>
Minor
</td>
<td>
1,777,347
</td>
<td>
3.8
</td>
</tr>
<tr>
<td>
No Party Affiliation
</td>
<td>
12,395,347
</td>
<td>
26.7
</td>
</tr>
<tr>
<td>
TOTAL
</td>
<td>
46,419,291
</td>
<td>
100.0
</td>
</tr>
</tbody>
</table>
<p>Registered Democrats have a <strong>8,028,245</strong> ballot request lead over registered Republicans.</p>
<p><em>Note:</em> Party registration statistics are for states that have party registration. These statistics are not actual votes. By federal law, election officials do not begin counting ballots until Election Day, although they may start the process of preparing ballots for counting in advance.</p>
</div>
<div id="mail-ballots-requested-by-race-and-ethnicity">
<h4>Mail Ballots Requested by Race and Ethnicity</h4>
<p>Reporting states with race or ethnicity data: NC, SC</p>
<table>
<thead>
<tr>
<th>
Race
</th>
<th>
Count
</th>
<th>
Percent
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Non-Hispanic White
</td>
<td>
1,611,987
</td>
<td>
61.4
</td>
</tr>
<tr>
<td>
Non-Hispanic Black
</td>
<td>
662,197
</td>
<td>
25.2
</td>
</tr>
<tr>
<td>
Hispanic
</td>
<td>
55,582
</td>
<td>
2.1
</td>
</tr>
<tr>
<td>
Non-Hispanic Asian American
</td>
<td>
70,625
</td>
<td>
2.7
</td>
</tr>
<tr>
<td>
Non-Hispanic Native American
</td>
<td>
6,530
</td>
<td>
0.2
</td>
</tr>
<tr>
<td>
Other/Multiple/Unknown
</td>
<td>
217,995
</td>
<td>
8.3
</td>
</tr>
<tr>
<td>
TOTAL
</td>
<td>
2,624,916
</td>
<td>
100.0
</td>
</tr>
</tbody>
</table>
<p><em>Note:</em> Race and ethnicity statistics are for states that ask registered voters to provide their race and ethnicity.</p>
</div>
<div id="mail-ballots-requested-by-age">
<h4>Mail Ballots Requested by Age</h4>
<p>Reporting states with age data: CO, OH (partial), PA</p>
<table>
<thead>
<tr>
<th>
Age
</th>
<th>
Count
</th>
<th>
Percent
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
18 to 24
</td>
<td>
787,532
</td>
<td>
7.0
</td>
</tr>
<tr>
<td>
25 to 34
</td>
<td>
1,490,695
</td>
<td>
13.2
</td>
</tr>
<tr>
<td>
35 to 44
</td>
<td>
1,447,576
</td>
<td>
12.9
</td>
</tr>
<tr>
<td>
45 to 54
</td>
<td>
1,462,539
</td>
<td>
13.0
</td>
</tr>
<tr>
<td>
55 to 64
</td>
<td>
2,038,519
</td>
<td>
18.1
</td>
</tr>
<tr>
<td>
65 and up
</td>
<td>
4,031,007
</td>
<td>
35.8
</td>
</tr>
<tr>
<td>
TOTAL
</td>
<td>
11,257,868
</td>
<td>
100.0
</td>
</tr>
</tbody>
</table>
</div>
<div id="mail-ballots-requested-by-gender">
<h4>Mail Ballots Requested by Gender</h4>
<p>Reporting states with gender data: CO, NC</p>
<table>
<thead>
<tr>
<th>
Gender
</th>
<th>
Count
</th>
<th>
Percent
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Female
</td>
<td>
1,300,832
</td>
<td>
56.0
</td>
</tr>
<tr>
<td>
Male
</td>
<td>
950,091
</td>
<td>
40.9
</td>
</tr>
<tr>
<td>
Unknown
</td>
<td>
72,043
</td>
<td>
3.1
</td>
</tr>
<tr>
<td>
TOTAL
</td>
<td>
2,322,966
</td>
<td>
100.0
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>




</div></div>]]>
            </description>
            <link>https://electproject.github.io/Early-Vote-2020G/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24553908</guid>
            <pubDate>Tue, 22 Sep 2020 13:01:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bottom Type in F#]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24553590">thread link</a>) | @adelarsq
<br/>
September 22, 2020 | https://tysonwilliams.coding.blog/2020-09-21_bottom_type_in_fsharp | <a href="https://web.archive.org/web/*/https://tysonwilliams.coding.blog/2020-09-21_bottom_type_in_fsharp">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="-codedoc-container"><p><em>It is ALMOST possible to express the bottom type in F#.</em></p><p>The <a href="https://en.wikipedia.org/wiki/Bottom_type">bottom type</a> is the type that has no values.  It is useful as the return type of a function when every execution either does not terminate or throws an exception.  <a href="https://en.wikipedia.org/wiki/Bottom_type#In_programming_languages">Some languages</a> include the bottom type.  The <a href="https://github.com/fsharp/fslang-suggestions/issues/349">suggestion for F#</a> to add the bottom type was declined.  Interestingly, the <a href="https://github.com/dotnet/csharplang/issues/538">suggestion for C#</a> to add the bottom type lives on.</p><p>Nevertheless, it is possible to define the bottom type in F#.  Here is how I would do it.</p><pre><code tabindex="0"><span><span></span><span></span><span></span><span></span></span><p><span>1<span><span data-ignore-text="">link</span></span></span><span>type</span> <span>Bottom</span> <span>=</span> <span>private</span> Bottom <span>of</span> <span>Bottom</span></p><br></code></pre><p>My original thought was to omit <code>of Bottom</code> and then have this line be alone in its module (so that a public function does not provide access to this private case constructor).  Then I read the suggestion to include <code>of Bottom</code> in <a href="https://www.reddit.com/r/fsharp/comments/5dmo1f/f_logical_void_type/">this post on Reddit</a> from three years ago.  By including <code>of Bottom</code>, this <a href="https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/discriminated-unions">discriminated union</a> is a <a href="https://fsharpforfunandprofit.com/posts/recursive-types-and-folds/">recursive type</a> but lacks a base case.  As such, it is clear from this one line that it is impossible to obtain an instance of <code>Bottom</code>.</p><p>In addition to having no values, there is another property of the bottom type.  In subtyping systems, it is a subtype of all types.  That is the motivation for the name "bottom".  It sits at the bottom of the type hierarchy.  My bottom type does not have this property.  I think the suggestion for F# to add the bottom type was specifically requesting this feature.</p><p><em>Added 2020-09-23</em></p><p>I just realized that this does not work as I had intended because <a href="https://fsharp.github.io/fsharp-core-docs/reference/fsharp-core-operators-unchecked.html#defaultof"><code>Unchecked.defaultof&lt;&gt;</code></a> exists.</p><pre><code tabindex="0"><span><span></span><span></span><span></span><span>F# Interactive</span></span><p><span>1<span><span data-ignore-text="">link</span></span></span><span>&gt;</span> <span>type</span> <span>Bottom</span> <span>=</span> Bottom <span>of</span> <span>Bottom</span><span>;</span><span>;</span></p><p><span>2<span><span data-ignore-text="">link</span></span></span><span>type</span> <span>Bottom</span> <span>=</span> <span>|</span> Bottom <span>of</span> <span>Bottom</span></p><p><span>3<span><span data-ignore-text="">link</span></span></span></p><p><span>4<span><span data-ignore-text="">link</span></span></span><span>&gt;</span> Unchecked<span>.</span>defaultof<span>&lt;</span>Bottom<span>&gt;</span><span>;</span><span>;</span></p><p><span>5<span><span data-ignore-text="">link</span></span></span><span>val</span> it <span>:</span> <span>Bottom</span> <span>=</span> <span>&lt;</span><span>null</span><span>&gt;</span></p><br></code></pre><p>This definition of <code>Bottom</code> is a reference type, and reference types can be <code>null</code>.  Now the recursive nature is a detriment.  I was aiming for a type with zero values, but I actually created a type with infinitely many values since <code>&lt;null&gt;</code>, <code>Bottom &lt;null&gt;</code>, <code>Bottom (Bottom &lt;null&gt;)</code>, and so on are a rather direct translation of the <a href="https://en.wikipedia.org/wiki/Church_encoding#Church_numerals">Church numerals</a>.</p><p>If we change <code>Bottom</code> to a <code>struct</code>, then the compiler complains about the recursive definition.</p><pre><code tabindex="0"><span><span></span><span></span><span></span><span></span></span><p><span>1<span><span data-ignore-text="">link</span></span></span><span><span>[&lt;</span><span>Struct</span><span>&gt;]</span></span></p><p><span>2<span><span data-ignore-text="">link</span></span></span><span>type</span> <span>Bottom</span> <span>=</span> Bottom <span>of</span> <span>Bottom</span></p><br></code></pre><blockquote><p>Error    FS0954 This type definition involves an immediate cyclic reference through a struct field or inheritance relation</p></blockquote><p>After removing <code>of Bottom</code> to make the type non-recursive, both the reference and struct variants of the definition now have exactly one value.  This is now just a more verbose way to define the <a href="https://en.wikipedia.org/wiki/Unit_type">unit type</a>, the type with just one value, which <a href="https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/unit-type">F# already has</a>.</p><p>My six-year-old daughter has been saying lately that "there is no perfect; there is only good enough".  I am going to take her advice here and be content with this approximation of the theoretically perfect bottom type in the pragmatic language that is F#.</p><hr><p>The <a href="https://connect-platform.github.io/coding-blog-plugin/tags">tags feature of Coding Blog Plugin</a> is still being developed.  Eventually the tags will link somewhere.</p><p><a data-page-tag="Type_Theory"><code><span>#</span>Type_Theory</code></a> <a data-page-tag="FSharp"><code><span>#</span>FSharp</code></a></p></div></div>]]>
            </description>
            <link>https://tysonwilliams.coding.blog/2020-09-21_bottom_type_in_fsharp</link>
            <guid isPermaLink="false">hacker-news-small-sites-24553590</guid>
            <pubDate>Tue, 22 Sep 2020 12:16:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Small Tech]]>
            </title>
            <description>
<![CDATA[
Score 296 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24553085">thread link</a>) | @luu
<br/>
September 22, 2020 | https://scattered-thoughts.net/writing/small-tech/ | <a href="https://web.archive.org/web/*/https://scattered-thoughts.net/writing/small-tech/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>Should you join a big company or start a startup?</p>
<p>This frequently debated question paints a picture of a world where the only choice is between being a cog at a giant semi-monopoly, or taking investment money in the hopes of one day growing to be head cog at a giant semi-monopoly.</p>
<p>Role models matter. So I made a list of small companies that I admire. Neither giants nor startups - just people making a living writing software on their own terms.</p>
<h2 id="sqlite"><a href="https://www.sqlite.org/index.html">sqlite</a></h2>
<p>Sqlite is an existence proof that you don't need a thousand engineers to have an impact. It's been around since the early 90s, is one of the <a href="https://www.sqlite.org/mostdeployed.html">most shipped</a> pieces of software in existence and is widely respected for its <a href="https://www.sqlite.org/testing.html">exceptional testing methodology</a>. Yet it was written almost entirely by a few people:</p>
<pre><span>jamie@machine:~/sqlite$ git shortlog -s -n | head -n 5
 14012	drh
  4297	dan
  1663	danielk1977
  1099	mistachkin
   175	shane
</span></pre>
<p>Their growth plans:</p>
<blockquote>
<p>Hwaci is a small company but it is also closely held and debt-free and has low fixed costs, which means that it is largely immune to buy-outs, take-overs, and market down-turns. Hwaci intends to continue operating in its current form, and at roughly its current size until at least the year 2050. We expect to be here when you need us, even if that need is many years in the future.</p>
</blockquote>
<p>Sqlite is also interesting because although the code is open source they operate on a cathedral model, generally <a href="https://www.sqlite.org/copyright.html">refusing or rewriting</a> outside contributions.</p>
<h2 id="pinboard"><a href="https://pinboard.in/">pinboard</a></h2>
<p>Pinboard is a single-person operation that has been making a solid income year on year <a href="https://blog.pinboard.in/2019/07/i_cant_stop_winning/">since 2010</a>. It somehow still leaves Maciej enough free time to go around <a href="https://idlewords.com/2019/5/">teaching politicians how to not get hacked</a> and <a href="https://idlewords.com/talks/senate_testimony.2019.5.htm">explaining privacy to the senate</a>. All the while <a href="https://blog.pinboard.in/2017/06/pinboard_acquires_delicious/">destroying his competition</a> using highly advanced tactics such as <a href="http://idlewords.com/talks/fan_is_a_tool_using_animal.htm">listening to his customers</a> and not operating at a loss.</p>
<p>But if Maciej had done nothing else he would still be on this list for his beautiful talk <a href="https://www.youtube.com/watch?v=5Vt8zqhHe_c">Barely Succeed! It's Easier!</a></p>
<blockquote>
<p>We live in a remarkable time when small teams (or even lone programmers) can successfully compete against internet giants. But while the last few years have seen an explosion of product ideas, there has been far less innovation in how to actually build a business. Silicon Valley is stuck in an outdated 'grow or die' mentality that overvalues risk, while investors dismiss sustainable, interesting projects for being too practical. So who needs investors anyway?</p>
</blockquote>
<blockquote>
<p>I'll talk about some alternative definitions of success that are more achievable (and more fun!) than the Silicon Valley casino. It turns out that staying small offers some surprising advantages, not just in the day-to-day experience of work, but in marketing and getting customers to love your project. Best of all, there's plenty more room at the bottom.</p>
</blockquote>
<blockquote>
<p>If your goal is to do meaningful work you love, you may be much closer to realizing your dreams than you think.</p>
</blockquote>
<h2 id="tarsnap"><a href="https://www.tarsnap.com/">tarsnap</a></h2>
<p>Another single-person operation, tarsnap has reported been profitable <a href="https://www.tarsnap.com/about.html">since 2009</a> and shows no sign of going anywhere, even lowering it's prices over the years as storage became cheaper. It's saved me on multiple occasions and it's so simple, reliable and cheap that I think of it almost like a utility - tarsnap having an outage would be more surprising to me than water not coming out of my taps.</p>
<p>One of the most interesting things about tarsnap is that it could <a href="https://www.kalzumeus.com/2014/04/03/fantasy-tarsnap/">easily be making more money</a>. If tarsnap had investors or shareholders, they would have <strong>already forced Colin to do so</strong>. In other words, the only reason that Colin gets to continue running the kind of business that he <strong>wants</strong> to run is that he owns it entirely and noone can force him to maximize profit instead.</p>
<h2 id="sublime"><a href="https://www.sublimetext.com/">sublime</a></h2>
<p>Version 1.0 came out in <a href="https://www.sublimetext.com/blog/articles/one-point-oh">2008</a> and they're still going strong today, making enough to <a href="https://scattered-thoughts.net/writing/small-tech/">hire an extra developer</a> and <a href="https://www.sublimetext.com/blog/articles/sublime-merge">launch a separate product</a>.</p>
<p>Sublime is notable for surviving despite massive open-source competition in a field where people hate to pay for tools. As <a href="https://thume.ca/2017/03/04/my-text-editor-journey-vim-spacemacs-atom-and-sublime-text/">Tristan Hume points out</a>, it's really the only editor that has managed to remain both fast and consistent while still supporting a large plugin ecosystem. These kind of end-to-end qualities are perhaps one of the main advantages for small teams with a long time horizon.</p>
<h2 id="zig"><a href="https://ziglang.org/">zig</a></h2>
<p>Zig has been around <a href="https://andrewkelley.me/post/full-time-zig.html">only a few years</a> but I'm already wildly impressed by the quality of thought and engineering that has gone into it.</p>
<p>The Zig Software Foundation is a non-profit, currently funded entirely by donations and working towards being able to employ a second full-time developer. They explicitly chose to be a non-profit out of concern over how profit incentives creep into daily decision making:</p>
<blockquote>
<p>In You Weren’t Meant to Have a Boss, Paul Graham makes an analogy between animals in the zoo (employees) and animals in the wild (startup founders). I think he’s on to something, but when you start a startup, you still have a boss. In fact, you have the same boss. At the end of the day, it is an inescapable fact that you must do what makes a profit, for the shareholders.</p>
</blockquote>
<blockquote>
<p>The difference in incentives makes a structural difference that permeates every part of an organization. Windows users wake up one day and find ads in their start menu. Will Debian Linux ever try to put ads into any of its software? The concept is absurd.</p>
</blockquote>
<p>Zig was explicitly designed to be a <a href="https://ziglang.org/#Small-simple-language">small, simple language</a>, allowing it to be maintained indefinitely by a small, committed team.</p>
<blockquote>
<p>ZSF is a small organization and makes efficient use of monetary resources.</p>
</blockquote>
<blockquote>
<p>I don’t see the need for ZSF to grow beyond a handful of people.</p>
</blockquote>
<p>As Maciej <a href="https://www.youtube.com/watch?v=5Vt8zqhHe_c&amp;feature=youtu.be&amp;t=2144">puts it</a>, one of the advantages of being small is that you can speak with a human voice. This comes across clearly from the zig team, who are <a href="https://www.youtube.com/watch?v=e8f48iYKxgg">resolutely human</a> instead of projecting some impassive facade of professionalism.</p>

</article></div>]]>
            </description>
            <link>https://scattered-thoughts.net/writing/small-tech/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24553085</guid>
            <pubDate>Tue, 22 Sep 2020 11:04:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bypassing ESP32 Encrypted Secure Boot]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 60 (<a href="https://news.ycombinator.com/item?id=24552482">thread link</a>) | @mleonhard
<br/>
September 22, 2020 | https://raelize.com/posts/espressif-esp32-bypassing-encrypted-secure-boot-cve-2020-13629/ | <a href="https://web.archive.org/web/*/https://raelize.com/posts/espressif-esp32-bypassing-encrypted-secure-boot-cve-2020-13629/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><p>We arrived at the last post about our <strong>Fault Injection</strong> research on the <strong>ESP32</strong>. Please read our previous posts as it provides context to the results described in this post.</p>
<ul>
<li><a href="https://raelize.com/posts/espressif-systems-esp32-bypassing-sb-using-emfi/">Espressif ESP32: Bypassing Secure Boot using EMFI</a></li>
<li><a href="https://raelize.com/posts/espressif-systems-esp32-controlling-pc-during-sb/">Espressif ESP32: Controlling PC during Secure Boot</a></li>
<li><a href="https://raelize.com/posts/espressif-systems-esp32-bypassing-flash-encryption/">Espressif ESP32: Bypassing Flash Encryption (CVE-2020-15048)</a></li>
</ul>
<p>During our <strong>Fault Injection</strong> research on the <strong>ESP32</strong>, we gradually took steps forward in order to identify the required vulnerabilities that allowed us to bypass <strong>Secure Boot</strong> and <strong>Flash Encryption</strong> with a single <strong>EM</strong> glitch. Moreover, we did not only achieve <strong>code execution</strong>, we also extracted the <strong>plain-text flash</strong> data from the chip.</p>
<p><strong>Espressif</strong> requested a <strong>CVE</strong>  for the attack described in this post: <a href="https://www.espressif.com/sites/default/files/advisory_downloads/Security%20Advisory%20CVE-2020-15048%2C%2013629%20EN%26CN.pdf" target="_blank">CVE-2020-13629</a>. Please note, that the attack as described in this post, is only applicable to <strong>ESP32</strong> silicon revision 0 and 1. The newer <strong>ESP32 V3</strong> silicon supports functionality to disable the <strong>UART bootloader</strong> that we leveraged for the attack.</p>

<p>The <strong>ESP32</strong> implements an <strong>UART bootloader</strong> in its <strong>ROM code</strong>. This feature allows, among other functionality, to program the external flash. It's not uncommon that such functionality is implemented in the <strong>ROM code</strong> as it's quite robust as the code cannot get corrupt easily. If this functionality would be implemented by code stored in the external flash, any corruption of the flash may result in a bricked device.</p>
<p>Typically, this type of functionality is accessed by booting the chip in a special <strong>boot mode</strong>. The <strong>boot mode</strong> selection is often done using one or more external strap pin(s) which are set before resetting the chip. On the <strong>ESP32</strong> it works exactly like this pin <code>G0</code> which is exposed externally.</p>
<p>The <strong>UART bootloader</strong> supports many interesting <a href="https://github.com/espressif/esptool/wiki/Serial-Protocol#command-opcodes" target="_blank">commands</a> that can be used to read/write memory, read/write registers and even execute a stub from <strong>SRAM</strong>.</p>
<h4 id="executing-arbitrary-code">Executing arbitrary code</h4>
<p>The <strong>UART bootloader</strong> supports loading and executing arbitrary code using the <code>load_ram</code> command. The <strong>ESP32</strong>'s SDK includes all the tooling required to compile the code that can be executed from <strong>SRAM</strong>. For example, the following code snippet will print <code>SRAM CODE\n</code> on the serial interface.</p>
<div><pre><code data-lang="C"><span>void</span> <span>__attribute__</span><span>((</span><span>noreturn</span><span>))</span> <span>call_start_cpu0</span><span>()</span>
<span>{</span>
    <span>ets_printf</span><span>(</span><span>"SRAM CODE</span><span>\n</span><span>"</span><span>);</span>
    <span>while</span> <span>(</span><span>1</span><span>);</span>
<span>}</span>
</code></pre></div><p>The <code>esptool.py</code> tool, which is part of the <strong>ESP32</strong>'s SDK, can be used to load the compiled binary into the <strong>SRAM</strong> after which it will be executed.</p>
<pre><code>esptool.py --chip esp32 --no-stub --port COM3 load_ram code.bin
</code></pre><p>Interestingly, the <strong>UART bootloader</strong> cannot disabled and therefore always accessible, even when <strong>Secure Boot</strong> and <strong>Flash Encryption</strong> are enabled.</p>
<h4 id="additional-measures">Additional measures</h4>
<p>Obviously, if no additional security measures would be taken, leaving the <strong>UART bootloader</strong> always accessible would render <strong>Secure Boot</strong> and <strong>Flash Encryption</strong> likely useless. Therefore, <strong>Espressif</strong> implemented additional security measures which are enabled using dedicated <strong>eFuses</strong>.</p>
<p>These are security configuration bits implemented in special memory, often referred to as <strong>OTP memory</strong>, which can typically only change from 0 to 1. This guarantees, that once enabled, is enabled forever. The following <strong>OTP memory</strong> bits are used to disable specific functionality when the <strong>ESP32</strong> is in the <strong>UART bootloader</strong> boot mode.</p>
<ul>
<li><strong>DISABLE_DL_ENCRYPT</strong>: disables flash encryption operation</li>
<li><strong>DISABLE_DL_DECRYPT</strong>: disables transparent flash decryption</li>
<li><strong>DISABLE_DL_CACHE</strong>: disables the entire MMU flash cache</li>
</ul>
<p>The most relevant <strong>OTP memory</strong> bit is <strong>DISABLE_DL_DECRYPT</strong> as it disables the transparent decryption of the flash data.</p>
<p>If not set, it would be possible to simply access the plain-text flash data while the <strong>ESP32</strong> is in its <strong>UART bootloader</strong> boot mode.</p>
<p>If set, any access to the flash, when the chip is in <strong>UART bootloader</strong> boot mode, will yield just the encrypted data. The <strong>Flash Encryption</strong> feature, which is fully implemented in hardware and transparent to the processor,  is only enabled in when the <strong>ESP32</strong> is in <strong>Normal</strong> boot mode.</p>
<p>The attacks described in this post have all these bits set to 1.</p>

<p>The <strong>SRAM</strong> memory that's used by the <strong>ESP32</strong> is typical technology that's used by many chips. It's commonly used to the <strong>ROM</strong>'s stack and executing the first bootloader from flash. It's convenient to use at early boot as it typically require no configuration before it can be used.</p>
<p>We know from previous experience that the data stored in <strong>SRAM</strong> memory is persistent until it's overwritten or the required power is removed from the physical cells. After a <strong>cold reset</strong> (i.e. power-cycle) of the chip, the <strong>SRAM</strong> will be reset to its default state. This often semi-random and unique per chip as the default value for each bit (i.e. 0 or 1) is different.</p>
<p>However, after a <strong>warm reset</strong>, where the entire chip is reset without removing the power, it may happen that the data stored in <strong>SRAM</strong> remains unaffected. This persistence of the data is visualized in the picture below.</p>
<p>
    <a href="https://raelize.com/img/esp32/esp32-sram-persistence.png">
        <img src="https://raelize.com/img/esp32/esp32-sram-persistence.png" width="700px">
    </a>
</p>
<p>We decided to figure out if this behavior holds up for the <strong>ESP32</strong> as well. We identified that the hardware <a href="https://docs.espressif.com/projects/esp-idf/en/latest/esp32/api-reference/system/wdts.html" target="_blank">watchdog</a> can be used to issue a <strong>warm reset</strong> from software. This <strong>watchdog</strong> can also be issued when the chip is in <strong>UART bootloader</strong> boot mode and therefore we can use it to reset the <strong>ESP32</strong> back into <strong>Normal</strong> boot mode.</p>
<p>Using some test code, loaded and executed in <strong>SRAM</strong> using the <strong>UART bootloader</strong>, we determined that the data in <strong>SRAM</strong> is indeed persistent after issuing a <strong>warm reset</strong> using the <strong>watchdog</strong>. Effectively this means we can boot the <strong>ESP32</strong> in <strong>Normal</strong> boot mode with the <strong>SRAM</strong> filled with controlled data.</p>
<p>But… how can we (ab)use this?</p>

<p>We envisioned that we may be able to leverage the persistence of data in <strong>SRAM</strong> across <strong>warm resets</strong> for an attack. The first attack we came up with is to fill the <strong>SRAM</strong> with code using the <strong>UART bootloader</strong> and issue a <strong>warm reset</strong> using the <strong>watchdog</strong>. Then, we inject a glitch while the <strong>ROM code</strong> is overwriting this code with the <strong>flash bootloader</strong> during a normal boot.</p>
<p>We got this ideas as during our previous experiments, where we <a href="">turned data transfers into code execution</a>, we noticed that for some experiments the chip started executing from the entry address before the bootloader was finished copying.</p>
<p>Sometimes you just need to try it…</p>
<h4 id="attack-code">Attack code</h4>
<p>The code that we load into the <strong>SRAM</strong> using the <strong>UART bootloader</strong> is shown below.</p>
<div><pre><code data-lang="C"><span>#define a "addi a6, a6, 1;"
</span><span>#define t a a a a a a a a a a
</span><span>#define h t t t t t t t t t t
</span><span>#define d h h h h h h h h h h
</span><span></span>
<span>void</span> <span>__attribute__</span><span>((</span><span>noreturn</span><span>))</span> <span>call_start_cpu0</span><span>()</span> <span>{</span>
    <span>uint8_t</span> <span>cmd</span><span>;</span>

    <span>ets_printf</span><span>(</span><span>"SRAM CODE</span><span>\n</span><span>"</span><span>);</span>

    <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{</span>

        <span>cmd</span> <span>=</span> <span>0</span><span>;</span>
        <span>uart_rx_one_char</span><span>(</span><span>&amp;</span><span>cmd</span><span>);</span>

        <span>if</span><span>(</span><span>cmd</span> <span>==</span> <span>'A'</span><span>)</span> <span>{</span>                                    <span>// 1
</span><span></span>            <span>*</span><span>(</span><span>unsigned</span> <span>int</span> <span>*</span><span>)(</span><span>0x3ff4808c</span><span>)</span> <span>=</span> <span>0x4001f880</span><span>;</span>
            <span>*</span><span>(</span><span>unsigned</span> <span>int</span> <span>*</span><span>)(</span><span>0x3ff48090</span><span>)</span> <span>=</span> <span>0x00003a98</span><span>;</span>
            <span>*</span><span>(</span><span>unsigned</span> <span>int</span> <span>*</span><span>)(</span><span>0x3ff4808c</span><span>)</span> <span>=</span> <span>0xc001f880</span><span>;</span>
        <span>}</span>
    <span>}</span>

    <span>asm</span> <span>volatile</span> <span>(</span> <span>d</span> <span>);</span>                                     <span>// 2
</span><span></span>
    <span>"movi a6, 0x40; slli a6, a6, 24;"</span>                       <span>// 3
</span><span></span>    <span>"movi a7, 0x00; slli a7, a7, 16;"</span>
    <span>"xor a6, a6, a7;"</span>
    <span>"movi a7, 0x7c; slli a7, a7, 8;"</span>
    <span>"xor a6, a6, a7;"</span>
    <span>"movi a7, 0xf8;"</span>
    <span>"xor a6, a6, a7;"</span>

    <span>"movi a10, 0x52; callx8  a6;"</span> <span>// R
</span><span></span>    <span>"movi a10, 0x61; callx8  a6;"</span> <span>// a            
</span><span></span>    <span>"movi a10, 0x65; callx8  a6;"</span> <span>// e               
</span><span></span>    <span>"movi a10, 0x6C; callx8  a6;"</span> <span>// l               
</span><span></span>    <span>"movi a10, 0x69; callx8  a6;"</span> <span>// i               
</span><span></span>    <span>"movi a10, 0x7A; callx8  a6;"</span> <span>// z               
</span><span></span>    <span>"movi a10, 0x65; callx8  a6;"</span> <span>// e               
</span><span></span>    <span>"movi a10, 0x21; callx8  a6;"</span> <span>// !               
</span><span></span>    <span>"movi a10, 0x0a; callx8  a6;"</span> <span>// \n               
</span><span></span>
    <span>while</span><span>(</span><span>1</span><span>);</span>
<span>}</span>
</code></pre></div><p>To summarize, the above code implements the following:</p>
<ol>
<li>Command handler with a single command to perform a <strong>watchdog</strong> reset</li>
<li>NOP-like padding using <code>addi</code> instructions</li>
<li>Assembly for printing <code>Raelize!</code> on the serial interface</li>
</ol>
<p>Please note, the listing's numbers match the numbers in the code.</p>
<h4 id="timing">Timing</h4>
<p>We target a reasonably small attack window at the start of <strong>F</strong> which is shown in the picture below. We know from previous experiments that during this moment the <strong>flash bootloader</strong> is copied.</p>
<p>
    <a href="https://raelize.com/img/esp32/esp32-spi-pin1-during-boot.png">
        <img src="https://raelize.com/img/esp32/esp32-spi-pin1-during-boot.png" width="600px">
    </a>
</p>
<p>The glitch must be injected before our code in <strong>SRAM</strong> is entirely overwritten by the valid <strong>flash bootloader</strong>.</p>
<h4 id="attack-cycle">Attack cycle</h4>
<p>We took the following steps for each experiment to determine if the attack idea actually works. A successful glitch will print <code>Raelize!</code> on the serial interface.</p>
<ol>
<li>Set pin <strong>G0</strong> to low and perform a <strong>cold reset</strong> to enter <strong>UART bootloader</strong> boot mode</li>
<li>Use the <code>load_ram</code> command to execute our <strong>attack code</strong> from <strong>SRAM</strong></li>
<li>Send an <code>A</code> to the program to issue a <strong>warm reset</strong> into <strong>normal</strong> boot mode</li>
<li>Inject a glitch while the <strong>flash bootloader</strong> is being copied by the <strong>ROM code</strong></li>
</ol>
<h4 id="results">Results</h4>
<p>After running these experiments for more than a day, resulting in more than 1 million experiments, we did not observe any successful glitch…</p>
<h4 id="an-unexpected-result">An unexpected result</h4>
<p>Nonetheless, while analyzing the results, we noticed something unexpected.</p>
<p>The <strong>serial interface</strong> output for one of the experiments, which is shown below, indicated that the glitch caused an <strong>illegal instruction</strong> exception.</p>
<pre><code>ets Jun  8 2016 00:22:57
rst:0x10 (RTCWDT_RTC_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)
configsip: 0, SPIWP:0xee
clk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00
mode:DIO, clock div:2
load:0x3fff0008,len:4
load:0x3fff000c,len:3220
load:0x40078000,len:4816
load:0x40080400,len:18640
entry 0x40080740
Fatal exception (0): IllegalInstruction
epc1=0x661b661b, epc2=0x00000000, epc3=0x00000000, 
excvaddr=0x00000000, depc=0x00000000
</code></pre><p>These type of exceptions happened quite often when glitches are injected in a chip. This was not different for the <strong>ESP32</strong>. For most the exceptions the <code>PC</code> register is set to a value that's expected (i.e. a valid address). It does not happen often the <code>PC</code> register is set to such an interesting value.</p>
<p>The <code>Illegal Instruction</code> exception is caused as there is no valid instruction stored at the <code>0x661b661b</code> address. We conclude this value must come from somewhere and that is cannot magically end up in the <code>PC</code> register.</p>
<p>We analyzed the code that we load into the <strong>SRAM</strong> in order to find an explanation. The …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://raelize.com/posts/espressif-esp32-bypassing-encrypted-secure-boot-cve-2020-13629/">https://raelize.com/posts/espressif-esp32-bypassing-encrypted-secure-boot-cve-2020-13629/</a></em></p>]]>
            </description>
            <link>https://raelize.com/posts/espressif-esp32-bypassing-encrypted-secure-boot-cve-2020-13629/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24552482</guid>
            <pubDate>Tue, 22 Sep 2020 09:16:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Designing Up Banking]]>
            </title>
            <description>
<![CDATA[
Score 112 | Comments 45 (<a href="https://news.ycombinator.com/item?id=24552043">thread link</a>) | @merricksb
<br/>
September 22, 2020 | https://up.com.au/blog/the-evolutionary-design-of-up/ | <a href="https://web.archive.org/web/*/https://up.com.au/blog/the-evolutionary-design-of-up/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><p>When we launched nearly 2 years ago, we declared our first principles approach to banking:</p>
<blockquote>
<p>“Challenge the assumptions and limitations in banking and re-evaluate them from a more current point of view."</p>
</blockquote>
<p>This statement did not come out of a contrived workshop, or committee, or a board of investors. It was the resonating sentiment of a team that had spent years in the weeds of banking software and its bureaucracy. Existing banks have too much baggage to be able to see the future of banking for what it could be. They lacked the courage to really re-invent themselves. Some questions that emerged from that period:</p>
<ul>
<li><em>Why doesn’t every transaction have a timestamp, a merchant logo?</em></li>
<li><em>Why are all my payments to a particular person not grouped?</em></li>
<li><em>Why isn’t it easier to pay someone I’ve paid before?</em></li>
<li><em>Why am I sitting on hold on my phone to resolve an issue?</em></li>
<li><em>How can saving money be fun and engaging?</em></li>
</ul>
<p>We knew that if we followed our noses and solved these user-centric problems, and many others of course, that winning customers would take care of itself. We now have over 280,000 Upsiders and counting.</p>
<h2>Making Feedback Easy</h2>
<p>Up does not do traditional user testing. There, I said it. That’s our dirty secret.</p>
<p>We think there are other techniques that are faster and better indicators of real-world behaviour. We also believe we have two advantages:</p>
<ul>
<li><strong>An innovation mindset</strong></li>
<li><strong>A deeper relationship with our customers than any financial institution in Australia</strong></li>
</ul>
<p>Much of banking is a known problem space. We feel like we're taking a more creative approach to the problem, but that's not to say we don't rely on user feedback. Our <em>Talk to Us</em> section encourages Upsiders to give us feedback or suggest ideas. To date, we have received over 6,000 ideas via this channel, making it one of the most crucial conduits into the minds of our customers. We also have other highly-engaged channels such as our socials and the newsletter where we hear, almost instantly, what customers think of what we’ve delivered.</p>
<figure>
  <img loading="lazy" src="https://up.com.au/449d9fa0873011802b194d6b669ffe9e/feedback-categories.gif" alt="Initiating a 'Talk to Us' chat with our team">
  <figcaption>
    Our <em>Talk to Us</em> trinity; ask for help, give feedback and report a bug
  </figcaption>
</figure>
<h2>Engagement Is the Name of the Game</h2>
<p>So where do we get our inspiration? It's rarely in the banking or fintech space. These are the apps that exist on billions of devices and set the bar for mobile digital experiences:</p>
<figure>
  <span>
      <a href="https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/c6b2e/inspire_apps.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="World leaders for designing mobile experiences" title="World leaders for designing mobile experiences" src="https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/40619/inspire_apps.jpg" srcset="https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/b0fd2/inspire_apps.jpg 175w,
https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/aaaf9/inspire_apps.jpg 350w,
https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/40619/inspire_apps.jpg 700w,
https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/e8c9e/inspire_apps.jpg 1050w,
https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/5814a/inspire_apps.jpg 1400w,
https://up.com.au/static/2e11befb2645e3c45227a91f56aed310/c6b2e/inspire_apps.jpg 2175w" sizes="(max-width: 700px) 100vw, 700px" loading="lazy">
  </a>
    </span>
  <figcaption>
    The leaders in mobile experience design
  </figcaption>
</figure>
<p>While they vary in purpose, their collective dominance in the landscape means they teach and familiarise patterns to expect when using our phones.</p>
<ul>
<li><em>How does TikTok allow you to react to content quickly and effortlessly?</em></li>
<li><em>What similarities are there between Whatsapp, Messenger and Instagram when conversing with friends?</em></li>
<li><em>When explaining new features what language do these platforms use? When do they use visualisations? When do they use words?</em></li>
<li><em>When is Snapchat playful and when is it serious?</em></li>
<li><em>Do I identify with my real name or username for Instagram? What about Facebook?</em></li>
</ul>
<p>As users we immerse ourselves in the apps that have nailed engagement. While acknowledging the patterns they establish, we also appreciate the balance between knowing when to follow them and when to do our own thing. Up has a handful of atypical patterns, but as long as they are usable and intuitive they can become distinct moments used to engage and delight.</p>
<p>Why is it so important that Upsiders engage with the Up app? Aside from the opportunity to nurture a relationship through frequent interactions, we also think that being more engaged with your money encourages better financial literacy. It’s common that people fall into credit trouble by continuing to use their plastic cards without checking their balance regularly. By making Up an engaging experience, we’re making Upsiders more confident and connected to their finances.</p>
<h2>Becoming an Upsider</h2>
<p>We’ve previously deep-dived on our <a href="https://up.com.au/blog/designing-a-super-powered-welcome-pack-experience/">card delivery experience</a>. Before we could even get that far, we had to solve the huge problem of enabling people to sign up for a bank account without leaving the comfort of a mobile app — something that had not been done before in Australia.</p>
<p>We knew that the sheer amount of information we needed from the user was going to be a challenge. Banking is highly regulated in Australia, so we had to cater for many types of identity data (passport, license etc). Most apps can get away with just asking for a username, email and password.</p>
<p>Mobile flows tend to be atomic with a single input per screen, so with identity verification we were anticipating quite a long flow. Our focus was to trim the fat wherever possible, not just by reducing the total number of screens but also by making it easy for people to understand what was being asked of them as they progressed.</p>
<figure>
  <span>
      <a href="https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/c6b2e/signup-flow.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="Large and complicated data capture flow" title="Large and complicated data capture flow" src="https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/40619/signup-flow.jpg" srcset="https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/b0fd2/signup-flow.jpg 175w,
https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/aaaf9/signup-flow.jpg 350w,
https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/40619/signup-flow.jpg 700w,
https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/e8c9e/signup-flow.jpg 1050w,
https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/5814a/signup-flow.jpg 1400w,
https://up.com.au/static/e381b2e19148dfb3bd4e3475c1d71c01/c6b2e/signup-flow.jpg 2175w" sizes="(max-width: 700px) 100vw, 700px" loading="lazy">
  </a>
    </span>
  <figcaption>
    A portion of the Sign-up flow
  </figcaption>
</figure>
<p>We fought to reduce the amount of data we needed to collect. Why does banking need to know your gender? We cut it. Do you need your card sent to a PO Box? We’ll give you a contextual experience based on answers you’ve already provided rather than a scrolling form full of fields. As we tested the new flow amongst ourselves and with beta users from our waitlist, we made a few changes:</p>
<ul>
<li>Anchoring buttons to the bottom of the viewport, and making them full-width. Close to your thumb and easy to hit.</li>
<li>Removing anything extra like images, so you could move faster without distractions.</li>
<li>Using conversational instructions (eg “What is your mobile number?” Instead of “Enter mobile number”). And only using secondary body text when it was really necessary.</li>
<li>Using example text for the input placeholder, so you'd know what the right info looks like.</li>
</ul>
<figure>
  <video autoplay="true" loop="true" muted="true" playsinline="true" alt="Signup flow, before and after">  
    <source type="video/mp4" src="https://up.com.au/85d89ef5fc1450db9c15cb25ceed64d1/signup-flow-tweaks.mp4">
  </video>
  <figcaption>    
    Tweaks made to the signup flow
  </figcaption>
</figure>
<p>All of these small iterations reduced the cognitive load for users and made it feel easy and fast, despite the number of screens. This granularity was important as there isn’t actually a single sign up flow, but several which vary depending on your circumstances and the information we are required to collect.</p>
<h2>Up Yeah!</h2>
<p>Once we became more confident in our onboarding flow and how streamlined it was becoming, you could feel the team were ready to ship it and move onto the next bit of work.</p>
<p>It’s easy to have the blinkers on when you have such a measurable and objective goal — get users into the app in under x time — but stepping back, we made the observation that although we’d nailed sign up speed, something was missing the first time you landed on your activity feed. A moment that should feel significant and celebratory — you’ve literally just opened a bank account in under 3 minutes through your phone — feels clinical and unimpressive. We appreciate great brand moments in digital experiences, most notably MailChimp’s use of emotive design throughout their email software.</p>
<figure>
  <img loading="lazy" src="https://up.com.au/b5b11a8931d184e937662bc553657582/mailchimp.gif" alt="Mailchimp animations">
  <figcaption>  
    Great emotive design through animations by MailChimp
  </figcaption>
</figure>
<p>Email campaigns are stressful exercises — you can’t unsend them once they go out. It’s the nature of the beast. Interestingly, it was the arm of their mascot beast Freddie that was used in these cute but situationally-aware animations. The red button before launch, the high five once your campaign is live, and the rock fist for scheduled campaigns.</p>
<p>Our creative director Pete was eager to create a moment post-signup that made you go “f*ck yeah” in celebration.</p>
<figure>
  <img loading="lazy" src="https://up.com.au/1c642ec78c4396636c3b1bc150d48a29/upyeahmoment-signup.gif" alt="Up Yeah! account created">
  <figcaption>  
    The first of many ‘Up Yeah!’ moments
  </figcaption>
</figure>
<p>Almost immediately after this launched it was being shared by new Upsiders and acknowledged through our feedback channels. Of course you only give yourself an opportunity for these moments if you are nailing the fundamentals. But this reinforcement encouraged us to lean into these seemingly frivolous treatments as a way to delight Upsiders while building a stronger brand.</p>
<h2>Logging in Sucks</h2>
<p>Sometimes the design process involves considering what you don’t see as much as what you do see. For Up, removing the login screen is a great example of this. Banking apps tend to have heavy authentication flows before you see your feed or landing screen. It’s easy to see how this came to be; mobile apps came after desktop banking sites, and so inherited their secure lockdown context. And perhaps it goes back even further back to the mindset of “money belongs in a vault, behind a locked door”.</p>
<p>With all the technological advances in mobile device security, from the humble PIN code to sophisticated biometric recognition, it’s worth questioning some of the assumptions and trade-offs made in the name of security.</p>
<blockquote>
<p>Logging in is a big friction point. Especially if we’re trying to help inform you about your finances.</p>
</blockquote>
<p>There’s an interesting distinction that’s prominent in tech — the difference between security and privacy. If we break out of the world of banking apps and look at some of the apps we use each and every day – imagine if you had to enter a passcode every time you opened your email, or your messages? Yet if someone had access to either, they could reset every password you have and really cause you some headaches. We found a helpful albeit simplified distinction when approaching this aspect of the user-experience:</p>
<br>
<table>
    <tbody><tr>
        <td><img loading="lazy" src="https://up.com.au/251bbe56c807fe4657239958a04f14e6/zap-coat.gif" alt="Mailchimp animations">
        </td>
        <td>
          <h3>Privacy</h3>
          Protecting information that is sensitive
          <em> "Read Only"</em>
        </td>
    </tr>
</tbody></table>
<table>
    <tbody><tr>
        <td><img loading="lazy" src="https://up.com.au/6e9f440aae0b397bd198cc1182838ed9/zap-lock.gif" alt="Mailchimp animations">
        </td>
        <td>
          <h3>Security</h3>
          Protecting against being compromised financially
          <em>"Write Access"</em>
        </td>
    </tr>
</tbody></table>

<p>A design decision we’ve made with this framework is removing the need to enter a passcode by default (which can be re-enabled in settings), and also when moving money where there isn’t any risk. Transfering between Savers doesn’t require you to use your phone’s authentication flow (e.g. passcode, Apple’s FaceID or Android’s BioAuth), but moving money into your spending account or sending money outside of your account (e.g. to another bank or via BPay) does.</p>
<p>Our philosophy is to ask for authentication where appropriate to maximise security, while also letting Upsiders enjoy the benefits of being more informed and connected with their money in a low-friction way.  Our push …</p></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://up.com.au/blog/the-evolutionary-design-of-up/">https://up.com.au/blog/the-evolutionary-design-of-up/</a></em></p>]]>
            </description>
            <link>https://up.com.au/blog/the-evolutionary-design-of-up/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24552043</guid>
            <pubDate>Tue, 22 Sep 2020 07:59:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mainline Linux on the MikroTik RB3011]]>
            </title>
            <description>
<![CDATA[
Score 95 | Comments 40 (<a href="https://news.ycombinator.com/item?id=24550846">thread link</a>) | @pabs3
<br/>
September 21, 2020 | https://www.earth.li/~noodles/blog/2020/09/rb3011-mainline.html | <a href="https://web.archive.org/web/*/https://www.earth.li/~noodles/blog/2020/09/rb3011-mainline.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <p>I upgraded my home internet connection to fibre (FTTP) <a href="https://www.earth.li/~noodles/blog/2019/10/native-ipv6-fttp.html">last October</a>. I’m still on an 80M/20M service, so it’s no faster than my old VDSL FTTC connection was, and as a result for a long time I continued to use my HomeHub 5A running <a href="https://openwrt.org/">OpenWRT</a>. However the FTTP ONT meant I was using up an additional ethernet port on the router, and I was already short, so I ended up with a GigE switch in use as well. Also my wifi is handled by a <a href="https://unifi-network.ui.com/">UniFi</a>, which takes its power via Power-over-Ethernet. That mean I had a router, a switch and a PoE injector all in close proximity. I wanted to reduce the number of devices, and ideally upgrade to something that could scale once I decide to upgrade my FTTP service speed.</p>

<p>Looking around I found the <a href="https://mikrotik.com/product/RB3011UiAS-RM">MikroTik RB3011UiAS-RM</a>, which is a rack mountable device with 10 GigE ports (plus an SFP slot) and a dual core <a href="https://www.qualcomm.com/products/ipq8064">Qualcomm IPQ8064</a> ARM powering it. There’s 1G RAM and 128MB NAND flash, as well as a USB3 port. It also has PoE support. On paper it seemed like an ideal device. I wasn’t particularly interested in running RouterOS on it (the provided software), but that’s based on Linux and there was some work going on within OpenWRT to add support, so it seemed like a worthwhile platform to experiment with (what, you expected this to be about me buying an off the shelf device and using it with only the supplied software?). As an added bonus a friend said he had one he wasn’t using, and was happy to sell it to me for a bargain price.</p>

<p><img alt="RB3011 router in use" src="https://www.earth.li/~noodles/blog/images/2020/rb3011.jpg"></p>

<p>I did try out RouterOS to start with, but I didn’t find it particularly compelling. I’m comfortable configuring firewalling and routing at a Linux command line, and I run some additional services on the router like my <a href="https://www.earth.li/~noodles/blog/2018/05/mqtt-broker.html">MQTT</a> broker, and <a href="https://www.earth.li/~noodles/blog/2018/09/netlink-arp-presence.html">mqtt-arp</a>, my wifi device presence monitor. I could move things around such that they ran on the <a href="https://www.earth.li/~noodles/blog/2019/07/upgrading-the-house-server.html">house server</a>, but I consider them core services and as a result am happier with them on the router.</p>

<p>The first step was to get something booting on the router. Luckily it has an RJ45 serial console port on the back, and a reasonably featured bootloader that can manage to boot via tftp over the network. It wants an ELF binary rather than a plain kernel, but Sergey Sergeev had done the hard work of getting <a href="https://github.com/adron-s/uboot-ipq806x">u-boot working for the IPQ8064</a>, which mean I could just build normal u-boot images to try out.</p>

<p>Linux upstream already had basic support for a lot of the pieces I was interested in. There’s a slight fudge around <code>AUTO_ZRELADDR</code> because the network coprocessors want a chunk of memory at the start of RAM, but there’s ongoing discussions about how to handle this cleanly that I’m hopeful will eventually mean I can drop that hack. Serial, ethernet, the QCA8337 switches (2 sets of 5 ports, tied to different GigE devices on the processor) and the internal NOR all had drivers, so it was a matter of crafting an appropriate DTB to get them working. That left niggles.</p>

<p>First, the second switch is hooked up via SGMII. It turned out the IPQ806x <code>stmmac</code> driver didn’t initialise the clocks in this mode correctly, and neither did the <code>qca8k</code> switch driver. So I need to fix up both of those (Sergey had handled the stmmac driver, so I just had to clean up and submit his patch). Next it turned out the driver for talking to the Qualcomm firmware (SCM) had been updated in a way that broke the old method needed on the IPQ8064. Some git archaeology figured that one out and provided a solution. Ansuel Smith helpfully provided the DWC3 PHY driver for the USB port. That got me to the point I could put a Debian armhf image onto a USB stick and mount that as root, which made debugging much easier.</p>

<p>At this point I started to play with configuring up the device to actually act as a router. I make use of a number of VLANs on my home network, so I wanted to make sure I could support those. Turned out the stmmac driver wasn’t happy reconfiguring its MTU because the IPQ8064 driver doesn’t configure the FIFO sizes. I found what seem to be the correct values and plumbed them in. Then the <code>qca8k</code> driver only supported port bridging. I wanted the ability to have a trunk port to connect to the upstairs switch, while also having ports that only had a single VLAN for local devices. And I wanted the switch to handle this rather than requiring the CPU to bridge the traffic. Thankfully it’s easy to find a copy of the QCA8337 datasheet and the kernel <a href="https://www.kernel.org/doc/html/latest/networking/dsa/index.html">Distributed Switch Architecture</a> is pretty flexible, so I was able to implement the necessary support.</p>

<p>I stuck with Debian on the USB stick for actually putting the device into production. It makes it easier to fix things up if necessary, and the USB stick allows for a full Debian install which would be tricky on the 128M of internal NAND. That means I can use things like <a href="https://wiki.nftables.org/">nftables</a> for my firewalling, and use the standard Debian packages for things like <a href="https://collectd.org/">collectd</a> and <a href="https://mosquitto.org/">mosquitto</a>. Plus for debug I can fire up things like tcpdump or tshark. Which ended up being useful because when I put the device into production I started having weird IPv6 issues that turned out to be a lack of proper Ethernet multicast filter support in the IPQ806x ethernet device. The driver would try and setup the multicast filter for the IPv6 NDP related packets, but it wouldn’t actually work. The fix was to fall back to just receiving all multicast packets - this is what the vendor driver does.</p>

<p>Most of this work will be present once the 5.9 kernel is released - the basics are already in 5.8. Currently not queued up that I can think of are the following:</p>

<ul>
  <li>stmmac IPQ806x FIFO sizes. I sent out an RFC patch for these, but didn’t get any replies. I probably just need to submit this.</li>
  <li>NAND. This is missing support for the QCOM ADM DMA engine. I’ve sent out the patch I found to enable this, and have had some feedback, so I’m hopeful it will get in at some point.</li>
  <li>LCD. AFAICT LCD is an ST7735 device, which has kernel support, but I haven’t spent serious effort getting the SPI configuration to work.</li>
  <li>Touchscreen. Again, this seems to be a zt2046q or similar, which has a kernel driver, but the basic attempts I’ve tried don’t get any response.</li>
  <li>Proper SFP functionality. The IPQ806x has a PCS module, but the stmmac driver doesn’t have an easy way to plumb this in. I have ideas about how to get it working properly (and it can be hacked up with a fixed link config) but it’s not been a high priority.</li>
  <li>Device tree additions. Some of the later bits I’ve enabled aren’t yet in the mainline RB3011 DTB. I’ll submit a patch for that at some point.</li>
</ul>

<p>Overall I consider the device a success, and it’s been entertaining getting it working properly. I’m running a mostly mainline kernel, it’s handling my house traffic without breaking a sweat, and the fact it’s running Debian makes it nice and easy to throw more things on it as I desire. However it turned out the RB3011 isn’t as perfect device as I’d hoped. The PoE support is passive, and the UniFi wants 802.1af. So I was going to end up with 2 devices. As it happened I picked up a cheap <a href="https://eu.dlink.com/uk/en/products/dgs-1210-series-gigabit-smart-plus-switches">D-Link DGS-1210-10P</a> switch, which provides the PoE support as well as some additional switch ports. Plus it runs Linux, so more on that later…</p>

  </article></div>]]>
            </description>
            <link>https://www.earth.li/~noodles/blog/2020/09/rb3011-mainline.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24550846</guid>
            <pubDate>Tue, 22 Sep 2020 04:27:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[RIP iCloud, Self-Hosting Part 5: Finale]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 66 (<a href="https://news.ycombinator.com/item?id=24550197">thread link</a>) | @walterbell
<br/>
September 21, 2020 | https://www.naut.ca/blog/2020/05/05/self-hosting-series-part-5-finale/ | <a href="https://web.archive.org/web/*/https://www.naut.ca/blog/2020/05/05/self-hosting-series-part-5-finale/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="https://www.naut.ca/blog/content/images/2020/05/Screen-Shot-2020-05-05-at-3.30.56-PM.jpeg" width="400px/"></p><p>Just yesterday, I turned off iCloud on <strong>all</strong> my Apple devices. I then took a moment to savour my liberation from Apple's walled garden.</p>
<p>It has been over two years since I first dabbled in hosting my own blog server to finally disabling my iCloud account. There is a good reason as to why it took so long. Apple has cultivated a beautiful hardware+software ecosystem over the years, resulting in an ecosystem filled with magical features such as Apple Pay, Home Sharing, Handoff, and Instant Hotspot. An iCloud account is apparently a requirement for all of those features, which is a shame. I didn't find out until after I logged out and lost those features, but maybe it was for the better. Anyway, here's the proof:<br>
<img src="https://www.naut.ca/blog/content/images/2020/05/Screen-Shot-2020-05-05-at-3.38.39-PM.jpeg" alt="Screen-Shot-2020-05-05-at-3.38.39-PM"><br>
Now this may sound odd, but I feel that the fallbacks and replacements to iCloud features are sometimes easier to understand and give more of a feeling of groundedness, albeit at the cost of convenience. For example, I no longer debate about whether to use Apple Pay or not, and I feel grounded knowing that the physical card is all I need to protect, and that my credit card won't run out of battery. I now plug in a cable to backup my iPhone, and I hear the hard disks on my server grinding away as the files are transferred. I'm confident that something, if anything, is happening. By physically self-hosting emails in the house, I feel secure that a company can't tell me that my account has vanished, a concept that is becoming <a href="https://news.ycombinator.com/item?id=23057365">increasingly common</a>.</p>
<p>As we switched from using physical devices such as floppy disks, CDs, and servers to storing data and logic online, we lost a sense of physicality and tangibility, replaced by an abstract notion of the <a href="https://www.youtube.com/watch?v=8GRPArTor7w">cloud</a>. Most programmers realize that the cloud is not a magical place and are comfortable with the notion, but I've noticed that the cloud instills fear, uncertainty, and doubt in others.</p>
<p>Given that I've been to hell and back setting up a self-hosted cloud (even my programmer friends stare at me quizzically), I sound crazy to mention that this has made things "easier". I'm definitely <strong>not</strong> saying that self-hosting is easier than using iCloud, but it has made me aware of what we are missing. In any interactive system, the true complexity must hide somewhere, and in this case, Apple is offering to manage it for you. In an attempt for trust, security, and ease of use, these services create a greater disconnect between you and your "interactee". To give an example, take transactions between you and a merchant. First there was bartering. Then there was cash. These are both easy to understand, and almost nobody has trouble understanding the end-to-end concepts. However, take Apple Pay. Here's a high-level example of what it actually does:</p>
<blockquote>
<p><em>NFC Coil in POS Terminal energizes iPhone antenna ⟶ sends data to NFC Chip ⟶ activates iPhone CPU ⟶ requests Face ID unlock ⟶ beams tiny Infrared dots at your face ⟶ Infrared Camera constructs 3D model using Machine Learning model ⟶ decrypts credit card details in Secure Enclave ⟶ creates credit card token ⟶ sends back to iPhone NFC chip ⟶ transmits to POS Terminal ⟶ encrypts with TLS ⟶ sends through internet to the credit card network ⟶ network replies back</em></p>
</blockquote>
<p>To reiterate, this is a <em>high-level</em> overview. So yeah, try feeling grounded with that. It's a miracle that it even works.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Overall, my self-hosting series has reduced the FUD surrounding these services for <em>me</em> (and hopefully at least another reader), since I now understand how the software works. I feel it is an accomplishment to be disconnected from Apple, knowing that I'm free to switch hardware whenever I please. Although increased privacy was one of the main reasons I started this series, I haven't really noticed anything different day-to-day. This series has been a very interesting journey, and it will be something that I will continue to explore with future blog posts. As somebody who is now examining iCloud from an outsider perspective for the first time, it is mind-boggling the amount of complexity that Apple manages and exerts power over, such as their COVID-19 Contact Tracing technology. I wonder what the future holds for Apple, and how its values will change over time.</p>
<h3 id="alternativestoicloud">Alternatives to iCloud:</h3>
<p><em>Note: E2EE software with easily exportable data is acceptable, e.g. Firefox Sync</em></p>
<p><strong>Rationale</strong></p>
<ul>
<li><a href="https://www.naut.ca/blog/2019/06/19/self-hosting-series-part-1-saying-bye-bye-to-icloud/">Self-Hosting Part 1: Why I'm Ditching iCloud</a></li>
<li><a href="https://medium.com/bugbountywriteup/how-apple-stored-all-your-email-metadata-for-years-on-their-servers-2a61b1a3232d">How Apple store all your email metadata for years on their servers</a></li>
</ul>
<p><strong>iCloud Mail, Notes</strong></p>
<ul>
<li><a href="https://www.naut.ca/blog/2019/10/06/self-hosting-series-part-2-mail-server/">Self-Hosting Part 2: Mail Server</a></li>
</ul>
<p><strong>iCloud Contacts, Calendar, Reminders</strong></p>
<ul>
<li><a href="https://www.naut.ca/blog/2019/11/16/self-hosting-series-part-3-radicale-server/">Self-Hosting Part 3: WebDAV Server</a></li>
</ul>
<p><strong>iCloud Safari</strong></p>
<ul>
<li><a href="https://hacks.mozilla.org/2018/11/firefox-sync-privacy/">Firefox Sync</a></li>
</ul>
<p><strong>iCloud Backup</strong></p>
<ul>
<li><a href="https://www.naut.ca/blog/2020/03/20/self-hosting-series-part-4-backup/">Self-Hosting Part 4: iOS + macOS Backup</a></li>
<li><a href="https://support.apple.com/en-ca/HT203977#computer">iTunes Local Backup</a></li>
</ul>
<p><strong>iCloud Drive</strong></p>
<ul>
<li><a href="https://9to5mac.com/2019/06/17/ios-13-beta-2-enables-smb-server-connectivity-in-the-files-app/">Samba Server</a></li>
<li><a href="https://nextcloud.com/">NextCloud</a></li>
</ul>
<p><strong>iCloud Photos</strong></p>
<ul>
<li><a href="https://nextcloud.com/">NextCloud</a></li>
<li><a href="https://support.apple.com/en-us/HT201313">iTunes Photo Sync</a></li>
</ul>
<p><strong>iCloud Keychain</strong></p>
<ul>
<li><a href="https://bitwarden.com/">Bitwarden (Self-hosted)</a></li>
<li><a href="https://www.enpass.io/">Enpass (Self-hosted)</a></li>
</ul>
<p><strong>iCloud Home</strong><br>
TBD. Currently, HomeKit requires iCloud Keychain to sync with your iOS devices. I am trying to develop a hub that rebroadcasts all HomeKit accessories allowing for multiple devices to connect to the same HomeKit device.</p>
<p><strong>Further Resources</strong></p>
<ul>
<li><a href="https://roll.urown.net/index.html">Roll Your Own Network</a></li>
</ul>
</div></div>]]>
            </description>
            <link>https://www.naut.ca/blog/2020/05/05/self-hosting-series-part-5-finale/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24550197</guid>
            <pubDate>Tue, 22 Sep 2020 02:07:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When feature flags do and don’t make sense (2019)]]>
            </title>
            <description>
<![CDATA[
Score 98 | Comments 40 (<a href="https://news.ycombinator.com/item?id=24549917">thread link</a>) | @nomdep
<br/>
September 21, 2020 | https://software.rajivprab.com/2019/12/19/when-feature-flags-do-and-dont-make-sense/ | <a href="https://web.archive.org/web/*/https://software.rajivprab.com/2019/12/19/when-feature-flags-do-and-dont-make-sense/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<div><figure><a href="https://www.redbubble.com/i/canvas-print/If-Else-Software-Developer-Joke-Beer-Lover-by-VaSkoy/33140544.5Y5V7" target="_blank"><img data-attachment-id="282" data-permalink="https://software.rajivprab.com/flag/" data-orig-file="https://softwarerajivprab.files.wordpress.com/2019/12/flag.png" data-orig-size="896,1480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="flag" data-image-description="" data-medium-file="https://softwarerajivprab.files.wordpress.com/2019/12/flag.png?w=182" data-large-file="https://softwarerajivprab.files.wordpress.com/2019/12/flag.png?w=620" src="https://softwarerajivprab.files.wordpress.com/2019/12/flag.png?w=182" alt="" srcset="https://softwarerajivprab.files.wordpress.com/2019/12/flag.png?w=182 182w, https://softwarerajivprab.files.wordpress.com/2019/12/flag.png?w=364 364w, https://softwarerajivprab.files.wordpress.com/2019/12/flag.png?w=91 91w" sizes="(max-width: 182px) 100vw, 182px"></a></figure></div>



<p>Over the past years, I’ve worked in multiple teams adopting very different strategies when it comes to feature flags. I’ve seen the pros and cons of both, and over time, I found myself disagreeing with any fundamentalist position on their use. There is a lot of nuance to this topic, and I think it is worth considering more carefully the various scenarios where feature flags do and do not make sense.</p>



<h2>The Reasons For</h2>



<p>There are a few major scenarios where feature flags make a lot of sense. The first is when it’s used for <a rel="noreferrer noopener" aria-label="A/B testing (opens in a new tab)" href="https://en.wikipedia.org/wiki/A/B_testing" target="_blank">A/B testing</a>, where you absolutely do want different behaviors for different users, based on their randomly assigned treatment. I’ve seen this strategy employed extremely well at Amazon where new features are gated by a “feature flag” that is actually controlled by an internal A/B testing framework. The framework randomly exposes some Amazon customers to the new feature, and then monitors their subsequent behavior in order to estimate the business impact of launching the feature.&nbsp;</p>



<p>I was initially skeptical, but was soon won over by how easy the framework was to use, and the valuable insights it provided on the benefits (or drawbacks) of certain features. “Flavor of the month” decisions were replaced with real data. And none of this is possible without the use of “feature flags” to dynamically toggle new features.</p>



<hr>



<p>Another great use case for feature flags, is when you’re working on a very complex epic that require many different sub-tasks to be completed in different parts of the system. Sub-tasks that are too numerous and invasive to be done in a single pull-request. </p>



<p>In such cases, trying to keep all these disparate changes in side-branches and coordinating a simultaneous merge and deployment, is a recipe for disaster. It’s far more manageable to gate any disruptive changes behind a master flag, merge and deploy all the sub-commits incrementally, and do a flag-flip once all the pieces are in place.</p>



<hr>



<p>One last use case for feature flags, is when you do not have control over your deployments. For example, consider the Facebook Android app, which contains code contributed by hundreds of different teams, all combined and deployed as a single binary. In such scenarios, performing rollbacks can be infeasible. For practical, political, bureaucratic or even marketing reasons. In such cases, feature flags allow your team to toggle new functionality or mitigate risky changes, without having to rollback or deploy any new binaries.</p>



<p><em><a href="https://www.reddit.com/r/programming/comments/i5zbvk/when_feature_flags_do_and_dont_make_sense/" target="_blank" rel="noreferrer noopener">Someone on Reddit pointed out</a> a similar use-case for feature flags: targeting a very specific launch date for marketing reasons, while still deploying your code much earlier, in order to ensure stability. You can then have a “dynamic” feature flag that automatically enables itself at a specific time. This is also a great use-case for similar reasons – changing functionality in situations where deploying a new binary is impractical.</em></p>



<h2>Risk Aversion</h2>



<p>The above are all fantastic use cases for feature flags, but I’ve also seen teams get bogged down by policies that overreach in their use. For example, mandating that every single code change should be behind a feature flag, <em>“just in case we made a mistake”</em>.</p>



<p>Risk management should indeed be a priority for all teams. But there are better ways of doing this than relying on feature flags, especially if your team has control over its own deployments. The <a rel="noreferrer noopener" aria-label=" (opens in a new tab)" href="https://software.rajivprab.com/2019/04/28/rethinking-software-testing-perspectives-from-the-world-of-hardware/" target="_blank">vast majority of your bugs should be caught by your automated test suite</a> and/or QA process. And the last few stragglers should be handled using <a rel="noreferrer noopener" aria-label="incremental deployments, production alarms and rollbacks (opens in a new tab)" href="https://software.rajivprab.com/2019/11/25/the-birth-of-legacy-software-how-change-aversion-feeds-on-itself/" target="_blank">incremental deployments, production alarms and rollbacks</a>.</p>



<p>Besides, as soon as any problem is detected, the recommendation at places like Google is to <a rel="noreferrer noopener" aria-label="rollback first and ask questions later (opens in a new tab)" href="https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons" target="_blank">rollback first and investigate the problem later</a>:</p>



<blockquote><p><em>We have seen this at Google any number of times, where a hastily deployed roll-forward fix either fails to fix the original problem, or indeed makes things worse. Even if it fixes the problem it may then uncover other latent bugs in the system; you’re taking yourself further from a known-good state, into the wilds of a release that hasn’t been subject to the regular strenuous QA testing. At Google, our philosophy is that “rollbacks are normal.” When an error is found or reasonably suspected in a new release, the releasing team rolls back first and investigates the problem second</em></p></blockquote>



<p> When things are on fire, the last thing you want to do is root-cause the bug and figure out which flag-flip will safely fix the problem. And that may not even fix things – there’s no guarantee that even if your teammate tried to put his changes behind a feature flag, he didn’t inadvertently introduce a bug that cannot be solved by a flag-flip.</p>



<p>Feature flags are a poor man’s alternative to binary rollbacks, and they definitely aren’t a substitute for having a great automated test suite and a robust QA process. If you’re relying on feature flags to remedy production bugs, you should stop and evaluate your team’s practices. Risk aversion is often a smell of your team <a rel="noreferrer noopener" aria-label="entering into a doom loop which will only get worse and worse with time (opens in a new tab)" href="https://software.rajivprab.com/2019/11/25/the-birth-of-legacy-software-how-change-aversion-feeds-on-itself/" target="_blank">entering into a doom loop which will only get worse and worse with time</a>.</p>



<h2>Death By Feature Flags</h2>



<p>You may be wondering at this point why we shouldn’t use feature flags anyway. After all, <em>“defense in depth” …</em> and it never hurts to have more fine-grain flexibility right?</p>



<p>While feature flags are great in some cases, we should also keep in mind their costs. Software engineering is primarily an exercise in managing complexity. And each feature flag immediately doubles the universe of corner cases that your programmers have to understand, and your code is required to handle. <em>“But what would happen if Foo is enabled, Bar is disabled, and we do independent A/B tests on Baz and Kaz on the same day?”</em> In my experience, this combinatorial explosion in complexity can and <strong>will</strong> lead to bugs. Not to mention slowing down the speed at which your team can make any changes.</p>



<p><a rel="noreferrer noopener" href="https://news.ycombinator.com/item?id=24549917" target="_blank">To quote an amusing anecdote shared online</a>: <em>“A flag that hasn’t been set to off in a year can be masking a major regression. At my last job we had two major outages in as many years from defunct flags defaulting to ‘off’ when the feature flag system failed to return flag states.”</em></p>



<p><em>“But these feature flags are only temporary. You should be removing them as soon as possible!”</em></p>



<p>Sure, and we should also not allow our tech debt to accumulate and we should follow every single best-practice religiously. Unfortunately, this never happens in any corporate environment. Even in great teams, tech debt often gets de-prioritized in the face of new requests. Newcomers to the team or those on their way out, aren’t always disciplined enough to clean up their flags after a successful rollout. And sometimes, these tasks simply slip through the cracks and get forgotten.</p>



<p><a rel="noreferrer noopener" href="https://news.ycombinator.com/item?id=24549917" target="_blank">Someone on HackerNews has pointed out</a> that as of Sept-2020, the release version of Windows <em>“has roughly 2500 feature flags. Some are permanently jammed into the on position, some off position, and the rest are configurable by its experimentation frameworks and hackers”</em>.</p>



<p>There is no better illustration of this than the KCG debacle where a financial firm lost half a billion dollars and almost went bankrupt in 30 minutes, partly due to dead code that was behind a feature flag.</p>



<blockquote><p><a href="https://www.bugsnag.com/blog/bug-day-460m-loss"><em>The cause of the failure</em></a><em> was due to multiple factors. However, one of the most important was that a flag which had previously been used to enable Power Peg… Power Peg had been obsolete since 2003, yet still remained in the codebase some eight years later.</em></p><p><em>In 2005, an alteration was made to the Power Peg code which inadvertently disabled safety-checks which would have prevented such a scenario. However, this update was deployed to a production system at the time, despite no effort having been made to verify that the Power Peg functionality still worked</em></p></blockquote>



<hr>



<p>Feature flags are a powerful tool that can help you experiment with new features, manage the rollout of complex epics, and mitigate the problems associated with not controlling your team’s deployments. </p>



<p>But they come at a significant cost, in the form of code complexity, tech debt, slower development speeds, and inevitably, bugs. </p>



<p>As tempting as it may be, there is no silver bullet here. Weigh the pros against the cons, and use this tool judiciously when it makes sense to do so.</p>



<hr>



<p><a rel="noreferrer noopener" href="https://www.reddit.com/r/programming/comments/i5zbvk/when_feature_flags_do_and_dont_make_sense/" target="_blank"><em>Discussion thread on /r/programming</em></a><br><em><a href="https://news.ycombinator.com/item?id=24549917" target="_blank" rel="noreferrer noopener">Discussion thread on Hacker News</a></em></p>
	</div></div>]]>
            </description>
            <link>https://software.rajivprab.com/2019/12/19/when-feature-flags-do-and-dont-make-sense/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24549917</guid>
            <pubDate>Tue, 22 Sep 2020 01:11:47 GMT</pubDate>
        </item>
    </channel>
</rss>
