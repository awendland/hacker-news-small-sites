<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 20 Aug 2020 00:50:28 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 20 Aug 2020 00:50:28 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Architecture of the Nintendo DS]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 38 (<a href="https://news.ycombinator.com/item?id=24195751">thread link</a>) | @Polylactic_acid
<br/>
August 17, 2020 | https://www.copetti.org/projects/consoles/nintendo-ds/ | <a href="https://web.archive.org/web/*/https://www.copetti.org/projects/consoles/nintendo-ds/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div class="page"><nav id="navbar"></nav><div><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul><div><div id="cover-diagram"><a href="https://www.copetti.org/images/consoles/nintendods/diagram.png"><picture>
<img alt="Diagram" src="https://www.copetti.org/images/consoles/nintendods/diagram.png" data-src="https://www.copetti.org/images/consoles/nintendods/diagram.png"></picture></a><figcaption>If you have trouble following the components: Top is only accessed by ARM9, bottom section is ARM7-only, middle section is shared</figcaption></div></div></div><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>This console is an interesting answer to many needs that weren’t possible to fulfil in the handheld ecosystem. There will be some innovation and a few compromises, but this combination may pave the way for new and ingenious content.</p><hr><h2 id="cpu">CPU</h2><p>As with Nintendo’s <a href="https://www.copetti.org/projects/consoles/game-boy-advance/">previous portable console</a>, the system revolves around a big chip named <strong>CPU NTR</strong>. ‘NTR’ is shorthand for ‘Nitro’, the codename of the original Nintendo DS.</p><p>Now, CPU NTR implements an interesting multi-processor architecture using two different ARM CPUs, this design was done before ARM Holdings officially released multi-processor solutions. So, their functioning may be considered a bit unorthodox taking into account the present technology available.</p><p>While this is not the first parallel system analysed for <a href="https://www.copetti.org/projects/consoles/">this series</a>, its design is very different from the rest. For instance, we are not talking about the ‘experimental’ master-slave configuration that the <a href="https://www.copetti.org/projects/consoles/sega-saturn/">Saturn</a> debuted or the ‘co-processor’ approach found on the <a href="https://www.copetti.org/projects/consoles/playstation/">PS1</a> or <a href="https://www.copetti.org/projects/consoles/nintendo-64/">N64</a>. The Nintendo DS includes two very independent computers that will perform exclusive operations, each one having a dedicated bus. This co-dependency will condition the overall performance of this console.</p><p>That being said, let’s take a look now at the two CPUs:</p><div><ul><li id="tab-1-1-arm7tdmi-link"><a href="#tab-1-1-arm7tdmi">ARM7TDMI</a></li><li id="tab-1-2-arm946e-s-link"><a href="#tab-1-2-arm946e-s">ARM946E-S</a></li></ul><div><div id="tab-1-1-arm7tdmi"><h4>ARM7TDMI</h4><div><a href="https://www.copetti.org/images/consoles/nintendods/cpu/arm7_core.8a9851c20df1dda3c252ae75f544a8ce7a6749026fa4bc870027741cda1003b4.png"><picture>
<img name="image_cover" alt="Image" src="https://www.copetti.org/images/consoles/nintendods/cpu/arm7_core.8a9851c20df1dda3c252ae75f544a8ce7a6749026fa4bc870027741cda1003b4.png" data-src="https://www.copetti.org/images/consoles/nintendods/cpu/arm7_core.8a9851c20df1dda3c252ae75f544a8ce7a6749026fa4bc870027741cda1003b4.png"></picture></a><figcaption>ARM7 structure and components</figcaption></div><p>Starting with the more familiar one, the <strong>ARM7TDMI</strong> is the same CPU found on the <a href="https://www.copetti.org/projects/consoles/game-boy-advance/#cpu">GameBoy Advance</a> but now running at <strong>~34 MHz</strong> (double its original speed). It still includes all its original features (especially <a href="https://www.copetti.org/projects/consoles/game-boy-advance/#whats-new">Thumb</a>).</p><p>Now for the changes: Because Nintendo’s engineers placed the ARM7 next to most of the I/O ports, this CPU will be tasked with arbitrating and assisting I/O operations. In fact, no other processor can directly connect to the I/O. As you can see, this is not the ‘main’ processor that will be in charge of the system, but rather the ‘sub-processor’ offloading the main CPU by passing data around many components.</p></div><div id="tab-1-2-arm946e-s"><h4>ARM946E-S</h4><div><a href="https://www.copetti.org/images/consoles/nintendods/cpu/arm9_core.213329ca27287083c84d30b27fb9da63edd81998406a10b9ee7289089d0fe94d.png"><picture>
<img name="image_cover" alt="Image" src="https://www.copetti.org/images/consoles/nintendods/cpu/arm9_core.213329ca27287083c84d30b27fb9da63edd81998406a10b9ee7289089d0fe94d.png" data-src="https://www.copetti.org/images/consoles/nintendods/cpu/arm9_core.213329ca27287083c84d30b27fb9da63edd81998406a10b9ee7289089d0fe94d.png"></picture></a><figcaption>ARM9 structure and components</figcaption></div><p>Here is the ‘main’ CPU of the Nintendo DS running at <strong>~67 MHz</strong>. If you ignore the ill-fated ARM8 series, you could say the ARM946E-S is the ‘next-gen’ version of the ARM7. Part of the <strong>ARM9 series</strong>, this core in particular not only inherits all the features of the <strong>ARM7TDMI</strong> but also includes some additional bits:</p><ul><li>The <strong>ARMv5TE ISA</strong>: Compared to the previous v4, features some new instructions and a faster multiplier.<ul><li>If you take a look at the core name, the letter ‘E’ means <strong>Enhanced DSP</strong> which implies that lots of these new instructions have to do with applications for signal processing.</li></ul></li><li><strong>5-stage Pipeline</strong>: This is another increment from the previous 3-stage pipeline.</li><li><strong>12 KB of L1 Cache</strong>: The core now features cache, where 8 KB are allocated for instructions and 4 KB for data.</li><li><strong>48 KB of Tightly-Coupled Memory</strong> or ‘TCM’: Similar to <a href="https://www.copetti.org/projects/consoles/playstation/#cpu">Scratchpad memory</a>, however this one discriminates between instructions (32 KB) and data (16 KB).</li></ul><p>Nintendo also added the following components around it:</p><ul><li>A <strong>Divisor and Square root unit</strong> to speed up these type of operations (the ARM9 by itself is not capable of performing this type of math).</li><li>A <strong>Direct Memory Access Controller</strong>: Accelerates memory transfers without depending on the CPU. Combined with the use of cache, both CPU and DMA can potentially work concurrently.<ul><li>Cache and DMA can provide a lot of performance but also create new problems, such as data integrity. So programmers will have to manually maintain memory consistency by flushing the <a href="https://www.copetti.org/projects/consoles/playstation-2/#preventing-past-mishaps">write-buffer</a> before triggering DMA, for instance.</li></ul></li></ul></div></div></div><p>I guess with hardware like this, it’s easy to figure out the <em>real</em> reason kids loved this console, eh?</p><h4 id="interconnection">Interconnection</h4><p>So far I’ve talked about how the two CPUs work individually. But to work as a whole, they require to co-operate constantly. To accomplish this, both CPUs directly ‘talk’ to each other using a dedicated <strong>FIFO unit</strong>, this block of data holds two 64-byte queues (up to 16 elements) for <strong>bi-directional communication</strong>.</p><div><a href="https://www.copetti.org/images/consoles/nintendods/cpu/fifo.4c452b5f9236fb1e98454d2f90d2cab902ee4c561e165e8eaf8a8fc0cd7a05f4.png"><picture>
<img name="image_cover" alt="Image" src="https://www.copetti.org/images/consoles/nintendods/cpu/fifo.4c452b5f9236fb1e98454d2f90d2cab902ee4c561e165e8eaf8a8fc0cd7a05f4.png" data-src="https://www.copetti.org/images/consoles/nintendods/cpu/fifo.4c452b5f9236fb1e98454d2f90d2cab902ee4c561e165e8eaf8a8fc0cd7a05f4.png"></picture></a><figcaption>Representation of FIFO unit</figcaption></div><p>This works as follows: The ‘sender’ CPU (that effectively needs to send the other a message) places a 32-bit block of data in the queue, the CPU acting as a ‘receiver’ can then pull that block from the queue and perform the required operations with it.</p><p>Whenever there’s a value written on the queue, either CPU can fetch it manually (<strong>polling</strong>) however, this requires to constantly check for new values (which can be expensive). Alternatively, an <strong>interrupt unit</strong> can be activated to notify the receiver whenever there’s a new value in the queue.</p><h4 id="main-memory">Main memory</h4><p>Just like its predecessor, RAM is spread around many different locations, enabling to prioritise data placement by speed of access. In summary, we have the following general-purpose memory available:</p><div><div><a href="https://www.copetti.org/images/consoles/nintendods/cpu/ram.99e9bd12e464182ef51ea4aa89a7fc60323a46a72550afbacd737957372cf190.png"><picture>
<img name="image_cover" alt="Image" src="https://www.copetti.org/images/consoles/nintendods/cpu/ram.99e9bd12e464182ef51ea4aa89a7fc60323a46a72550afbacd737957372cf190.png" data-src="https://www.copetti.org/images/consoles/nintendods/cpu/ram.99e9bd12e464182ef51ea4aa89a7fc60323a46a72550afbacd737957372cf190.png"></picture></a><figcaption>RAM model of this console</figcaption></div><ul><li><strong>32 KB of WRAM</strong> (Work RAM) using a <strong>32-bit</strong> bus: To hold fast data shared between the ARM7 and ARM9.<ul><li>Bear in mind that only one CPU can access the same address at a time.</li></ul></li><li><strong>64 KB of WRAM</strong> using a <strong>32-bit</strong> bus: For fast data as well, but only accessible from the ARM7, like the GBA had.</li><li><strong>4 MB of PSRAM</strong> using a <strong>16-bit</strong> bus: A slower type, available from either CPU and it’s controlled by a memory interface unit.<ul><li>Pseudo SRAM or ‘PSRAM’ is a variant of DRAM which, by contrast, performs refreshes from within the chip. Therefore, behaving like SRAM (the faster, but more expensive alternative to DRAM). This design reminds me of <a href="https://www.copetti.org/projects/consoles/gamecube/#clever-memory-system">1T‑SRAM</a>.</li></ul></li></ul></div><h4 id="backwards-compatibility">Backwards compatibility</h4><p>Even though the architecture is significantly different from its predecessor, it still managed to maintain the critical bits that would grant it native compatibility with GameBoy Advance games.</p><p>But for the DS to revert to an ‘internal’ GBA, the former includes a set of software routines that set the console in <strong>AGB Compatibility Mode</strong>. In doing so, it effectively halts the ARM9, disables most of the ‘special’ hardware, redirects the buses, puts the ARM7 in charge and slows it down at 16.78 MHz. Finally, the ARM7 proceeds to boot the original AGB BIOS which bootstraps the GamePak cartridge (just like an original GameBoy Advance). This mode still exhibits some features not found in the original console, such as displaying the game with black margins (we’ll see in the next section that the new screen resolution happens to be bigger). Moreover, since the DS has two screens, users can set which screen will be used to display the GBA game.</p><p>Once in GBA mode <strong>there’s no going back</strong>, the console must be reset to re-activate the rest of the hardware.</p><h4 id="secrets-and-limitations">Secrets and limitations</h4><p>With so many sophisticated components fitted in a single and inexpensive chip, it’s no mystery that some issues emerged due to the way they were forced to work with each other.</p><div><ul><li id="tab-2-1-unused-power-link"><a href="#tab-2-1-unused-power">Unused Power</a></li><li id="tab-2-2-a-question-about-the-hardware-choice-link"><a href="#tab-2-2-a-question-about-the-hardware-choice">A question about the hardware choice</a></li></ul><div><div id="tab-2-1-unused-power"><h4>Unused Power</h4><p>Sometimes I wonder how Nintendo planned the way the two CPU’s would be used, and if they already assumed some performance would be hit by the design they chose.</p><p>Let me start with the ARM9, this CPU runs at twice the speed of the ARM7, but most (if not all) of the I/O depends on the ARM7, so the ARM9 is vulnerable to excessive stalling until the ARM7 answers. If that wasn’t enough, <strong>ARM9’s external bus runs at half the speed</strong>, so there are a few bottlenecks identified.</p><p>Additionally, the Main Memory bus is only <strong>16-bit wide</strong>. Thus, whenever any CPU needs to fetch a word (32-bit wide) from memory, the interface <strong>stalls the CPU</strong> (up to 3 ‘wait’ cycles) until a full world is reconstructed. The worst impact happens when memory access is not sequential, which makes it stall for every single access. This issue will also arise when instructions are fetched (unfortunately, ARM didn’t support sequential opcode fetching back then) which, to my dismay, also affects Thumb code (since every 16-bit fetch is done as a 32-bit block). On the other hand, this penalty (as some sources call it) can be alleviated by making full use of cache and TCM.</p><p>All in all, this means that in the worst case, the ‘whooping’ ARM9’s 66 MHz horsepower is practically reduced to a mere ~8&nbsp;MHz. That is if the program makes an abysmal use of cache/TCM.</p></div><div id="tab-2-2-a-question-about-the-hardware-choice"><h4>A question about the hardware choice</h4><p>Back when I read about the CPU of the GameBoy Advance, I was really surprised by the potential of the ARM7: The CPU not only performed its designated tasks, but could also assist with others, such as providing audio sequencing or pseudo-3D graphics.</p><p>Now, during the commercialisation ARM7, ARM Holdings joined forces with DEC to design a high-end version of ARM’s chips. For this, DEC grabbed the datapath design of their processor, <strong>Alpha</strong>, and mixed it with ARM’s. The result was a new series of CPUs called <strong>StrongARM</strong> which was surprisingly <em>fast</em>. At the expense of removing certain features (Thumb and debug), DEC managed to cross the megahertz threshold by reaching speeds of up to 233 MHz. As a normal user prepared to buy a new ARM PC (let’s say a <em>RiscPC</em>), you could either choose one with the old ARM710 at 40 MHz or another one with a StrongARM running ~582% faster. The impact of StrongARM was so disruptive that ARM Holdings absorbed some of StrongARMs features to produce their next line of CPUs, starting with ARM9. And the rest is history.</p><p>But here’s where my question resides: Considering the new developments in the ARM world, why did Nintendo ultimately choose an awfully slow ARM9 combined by an even slower ARM7, instead of a faster ARM9 (or even a StrongARM)? To give you an idea, other companies like Apple just adopted the StrongARM with their Newton PDA line.</p><p>I don’t mean to criticise Nintendo’s choice, but I believe the amount of emerging technology was just too great for me to ignore. I guess their choice was done in an effort to preserve battery life and maintain production costs (by using the same CPU found in the GBA).</p></div></div></div><hr><h2 id="graphics">Graphics</h2><p>This section is a bit unusual because not only this console has multiple screens to draw, but also a combination of traditional tile engines working …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.copetti.org/projects/consoles/nintendo-ds/">https://www.copetti.org/projects/consoles/nintendo-ds/</a></em></p>]]>
            </description>
            <link>https://www.copetti.org/projects/consoles/nintendo-ds/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24195751</guid>
            <pubDate>Tue, 18 Aug 2020 06:06:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Let’s Learn x86-64 Assembly: Part 0 – Setup and First Steps]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 16 (<a href="https://news.ycombinator.com/item?id=24195627">thread link</a>) | @nice_byte
<br/>
August 17, 2020 | https://gpfault.net/posts/asm-tut-0.txt.html | <a href="https://web.archive.org/web/*/https://gpfault.net/posts/asm-tut-0.txt.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
    <main role="main">

        
        
<p><img src="https://gpfault.net/assets/post-img/asm-tut-0/header.png">
</p>

<p>
The way I was taught x86 assembly at the university had been completely outdated for many years by the time I had my first class. It was around 2008 or 2009, and 64-bit processors had already started becoming a thing even in my neck of the woods. Meanwhile, we were doing DOS, real-mode, memory segmentation and all the other stuff from the bad old days.
</p>

<p>
Nevertheless, I picked up enough of it during the classes (and over the subsequent years) to be able to understand the stuff coming out of the other end of a compiler, and that has helped me a few times. However, I've never manually written any substantial amount of x86 assembly for something non-trivial. Due to being locked up inside (on account of a global pandemic), I decided to change that situation, to pass the time.
</p>

<p>
I wanted to focus on x86-64 specifically, and completely forget/skip any and all legacy crap that is no longer relevant for this architecture. After getting a bit deeper into it, I also decided to publish my notes in the form of tutorials on this blog since there seems to be a desire for this type of content.
</p>

<p>
Everything I write in these posts will be a normal, 64-bit, Windows program. We'll be using Windows because that is the OS I'm running on all of my non-work machines, and when you drop down to the level of writing assembly it starts becoming incresingly impossible to ignore the operating system you're running on. I will also try to go as "from scratch" as possible - no libraries, we're only allowed to call out to the operating system and that's it.
</p>

<p>
In this first, introductory part (yeah, I'm planning a series and I know I will regret this later), I will talk about the tools we will need, show how to use them, explain how I generally think about programming in assembly and show how to write what is perhaps the smallest viable Windows program.
</p>

<h2>Getting the Tools</h2>
<p>There are two main tools that we will use throughout this series.</p>

<h3>Assembler</h3>

<p>
CPUs execute machine code - an efficient representation of instructions for the processor that is almost completely impenetrable to humans. The assembly language is a human-readable representation of it. A program that converts this symbolic representation into machine code ready to be executed by a CPU is called an <b>assembler</b>.
</p>

<p>
There is no single, agreed-upon standard for x86-64 assembly language. There are many assemblers out there, and even though some of them share a great deal of similarities, each has its own set of features and quirks. It is therefore important which assembler you choose. In this series, we will be using 
<a href="http://flatassembler.net/">Flat Assembler</a> (or FASM for short). I like it because it's small, easy to obtain and use, has a nice macro system and comes with a handy little editor.</p>

<h3>Debugger</h3>

<p>
Another important tool is the debugger. We'll use it to examine the state of our programs. While I'm pretty sure it's possible to use Visual Studio's integrated debugger for this, I think a standalone debugger is better when all you want to do is look at the disassembly, memory and registers. I've always used <a href="http://ollydbg.de/">OllyDbg</a> for stuff like that, but unfortunately it does not have a 64-bit version. Therefore we will be using <a href="https://www.microsoft.com/en-us/p/windbg-preview/9pgjgd53tn86?activetab=pivot:overviewtab">WinDbg</a>. The version linked here is a revamp of this venerable tool with a slightly nicer interface. Alternatively, you can get the non-Windows-store version <a href="https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/debugger-download-tools">here</a> as part of the Windows 10 SDK. Just make sure you deselect everything else besides WinDbg during installation. For our purposes, the two versions are mostly interchangeable.
</p>

<h2>Thinking in Assembly</h2>

<p>
Now that we have our tools, I want to spend a bit of time to discuss some basics. For the purpose of these tutorials I'm assuming some knowledge of languages like C or C++, but little or no previous exposure to assembly, therefore many readers will find this stuff familiar.
</p>
<h3>A 10000-foot view</h3>
<p>
CPUs only "know" how to do a fixed number of certain things. When you hear someone talk about an "instruction set", they're referring to the set of things a particular CPU has been designed to do, and the term "instruction" just means "one of the things a CPU can do". Most instructions are parameterized in one way or another, and they're generally really simple. Usually an instruction is somthing along the lines of "write a given 8-bit value to a given location in memory", or "interpreting the values from registers A and B as 16-bit signed integers, multiply them and record the result into register A".
</p>

<p>
Below is a simple mental model of the architecture that we'll start with.
</p>

<p><img src="https://gpfault.net/assets/post-img/asm-tut-0/diag0.png">
</p>

<p>
 This skips a <i>ton</i> of things (there can be more than one core executing instructions and reading/writing memory, there's different levels of cache, etc. etc.), but should serve as a good starting point.
</p>

<p>
 To be effective at low-level programming or debugging you need to understand that every high-level concept eventually maps to this low-level model, and learning how the mapping works will help you.
</p>

<h3>Registers</h3>
<p>
 You can think of <b>registers</b> as a special kind of memory built right into the CPU that is very small, but extremely fast to access. There are many different kinds of registers in x86-64, and for now we'll concern ourselves only with the so-called <i>general-purpose</i> registers, of which there are sixteen. Each of them is 64 bits wide, and for each of them the lower byte, word and double-word can be addressed individually (incidentally, 1 "word" = 2 bytes, 1 "double-word" = 4 bytes, in case you haven't heard this terminology before).
 </p>
 
 <table>
 <tbody><tr>
  <td><b>Register</b></td>
  <td><b>Lower byte</b></td>
  <td><b>Lower word</b></td>
  <td><b>Lower dword</b></td>
 </tr>
 <tr>
  <td>rax</td> <td>al</td> <td>ax</td> <td>eax</td>
 </tr>
 <tr>
  <td>rbx</td> <td>bl</td> <td>bx</td> <td>ebx</td>
 </tr>
 <tr>
  <td>rcx</td> <td>cl</td> <td>cx</td> <td>ecx</td>
 </tr>
 <tr>
  <td>rdx</td> <td>dl</td> <td>dx</td> <td>edx</td>
 </tr>
 <tr>
  <td>rsp</td> <td>spl</td> <td>sp</td> <td>esp</td>
 </tr>
 <tr>
  <td>rsi</td> <td>sil</td> <td>si</td> <td>esi</td>
 </tr>
 <tr>
  <td>rdi</td> <td>dil</td> <td>di</td> <td>edi</td>
 </tr>
 <tr>
  <td>rbp</td> <td>bpl</td> <td>bp</td> <td>ebp</td>
 </tr>
 <tr>
  <td>r8</td> <td>r8b</td> <td>r8w</td> <td>r8d</td>
 </tr>
 <tr>
  <td>r9</td> <td>r9b</td> <td>r9w</td> <td>r9d</td>
 </tr>
 <tr>
  <td>r10</td> <td>r10b</td> <td>r10w</td> <td>r10d</td>
 </tr>
 <tr>
  <td>r11</td> <td>r11b</td> <td>r11w</td> <td>r11d</td>
 </tr>
 <tr>
  <td>r12</td> <td>r12b</td> <td>r12w</td> <td>r12d</td>
 </tr> 
 <tr>
  <td>r13</td> <td>r13b</td> <td>r13w</td> <td>r13d</td>
 </tr> 
 <tr>
  <td>r14</td> <td>r14b</td> <td>r14w</td> <td>r14d</td>
 </tr> 
 <tr>
  <td>r15</td> <td>r15b</td> <td>r15w</td> <td>r15d</td>
 </tr>   
</tbody></table>  

<p>Additionally, the higher 8 bits of <code>rax</code>, <code>rbx</code>, <code>rcx</code> and <code>rdx</code> can be referred to as <code>ah</code>, <code>bh</code>, <code>ch</code> and <code>dh</code>.</p>

<p>
Note that even though I said those were "general-purpose" registers, some instructions can only be used with certain registers, and some registers have special meaning for certain instructions. In particular, <code>rsp</code> holds the stack pointer (which is used by instructions like <code>push</code>, <code>pop</code>, <code>call</code> and <code>ret</code>), and <code>rsi</code> and <code>rdi</code> serve as source and destination index for "string manipulation" instructions. Another example where certain registers get "special treatment" are the multiplication instructions, which require one of the multiplier values to be in the register <code>rax</code>, and write the result into the pair of registers <code>rax</code> and <code>rdx</code>.
</p>

<p>
In addition to these registers, we will also consider the special registers <code>rip</code> and <code>rflags</code>. <code>rip</code> holds the address of the next instruction to execute. It is modified by control flow instructions like <code>call</code> or <code>jmp</code>. <code>rflags</code> holds a bunch of binary flags indicating various aspects of the program's state, such as whether the result of the last arithmetic operation was less, equal or greater than zero. The behavior of many instructions depends on those flags, and many instructions update certain flags as part of their execution. The flags register can also be read and written "wholesale" using special instructions.
</p>

<p>
There are a lot more registers on x86-64. Most of them are used for SIMD or floating-point instructions, and we'll not be considering them in this series.
</p>

<h3>Memory and Addresses</h3>

<p>
You can think of memory as a large array of byte-sized "cells", numbered starting at 0. We'll call these numbers "memory addresses". Simple, right?
</p>
<p>
Well... addressing memory used to be rather annoying back in the old days. You see, registers in old x86 processors used to be only 16-bit wide. Sixteen bits is enough to address 64 kilobytes worth of memory, but not more. The hardware was actually capable of using addresses as wide as 20 bits, but you had put a "base" address into a special segment register, and instructions that read or wrote memory would use a 16-bit offset into that segment to obtain the final 20-bit "linear" address. There were separate segment registers for code, data and stack portions (and a few more "extra" ones), and segments could overlap. 
</p>
<p>
In x86-64 these concerns are non-existant. The segment registers for code, data and stack are still present, and they're loaded with some special values, but as a user-space programmer you needn't concern yourself with them. For all intents and purposes you can assume that all segments start at 0 and extend for the entire addressable length of memory. So, as far as we're concerned, on x86-64 our programs see memory as a "flat" contiguous array of bytes, with sequential addresses, starting at 0, just like we said in the beginning of this section...
</p>
<p>
Okay, I may have distorted the truth a little bit. Things aren't quite as simple. While it is true that on 64-bit Windows your programs see memory as a flat contiguous array of bytes with addresses starting at 0, it is actually an elaborate illusion maintained by the OS and CPU working together.
</p>
<p>
The truth is, if you were really able to read and write any byte in memory willy-nilly, you'd stomp all over other programs' code and data (something that indeed could happen in the Bad Old Days). To prevent that, special protection mechanisms exist. I won't get too deep into their inner workings here because this stuff matters mostly for OS developers. Nevertheless, here's a very short overview:
</p>
<p>
Each process gets a "flat" address space as described above (we'll call it the "virtual address space"). For each process, the OS sets up a <a href="https://wiki.osdev.org/Paging">mapping</a> between its virtual addresses and actual physical addresses in memory. This mapping is respected by the hardware: the "virtual" addresses get translated to physical addresses dynamically at runtime. Thus, the same address (e.g. 0x410F119C) can map to two different locations in physical memory for two different processes. This, in a nutshell, is how the separation between processes in enforced.
</p>

<p>
The final thing I want to invite your attention to here is how the instructions and data which they operate on are held in the same …</p></main></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://gpfault.net/posts/asm-tut-0.txt.html">https://gpfault.net/posts/asm-tut-0.txt.html</a></em></p>]]>
            </description>
            <link>https://gpfault.net/posts/asm-tut-0.txt.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24195627</guid>
            <pubDate>Tue, 18 Aug 2020 05:39:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using, Understanding, and Unraveling the OCaml Language]]>
            </title>
            <description>
<![CDATA[
Score 65 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24193795">thread link</a>) | @rabidsnail
<br/>
August 17, 2020 | http://caml.inria.fr/pub/docs/u3-ocaml/index.html | <a href="https://web.archive.org/web/*/http://caml.inria.fr/pub/docs/u3-ocaml/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tbody><tr><td><h3><a href="http://cristal.inria.fr/~remy"><span size="5">Didier R</span><span size="5">é</span><span size="5">my</span></a><span size="5">
</span></h3><h3><a href="http://www-sop.inria.fr/oasis/Caminha00/index.html">APPSEM'2000 summer school</a><sup><a name="text1" href="#note1">1</a></sup></h3></td></tr>
</tbody></div><p>Copyright ©&nbsp;2000, 2001 by Didier Rémy.
</p><p>
To correctly preview mathematical symbols, you may need 
to adjust your 
<a href="http://pauillac.inria.fr/~maranget/hevea/doc/browser.html">browser configuration</a>. 
</p></div>]]>
            </description>
            <link>http://caml.inria.fr/pub/docs/u3-ocaml/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24193795</guid>
            <pubDate>Tue, 18 Aug 2020 00:08:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A circuit-like notation for lambda calculus (2015)]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 37 (<a href="https://news.ycombinator.com/item?id=24193313">thread link</a>) | @apsec112
<br/>
August 17, 2020 | https://csvoss.com/circuit-notation-lambda-calculus | <a href="https://web.archive.org/web/*/https://csvoss.com/circuit-notation-lambda-calculus">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <p>Lately, I’ve been playing around with inventing a visual writing system for lambda calculus.</p>

<p><a href="http://en.wikipedia.org/wiki/Lambda_calculus">Lambda calculus</a> (λ-calculus) is a sort of proto-functional-programming, originally invented by Alonzo Church while he was trying to solve <a href="http://en.wikipedia.org/wiki/Entscheidungsproblem">the same problem</a> that led Turing to invent Turing machines. It’s another way of reasoning about computation.</p>

<p>Python’s lambda is an idea that was borrowed from λ-calculus. In Python, you can use a <a href="https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions">lambda expression</a> like the following in order to define a function that returns the square of a number:</p>



<p>In λ-calculus, the idea is the same: we create a function by using <code>λ</code> to specify which arguments a function takes in, then we give an expression for the function’s return value. Pure lambda calculus doesn’t include operators of any sort –&nbsp;just functions being applied to other functions –&nbsp;so if we try to write a <code>square</code> function, we have to suppose that <code>multiply</code> is a function of two variables that has already been defined:</p>

<div><div><pre><code>square = λx. multiply x x
</code></pre></div></div>

<p>The <code>square</code> function, once defined, can be applied to arguments and evaluated into something simpler.</p>

<div><div><pre><code>square 4 = (λx. multiply x x) 4
         = multiply 4 4
         = 16
</code></pre></div></div>

<p>One of the cool things about lambda calculus is that we can represent most common programming abstractions using λ-calculus, even though it’s nothing but functions: numbers, arithmetic, booleans, lists, if statements, loops, recursion… the list goes on. Before I introduce the visual writing system I’ve been using, let’s take a detour and discuss how we can represent numbers and arithmetic using lambda calculus.</p>

<h2 id="church-numerals-in-lambda-calculus">Church numerals, in lambda calculus</h2>
<p>Alonzo Church figured out how to represent numbers as lambda functions; these numbers are referred to as Church numerals.</p>

<p>We can represent any nonnegative integer as long as we have two things: (1) a value for <strong>zero</strong>, and (2) a <strong>successor</strong> function, which returns <code>n + 1</code> for any number <code>n</code>. To represent numbers as functions, then, we require that <code>z</code> (zero) and <code>s</code> (successor) be passed in as arguments, and go from there. Each number is actually secretly a function of those two inputs.</p>

<div><div><pre><code>zero = λs. λz. z
one = λs. λz. s z
two = λs. λz. s (s z)
three = λs. λz. s (s (s z))
</code></pre></div></div>

<p>The actual details of how to implement zero and successor should be implemented as are left as someone else’s problem — we can survive without them. All we care about is that our numbers do the right thing, given whatever zero and successor someone may provide.</p>

<p>What about <strong>addition</strong>? Addition is a function that takes in two numbers (let’s call them <code>x</code> and <code>y</code>), and produces a number representing their sum. To sum them together, we’ll want to produce a number that applies <code>s</code>, the successor function, a total of <code>x + y</code> times. For example, we could first apply it <code>y</code> times to the zero, then apply it <code>x</code> more times to that result.</p>

<div><div><pre><code>plus = λx. λy. (λs. λz. x s (y s z))
</code></pre></div></div>

<p>Let’s try proving that one plus one equals two. In λ-calculus, this proof looks like the following:</p>

<div><div><pre><code>one = λs. λz. s z
two = λs. λz. s (s z)

plus = λx. λy. (λs. λz. x s (y s z))

plus one one = (λx. λy. (λs. λz. x s (y s z))) one one
             = λs. λz. one s (one s z)
             = λs. λz. (λs. λz. s z) s (one s z)
             = λs. λz. s (one s z)
             = λs. λz. s ((λs. λz. s z) s z)
             = λs. λz. s (s z)
             = two
</code></pre></div></div>

<p>(Long, but at least conciser than <a href="http://en.wikipedia.org/wiki/Principia_Mathematica">Bertrand Russell’s</a>.)</p>

<h2 id="lambda-circuitry">Lambda circuitry</h2>

<p>There are a lot of lambdas, parentheses, and arguments being pushed around in that proof. Mentally matching up parentheses is annoying. Scope is especially annoying: which <code>s</code> am I looking at again in <code>λs. λz. (λs. λz. s z) s (one s z)</code>, the inner one or the outer one?</p>

<p>A linear string of lambdas and parentheses is an ineffective way to provide intuition for the computations that are taking place. This problem isn’t unique to lambda calculus, either; consider trying to represent a binary tree using a linear string:</p>

<div><div><pre><code>Node(2, Node(7, Leaf(2), Node(6, Leaf(5), Leaf(11))), Node(5, None, Node(9, Leaf(4), None)))
</code></pre></div></div>

<p>Unambiguous, but not very intuitive. Contrast that representation with the diagram we use when we’re trying to explain that same binary tree at a chalkboard, a more visual notation:</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Binary_tree.svg/288px-Binary_tree.svg.png" alt="Binary tree diagram, from Wikipedia"></p>

<p><em>Image from <a href="https://commons.wikimedia.org/wiki/File:Binary_tree.svg">Wikipedia</a>.</em></p>

<p>I remember programming constructs better when I can reason about them visually like this: when I imagine cutting an array in half for binary search, when I imagine pointers in a linked list being shuffled around to insert a new element, and when I imagine traversing up and down the branches of a binary tree.</p>

<p>Why can’t lambda calculus get some visual intuitions, in the same way? Lambda calculus is a dance of variables flowing through and being manipulated by functions, and I want a writing system for lambda calculus that will visually display this dance. It shouldn’t look like strings of parentheses and symbols: it should create visual intuition.</p>

<p>After some trial and error, here is the system I came up with. I aimed for something that would resemble circuitry.</p>

<p><strong>Values</strong> flow along wires, where they may be passed in as arguments to functions or applied as functions themselves. Some are inputs, some are outputs.</p>

<p><strong>Functions</strong> are represented as boxes which are applied to their inputs on one side and produce a single output on the other. The notation must indicate which function is applied; this may either be drawn within the box itself, or wired in to the middle of the box from some other value.</p>

<p><strong>Arguments</strong> are represented as inputs, coming in from the right side of the diagram; these arguments might pass through functions, or they might be functions-to-apply themselves. If an argument has not been passed in yet, it’s an empty arrow beginning a wire; if an argument has been passed in, its value is attached to the wire. Arguments are always passed in from top to bottom, in order.</p>

<p>As an example, here’s a function which takes in two functions, <code>f</code> and <code>g</code>, then a value <code>x</code>, and returns <code>f (g x)</code>:</p>

<p><img src="https://csvoss.com/images/lambda-f-g-x.png" alt="lambda f. lambda g. lambda x. f (g x)"></p>

<p>As another example, here’s the M combinator <code>M = λx. x x</code> (the “mockingbird” in <a href="http://smile.amazon.com/gp/product/B00A1P096Y"><em>To Mock a Mockingbird</em></a>):</p>

<p><img src="https://csvoss.com/images/m-combinator.png" alt="lambda x. x x"></p>

<h2 id="church-numerals-in-lambda-circuitry">Church numerals, in lambda circuitry</h2>

<p>Here’s the Church numeral <code>four = λs. λz. s (s (s (s z)))</code>, drawn out in lambda circuitry:</p>

<p><img src="https://csvoss.com/images/lambda-four.png" alt="Four, in lambda circuitry"></p>

<p>Let’s take that proof from earlier that one plus one is two. What does it look like to draw that proof in lambda circuitry, instead?</p>

<p><img src="https://csvoss.com/images/lambda-oneplusoneistwo.png" alt="Proof that one plus one is two, in lambda circuitry"></p>

<p>∎</p>

<p>We could also consider <strong>multiplication</strong>. A multiply function would take in two numbers, m and n, and computes a new number which is their product. In lambda calculus, we’d write:</p>

<div><div><pre><code>multiply = λm. λn. λs. λz. m (n s) z
</code></pre></div></div>

<p>In the notation of lambda circuitry, this looks like this:</p>

<p><img src="https://csvoss.com/images/lambda-multiply.png" alt="Multiplication function, in lambda circuitry"></p>

<p>Using this function, we can check that <code>multiply 2 3</code> evaluates to <code>6</code>:</p>

<p><img src="https://csvoss.com/images/lambda-multiply-1.png" alt="Multiply(2, 3), step 1"></p>

<p><img src="https://csvoss.com/images/lambda-multiply-2.png" alt="Multiply(2, 3), step 2"></p>

<p><img src="https://csvoss.com/images/lambda-multiply-3.png" alt="Multiply(2, 3), step 3"></p>

<p><img src="https://csvoss.com/images/lambda-multiply-4.png" alt="Multiply(2, 3), step 4"></p>

<p><img src="https://csvoss.com/images/lambda-multiply-5.png" alt="Multiply(2, 3), step 5"></p>

<p>∎</p>

<h2 id="sidenote-de-bruijn-indices">Sidenote: De Bruijn indices</h2>
<p>One of the nice things about lambda circuitry is that it completely removes the need for variable names.</p>

<p>There’s another notation for lambda calculus that does this too: <a href="https://en.wikipedia.org/wiki/De_Bruijn_index"><em>De Bruijn indices</em></a>. A lambda expression written with De Bruijn indices indicates which variables are used where with a positive integer; the smaller the integer, the more recently the argument it refers to was passed in.</p>

<p>For example, the identity function <code>λx. x</code> may be written with De Bruijn indices like so:</p>



<p>The Church numeral for two, <code>λs. λz. s (s z)</code>, may be written like so:</p>



<p>The addition function, <code>λx. λy. (λs. λz. x s (y s z))</code>, may be written like so:</p>

<div><div><pre><code>plus = λ λ (λ λ 4 2 (3 2 1))
</code></pre></div></div>

<p>An evaluation of <code>plus one one</code> looks like this:</p>

<div><div><pre><code>plus one one = (λ λ (λ λ 4 2 (3 2 1))) (λ λ 2 1) (λ λ 2 1)
             = (λ (λ λ (λ λ 2 1) 2 (3 2 1))) (λ λ 2 1)
             = λ λ (λ λ 2 1) 2 ((λ λ 2 1) 2 1)
             = λ λ (λ λ 2 1) 2 (2 1)
             = λ λ 2 (2 1)
</code></pre></div></div>

<p>One of the tricky things about writing a lambda calculus interpreter is getting the renaming rules right; De Bruijn indices are convenient because they remove the need for this. Lambda circuitry is similar in spirit to De Bruijn indices in that it doesn’t require variable names at all, but instead indicates which variables are passed where by connecting values directly to an arrow indicating when they were passed in.</p>

<h2 id="argument-switching-function-in-lambda-circuitry">Argument-switching function, in lambda circuitry</h2>

<p>I’ll provide more examples just to further demonstrate how the notation works in different situations. Let’s consider the “argument-switching” function <code>C</code>, where <code>C f x y</code> returns <code>f y x</code>. (This is actually the <a href="https://en.wikipedia.org/wiki/B,C,K,W_system">C combinator</a>.)</p>



<p><img src="https://csvoss.com/images/c-combinator.png" alt="C combinator"></p>

<p>Suppose we try applying this to a silly function <code>f</code> where <code>f x y</code> discards <code>y</code> and just returns <code>x</code>. Then, <code>C f</code> should switch around <code>f</code>‘s arguments and create a function which returns <code>y</code> instead. Let’s check:</p>

<div><div><pre><code>f = λx. λy. x

C f = λf. λx. λy. (f y x) f
    = λx. λy. f y x
    = λx. λy. (λx. λy. x) y x
    = λx. λy. y
</code></pre></div></div>

<p><img src="https://csvoss.com/images/lambda-f.png" alt="f = lambda x. lambda y. x"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-f-1.png" alt="C(f), step 1"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-f-2.png" alt="C(f), step 2"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-f-3.png" alt="C(f), step 3"></p>

<p>∎</p>

<p>We could also try a function <code>g</code> where <code>g x y</code> returns <code>x y</code>. Then <code>C g x y</code> should return <code>y x</code>. Let’s check:</p>

<div><div><pre><code>g = λx. λy. x y

C g x y = λf. λx. λy. (f y x) g x y
        = g y x
        = (λx. λy. x y) y x
        = y x
</code></pre></div></div>

<p><img src="https://csvoss.com/images/lambda-g.png" alt="g = lambda x. lambda y. x(y)"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-g-1.png" alt="C(g), step 1"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-g-2.png" alt="C(g), step 2"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-g-3.png" alt="C(g), step 3"></p>

<p><img src="https://csvoss.com/images/c-combinator-of-g-4.png" alt="C(g), step 4"></p>

<p>∎</p>

<p><em>Exercise</em>: Show that applying <code>C</code> twice reverses it. That is, show that <code>C (C f)</code> returns <code>f</code>, for any <code>f</code>.
(Note that <code>C f</code> is a function which takes in two arguments, <code>x</code> and <code>y</code>, and returns <code>f y x</code>. Applying <code>C</code> only to <code>f</code> like this is <a href="http://en.wikipedia.org/wiki/Partial_application">partial application</a>.)</p>

<h2 id="prior-work">Prior work</h2>
<p>There are some other systems that give visual intuition to lambda calculus.</p>

<p><a href="http://dkeenan.com/Lambda/"><em>To Dissect a Mockingbird</em></a> describes a notation that is actually very similar to the one I’ve described, and demonstrates it on various problems from <em>To Mock a Mockingbird</em>. I like the way this looks, especially how every function is enclosed by two halves of a circle which make it obvious how that function might be applied. My notation doesn’t have this feature, but requires drawing fewer enclosing boxes as a result.</p>

<p>Visual Lambda (<a href="https://code.google.com/p/visual-lambda/">code</a>, <a href="http://bntr.planet.ee/lambda/visual_lambda_bubble_notation.gif">basics</a>, <a href="http://bntr.planet.ee/lambda/work/visual_lambda.pdf">paper</a>) represents lambda expressions as colored bubbles, and provides an interface for manipulating them.</p>

<p><a href="http://worrydream.com/AlligatorEggs/">Alligator Eggs</a> is a description of a puzzle game based on lambda calculus, which also happens to provide a visual way of working with and evaluating lambda expressions.</p>

<p>These last two don’t happen to …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://csvoss.com/circuit-notation-lambda-calculus">https://csvoss.com/circuit-notation-lambda-calculus</a></em></p>]]>
            </description>
            <link>https://csvoss.com/circuit-notation-lambda-calculus</link>
            <guid isPermaLink="false">hacker-news-small-sites-24193313</guid>
            <pubDate>Mon, 17 Aug 2020 23:02:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Build an Iconic Company – Keith Rabois [audio]]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24189580">thread link</a>) | @craigcannon
<br/>
August 17, 2020 | https://nugget.fm/rabois/ | <a href="https://web.archive.org/web/*/https://nugget.fm/rabois/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div width="1"><p>Keith Rabois is a General Partner at Founders Fund. He's been a professional investor for the last seven years at Khosla Ventures and Founders Fund. Before that he spent thirteen years leading organizations such as PayPal, LinkedIn, and Square. He's served on the board of directors from inception to IPO of Yelp and Zoom. He's also an angel investor in Airbnb, Lyft, and other companies.</p></div></div>]]>
            </description>
            <link>https://nugget.fm/rabois/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24189580</guid>
            <pubDate>Mon, 17 Aug 2020 17:38:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What’s Flying Above Us?]]>
            </title>
            <description>
<![CDATA[
Score 487 | Comments 134 (<a href="https://news.ycombinator.com/item?id=24188661">thread link</a>) | @zuhayeer
<br/>
August 17, 2020 | https://skycircl.es/donate/ | <a href="https://web.archive.org/web/*/https://skycircl.es/donate/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
            <figure>
                <!-- <picture>
                    <source srcset="cbp-reaper-over-minneapolis.webp" type="image/webp">
                    <source srcset="cbp-reaper-over-minneapolis.jpg" type="image/jpeg">
                    <img class="hero" src="cbp-reaper-over-minneapolis.png">
                </picture> -->
                <picture>
                    <source srcset="https://skycircl.es/donate/buzzfeed-circles.webp" type="image/webp">
                    <source srcset="https://skycircl.es/donate/buzzfeed-circles.jpg" type="image/jpeg">
                    <img src="https://skycircl.es/donate/buzzfeed-circles.png">
                </picture>
                <figcaption>
                    DHS &amp; DOJ aircraft over Los Angeles (Buzzfeed)
                </figcaption>
            </figure>

            <p>
                What's flying above us?
            </p>
            <p>
                I'm working to make it easy to find out.
            </p>
            <p>
                Here's how:
            </p>

            
            <p>
                My <a href="https://twitter.com/lemonodor/status/1294002338215034880">Advisory Circular network of
                    twitter bots</a> post in real-time whenever they detect aircraft
                flying in circles over cities around the world, including Los Angeles, Baltimore, Portland, Minneapolis,
                and London. The bots often tweet about news and fire aircraft, and because they use an uncensored source
                of data they also tweet police, FBI, DHS, DEA, CBP, and military
                aircraft. They look for circles because it means an aircraft is <i>doing something</i> instead of
                <i>going somewhere</i>. If you've ever asked “what is that helicopter/plane?” there’s a good chance my
                bots can answer your question—even if it's an advanced military surveillance plane:
            </p>

            <blockquote>
                <p lang="en" dir="ltr">91-00504, a military Swearingen RC-26B Metroliner, is circling over Bancroft,
                    Minneapolis at 8675 feet, squawking 0243, 0.06 miles from 39 St E #91_00504 <a href="https://t.co/ZAAowIcvwi">https://t.co/ZAAowIcvwi</a> <a href="https://t.co/9WHKOSiskZ">pic.twitter.com/9WHKOSiskZ</a></p>— Advisory Circular
                Minneapolis-St. Paul (@SkyCirclesMPLS) <a href="https://twitter.com/SkyCirclesMPLS/status/1267681883862650887?ref_src=twsrc%5Etfw">June 2,
                    2020</a>
            </blockquote>
            

            
            <p>
                A few years ago I <a href="https://docs.google.com/presentation/d/1sowJrQQfgxnLCErb-CvUV8VGXdtca6SWYWWLRPZgaHI/edit?usp=sharing">discovered
                    a secret FBI aerial surveillance program</a>, involving more than 100 aircraft
                registered to front companies. I was one of the first people to <a href="https://news.ycombinator.com/item?id=9508812">post online</a> about the program.
            </p>
            <p>From a <a href="https://web.archive.org/web/20150605064602/http://fusion.net/story/143739/how-you-can-track-the-fbis-spy-planes/">Fusion
                    Media
                    article</a>:</p>
            <blockquote>
                This week, the Associated Press reported that the FBI is regularly flying “spy planes” over American
                cities.

                The report, which revealed the front companies the FBI uses to fly the planes, wasn’t a surprise to John
                Wiseman, a technologist in Los Angeles. Based on public records, he had already figured out some of the
                planes the FBI was flying and, using a
                device he programmed to intercept airplane transmissions, had identified over the last month the ones
                flying overhead in L.A. in real time.
                Wiseman wrote in a Hacker News comment in May about his findings, revealing a month ago what the AP
                reported today.
            </blockquote>

            <p>
                I continue to help journalists with stories related to government aerial surveillance.
            </p>
            
            <p>
                I created a <a href="https://twitter.com/lemonodor/status/1238149529469202433">Siri shortcut</a> that
                lets you ask what's overhead at any time, and it will tell you what
                the nearest aircraft is and who it's registered to. It uses the same uncensored data source as
                the Advisory Circular bots.
            </p>

            
            <p>
                I've always funded this work myself, but now I'm in the situation of having been furloughed without pay
                since April.
            </p>
            <p>
                I’ve spent hundreds of hours in development work trying to bring the public this essential
                information for free. I'd like to continue this work, both supporting existing projects and implementing
                new ideas (I'd also like to replace my broken laptop).
            </p>
            <p>
                If you would like to support this effort and help keep the servers running, please donate below. <b>You
                    can
                    give a one-time donation with PayPal or Venmo, or a recurring donation with PayPal.</b>
            </p>

            <p>
                Thanks for your help!<br>
                John Wiseman
            </p>
            <p>psst... if you're a nerd, coder, and/or planetracker and want some more details, check out
                <a href="https://skycircl.es/donate-nerd-mode/">this page.</a></p>

            

            <p><a href="https://skycircl.es/donate/venmo-qr-code.jpg"><img alt="Venmo button" src="https://skycircl.es/donate/venmo-button.png" width="200" height="59"></a></p><!-- <a href="bitcoin:13FjNbDnUrJxku6h8ceg7XzhKTCgJkVSRB"><img alt="bitcoin button" src="bitcoin-button.png" width="200" height="76""></a> -->
            
        </div></div>]]>
            </description>
            <link>https://skycircl.es/donate/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24188661</guid>
            <pubDate>Mon, 17 Aug 2020 16:18:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chat server on a WiFi-enabled SD card]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 46 (<a href="https://news.ycombinator.com/item?id=24188648">thread link</a>) | @l00sed
<br/>
August 17, 2020 | https://l-o-o-s-e-d.net/wartor | <a href="https://web.archive.org/web/*/https://l-o-o-s-e-d.net/wartor">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            <p><h2>warTOR</h2></p>
            
            <div>
              <p>09:00am | 02/08/2020<br>Daniel Tompkins</p>
              

 
            </div>
            <p>Oddly, I don't remember when or how I got my hands on the <a target="_blank" href="https://www.toshiba-memory.com/products/toshiba-wireless-sd-cards-flashair-w-04/">Toshiba <em>FlashAir</em></a> card. These WiFi-enabled SD cards are made to transfer photos from a digital camera to a computer.</p>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wartor/flashair.png">
              <p><img data-src="assets/img/wartor/flashair.png" src="https://l-o-o-s-e-d.net/assets/img/wartor/flashair.png">
              </p>
            </a>

            <div>
              <p>Imaginging that someone would use a 4MBps wireless connection to transfer photos when the hardware transfer is closer to 70MBps seems ridiculous.</p>
              <p>However, it could come in handy if you're in a situation where you don't have access to an SD card reader, or want to easily preview photos on a phone or tablet.</p>
            </div>

            <p>
              <h3>Digging into the <em>FlashAir</em></h3>
            </p>
            <div>
              <p>What's far more interesting about these Toshiba cards is the fact that they are essentially programmed to act as a <a target="_blank" href="https://en.wikipedia.org/wiki/Wireless_access_point">wireless access point (WAP)</a>. You can setup a custom SSID and password— connecting directly to the card over 2.4GHz.</p>
              <p>After peeking at the filesystem, I realized that the main photo gallery application hosted on the card is essentially a static website— served as basic HTML, CSS and JavaScript.</p>
            </div>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wartor/flashair-filesystem.png">
              <p><img data-src="assets/img/wartor/flashair-filesystem.png" src="https://l-o-o-s-e-d.net/assets/img/wartor/flashair-filesystem.png">
              </p>
            </a>

            <p>My excitement with this bit of tech was the possibility of building a totally discrete microserver that would be capable of hosting a few pages and services.</p>

            <p>
              <h3>Project Inspiration</h3>
            </p>
            <div>
              <p>My fourth-year undergraduate architecture studio, entitled <em>Dark Rooms</em>, was taught by <a target="_blank" href="https://m-a-u-s-e-r.net/">Mona Mahall</a>. The studio was split into three exhibitions: <em>Pyramid, Server and Backstage</em>. These exhibitions were meant to explore the "spaces between visibility and invisibility".</p>
              <p>In the second exhibition, <em>Server</em>, we were asked to consider the design of an anti-human space— a dark, cold, electronic archive built exclusively for machines. In my preliminary research, I was particularly interested in artists like <a target="_target" href="https://arambartholl.com/dead-drops/">Adam Bartholl</a>.</p>
              <p>His project, a USB "<a target="_blank" href="http://deaddrops.com/">dead drop</a>", drew inspiration from an information-sharing tactic used by spies. A predetermined secret location— such as a hollowed-out rock, brick, log or other object— would be used to discreetly stash an important item.</p>
            </div>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wartor/USB-dead-drop.png">
              <p><img data-src="assets/img/wartor/USB-dead-drop.png" src="https://l-o-o-s-e-d.net/assets/img/wartor/USB-dead-drop.png">
              </p>
            </a>

            <p>Once the item— perhaps a slip of paper or a key— is "dropped", a second party could then retrieve it without interacting directly with the other person or being detected in the exchange.</p>

            <p>
              <h3>Wireless Anonymous Repository</h3>
            </p>
            <div>
              <p>While a USB is already a much more "invisible" way of storing and exchanging information, I wanted to take this concept a step further. Using the <em>FlashAir</em> cards, I proposed a new wireless dead drop that could be just as affordable and simple as the USB, but with a myriad of superior qualities.</p>
              <p>Writing directly to the card requires an SD card slot; however, the wireless functionality can be powered without a desktop or laptop. A typical SD card takes 2.4-3.6V at about 30mA. I found that by using an SD-to-USB adapter, one could power the wireless module from a standard 5V USB outlet.</p>
            </div>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wartor/wartor-AC.png">
              <p><img data-src="assets/img/wartor/wartor-AC.png" src="https://l-o-o-s-e-d.net/assets/img/wartor/wartor-AC.png">
              </p>
            </a>

            <p>Depending on the location of the dead drop, this could be concealed within a false junction box that would be plugged in over the top of a standard 2-outlet 120V AC. If— in the spirit of the original USB dead drop— you wanted to embed the card in a brick wall, then the device could be powered from a USB powerbank.</p>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wartor/wartor-outlet.png">
              <p><img data-src="assets/img/wartor/wartor-outlet.png" src="https://l-o-o-s-e-d.net/assets/img/wartor/wartor-outlet.png">
              </p>
            </a>

            <p>I named the project <em>warTOR</em> for "wireless anonymous repo" + <a target="_blank" href="https://en.wikipedia.org/wiki/Tor_(anonymity_network)">TOR</a>. Though it's not connected to the Tor network in any way, I thought this title captured the spirit of anonymity (plus I wanted to use some Pokémon sprites). If someone would like to try it, the Toshiba cards <em>can</em> be setup as a wireless bridge... from a Tor gateway?</p>

            <p>
              <h3>Future Plans</h3>
            </p>
            <div>
              <p>Something else that I'd like to try is powering the card via a small solar cell, but I haven't gotten there yet... The greatest benefit of the wireless dead drop is that one could log into the WAP from their cellphone, upload or download a file, and no one could discern that any sort of exchange was happening. However, I'm also working on a clientside chat application that could be hosted on the cards.</p>
              <p>With this implemented, then two people could sit down in a café and send messages back and forth over the private network; or, someone could login to the network and type out a message for the other person to see at a later date.</p>
            </div>

            <a target="_blank" href="https://l-o-o-s-e-d.net/assets/img/wartor/wartor-screenshots.jpg">
              <p><img data-src="assets/img/wartor/wartor-screenshots.jpg" src="https://l-o-o-s-e-d.net/assets/img/wartor/wartor-screenshots.jpg">
              </p>
            </a>

            <div>
              <p>Here are some screenshots from the application 👆. I'll be doing a follow-up post with a GitHub link to the <em>FlashAir</em> CONFIG file that I'm using on my card. I'll also upload the source code for the original <em>warTOR</em> server with instructions on how you can deploy your own <em>warTOR</em> in the wild!</p>
              <p>Hope you enjoyed this project. If you'd like, please subscribe for updates (livestreams, new posts) or follow me on Twitter or leave a note below! Thanks for reading <em>loosed</em>.</p>
            </div>
          </div>
        </div></div>]]>
            </description>
            <link>https://l-o-o-s-e-d.net/wartor</link>
            <guid isPermaLink="false">hacker-news-small-sites-24188648</guid>
            <pubDate>Mon, 17 Aug 2020 16:17:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using a Yubikey as a touchless, magic unlock key for Linux]]>
            </title>
            <description>
<![CDATA[
Score 145 | Comments 61 (<a href="https://news.ycombinator.com/item?id=24188172">thread link</a>) | @Pneumaticat
<br/>
August 17, 2020 | https://kliu.io/post/yubico-magic-unlock/ | <a href="https://web.archive.org/web/*/https://kliu.io/post/yubico-magic-unlock/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>
        <p>Yubikeys are great for security, but their benefits decrease somewhat when you leave them in your computer unattended.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> I unfortunately have a habit of forgetting my key when I walk away from the computer. I also have login passwords that are way too long and easy to typo.</p>
<p>Thankfully, there’s a way to solve both of these problems: use a Yubikey to unlock your computer when you put it in and lock your computer when you remove it!</p>
<h2 id="prior-art">Prior art<a href="#prior-art" arialabel="Anchor">⌗</a> </h2>
<p>The first example I remember seeing of this concept years ago was <a href="https://www.predator-usb.com/predator/en/index.php">Predator</a>, Windows software (with a delightfully retro website) that locks your computer when you remove a special USB drive. Similar examples for Linux include <a href="https://wiki.debian.org/pamusb">pamusb</a>, which allows you to login using Linux’s PAM by inserting a specially-formatted USB stick.</p>
<p>Of course, nowadays most people use Yubikeys to accomplish this, and Yubico has <a href="https://developers.yubico.com/yubico-pam/Authentication_Using_Challenge-Response.html">convenient guides</a> on how to accomplish this very task. However, I wanted to make it <em>touchless</em> – that is, I wanted to be able to plug in my Yubikey and instantly unlock my laptop, without clicking through logins or touching the Yubikey button. Upon removal, I wanted to instantly lock my computer.</p>

<p>There are some guides on how to do this online (unlock when you plug in, lock when you remove), but unfortunately most of them fall prey to the problem described in <a href="https://medium.com/@d0znpp/how-to-sacrifice-security-using-a-public-yubikey-linux-guides-c823c4c6e2">this article</a>. A lot of them use udev to detect when the Yubikey is plugged in, but they don’t actually authenticate the key beyond checking its vendor ID, model ID, and sometimes serial number, which all can <a href="https://forums.anandtech.com/threads/changing-creating-a-custom-serial-id-on-a-flash-drive-low-level-blocks.2099116/">easily be faked</a>.</p>
<p>To provide actual security, most official guides use either <code>pam_u2f</code> (which authenticates a Yubikey through the <a href="https://en.wikipedia.org/wiki/Universal_2nd_Factor">U2F protocol</a>) or <code>pam_yubico</code> (which uses either online validation through YubiCloud or offline validation through a challenge-response protocol). The U2F method requires a tap on the Yubikey, while the challenge-response process can be done without user interaction, so I went with the latter. I set up traditional Yubikey authentication using <a href="https://support.system76.com/articles/yubikey-login/">this great guide from System76</a>.</p>
<p>However, I still needed some way to test the challenge-response for success when I plugged in the key. Usually, <code>pam_yubico</code> is run when you login or unlock your computer (i.e. when pressing the enter key on the lockscreen). But I didn’t want <em>any</em> clicks, so I needed a way to run it without interaction.</p>
<p>Enter <code>udev</code> (again) and <code>pamtester</code>!</p>
<p>Here’s the udev rules I included:</p>
<pre><code>kevin@you:~ » cat /etc/udev/rules.d/yubikey.rules
ACTION=="remove", ENV{DEVTYPE}=="usb_device", ENV{PRODUCT}=="1050/407*", RUN+="/usr/local/sbin/ykunlock.sh lock"
ACTION=="add", ENV{DEVTYPE}=="usb_device", ENV{ID_BUS}=="usb", ENV{PRODUCT}=="1050/407*", RUN+="/usr/local/sbin/ykunlock.sh unlock"
</code></pre><p>These rules effectively call a script when inserting and removing the key, so I can trigger any action from the script. Note that the script <strong>should not immediately unlock the computer</strong>, to avoid the security issues mentioned earlier.</p>
<p>To actually test the challenge-response from the Yubikey on inserting, I decided to use <a href="http://pamtester.sourceforge.net/">pamtester</a>, a simple utility that pretends to trigger a PAM authentication from the command line. Since <code>pam_yubico</code> is installed, this will naturally test the challenge-response if a Yubikey is plugged in.</p>
<p>Here’s the final script:</p>
<div><pre><code data-lang="bash"><span>#!/bin/bash
</span><span></span>exec 1&gt; &gt;<span>(</span>logger -s -t <span>"</span><span>$(</span>basename <span>"</span>$0<span>"</span><span>)</span><span>"</span><span>)</span> 2&gt;&amp;<span>1</span>
echo <span>"RUN"</span>
<span>if</span> <span>[</span> <span>"</span>$1<span>"</span> <span>=</span> <span>"lock"</span> <span>]</span>; <span>then</span>
        pkill -USR1 swayidle
<span>else</span>
        <span># unlock</span>
        <span>if</span> echo <span>""</span> | pamtester login kevin authenticate; <span>then</span>
                <span># PAM login successful</span>
                <span># kill locker</span>
                kill -KILL <span>$(</span>pgrep swaylock<span>)</span>
                ps aux | grep swaylock
                <span># turn on displays</span>
                SWAYSOCK<span>=</span><span>$(</span>ls /run/user/1000/sway-ipc.*.sock<span>)</span> swaymsg <span>"output * dpms on"</span>
        <span>fi</span>
<span>fi</span>
exit <span>0</span>
</code></pre></div><ul>
<li>On lock, it immediately locks my desktop (by sending a SIGUSR1 to swayidle, the program that manages locking on the Sway window manager)</li>
<li>On unlock, it first sees if it can authenticate using pamtester without interaction (when no Yubikey is inserted or if the key is invalid, pamtester asks for a password). If it can, it kills the lockscreen and turns on all displays using Sway WM protocols.</li>
</ul>
<p>The final result is amazingly convenient, and has successfully made me remember to pull out my Yubikey when leaving my computer unattended more than once<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>! Mission success.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>I’ve significantly downgraded this statement in severity after some excellent comments on Hacker News have pointed out that (1) <a href="https://news.ycombinator.com/item?id=24192390">stealing a Yubikey is incredibly unlikely unless you’re a person of interest</a>; (2) <a href="https://news.ycombinator.com/item?id=24192138">even if you have the Yubikey, you still can’t directly extract e.g. a private key</a>; and (3) a <a href="https://news.ycombinator.com/item?id=24190313">Yubikey protects against SSH/GPG fraud because it can require a PIN and lock out over time</a>. Case in point, Yubikeys are good. I’d argue that it’s still not good to have your key stolen (e.g. perhaps if you’re targeted by a government/<a href="https://news.ycombinator.com/item?id=24194081">industrial espionage</a>, or the malicious significant other attack, where they know your password and can steal your key for 2FA if unattended), but I see that it’s not as much of a risk as I originally thought. <a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>You might call it Yubikey: Coronavirus Edition. <a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Yes, yes, I know there’s not too much of a danger because we’re all stuck at home right now. But who knows – maybe this will be helpful when we <em>eventually</em> get back on campus. <a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

      </div></div></div>]]>
            </description>
            <link>https://kliu.io/post/yubico-magic-unlock/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24188172</guid>
            <pubDate>Mon, 17 Aug 2020 15:29:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pathological Lying: Theoretical and Empirical Support for a Diagnostic Entity]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 103 (<a href="https://news.ycombinator.com/item?id=24187224">thread link</a>) | @InInteraction
<br/>
August 17, 2020 | https://psychnewsdaily.com/about-13-percent-of-people-are-pathological-liars/ | <a href="https://web.archive.org/web/*/https://psychnewsdaily.com/about-13-percent-of-people-are-pathological-liars/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A new study has found that 13% of people think of themselves as pathological liars, or say that others consider them to be pathological liars. Those 13% reported telling about 10 lies per day.</p><p>The study, published in the journal <a href="https://prcp.psychiatryonline.org/doi/10.1176/appi.prcp.20190046#.XvDDrEQ2jzE%20">Psychiatric Research and Clinical Practice</a>,&nbsp;included 623 people. The researchers recruited them in 2019 from various mental health forums, social media, and a university.</p><p>The participants spanned a range of ages, ethnicities, education levels, and income levels. The researchers asked them whether they thought of themselves as pathological liars, or if others thought of them that way. The respondents also took a lie frequency assessment and other questionnaires.</p><h2>Greater distress and impaired functioning</h2><p>The study found the pathological liars were more likely to experience distress and impaired functioning, especially in social relationships. This diminished functioning also applied to legal contexts, work, and finances. Their distress often had to do with worries about whether their lies would be be discovered.</p><p>The pathological liars in the group also reported telling lies for no specific reason, and said many of their lies grew out of an initial lie.</p><p>The majority of participants in the pathological liars group indicated that their problematic lying began during adolescence. People in this group were also more likely to say their lying was out of their control, indicating a kind of compulsiveness. Likewise, people in this group said they felt less anxious after lying.</p><h2>From <em>pseudologia phantastica</em> to pathological liars</h2><p>The phenomenon of the “pathological liar” was first recorded in 1891 by psychiatrist <a aria-label="undefined (opens in a new tab)" href="https://www.amazon.co.uk/pathologische-L%C3%BCge-psychisch-abnormen-Schwindler/dp/3226034618/ref=sr_1_2?dchild=1&amp;qid=1594994372&amp;refinements=p_27%3AAnton+Delbr%C3%BCck&amp;s=books&amp;sr=1-2" target="_blank" rel="noreferrer noopener">Anton Delbr?ck</a>. He initially called it <em>pseudologia phantastica</em>, and used the term to describe people who told so many outrageous lies that their behavior could be considered pathological.</p><p>Since then, research into pathological lying has been surprisingly scant. One <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1600-0447.1988.tb05068.x">analysis</a> of prior case studies found <a aria-label="undefined (opens in a new tab)" href="https://en.wikipedia.org/wiki/Pathological_lying" target="_blank" rel="noreferrer noopener">pathological lying</a> equally represented among men and women, with the <a aria-label="undefined (opens in a new tab)" href="https://psychnewsdaily.com/category/iq/" target="_blank" rel="noreferrer noopener">IQs</a> of the liars in the average to above average range.</p><h2>Formal recognition</h2><p>Pathological lying has not (yet) been classified as a diagnostic entity in either the <a aria-label="undefined (opens in a new tab)" href="https://www.psychiatry.org/psychiatrists/practice/dsm" target="_blank" rel="noreferrer noopener">DSM-5</a> or the <a aria-label="undefined (opens in a new tab)" href="https://en.wikipedia.org/wiki/ICD-10" target="_blank" rel="noreferrer noopener">ICD-10</a>, but the researchers behind the present study are hoping to change that. As they write, “we suggest that PL should be defined as a persistent, pervasive, and often compulsive pattern of excessive lying behavior that leads to clinically significant impairment of functioning in social, occupational, or other areas.”</p><p>Pathological lying also causes distress, they say, and poses a risk to the self or others; an example of this risk is if a pathological liar conceals the presence of suicidal thoughts.</p><p>Formal recognition of pathological lying as a disorder would bring many benefits, as researchers would then be better able to examine its features and causes. And effective treatments, such as cognitive-behavioral therapy and possibly pharmaceutical drugs, could then be more thoroughly investigated.</p><hr><p><strong>Study:</strong> P<em>athological Lying: Theoretical and Empirical Support for a Diagnostic Entity</em><br><strong>Authors: </strong><a href="https://prcp.psychiatryonline.org/doi/10.1176/appi.prcp.20190046#">Drew A. Curtis</a> and <a href="https://prcp.psychiatryonline.org/doi/10.1176/appi.prcp.20190046#">Christian L. Hart</a> <br><strong>Published: </strong>22 Jun 2020, <a href="https://doi.org/10.1176/appi.prcp.20190046">https://doi.org/10.1176/appi.prcp.20190046</a><br><strong>Image: </strong>via <a aria-label="undefined (opens in a new tab)" href="https://www.flickr.com/photos/80641068@N07/with/8686708312/" target="_blank" rel="noreferrer noopener">Flickr</a>, Creative Commons Attribution 2.0 Generic <a aria-label="undefined (opens in a new tab)" href="https://creativecommons.org/licenses/by/2.0/deed.en" target="_blank" rel="noreferrer noopener">license</a>.</p></div></div>]]>
            </description>
            <link>https://psychnewsdaily.com/about-13-percent-of-people-are-pathological-liars/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24187224</guid>
            <pubDate>Mon, 17 Aug 2020 13:55:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Introduction to OpenBSD [video]]]>
            </title>
            <description>
<![CDATA[
Score 196 | Comments 90 (<a href="https://news.ycombinator.com/item?id=24185985">thread link</a>) | @asicsp
<br/>
August 17, 2020 | https://blog.lambda.cx/posts/openbsd-introduction-talk/ | <a href="https://web.archive.org/web/*/https://blog.lambda.cx/posts/openbsd-introduction-talk/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p><img src="https://blog.lambda.cx/posts/openbsd-introduction-talk/cover.jpg" alt="The first slide of the OpenBSD introduction presentation" title="An Introduction to OpenBSD"></p><p>
  <iframe src="https://www.youtube.com/embed/EkDVKthufAM" allowfullscreen="" title="YouTube Video"></iframe>
</p>


<p>
I recently gave a talk at work to help introduce OpenBSD to my
colleagues. It's a broad introduction to the fundamentals of security
in OpenBSD, as well as some basic system administration tips and
suggestions anyone coming from a Linux background might find useful.
</p>
<p>
It's roughly split up into four sections; the history of OpenBSD,
what sets it apart from other operating systems, a guided
installation, and the system administration introduction.
</p>
<p>
In the original presentation the guided installation was done
interactively with the participants installing OpenBSD in a VM on
their machines to follow along with the slides.
</p>
<p>
I've tried my best to make it as accessible as possible while still
covering the most important beats. If you find any errors please let
me know so I can correct them, my contact info is on the <a href="https://blog.lambda.cx/about/">about</a> page.
</p>
<p>
I've corrected several small issues with the slides since the
recording. I've replaced the file name <code>/etc/mygateway</code> with <code>/etc/mygate</code>,
replaced the smartquotes with regular quotes, and removed the rebound
program, to name the biggest fixes. These corrections are available at
the slides linked below.
</p>
<p>
<a href="https://blog.lambda.cx/posts/openbsd-introduction-talk/openbsd-introduction.pdf">An Introduction to OpenBSD slides</a>
</p>

    </div></div>]]>
            </description>
            <link>https://blog.lambda.cx/posts/openbsd-introduction-talk/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24185985</guid>
            <pubDate>Mon, 17 Aug 2020 10:55:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A-Levels: The Model is not the Student]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 127 (<a href="https://news.ycombinator.com/item?id=24185621">thread link</a>) | @tosh
<br/>
August 17, 2020 | http://thaines.com/post/alevels2020 | <a href="https://web.archive.org/web/*/http://thaines.com/post/alevels2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://thaines.com/post/alevels2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-24185621</guid>
            <pubDate>Mon, 17 Aug 2020 09:47:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Response to Google open letter]]>
            </title>
            <description>
<![CDATA[
Score 365 | Comments 305 (<a href="https://news.ycombinator.com/item?id=24185374">thread link</a>) | @ajdlinux
<br/>
August 17, 2020 | https://www.accc.gov.au/media-release/response-to-google-open-letter | <a href="https://web.archive.org/web/*/https://www.accc.gov.au/media-release/response-to-google-open-letter">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="content-column">
              
            <div id="readspeaker-process">
        <section>
                                
                            </section>
        <div>
          <section>
                                                                                                    <div>
    <section id="block-system-main">

      
  <div>
    <article id="node-87993" about="/media-release/response-to-google-open-letter" typeof="sioc:Item foaf:Document">
    <header>
            <span property="dc:title" content="Response to Google open letter"></span>      </header>
    <div><div><div property="content:encoded"><p>The open letter published by Google today contains misinformation about the draft news media bargaining code which the ACCC would like to address.&nbsp;</p>

<p>Google will not be required to charge Australians for the use of its free services such as Google Search and YouTube, unless it chooses to do so.</p>

<p>Google will not be required to share any additional user data with Australian news businesses unless it chooses to do so.</p>

<p>The draft code will allow Australian news businesses to negotiate for fair payment for their journalists’ work that is included on Google services.</p>

<p>This will address a significant bargaining power imbalance between Australian news media businesses and Google and Facebook.</p>

<p>A healthy news media sector is essential to a well-functioning democracy.</p>

<p>We will continue to consult on the draft code with interested parties, including Google.</p>

<p><a href="https://www.accc.gov.au/focus-areas/digital-platforms/news-media-bargaining-code/draft-legislation">Consultation</a> closes on 28 August 2020.</p>

<p>More information about the draft news media bargaining code can be found here:&nbsp;<a href="https://www.accc.gov.au/media-release/australian-news-media-to-negotiate-payment-with-major-digital-platforms">Australian news media to negotiate payment with major digital platforms</a></p>
</div></div></div>    </article>
  </div>

</section> <!-- /.block -->
<section id="block-service-links-service-links">

        <p>
      <h2>Share</h2>
    </p>
    
  

</section> <!-- /.block -->
  </div>
                                  </section>
                  </div>
      </div>
    </section></div>]]>
            </description>
            <link>https://www.accc.gov.au/media-release/response-to-google-open-letter</link>
            <guid isPermaLink="false">hacker-news-small-sites-24185374</guid>
            <pubDate>Mon, 17 Aug 2020 08:59:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Handmade: A Community for Self-Rolled Performant Software (2016)]]>
            </title>
            <description>
<![CDATA[
Score 172 | Comments 85 (<a href="https://news.ycombinator.com/item?id=24184688">thread link</a>) | @TheUndead96
<br/>
August 16, 2020 | https://handmade.network/manifesto | <a href="https://web.archive.org/web/*/https://handmade.network/manifesto">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
        
            <div>
                
                
    <div>
        
        
            

<p> Modern computer hardware is amazing. Manufacturers have orchestrated billions of pieces of silicon into terrifyingly complex and efficient structures that sweep electrons through innumerable tangled paths, branchings, and reunions with the sole purpose of performing computations at more than a billion times per second. This awe-inspiring piece of computational wizardry has at its disposal multiple billions of uniquely addressible silicon plates where it can store the results of millions of computations in an array of several vanishingly small chips. All of this hardware, though each component often sits no further than 7 or 8 centimeters away from the others, cycles so fast that the speed of light, a physical law of the universe, limits the rate at which they communicate with each other.
</p><p><span>So why is software still slow?</span>
</p><p>Why does it take your operating system 10 seconds, 30 seconds, a minute to boot up? Why does your word processor freeze when you save a document on the cloud? Why does your web browser take 3, 4, 10 seconds to load a web page? Why does your phone struggle to keep more than a few apps open at a time? And why does each update somehow make the problem worse?
</p><p><span>We made it slow</span>.
</p><p>Not necessarily you, not necessarily me, not necessarily any single person in particular. But we, the software development community, made it slow by ignoring the fundamental reality of our occupation. We write code, code that runs on computers. Real computers, with central processing units and random access memory and hard disk drives and display buffers. Real computers, with integer and bitwise math and floating point units and L2 caches, with threads and cores and a tenuous little network connection to a million billion other computers. Real computers not built for ease of human understanding but for blindingly, incomprehensibly fast speed.
</p><p><span>A lot of us have forgotten that</span>.
</p><p>In our haste to get our products, our projects, the works of our hands and minds, to as many people as possible, we take shortcuts. We make assumptions. We generalize, and abstract, and assume that just because these problems have been solved before that they never need to be solved again. We build abstraction layers, then forget we built them and build more on top.
</p><p>And it's true that many of us think we do not have the time, the money, the mental bandwidth to always consider these things in detail. The deadline is approaching or the rent is due or we have taxes to fill out and a manager on our back and someone asking us why we always spend so much time at the office, and we just have to stick the library or virtual machine or garbage collector in there to cover up the places we can't think through right now.
</p><p>Others of us were never taught to think about the computer itself. We learned about objects and classes and templates and how to make our code clean and pretty. We learned how to write code to make the client or the manager or the teacher happy, but made the processor churn. And because we did, that amazing speed we'd been granted was wasted, by us, in a death by a thousand abstraction layers.
</p><p><span>But some of us aren't satisfied with that.</span>
</p><p>Some of us take a few extra steps into the covered territory, the wheels sitting, motionless, in a pile behind us, examine their designs and decide there is a better way. The more experienced among us remember how software used to be, the potential that we know exists for computer programs to be useful, general, <em>and</em> efficient. Others of us got fed up with the tools we were expected to use without complaint, but which failed us time and time again. Some of us are just curious and don't know what's good for us. Don't trust what we've been told is good for us.
</p><p>We sat down and looked at our hardware, and examined our data, and thought about how to use the one to transform the other. We tinkered, and measured, and read, and compared, and wrote, and refined, and modified, and measured again, over and over, until we found we had built the same thing, but 10 times faster and incomparably more useful to the people we designed it for. And we had built it by hand.
</p><p>That is what Handmade means. It's not a technique or a language or a management strategy, it isn't a formula or a library or an abstraction. It's an idea. The idea that we can build software that works with the computer, not against it. The idea that sometimes an individual programmer can be more productive than a large team, that a small group can do more than an army of software engineers and *do it better*. The idea that programming is about transforming data and we wield the code, the tool we use to bend that data to our will.
</p><p> It doesn't require a degree, or a dissertation, or a decade of experience. You don't need an
expensive computer or a certificate or even prior knowledge. All you need is an open mind and a sense of
curiosity. We'll help you with the rest.
</p><p><span>Will you join us?</span>
</p><p>Will you build your software by hand?</p>

        
        
        <p>
            
                Last updated by Andrew Chronister on April 23, 2016, 1:39 a.m.
            
        </p>
    </div>

                
            </div>
        
    </div>
    
</div></div>]]>
            </description>
            <link>https://handmade.network/manifesto</link>
            <guid isPermaLink="false">hacker-news-small-sites-24184688</guid>
            <pubDate>Mon, 17 Aug 2020 06:56:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How long since Google said a Google Drive Linux client is coming?]]>
            </title>
            <description>
<![CDATA[
Score 147 | Comments 108 (<a href="https://news.ycombinator.com/item?id=24183399">thread link</a>) | @zdw
<br/>
August 16, 2020 | https://abevoelker.github.io/how-long-since-google-said-a-google-drive-linux-client-is-coming/ | <a href="https://web.archive.org/web/*/https://abevoelker.github.io/how-long-since-google-said-a-google-drive-linux-client-is-coming/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
        
        <p>
          have elapsed since <a href="https://productforums.google.com/forum/#!category-topic/drive/report-a-problem/KeC7Ax76dAA">Google said to "hang tight" about Linux support for Google Drive</a>.
        </p>
        <p>
          <strong><a href="https://tools.google.com/dlpage/drive">We're still waiting</a></strong>.
        </p>
        <p><a href="https://productforums.google.com/forum/#!category-topic/drive/report-a-problem/KeC7Ax76dAA">
          <img src="https://abevoelker.github.io/how-long-since-google-said-a-google-drive-linux-client-is-coming/img/waiting.gif">
        </a></p><hr>
        <p>
          Made with frustration by <a href="https://twitter.com/abevoelker">@abevoelker</a>
        </p>
      </div></div>]]>
            </description>
            <link>https://abevoelker.github.io/how-long-since-google-said-a-google-drive-linux-client-is-coming/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24183399</guid>
            <pubDate>Mon, 17 Aug 2020 02:47:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Factorio and Software Engineering]]>
            </title>
            <description>
<![CDATA[
Score 400 | Comments 159 (<a href="https://news.ycombinator.com/item?id=24181783">thread link</a>) | @nindalf
<br/>
August 16, 2020 | https://blog.nindalf.com/posts/factorio-and-software-engineering/ | <a href="https://web.archive.org/web/*/https://blog.nindalf.com/posts/factorio-and-software-engineering/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="wrapper">
            <article>
                <header>
                    
                    <h2>
                    Aug 16, 2020 00:15
                    · 1364 words
                    · 7 minute read
                      <span>
                      
                      
                          
                              <a href="https://blog.nindalf.com/tags/tech">tech</a>
                          
                      
                      
                      </span>
                    </h2>
                </header>
                
                <section id="post-body">
                    <p>I’ve been a software engineer a while now and I can say this with confidence - it is fun. It’s great and I wouldn’t trade it for anything else. It’s so much fun that some folks try to capture the most enjoyable elements and put them into games.</p>

<p>I’ve played two such games. The first is <strong><a href="https://store.steampowered.com/app/504210/SHENZHEN_IO/">Shenzhen.io</a></strong>. This one looks similar to what an engineer working on embedded devices would see. You solve puzzles by writing assembly code on low power devices. What makes this game great is that they remove the annoying parts of writing and shipping code.</p>

<ul>
<li>The requirements are clear, rarely true on the job.</li>
<li>The edit-debug-compile loop is lightning quick. That combined with a great test suite means you can try several potential solutions in a minute.</li>
<li>The platform your code depends on (the game itself) doesn’t have bugs. You don’t need to fix your dependencies before you start on your own code.</li>
</ul>

<p>Should a software engineer play Shenzhen.io? The gameplay isn’t for everyone. For some people it feels too much “like work”. At the end of the day you want to relax, not work at tasks that feel similar to what you did for 8 hours. Despite that, I think it’s worthwhile just to see how much more fun a task becomes when the requirements are clear and developer tools are fast. Everyone knows that investing in our direction and our tools will help, but having fun playing this game reinforces that feeling.</p>

<!-- There's a great article about Shenzhen.io that I couldn't quite work in - https://probablydance.com/2016/11/07/lessons-learned-from-shenzhen-io/ -->

<p>The second game is <strong><a href="https://factorio.com/">Factorio</a></strong> which released last Friday, though it has been available as an early access preview for about 4 years now. Those who’ve played it are probably scratching their heads right now - this is a game about building a factory, not coding. You work with conveyor belts, metals, oil products to craft products necessary to make a spacecraft.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/BqaAjgpsoW8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>And yet, this game reminds me of software engineering more than any other. Let me explain why.</p>

<ul>
<li><strong>Technical debt</strong>. Do we hack it for now or do we implement it right? The answer as always - it depends. Hacking it gets us closer to our goals right now but we have to pay off the debt eventually. Novice players (like me!) start out connecting various parts of our base by conveyor belts till our base resembles spaghetti, similar to poorly maintained codebases. Eventually we learn techniques to tame this complexity so our base/codebase becomes easier to reason about.</li>
<li><strong>Don’t Repeat Yourself (DRY)</strong>. One of those techniques is reducing duplication. If you have a component that’s needed in multiple places, do you make it once and use it everywhere or do you copy paste in each place it’s needed? The answer is “it depends”. As an engineer, sometimes you use a library while other times you copy paste. It depends on the complexity of the component - a couple of functions can be copy pasted while something complex probably shouldn’t. So it is in Factorio, a certain component (electronic circuits) was being produced in 4-5 places. I eventually replaced them all with one centralised production array to simplify the factory.</li>
<li><strong>Scaling</strong>. A repeated theme in this game is building a production array and later finding we need 3-5x the throughput. The first few times this happens, it requires redoing from scratch. After we wise up, we start design our arrays with space for scaling up. So it is with software - our systems need to scale to many more users, sometimes without much warning. We design our systems keeping that in mind.</li>
<li><strong>Rebuilding</strong>. When we’re rebuilding a component in a single player game or a personal software project we don’t usually care if the component or the whole system stops working briefly. I was playing multiplayer with friends though, so I tried to make sure components I was working on didn’t break anything else. I created a new oil refinery system, shifted existing customers to the new one before decommissioning the old one. Zero downtime.</li>
<li><strong>Debugging</strong> to find root causes. Our factory is far from perfect, so something is always breaking as we add stuff to it. Finding the root causes of these issues is tricky, especially when fixing those leads to other problems, like playing whack-a-mole. An example from yesterday - We don’t have enough electricity, so we add more boilers, but now the water pipes need fixing. Then the water is fine, but we don’t have enough coal. That mirrors real life</li>
<li><strong>Teamwork</strong>. Most things are possible solo given enough time. But it’s quicker and more fun working with a team you like. We were able to move fast by splitting responsibilities among the team. We have one oil guy (me), one trains guy, one secretary of defense, among other roles. The others don’t care about the internals of the refinery system, just the interface - they use the outputs and let me know if it’s broken. It’s the same on large software projects - everyone can’t learn the intricacies of the whole system. Instead everyone learns the APIs of all components while a few are responsible for the implementation.</li>
<li><strong>Researching</strong>. We spend most of our time exploiting our existing knowledge to keep us in our local maxima of output. However, a smart player dedicates some time to learning new techniques. In our game, coal power wasn’t working out for us and I had dismissed nuclear power as an option because we didn’t have enough uranium ore. I looked into it again when we were desperately short of power and producing too much pollution. Turns out even a little ore is enough to power the base for a 100 hours if we do it right. It’s the same with software - our existing stack probably works well, but it’s smart to see what’s out there and possibly learn from others. New team mates can help here too - everything is new to them so they’re spending more time than you on learning. They can come across stuff that we didn’t know about and use that knowledge to find a higher maxima. A fresh pair of eyes is always good.</li>
<li><strong>Automation</strong>. You can do most things manually, it just takes time. But if you’re doing something repeatedly, it should be automated. The game gently eases you towards this idea by requiring a few items be automated. It gets better later in the game you unlock construction robots. You can tell them the layout of the production array you’ve designed, give them the materials and they will construct it for you. This reminded me of AWS CloudFormation and similar tools - although it’s possible to set up servers by hand, it’s quicker and less error prone to specify the end state and let a tool do it for us. But if you’ve developed software, you know that automation isn’t just a means to an end - it’s an end in and of itself. We do it because it makes us happy, even if we do too much of it and <a href="https://xkcd.com/1319/">forget what we were doing in the first place</a>.</li>
<li><strong>Putting out fires</strong>. Sometimes it’s hard to implement new features because we’re being pulled away to deal with alerts - all too common on software engineering teams. The typical solution is to have one team member deal with alerts while the rest focus on adding features. That’s we did in the game too.</li>
</ul>

<p>But more than any one thing, the game is about managing complexity. Designing a specification and implementing systems that fulfill that specification. Maintaining and growing that system over time.</p>

<p>IMO, playing Factorio will not make you a better software engineer. But if you’re a software engineer, you’ll likely find the game fun. And conversely, if you’re good at the game you should give software engineering a shot.</p>

<p>You can get Factorio at on the <a href="https://factorio.com/">official website</a> or <a href="https://store.steampowered.com/app/427520/Factorio/">Steam</a>. There’s also a <a href="https://factorio.com/download">free demo</a>, in case you’d like to try before you buy. (Just one thing - don’t wait for a sale. This game has never gone on sale and possibly never will.)</p>

<hr>

<p>Thanks to Minesh Patel for reading drafts of this and suggesting improvements.</p>

<p>Check out the comments on <a href="https://news.ycombinator.com/item?id=24181783">Hacker News</a> and <a href="https://blog.nindalf.com/posts/factorio-and-software-engineering/">reddit</a>.</p>

                </section>
            </article>

            

            

            

            

        </section></div>]]>
            </description>
            <link>https://blog.nindalf.com/posts/factorio-and-software-engineering/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24181783</guid>
            <pubDate>Sun, 16 Aug 2020 22:08:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pure Skill Minesweeper]]>
            </title>
            <description>
<![CDATA[
Score 272 | Comments 99 (<a href="https://news.ycombinator.com/item?id=24181772">thread link</a>) | @ColinWright
<br/>
August 16, 2020 | https://github.andrewt.net/mines/ | <a href="https://web.archive.org/web/*/https://github.andrewt.net/mines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
	
	<p>
		If you're around my age, you'll remember Windows Minesweeper. It was one of several games included with Windows back in the day to try to train users in the use of the then-newfangled â€˜mouseâ€™. It doesnâ€™t come with Windows any more. You can get Minesweeper for Windows 10, but the developers haveâ€¦ made some choices.
	</p>
	<p>
		If you havenâ€™t played it, Minesweeper is a grid of squares, some of which have mines underneath, and your job is to click on all the others. If you click on a mine, you lose. You can also flag squares with the right mouse button to record that you think they are a mine. Your only clue as to where the mines are is that when you click on a square that isnâ€™t a mine, it tells you how many of the eight adjacent squares are mines. If that number is zero, the game automatically clicks on all the adjacent squares for you so you can get a little patch to start from. Hopefully that will include a 1 in a tight corner so thereâ€™s only one place its mine can go, and that will complete a nearby 1 so you can get some more numbers to work from, eventually leading you to complete the game.
	</p>
	<p>
		If youâ€™re not so lucky, you might not get a solvable board. A common situation is that you get down to the last two spaces and you know one of them is a mine but thereâ€™s no wat to predict which one. You have to guess. Heads you win, tails you just wasted 10 minutes carefully clearing the whole board only to be scuppered by a forced guess.
	</p>
	<p>
		<a href="https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/mines.html">Thereâ€™s a version of Minesweeper that guarantees you never have to guess</a>, which is very nice. It neatly solves the problem of unwinnable games, and while I find that always knowing you can make a deduction can be too big a clue, in regular Minesweeper you could even lose straight off the bat â€” you have to guess the first move and maybe itâ€™ll be a mine. In this case, Windows Minesweeper quietly chooses a different arrangement of mines and pretends like you got lucky after all â€” but it doesnâ€™t do that if youâ€™re stuck at the <em>end</em>.
	</p>
	<p>
		So what if we went further? Below is a version of Minesweeper where you will <em>never</em> be penalised when forced to guess. Any time there simply isnâ€™t enough information to deduce a safe move, weâ€™ll pull Windowsâ€™s trick and quietly rearrange the mines so you donâ€™t get penalised.
	</p>
	<p>
		But since that would on its own make the game too easy, the flip side of this coin is that if you <em>can</em> in theory deduce a safe move but guess anyway, the code will quietly rearrange the mines so that your unnecessary risk backfires and you lose.
	</p>
	<p><label>Width </label>
		<label>Height </label>
		<label>Mines </label>
		
	</p>
	
	<p>
		This game is also rigged to give you a zero on your first click rather than simply a non-mine space. To be honest this is mostly to avoid situations that are computationally difficult â€” this is not what I would call carefully optimised code. If I wanted to expand this I would (eg) make it precalculate things while youâ€™re staring at the screen instead of waiting for you to choose a square and then trying to work out what to do about it.
	</p>
	<p>
		More importantly, I would like to make the game recognise when thereâ€™s a forced guess _coming_ â€” currently if thereâ€™s a 50/50 chance in one corner of the grid that youâ€™re clearly going to have to take eventually, you have to wait until thatâ€™s <em>all</em> there is. If you take the chance when there was a safe space available elsewhere, youâ€™ll always lose.
	</p>
	<p>
		If I was feeling especially mean, I could insist that the player choose the square with the best odds of being safe, rather than simply judging whether or not each square is theoretically knowable. But I think that would be taking things too far.
	</p>
	

</div>]]>
            </description>
            <link>https://github.andrewt.net/mines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24181772</guid>
            <pubDate>Sun, 16 Aug 2020 22:07:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I tried to use WordPress with GitHub Pages]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 114 (<a href="https://news.ycombinator.com/item?id=24181153">thread link</a>) | @mdu96
<br/>
August 16, 2020 | https://www.melissadu.com/i-tried-to-use-wordpress-with-github-pages-and-it-was-a-nightmare/ | <a href="https://web.archive.org/web/*/https://www.melissadu.com/i-tried-to-use-wordpress-with-github-pages-and-it-was-a-nightmare/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><!-- .entry-header --><div><p>Recently I migrated this website over to WordPress.org. I tried to set it up so I could use my WordPress site with Github Pages. This post details the tribulations I faced while doing so. Here are the different sections; feel free to skip around however you like:&nbsp;</p><ol><li><a href="#backstory">The backstory: my previous setup and why I switched</a></li><li><a href="#requirements">Requirements for a new solution</a></li><li><a href="#mistakes">Two big initial mistakes</a></li><li><a href="#warning">A warning (because hindsight is 20/20)</a></li><li><a href="#run-locally">The treacherous journey to run WordPress locally</a></li><li><a href="#annoying-issues">More annoying issues I faced</a></li><li><a href="#is-it-working">Finally, it’s working?</a></li><li><a href="#conclusion">The Conclusion</a></li><li><a href="#postscript">Postscript: I’m never satisfied and here are a few more things about WordPress that I’m annoyed with</a></li></ol><h2 id="backstory"><strong>The backstory: my previous setup and why I switched</strong> to WordPress</h2><p>Prior to WordPress, I had a nice set up with <a href="https://jekyllrb.com/">Jekyll</a> (static site generator) and <a href="https://pages.github.com/">Github/Github Pages</a> (hosting &amp; serving). I liked this old setup because it was simple and free. I also liked Jekyll’s plain, minimal look. Jekyll’s visual style is very barebones and that’s exactly what I wanted.</p><p>My old workflow for editing and publishing posts was simple. I’d open a text editor, write a post in Markdown, and build and run my website locally to test the changes. Next, I’d push the new post to Github, where I’d re-deploy my website with the changes.</p><p>I first started to get annoyed with this setup when I had to re-remind myself how Jekyll glued together and rendered my site each time I wanted to publish a post. I got even more annoyed when I wanted to make changes to the layout. For example, I tried and failed multiple times to add a <code>&lt;meta&gt;</code> tag to the head of my <code>index.html</code> file. This theoretically should have been a very simple change.&nbsp;</p><p>If this happened once or twice, I’d be fine with it. The bigger problem is that the purpose of my website is to encourage me to write more. I started to procrastinate writing because there was always some small thing I wanted to fix with Jekyll first. Then, when I made time to tinker around with Jekyll, I was annoyed that I spent more time doing that than actually writing.</p><h2 id="requirements"><strong>Requirements for a new solution</strong></h2><p>So, I decided to make it was time to change my setup. My new setup had to fulfill just two requirements:</p><ol><li><strong>Easy content management. </strong>I really like the idea of just writing something, editing it, and then hitting a button to publish it once it’s ready. I didn’t like diving into Ruby files just to fix configuration issues, deploy issues, UI issues, etc. (what was happening with Jekyll).&nbsp;</li><li><strong>Be a writing first experience. </strong>I’ve never quite liked writing in text editors. Something about it just doesn’t <em>feel</em> right, though I can’t put my finger on exactly what.&nbsp;</li></ol><p>I decided to go with WordPress because it seemed like it fulfilled both of those requirements really well. Also <a href="https://wordpress.org/">WordPress.org</a> is free, unlike Webflow and Ghost, two alternatives I considered that are both quite pricey ($20/month for Webflow and $29/month for Ghost) and loaded with a bunch of features I don’t need yet.</p><h2 id="mistakes"><strong>Two big initial mistakes</strong></h2><p>As mentioned above, previously I used Github/Github Pages to host and serve my website. Github was free and worked great, so I wanted to rig up my WordPress setup to also use Github and Github Pages.&nbsp;</p><p>The only requirement to use Github Pages is that your site has to be static. Some light Googling showed me that there were a few plugins that converted WordPress site files to static pages. There were even <a href="https://www.hywel.me/static/site/wordpress/2016/07/17/fast-free-static-website-with-wordpress-and-github-pages.html">a few tutorials</a> describing how to do this, so I was confident this would be possible. That was my first big mistake.</p><p>Next, I began figuring out how to get WordPress to run locally. That was my second big mistake. My logic here was simple. Previously with Jekyll, I’d run my website locally, add my changes, and then push the new version of my website to Github to redeploy it. Thus, if I wanted to use WordPress with Github and Github Pages, then I should run my WordPress site locally, add my changes, and then push the new version of the site to Github.&nbsp;</p><p>While this logic wasn’t bad, I really should have tested the core assumption that it relied on first (the fact that the WordPress site <em>could</em> be converted into a static website). Or I should have done a bit more research than just some “light Googling” before venturing into a solution and stubbornly refusing to give up.&nbsp;</p><h2 id="warning"><strong>A warning (because hindsight is 20/20)</strong></h2><p>Here’s what I know now: running WordPress locally is a nightmare. Successfully figuring it out was way harder than it should have been. If you’re trying to do this yourself, don’t. You most likely don’t need to, and there’s probably another solution you can use to get to what you want.&nbsp;</p><p>(I should also caveat that I know almost nothing about Apache, MySQL, FTP, and PHP. If you’re familiar with them, then you might not have the same issues. However, I really don’t think I should have needed to learn about all of those technologies just to set up WordPress locally.)</p><h2 id="run-locally"><strong>The treacherous journey to run WordPress locally</strong></h2><p>First, I downloaded WordPress.org and installed it locally. The <a href="https://wordpress.org/support/article/installing-wordpress-on-your-own-computer/">official WordPress installation page</a> implied that I also needed some combination of Apache, MySQL, and PHP. A few of the links on that page were broken, a red flag that I promptly ignored.&nbsp;</p><p>I read a few other tutorials and subsequently downloaded and installed <a href="https://sourceforge.net/projects/xampp/files/">XAMPP</a>, making sure to include Apache, MySQL, PHP, and phpMyAdmin in the installation. I opened up XAMPP and pressed the “Start Application” button. That took me to an error page.</p><p>I clicked through some XAMPP tabs and pressed a few more buttons that started Apache, MySQL, and ProFTD. Now pressing “Start Application” took me to an XAMPP page. This seemed promising, but I still got an error page when trying to access my WordPress site which should have been running locally.</p><p>I bopped around a bit more and figured out that I had to enable local forwarding in XAMPP on an available port. I roll my eyes and question why XAMPP is so difficult to use.</p><p>Now that I could <em>see</em> my WordPress site locally, I still couldn’t do anything (e.g. create post). I learned that I needed to create a database locally via phpMyAdmin. Again, this is something that should have been easy, but the interface SUCKED. I had such difficulty performing such a simple action. I mean…just look at it:&nbsp;</p><figure><img width="1024" height="544" src="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-1024x544.png" alt="phpMyAdmin home screen" srcset="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-1024x544.png 1024w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-300x159.png 300w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-768x408.png 768w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-1536x817.png 1536w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-2048x1089.png 2048w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-600x319.png 600w" sizes="(max-width: 1024px) 100vw, 1024px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-1024x544.png" data-srcset="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-1024x544.png 1024w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-300x159.png 300w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-768x408.png 768w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-1536x817.png 1536w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-2048x1089.png 2048w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.01.03-PM-600x319.png 600w"><figcaption><em>phpMyAdmin home screen</em></figcaption></figure><figure><img width="1022" height="486" src="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM.png" alt="bad database creation UI" srcset="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM.png 1022w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM-300x143.png 300w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM-768x365.png 768w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM-600x285.png 600w" sizes="(max-width: 1022px) 100vw, 1022px" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM.png" data-srcset="https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM.png 1022w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM-300x143.png 300w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM-768x365.png 768w, https://www.melissadu.com/wp-content/uploads/2020/08/Screen-Shot-2020-08-15-at-10.02.19-PM-600x285.png 600w"><figcaption><em>phpMyAdmin “Create Database” page</em></figcaption></figure><p>Looking at these screenshots now makes me annoyed all over again.</p><h2 id="annoying-issues"><strong>More annoying issues I faced</strong></h2><p>I then ran into some issues that took me a long time to figure out. Surprisingly, there was not a lot of advice out there on good solutions. I spent more time than I’d like to admit on WordPress support forums and arcane blogs. I’ll include the two most annoying issues here for posterity and in the event that another poor soul is also sifting through Google searching for a solution.&nbsp;</p><p><strong><em>Annoying error #1: “to perform the requested action WordPress needs to access your web server”</em></strong></p><p>To fix this, add <code>define( 'FS_METHOD', 'direct' );</code> to your <code>wp-config.php</code> file.</p><p><strong><em>Annoying error #2: fix the file and folder permissions error in WordPress / “installation failed could not create directory”</em></strong></p><p>To fix this, you need to give XAMPP write access to the local directory you have WordPress set up in. I spent awhile messing around with quite a few config files (like .htaccess) and FTP permission codes to no avail. The solution was ultimately simple: right-click on your local WordPress directory, click “Get Info”, scroll to “Sharing &amp; Permissions”, and change the permissions on your directory to “Read and Write (anyone)”.</p><h2 id="is-it-working"><strong>Finally, it’s working?</strong></h2><p>At long last, all of this is finally working and I now have WordPress running locally.&nbsp;</p><p>Anyways, the next step is to install a plugin to convert my site into a bunch of static pages. I go with <a href="https://www.simplystatic.co/">SimplyStatic</a>, the most popular and highly recommended plugin to do this.</p><p>I immediately run into quite a few errors with SimplyStatic. After progressively resolving each one, I ultimately run into one I can’t fix. I search some more and realize that it’s because SimplyStatic was only compatible with an earlier version of WordPress than the one I was running. I dig further and realize that the plugin hasn’t been updated for over two years. And there don’t seem to be <em>any</em> currently updated plugins that do this conversion. Epic FAIL.&nbsp;</p><p>I didn’t want to sink in even more time to learn how to convert the WordPress PHP files into static pages. I decided to cut my losses right there and abandon ship.&nbsp;</p><h2 id="conclusion"><strong>The Conclusion</strong></h2><p><strong><span>TL;DR? Don’t try to run WordPress locally, and don’t try to use WordPress with Github Pages to host and serve it. It won’t work.</span></strong></p><p>I ultimately ended up going with <a href="http://siteground.com/">Siteground</a> for managed hosting. Siteground’s interface is awful, but after poking around a bit I got most of it set up. I had a few minor configuration issues when enabling HTTPS and spent ~30 minutes chatting with a customer support agent, who was actually quite helpful.&nbsp;</p><h2 id="postscript"><strong>Postscript: I’m never satisfied and here are a few more things about WordPress that I’m annoyed with</strong></h2><ol><li>Unless you want your WordPress site to look bad or just like everyone else’s, you’ll probably want to customize an existing theme to your own liking. <strong>To do this, you’ll still need to hack around with code<em> </em></strong><em>(in this case, a combination of HTML, CSS and PHP)</em>. Theoretically, you could probably install a plugin to help you do this, but that’d probably slow down your site and further obscure how things actually work.&nbsp;</li><li>Getting used to WordPress itself was a bit of a curve. The main dashboard, for instance, is completely useless to me. I’ve just been ignoring all parts of the interface I don’t need, which thus far has been working pretty well.&nbsp;</li></ol><hr><p>If you liked this post, add your email address below to stay updated whenever there’s a new one.  Or you can <a href="https://www.twitter.com/melissadooo">follow me on Twitter</a>. I like talking to people there.</p></div><!-- .entry-content --></article></div>]]>
            </description>
            <link>https://www.melissadu.com/i-tried-to-use-wordpress-with-github-pages-and-it-was-a-nightmare/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24181153</guid>
            <pubDate>Sun, 16 Aug 2020 20:45:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[WebAssembly without the browser]]>
            </title>
            <description>
<![CDATA[
Score 134 | Comments 59 (<a href="https://news.ycombinator.com/item?id=24180303">thread link</a>) | @pacificat0r
<br/>
August 16, 2020 | https://alexene.dev/2020/08/17/webassembly-without-the-browser-part-1.html | <a href="https://web.archive.org/web/*/https://alexene.dev/2020/08/17/webassembly-without-the-browser-part-1.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        

<article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Most WebAssembly tutorials and examples you will find online focus on using it inside the browser in order to accelerate various functionality of a website or web app.<br>
However, there is an area where WebAssembly is really powerful but not talked too much about: outside the browser usage scenarios. That is what we’ll focus on in this series of posts.</p>

<h2 id="what-is-webassembly">What is WebAssembly?</h2>
<p>Web people are on a roll of giving bad names to things (web-gpu is another example).<br>
WebAssembly is neither web or assembly, but a bytecode that can be targeted from languages like C++, C#, Rust and others. This means you can write some Rust code, compile it into WebAssembly and run that code in a WebAssembly virtual machine.</p>

<p>This is powerful because you won’t have to deal with garbage collected scripted languages anymore, and essentially use Rust or C++ as your <em>scripting language</em>.  WebAssembly enables predictable and stable performance because it doesn’t require garbage collection like the usual options (LUA/JavaScript).</p>

<p>It’s a relatively new product and there are a lot of rough edges, especially for out-of-browser scenarios. One of the roughest ones in my experience has been documentation for out-of-browser scenarios and this is the reason for my blog posts, to document my findings and hopefully help some people that may be interested in this subject.</p>

<h2 id="why-would-we-want-to-run-webassembly-outside-of-a-browser">Why would we want to run WebAssembly outside of a browser?</h2>
<p>For out of browser scenarios, one of its main advantage is that it provides system level access without compromising on security. This is done through WASI, the Web Assembly System Interface. <a href="https://wasi.dev/">WASI</a> is a collection of C-like functions that provide access to functionality such as <code>fd_read</code>, <code>rand</code>, <code>fd_write</code>, threads (WIP), in a safe way.</p>

<p>Here are a few scenarios where you would be able to use web-assembly outside of a browser:</p>
<ul>
  <li>A scripting language for a video game.</li>
  <li>To run some code with minimal overhead as Fastly/Cloudflare are doing with their compute-at-edge scenarios.</li>
  <li>To run some easy to update code on IoT devices safely and with minimal runtime overhead.</li>
  <li>Extreamly fast programs in environments where you can’t JIT for <em>reasons</em>.</li>
</ul>

<h2 id="prerequisites">Prerequisites</h2>
<p>For the best experience in this adventure, I suggest using <a href="https://code.visualstudio.com/">Visual Studio Code</a> as your IDE and install the following extensions:</p>
<ul>
  <li><code>rust-analyzer</code>: for autocomplete and other great features.</li>
  <li><code>Code-LLDB</code>: For debugging with LLDB (even works on Windows)</li>
  <li><code>WebAssembly by the WebAssembly foundation</code>: Allows you to disassemble and inspect <code>.wasm</code> binaries.</li>
</ul>


<p>First you need a Virtual Machine (VM) that can run your WebAssembly program. This VM needs to be embeddable, so you can add it in your game engine, or what we will call from now on <strong>host program</strong>. There are a few to pick from: <a href="https://github.com/wasm3/wasm3">WASM3</a>, <a href="https://github.com/bytecodealliance/wasmtime">Wasmtime</a>, <a href="https://github.com/bytecodealliance/wasm-micro-runtime">WAMR</a>, and many others. They have various characteristics, such as supporting JIT, using as little memory as possible and so on and you have to choose one one that fits your target platform and scenario.</p>

<p>It doesn’t matter too much what VM you’re choosing besides runtime properties, with the exception of debugging. The only VM that allows for a seamless debugging experience that I’ve found is Wasmtime (this is another one of those rough edges). So even if you don’t plan on deploying that anywhere due to other constraints, I suggest using it as the <strong>debug VM</strong>. Whenever you’d want to debug some WASM code you can launch it with Wasmtime.</p>



<p>First, we need to create a new <code>lib</code> project:</p>
<div><div><pre><code>cargo new <span>--lib</span> wasm_example
</code></pre></div></div>

<p>In <code>Cargo.toml</code> add the following:</p>

<div><div><pre><code>[lib]
crate-type = ["cdylib"]
</code></pre></div></div>

<p>Now we can edit <code>lib.rs</code> and export the following <code>C</code> FFI compatible function from it:</p>

<div><div><pre><code><span>#[no_mangle]</span>
<span>extern</span> <span>"C"</span> <span>fn</span> <span>sum</span><span>(</span><span>a</span><span>:</span> <span>i32</span><span>,</span> <span>b</span><span>:</span> <span>i32</span><span>)</span> <span>-&gt;</span> <span>i32</span> <span>{</span>
    <span>let</span> <span>s</span> <span>=</span> <span>a</span> <span>+</span> <span>b</span><span>;</span>
    <span>println!</span><span>(</span><span>"From WASM: Sum is: {:?}"</span><span>,</span> <span>s</span><span>);</span>
    <span>s</span>
<span>}</span>
</code></pre></div></div>

<p>This a function that takes two numbers, adds them, then prints the result before returning their sum.<br>
WebAssembly doesn’t define a default function that’s executed after a module is loaded, so in the host program you need to get a function by it’s signature, and run it (quite similar to how <code>dlopen</code>/<code>dlsym</code> works).</p>

<p>We expose this <code>sum</code> function (and any other functions we want to call from the host VM) as a function that’s callable from <code>C</code>, using <code>[#no_mangle]</code> and <code>pub extern "C"</code>. If you’re coming here from some WASM for the browser tutorials, you may notice we don’t need to use <code>wasm-bindgen</code> at all.</p>

<h2 id="how-do-we-compile-it">How do we compile it?</h2>
<p>Rust supports two targets for WebAssembly: <code>wasm32-unknown-unknown</code> and <code>wasm32-wasi</code>. The first one is bare-bones WebAssembly. Think of it like the <code>[#no-std]</code> of WebAssembly. It’s the kind you’d use for the browser that doesn’t assume any system functions are available.</p>

<p>At the other end, <code>wasm32-wasi</code> assumes that the VM exposes the <code>WASI</code> functionality, allowing a different implementation of the standard library to be used (the implementation that depends on the WASI functions to be available).</p>

<p>You can take a look at the available implementations for the Rust’s stdlib here: <a href="https://github.com/rust-lang/rust/tree/master/library/std/src/sys">https://github.com/rust-lang/rust/tree/master/library/std/src/sys</a>  <br>
This is the implementation that assumes WASI functions are available to the rust program when running in a WebAssembly VM: <a href="https://github.com/rust-lang/rust/tree/master/library/std/src/sys/wasi">https://github.com/rust-lang/rust/tree/master/library/std/src/sys/wasi</a>.</p>

<p>To comile for wasm32-wasi run:</p>
<div><div><pre><code><span># Run this just once</span>
rustup target add wasm32-wasi

<span># Compile for the wasm32-wasi target.</span>
cargo build <span>--target</span> wasm32-wasi
</code></pre></div></div>

<h2 id="but-how-does-println-work">But how does <code>println!()</code> work?</h2>
<p>You may have noticed that we’re calling <code>println!()</code> and expecting the program to work and print to the console, but how does a WebAssembly program knows how to do that?</p>

<p>This is why we’re using <code>wasm32-wasi</code>. This target selects for the rust stdlib the version that assumes some functionality to be there (the <code>WASI</code> functions). Printing to the console means just writing to a special file descriptor. Most VMs allow that by default so we don’t need to do any special settings, besides compiling the correct <code>wasm32-wasi</code> target.</p>

<p>If you have installed the required extensions for vscode, you can now right click on <code>target/wasm32-wasi/debug/wasm_example.wasm</code> and select <code>Show WebAssembly</code> and you should have a new file open in vscode that looks like this:</p>

<div><div><pre><code><span>(</span><span>module</span><span>
  </span><span>....</span><span>
  </span><span>(</span><span>type</span><span> </span><span>$</span><span>t15</span><span> </span><span>(</span><span>func</span><span> </span><span>(</span><span>param</span><span> </span><span>i64</span><span> </span><span>i32</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>result</span><span> </span><span>i32</span><span>)))</span><span>
  </span><span>(</span><span>import</span><span> </span><span>"wasi_snapshot_preview1"</span><span> </span><span>"fd_write"</span><span> </span><span>(</span><span>func</span><span> </span><span>$</span><span>_</span><span>ZN4wasi13lib_generated22wasi_snapshot_preview18fd_write17h6ec13d25aa9fb6acE</span><span> </span><span>(</span><span>type</span><span> </span><span>$</span><span>t8</span><span>)))</span><span>
  </span><span>(</span><span>import</span><span> </span><span>"wasi_snapshot_preview1"</span><span> </span><span>"proc_exit"</span><span> </span><span>(</span><span>func</span><span> </span><span>$</span><span>__</span><span>wasi_proc_exit</span><span> </span><span>(</span><span>type</span><span> </span><span>$</span><span>t0</span><span>)))</span><span>
  </span><span>(</span><span>import</span><span> </span><span>"wasi_snapshot_preview1"</span><span> </span><span>"environ_sizes_get"</span><span> </span><span>(</span><span>func</span><span> </span><span>$</span><span>__</span><span>wasi_environ_sizes_get</span><span> </span><span>(</span><span>type</span><span> </span><span>$</span><span>t2</span><span>)))</span><span>
  </span><span>(</span><span>import</span><span> </span><span>"wasi_snapshot_preview1"</span><span> </span><span>"environ_get"</span><span> </span><span>(</span><span>func</span><span> </span><span>$</span><span>__</span><span>wasi_environ_get</span><span> </span><span>(</span><span>type</span><span> </span><span>$</span><span>t2</span><span>)))</span><span>
  </span><span>(</span><span>func</span><span> </span><span>$</span><span>_</span><span>ZN4core3fmt9Arguments6new_v117hb11611244be67330E</span><span> </span><span>(</span><span>type</span><span> </span><span>$</span><span>t9</span><span>)</span><span> </span><span>(</span><span>param</span><span> </span><span>$</span><span>p0</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>param</span><span> </span><span>$</span><span>p1</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>param</span><span> </span><span>$</span><span>p2</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>param</span><span> </span><span>$</span><span>p3</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>param</span><span> </span><span>$</span><span>p4</span><span> </span><span>i32</span><span>)</span><span>
    </span><span>(</span><span>local</span><span> </span><span>$</span><span>l5</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>local</span><span> </span><span>$</span><span>l6</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>local</span><span> </span><span>$</span><span>l7</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>local</span><span> </span><span>$</span><span>l8</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>local</span><span> </span><span>$</span><span>l9</span><span> </span><span>i32</span><span>)</span><span> </span><span>(</span><span>local</span><span> </span><span>$</span><span>l10</span><span> </span><span>i32</span><span>)</span><span>
    </span><span>global.get</span><span> </span><span>$</span><span>g0</span><span>
    </span><span>local.set</span><span> </span><span>$</span><span>l5</span><span>
  </span><span>...</span><span>
</span></code></pre></div></div>

<p>This is a <code>wat</code> file. <code>wat</code> stands for WebAssembly text format. It’s kind of like looking at x64/ARM ASM instructions when disassembling a binary, just uglier and harder to understand. I have read that this was because the creators of WebAssembly couldn’t decide on a text format so they just left it in this ugly s-expression form.</p>

<p>The import statements here tell us that the WASM program needs the following functions <code>proc_exit</code>, <code>fd_write</code>, <code>environ_get</code>, <code>environ_sizes_get</code> to exist in the <code>wasi_snapshot_preview1</code> namespace.<br>
All imported or exported functions from a WebAssembly module require a namespace. <code>wasi_snapshot_preview1</code> is the WASI namespace so you can think of it as a reserved namespace for these functions. <code>println!</code> needs <code>wasi_snapshot_preview1::fd_write</code> to write to stdout.</p>

<h2 id="the-host-program">The host program</h2>
<p>You can pick any VM that has WASI available. I will use Wasmtime because later on I want to show you how to debug WebAssembly and this VM is the only one where debugging works at the moment.</p>

<p>The program loads the wasm binary file from the path: <code>examples/wasm_example.wasm</code>.<br>
This is the file you have previously compiled that you can find in <code>wasm_example/target/wasm32-wasi/debug/wasm_example.wasm</code>. <strong>Make sure you move it in the right place before running the host program.</strong></p>

<p>Here is the full listing of the host VM rust program that initializes the Wasmtime VM, loads the module, links against WASI and loads and executes the exported <code>sum</code> function from the WASM module:</p>

<div><div><pre><code><span>use</span> <span>std</span><span>::</span><span>error</span><span>::</span><span>Error</span><span>;</span>
<span>use</span> <span>wasmtime</span><span>::</span><span>*</span><span>;</span>
<span>use</span> <span>wasmtime_wasi</span><span>::{</span><span>Wasi</span><span>,</span> <span>WasiCtx</span><span>};</span>

<span>fn</span> <span>main</span><span>()</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(),</span> <span>Box</span><span>&lt;</span><span>dyn</span> <span>Error</span><span>&gt;&gt;</span> <span>{</span>
    <span>// A `Store` is a sort of "global object" in a sense, but for now it suffices</span>
    <span>// to say that it's generally passed to most constructors.</span>
    <span>// let store = Store::default();</span>
    <span>let</span> <span>engine</span> <span>=</span> <span>Engine</span><span>::</span><span>new</span><span>(</span><span>Config</span><span>::</span><span>new</span><span>()</span><span>.debug_info</span><span>(</span><span>true</span><span>));</span>
    <span>let</span> <span>store</span> <span>=</span> <span>Store</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>engine</span><span>);</span>

    <span>// We start off by creating a `Module` which represents a compiled form</span>
    <span>// of our input wasm module. In this case it'll be JIT-compiled after</span>
    <span>// we parse the text format.</span>
    <span>let</span> <span>module</span> <span>=</span> <span>Module</span><span>::</span><span>from_file</span><span>(</span><span>&amp;</span><span>engine</span><span>,</span> <span>"examples/wasm_example.wasm"</span><span>)</span><span>?</span><span>;</span>

    <span>// Link the WASI module to our VM. Wasmtime allows us to decide if WASI is present.</span>
    <span>// So we need to load it here, as our module rquires certain functions to be present from the</span>
    <span>// wasi_snapshot_preview1 namespace as seen above.</span>
    <span>// This makes println!() from our WASM program to work. (it uses fd_write).</span>
    <span>let</span> <span>wasi</span> <span>=</span> <span>Wasi</span><span>::</span><span>new</span><span>(</span><span>&amp;</span><span>store</span><span>,</span> <span>WasiCtx</span><span>::</span><span>new</span><span>(</span><span>std</span><span>::</span><span>env</span><span>::</span><span>args</span><span>())</span><span>?</span><span>);</span>
    <span>let</span> <span>mut</span> <span>imports</span> <span>=</span> <span>Vec</span><span>::</span><span>new</span><span>();</span>
    <span>for</span> <span>import</span> <span>in</span> <span>module</span><span>.imports</span><span>()</span> <span>{</span>
        <span>if</span> <span>import</span><span>.module</span><span>()</span> <span>==</span> <span>"wasi_snapshot_preview1"</span> <span>{</span>
            <span>if</span> <span>let</span> <span>Some</span><span>(</span><span>export</span><span>)</span> <span>=</span> <span>wasi</span><span>.get_export</span><span>(</span><span>import</span><span>.name</span><span>())</span> <span>{</span>
                <span>imports</span><span>.push</span><span>(</span><span>Extern</span><span>::</span><span>from</span><span>(</span><span>export</span><span>.clone</span><span>()));</span>
                <span>continue</span><span>;</span>
            <span>}</span>
        <span>}</span>
        <span>panic!</span><span>(</span>
            <span>"couldn't find import for `{}::{}`"</span><span>,</span>
            <span>import</span><span>.module</span><span>(),</span>
            <span>import</span><span>.name</span><span>()</span>
        <span>);</span>
    <span>}</span>
    <span>// After we have a compiled `Module` we can then instantiate it, creating</span>
    <span>// an `Instance` …</span></code></pre></div></div></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://alexene.dev/2020/08/17/webassembly-without-the-browser-part-1.html">https://alexene.dev/2020/08/17/webassembly-without-the-browser-part-1.html</a></em></p>]]>
            </description>
            <link>https://alexene.dev/2020/08/17/webassembly-without-the-browser-part-1.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24180303</guid>
            <pubDate>Sun, 16 Aug 2020 19:08:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Recent Advances in Natural Language Processing]]>
            </title>
            <description>
<![CDATA[
Score 273 | Comments 170 (<a href="https://news.ycombinator.com/item?id=24179795">thread link</a>) | @saadalem
<br/>
August 16, 2020 | https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/ | <a href="https://web.archive.org/web/*/https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1118">
	<!-- .entry-header -->

	<div>
		<p><em>If you enjoy this article, please check out my free book by clicking <a href="https://deponysum.com/2020/03/30/something-to-read-in-quarantine-essays-2018-to-2020/">Here: “Something to Read in Quarantine: Essays 2018-2020.”</a></em></p>
<p id="b9a9">Natural Language Processing (NLP) per Wikipedia:</p>
<p id="34cf">“Is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.”</p>
<p id="7dd8">The field has seen tremendous advances during the recent explosion of progress in machine learning techniques.</p>
<p id="b5f8">Here are some of its more impressive recent achievements:</p>
<p id="cb15"><strong>A)</strong>&nbsp;The Winograd Schema is a test of common sense reasoning- easy for humans, but historically almost impossible for computers- which requires the test taker to indicate which noun an ambiguous pronoun stands for. The correct answer hinges on a single word, which is different between two separate versions of the question. For example:</p>
<p id="512d"><em>The city councilmen refused the demonstrators a permit because they feared violence.</em></p>
<p id="ece3"><em>The city councilmen refused the demonstrators a permit because they advocated violence.</em></p>
<p id="0ddf">Who does the pronoun “They” refer to in each of the instances?</p>
<p id="d33c">The Winograd schema test was originally intended to be a more rigorous replacement for the Turing test, because it seems to require deep knowledge of how things fit together in the world, and the ability to reason about that knowledge in a linguistic context. Recent advances in NLP have allowed computers to achieve near human scores:(<a href="https://gluebenchmark.com/leaderboard/" target="_blank" rel="noopener nofollow">https://gluebenchmark.com/leaderboard/</a>).</p>
<p id="0a43"><strong>B)</strong>&nbsp;The New York Regent’s science exam is a test requiring both scientific knowledge and reasoning skills, covering an extremely broad range of topics. Some of the questions include:</p>
<p id="a36d"><em>1.Which equipment will best separate a mixture of iron filings and black pepper? (1) magnet (2) filter paper (3) triplebeam balance (4) voltmeter</em></p>
<p id="9797"><em>2. Which form of energy is produced when a rubber band vibrates? (1) chemical (2) light (3) electrical (4) sound</em></p>
<p id="966a"><em>3. Because copper is a metal, it is (1) liquid at room temperature (2) nonreactive with other substances (3) a poor conductor of electricity (4) a good conductor of heat</em></p>
<p id="6d70"><em>4. Which process in an apple tree primarily results from cell division? (1) growth (2) photosynthesis (3) gas exchange (4) waste removal</em></p>
<p id="3a8a">On the 8th grade, non-diagram based questions of the test, a program was recently able to score 90%. (&nbsp;<a href="https://arxiv.org/pdf/1909.01958.pdf" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1909.01958.pdf</a>&nbsp;)</p>
<p id="2f49"><strong>C)</strong></p>
<p id="267a">It’s not just about answer selection either. Progress in text generation has been impressive. See, for example, some of the text samples created by Megatron:&nbsp;<a href="https://arxiv.org/pdf/1909.08053.pdf" target="_blank" rel="noopener nofollow">https://arxiv.org/pdf/1909.08053.pdf</a></p>
<p id="b8f7"><strong>2.</strong></p>
<p id="db13">Much of this progress has been rapid. Big progress on the Winograd schema, for example, still looked like it might be decades away back in (from memory) much of 2018. The computer science is advancing very fast, but it’s not clear our concepts have kept up.</p>
<p id="1a6c">I found this relatively sudden progress in NLP surprising. In my head- and maybe this was naive- I had thought that, in order to attempt these sorts of tasks with any facility, it wouldn’t be sufficient to simply feed a computer lots of text. Instead, any “proper” attempt to understand language would have to integrate different modalities of experience and understanding, like visual and auditory, in order to build up a full picture of how things relate to each other in the world. Only on the basis of this extra-linguistic grounding could it deal flexibly with problems involving rich meanings- we might call this the multi-modality thesis. Whether the multi-modality thesis is true for some kinds of problems or not, it’s certainly true for far fewer problems than I, and many others, had suspected.</p>
<p id="5a4d">I think science-fictiony speculations generally backed me up on this (false) hunch. Most people imagined that this kind of high-level language “understanding” would be the capstone of AI research, the thing that comes after the program already has a sophisticated extra-linguistic model of the world. This sort of just seemed obvious- a great example of how assumptions you didn’t even know you were making can ruin attempts to predict the future.</p>
<p id="f943">In hindsight it makes a certain sense that reams and reams of text alone can be used to build the capabilities needed to answer questions like these. A lot of people remind us that these programs are really just statistical analyses of the co-occurence of words, however complex and glorified. However we should not forget that the relationships between words are isomorphic to the relations between things- that isomorphism is why language works. This is to say the patterns in language use mirror the patterns of how things are(1). Models are transitive- if x models y, and y models z, then x models z. The upshot of these facts are that if you have a really good statistical model of how words relate to each other, that model is also implicitly a model of the world.</p>
<p id="12db">It might be instructive to think about what it would take to create a program which has a model of eighth grade science sufficient to understand and answer questions about hundreds of different things like “growth is driven by cell division”, and “What can magnets be used for” that wasn’t NLP led. It would be a nightmare of many different (probably handcrafted) models. Speaking somewhat loosely, language allows for intellectual capacities to be greatly compressed. From this point of view, it shouldn’t be surprising that some of the first signs of really broad capacity- common sense reasoning, wide ranging problem solving etc., have been found in language based programs- words and their relationships are just a vastly more efficient way of representing knowledge than the alternatives.</p>
<p id="1e8b">So I find myself wondering if language is not the crown of general intelligence, but a potential shortcut to it.</p>
<p id="beea"><strong>3.</strong></p>
<p id="0095">A couple of weeks ago I finished this essay, read through it, and decided it was not good enough to publish. The point about language being isomorphic to the world, and that therefore any sufficiently good model of language&nbsp;<em>is</em>&nbsp;a model of the world, is important, but it’s kind of abstract, and far from original.</p>
<p id="e0e2">Then today I read&nbsp;<a href="https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/" target="_blank" rel="noopener nofollow">this report</a>&nbsp;by Scott Alexander of having trained GPT-2 (a language program) to play chess. I realised this was the perfect example. GPT-2 has no (visual) understanding of things like the arrangement of a chess board. But if you feed it enough sequences of alphanumerically encoded games- 1.Kt-f3, d5 and so on- it begins to understand patterns in these strings of characters which are isomorphic to chess itself. Thus, for all intents and purposes, it develops a model of chess.</p>
<p id="1a14">Exactly how strong this approach is- whether GPT-2 is capable of some limited analysis, or can only overfit openings- remains to be seen. We might have a better idea as it is optimised — for example, once it is fed board states instead of sequences of moves. Either way though, it illustrates the point about isomorphism.</p>
<p id="a87a">Of course everyday language stands in a woolier relation to sheep, pine cones, desire and quarks than the formal language of chess moves stands in relation to chess moves, and the patterns are far more complex. Modality, uncertainty, vagueness and other complexities enter but the isomorphism between world and language is there, even if inexact.</p>
<p id="b6c0"><strong>Postscript- The Chinese Room Argument</strong></p>
<p id="f901">After similar arguments are made, someone usually mentions the Chinese room thought experiment. There are, I think, two useful things to say about it:</p>
<p id="3d7e">A) The thought experiment is an argument about understanding in itself, separate from capacity to handle tasks, a difficult thing to quantify or understand. It’s unclear that there is a practical upshot for what <em>AI can actually do.</em></p>
<p id="1f79">B) A lot of the power of the thought experiment hinges on the fact that the room solves questions using a lookup table, this stacks the deck. Perhaps we be more willing to say that the room as a whole understood language if it formed an (implicit) model of how things are, and of the current context, and used those models to answer questions? Even if this doesn’t deal with all the intuition that the room cannot understand Chinese, I think it takes a bite from it.</p>
<p id="96a9">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p>
<p id="4642">(1)- Strictly of course only the patterns in true sentences mirror, or are isomorphic to, the arrangement of the world, but most sentences people utter are at least approximately true.</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

		<!-- .entry-auhtor -->
</article></div>]]>
            </description>
            <link>https://deponysum.com/2020/01/16/recent-advances-in-natural-language-processing-some-woolly-speculations/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24179795</guid>
            <pubDate>Sun, 16 Aug 2020 18:03:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Error recovery with parser combinators, using Rust and nom]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24178208">thread link</a>) | @fanf2
<br/>
August 16, 2020 | https://www.eyalkalderon.com/nom-error-recovery/ | <a href="https://web.archive.org/web/*/https://www.eyalkalderon.com/nom-error-recovery/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<article>

<ul id="frontmatter">
<li>
<time datetime="2020-04-01">April 01, 2020</time>
</li>
<span></span>
<li> 3534 words </li>
<span></span>
<li> 18 min </li>
</ul>
<p>As the COVID-19 pandemic continues to ravage the globe, lots of people are stuck
at home, either working remotely or sitting around without much to do. The
previous afternoon, I had stumbled across an online announcement that the
<a href="https://www.acm.org/articles/bulletins/2020/march/dl-access-during-covid-19">ACM Digital Library has been made free to all to read and download</a> to
help foster research, discovery, and learning during this time of crisis.
Feeling curious, and having previously wanted to read certain research papers
from the ACM DL previously, I took the opportunity to peruse through its library
and read as much content as I could. As I was doing so, I stumbled across a very
useful paper called <a href="https://dl.acm.org/doi/10.1145/3167132.3167261"><strong>"Syntax error recovery in parsing expression
grammars"</strong></a> by (Medeiros, S. and Fabio Mascarenhas, 2018) that I would
like to share, and I'll be testing some of its concepts using a prototype parser
written in <a href="https://www.rust-lang.org/">Rust</a> with the help of the <a href="https://docs.rs/nom"><code>nom</code></a> crate.</p>
<h2 id="some-background">Some background</h2>
<p>Language parsing is a very broad and interesting topic, with a swathe of varying
approaches and tools to choose from depending on the requirements of the task at
hand, but the basic premise is simple: the goal of a parser is to consume some
data as input, break it down into its component parts according to some grammar,
and derive meaning or understanding from it (<a href="https://en.wikipedia.org/wiki/Parsing">wiki</a>). I personally happen to
enjoy working with <em>parsing expression grammars</em> (PEGs) and <em>parser combinators</em>
when writing my own projects.</p>
<p>In case you are not familiar, PEG is a kind of declarative formal language for
describing other languages in terms of string pattern matching. That is, PEG
allows the parser author to declare the grammar of the language they wish to
parse using sets of expressions like those shown below:</p>
<pre><span>expr    ← sum
sum     ← product (('+' / '-') product)*
product ← value (('*' / '/') value)*
value   ← [0-9]+ / '(' expr ')'
</span></pre>
<p>These PEG rules would then be able to describe the rules to a simple arithmetic
language that behaves like this:</p>
<table><thead><tr><th>Input</th><th>Parsed syntax tree</th></tr></thead><tbody>
<tr><td><code>123</code></td><td><code>Value(123)</code></td></tr>
<tr><td><code>1 + 2</code></td><td><code>Sum(Value(1), Value(2))</code></td></tr>
<tr><td><code>1 + 2 * 3</code></td><td><code>Sum(Value(1), Product(Value(2), Value(3)))</code></td></tr>
<tr><td><code>(1 + 2) * 3</code></td><td><code>Product(Sum(Value(1), Value(2)), Value(3))</code></td></tr>
</tbody></table>
<p>Any PEG expression can be converted directly into a <a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">recursive descent parser</a>,
either automatically using a parser generator or crafted by hand in the
programming language of your choice.</p>
<p>I really enjoy using parser combinator frameworks like <a href="https://docs.rs/nom"><code>nom</code></a> as a nice middle
ground between the two options, since they grant you the freedom and flexibility
of writing your parser fully in the host language (in this example, Rust), but
the resulting code is succinct, fairly declarative, and looks somewhat PEG-ish,
if you tilt your head and squint hard enough.</p>
<pre><span>fn </span><span>expr</span><span>(</span><span>input</span><span>: &amp;</span><span>str</span><span>) -&gt; IResult&lt;&amp;</span><span>str</span><span>, &amp;</span><span>str</span><span>&gt; {
    </span><span>sum</span><span>(input)
}

</span><span>fn </span><span>sum</span><span>(</span><span>input</span><span>: &amp;</span><span>str</span><span>) -&gt; IResult&lt;&amp;</span><span>str</span><span>, &amp;</span><span>str</span><span>&gt; {
    </span><span>let</span><span> op = </span><span>alt</span><span>((</span><span>char</span><span>('</span><span>+</span><span>'), </span><span>char</span><span>('</span><span>-</span><span>')));
    </span><span>recognize</span><span>(</span><span>pair</span><span>(product, </span><span>many0</span><span>(</span><span>pair</span><span>(op, product))))(input)
}

</span><span>fn </span><span>product</span><span>(</span><span>input</span><span>: &amp;</span><span>str</span><span>) -&gt; IResult&lt;&amp;</span><span>str</span><span>, &amp;</span><span>str</span><span>&gt; {
    </span><span>let</span><span> op = </span><span>alt</span><span>((</span><span>char</span><span>('</span><span>*</span><span>'), </span><span>char</span><span>('</span><span>/</span><span>')));
    </span><span>recognize</span><span>(</span><span>pair</span><span>(value, </span><span>many0</span><span>(</span><span>pair</span><span>(op, value))))(input)
}

</span><span>fn </span><span>value</span><span>(</span><span>input</span><span>: &amp;</span><span>str</span><span>) -&gt; IResult&lt;&amp;</span><span>str</span><span>, &amp;</span><span>str</span><span>&gt; {
    </span><span>recognize</span><span>(</span><span>alt</span><span>((digit1, </span><span>delimited</span><span>(</span><span>char</span><span>('</span><span>(</span><span>'), expr, </span><span>char</span><span>('</span><span>)</span><span>')))))(input)
}
</span></pre>
<p>Each of the four parsers above corresponds to a PEG rule, and since each one is
represented as a pure function, they compose nicely in code and each one can
easily be tested in isolation from the others, e.g. with inline unit tests. All
in all, I enjoy working with PEG and parser combinators!</p>
<h2 id="motivation">Motivation</h2>
<p>I've been hacking on a parser and <a href="https://microsoft.github.io/language-server-protocol/">language server</a> for the <a href="https://nixos.org/nix/">Nix programming
language</a> as a side project (<a href="https://github.com/ebkalderon/nix-language-server">GitHub</a>) for some time now, and this extended
period of being stuck at home renewed my interest in working on it. This
language server aims to supply code analysis, and auto-completion for compatible
third-party text editors and IDEs. This project has been very challenging for me
to work on, in a good way, because language servers tend to have very strict
requirements of their underlying parsers.</p>
<p>Most compilers and static analysis tools are <a href="https://en.wikipedia.org/wiki/Batch_program">batch programs</a> which act like a
dumb pipe, consuming source code in one end and spitting an executable out the
other (yes, incremental compilation and artifact caching bends this analogy a
bit, but the basic premise still holds). This means that their parsers and
resulting <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract syntax trees</a> are optimized for very different things than
what an interactive IDE would want.</p>
<p>Since the user is continuously modifying the source text and entering keystrokes
into their editor, the parser providing syntax checking for their editor is very
frequently exposed to incomplete or downright invalid snippets of code more
often than not. This means that halting parsing and bailing with an error
message whenever the first error is encountered, like many traditional parsers
do, is <em>simply not an option</em>.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/76758522-1ee9ad80-678a-11ea-84cd-111739ecd379.gif" alt="rust-analyzer in action">
<em>Source: <a href="https://rust-analyzer.github.io/thisweek/2020/03/16/changelog-16.html">rust-analyzer Changelog #16</a></em></p>
<p>Instead, the parser needs to be as fault-tolerant as possible, always producing
a syntax tree of some kind on every single parse and deriving as much syntactic
and semantic meaning as it can from user input, however malformed it might be.
Your editor should still be able to provide meaningful code completion, hover
documentation, go-to-definition, and symbol searching regardless of whether
there is a missing semicolon somewhere halfway down the page.</p>
<h2 id="a-naive-approach">A naive approach</h2>
<p>When I first started working on this project, I had chosen to implement my Nix
parser in Rust using <code>nom</code> 5.0, since that was the tool I was most comfortable
using for writing parsers at the time.</p>
<p>As I was writing up my parsers, I very quickly realized that bailing early from
parsing with an <code>Err(nom::Err::Error(_))</code> or <code>Err(nom::Error::Failure(_))</code>
wasn't a good idea for emitting errors. The former triggers a backtrack, which I
didn't always want, and the latter would halt parsing altogether with an error,
which I never wanted. <code>Err(nom::Error::Incomplete(_))</code> sounded promising due to
the name, but it too ended up being useless given the design constraints I had
in mind. I needed some way to log that a non-fatal parse error had been
encountered and resume parsing as though nothing had happened, but
unfortunately, there seemed to be nothing in the vast <code>nom</code> parser combinator
toolbox that could help me deal with this.</p>
<p>Given that <code>nom</code> parser combinators are pure functions whose signatures are
structured like this:</p>
<pre><span>impl </span><span>Fn(Input) -&gt; IResult&lt;Input, Output, Error&gt;
</span></pre>
<p>which maps to:</p>
<pre><span>impl </span><span>Fn(Input) -&gt; Result&lt;(Remaining, Output), Error&gt;
</span></pre>
<p>I decided to carry these non-fatal parse errors through the <code>Output</code> instead of
returning them through <code>Result::Err(nom::Error::Error(_))</code> using a custom data
structure which I had named <a href="https://github.com/ebkalderon/nix-language-server/blob/master/nix-parser/src/parser/partial.rs#L64-L68"><code>Partial</code></a>. This was a monadic data structure which
was essentially:</p>
<pre><span>pub struct </span><span>Partial&lt;T&gt; {
    </span><span>value</span><span>: Option&lt;T&gt;,
    </span><span>errors</span><span>: Vec&lt;Error&gt;,
}

</span><span>impl</span><span>&lt;T&gt; Partial&lt;T&gt; {
    </span><span>pub fn </span><span>map</span><span>&lt;U, F&gt;(</span><span>self</span><span>, </span><span>f</span><span>: F) -&gt; Partial&lt;U&gt;
    </span><span>where</span><span>
        F: FnOnce(T) -&gt; U
    { ... }

    </span><span>pub fn </span><span>flat_map</span><span>&lt;U, F&gt;(</span><span>mut </span><span>self</span><span>, </span><span>f</span><span>: F) -&gt; Partial&lt;U&gt;
    </span><span>where</span><span>
        F: FnOnce(T) -&gt; Partial&lt;U&gt;
    { ... }

    </span><span>pub fn </span><span>value</span><span>(&amp;</span><span>self</span><span>) -&gt; Option&lt;&amp;T&gt; {
        ...
    }

    </span><span>pub fn </span><span>errors</span><span>(&amp;</span><span>self</span><span>) -&gt; &amp;[Error] {
        ...
    }

    </span><span>pub fn </span><span>verify</span><span>(</span><span>self</span><span>) -&gt; Result&lt;T, Vec&lt;Error&gt;&gt; {
        ...
    }
}
</span></pre>
<p>This data structure was complemented with a bunch of custom <code>nom</code> combinators,
e.g. <code>map_partial()</code>, <code>expect_terminated()</code>, and <code>skip_if_err()</code>, which would
allow me to compose these fault-tolerant parsers together while accumulating
errors in the <code>errors</code> field.</p>
<p>The consumer of this data structure would then choose to either:</p>
<ol>
<li>Assert that they need a valid AST without errors by calling <code>expr.verify()</code>,
transforming the <code>Partial&lt;T&gt;</code> into a <code>Result&lt;T, Vec&lt;Error&gt;&gt;</code>. This option
would be useful for traditonal batch compiler authors, as well as for testing
and debugging.</li>
<li>Extract and examine the contents of the <code>value</code> and <code>errors</code> field
separately. This is what the language server would do: publish the
accumulated errors to the user's editor in the form of diagnostics and then
perform further analysis on the syntax tree contained in <code>value</code>.</li>
</ol>
<p>All the parser combinators would have this function signature instead:</p>
<pre><span>impl </span><span>Fn(Input) -&gt; IResult&lt;Input, Partial&lt;Output&gt;, Error&gt;
</span></pre>
<p>While this approach seemed to work well initially, it spiralled out of control
once the parser grew beyond a certain size. The number of <code>Partial</code> specific
combinators grew, the parser logic got hairier, more imperative, and trickier to
debug, and the performance implications of carrying around a heavy stack of
errors from function to function <a href="https://github.com/ebkalderon/nix-language-server/commit/4cd939a2917709a527bd1967f4a29bfd9f2767cc">were astonishingly awful</a>. It didn't
look and feel that much like PEG anymore.</p>
<p>I will admit I learned a lot about a breadth of topics during this time, from
benchmarking functions with <a href="https://github.com/bheisler/criterion.rs"><code>criterion</code></a> to generating flamegraphs with
<a href="https://github.com/ferrous-systems/flamegraph"><code>cargo-flamegraph</code></a>, and going to extreme lengths to avoid heap allocations to
make the parser as fast as possible. I used <a href="https://docs.rs/nom_locate"><code>nom_locate</code></a> to retain string span
information and be as zero-copy as possible when constructing the syntax tree.
But ultimately, I couldn't fix all the warts and fundamental flaws. I needed a
new approach.</p>
<h2 id="the-paper-s-solution">The paper's solution</h2>
<p>Finally, back to the paper that originally inspired this article! I shelved this
project some months ago due to work and personal life matters, but came back
to it last month with some fresh ideas and a better intuition of where to look.
Discouraged by the previous setbacks, I was questioning whether parser
combinators in general were flexible enough to express parsers which were both
permissive and fault-tolerant, while also emitting good hand-crafted
diagnostics. But then I stumbled upon the <a href="https://dl.acm.org/doi/10.1145/3167132.3167261">"Syntax error recovery in parsing
expression grammars" (2018)</a> paper while scouring the ACM DL search
engine for interesting articles last night.</p>
<p>The authors of this paper actually managed to get pretty great results parsing
the <a href="https://www.lua.org/">Lua programming language</a> using a set of extended PEGs, producing excellent
tailor-made diagnostics rivaling the automatic error recovering capabilities of
their control, a top-down <a href="https://en.wikipedia.org/wiki/LL_parser">LL parser</a>…</p></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.eyalkalderon.com/nom-error-recovery/">https://www.eyalkalderon.com/nom-error-recovery/</a></em></p>]]>
            </description>
            <link>https://www.eyalkalderon.com/nom-error-recovery/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24178208</guid>
            <pubDate>Sun, 16 Aug 2020 14:43:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hiding messages in x86 binaries using semantic duals]]>
            </title>
            <description>
<![CDATA[
Score 168 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24178188">thread link</a>) | @todsacerdoti
<br/>
August 16, 2020 | https://blog.yossarian.net/2020/08/16/Hiding-messages-in-x86-binaries-using-semantic-duals | <a href="https://web.archive.org/web/*/https://blog.yossarian.net/2020/08/16/Hiding-messages-in-x86-binaries-using-semantic-duals">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">


<h2><em>Programming, philosophy, pedaling.</em></h2>

<ul>
    <li><a href="https://blog.yossarian.net/">Home</a></li>
    <li><a href="https://blog.yossarian.net/tags">Tags</a></li>
    <li><a href="https://blog.yossarian.net/favorites">Favorites</a></li>
    <li><a href="https://blog.yossarian.net/archive">Archive</a></li>
    
      <li><a href="https://yossarian.net/">Main Site</a></li>
    
</ul>

<hr>



<h2>
  <em>Aug 16, 2020</em>
</h2>

  <p>Tags:
  
    
    <a href="https://blog.yossarian.net/tags#rust">rust</a>,
    
  
    
    <a href="https://blog.yossarian.net/tags#devblog">devblog</a>,
    
  
    
    <a href="https://blog.yossarian.net/tags#programming">programming</a>
    
  
  </p>


<p>This is a quick writeup of a <a href="https://en.wikipedia.org/wiki/Steganography">steganographic</a> tool
that I threw together: <a href="https://github.com/woodruffw/steg86"><em>steg86</em></a>. It uses a quirk of the
x86 instruction encoding format to hide messages in pre-existing binaries with no size or
performance overhead.</p>

<p>You can play with it yourself by fetching it with <code>cargo</code>:</p>

<div><div><pre><code><span>$ </span>cargo <span>install </span>steg86
<span>$ </span>steg86 <span>--help</span>
</code></pre></div></div>

<h2 id="a-quick-steganography-refresher">A quick steganography refresher</h2>

<p>Steganography is the practice of concealing data within other data. A variety of different
common formats open themselves up to steganographic channels:</p>

<ul>
  <li>Image formats: using the least significant bits of the color space</li>
  <li>Audio formats: hiding information in background noise, or hiding an image
in the <a href="https://en.wikipedia.org/wiki/Spectrogram">spectrogram</a></li>
  <li>Plain text: using <a href="https://en.wikipedia.org/wiki/IDN_homograph_attack">homographs</a> and
<a href="https://en.wikipedia.org/wiki/Zero-width_joiner">special spacing characters</a> to encode information</li>
</ul>

<p>On its own, steganography is <em>not</em> a replacement for cryptography: it’s strictly
obfuscatory, and does not provide any deniablity of the embedded message if revealed.</p>

<p>That being said, there exist a variety of use cases for steganographic schemes:</p>

<ul>
  <li>Coupled with strong cryptography, they <em>do</em> allow for deniability: depending on the format,
an extracted message can be made indistinguishable from random noise prior to decryption.</li>
  <li>Steganography can be used for fingerprinting and asset management.
<a href="https://en.wikipedia.org/wiki/Machine_Identification_Code">Commercial printers</a> are well-known for doing this;
video games have also used it to <a href="https://www.schneier.com/blog/archives/2012/09/steganography_i_1.html">track users and their machines</a>.</li>
</ul>

<h2 id="doing-steganography-on-computer-programs">Doing steganography on computer programs</h2>

<p>There are two obvious avenues for doing steganography on compiled programs. <em>steg86</em> does
<strong>neither</strong> of these, but for the sake of completeness:</p>

<h3 id="instrumenting-the-program-via-the-compiler">Instrumenting the program via the compiler</h3>

<p>Compilers perform instruction selection to lower their internal representations into machine code.
Instruction selection can be informed by any number of weights, including
<a href="https://en.wikipedia.org/wiki/Register_allocation">register pressure</a>, individual instruction
performance (clock cycles, use of shared units), and feature availability
(e.g., selecting an older version of <a href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions">SSE</a>
to be compatible with more CPUs).</p>

<p>In this approach, the steganographer would control the compiler and its instruction selection
weights, encoding information in particular selections of instructions (or other parts of
code generation, like stack object order).</p>

<p>This affords a great deal of flexibility and control, but also makes it more difficult for the
compiler to perform optimal selections. It also requires the steganographer to maintain
their (forked) compiler. Not a bad idea, but requires a decent amount of effort.</p>

<h3 id="instrumenting-the-container-format">Instrumenting the container format</h3>

<p>Another approach is to ignore the compiled instructions themselves, and focus on the binary
container: <a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format">ELF</a>,
<a href="https://en.wikipedia.org/wiki/Mach-O">Mach-O</a>, or
<a href="https://en.wikipedia.org/wiki/Portable_Executable">PE</a>, among others.</p>

<p>These formats afford many opportunities for embedding information: the order
(both file and virtual address) of segments and sections, the order and layout
of <a href="https://en.wikipedia.org/wiki/Relocation_(computing)">relocation tables</a> and the composition of
their internal rules, and so on.</p>

<p>Instrumenting the binary format rather than the executable code means not having
to muck with instruction selection, but is also less bountiful in terms of
information density. It’s also not portable, e.g. tricks for hiding information
in ELF relocation entries would have to be tweaked for PEs.</p>

<h2 id="how-steg86-works">How <em>steg86</em> works</h2>

<p>Instead of controlling instruction selection or tweaking the details of the containing binary
format, <em>steg86</em> takes a third route: it uses the presence of <em>semantic duals</em> in the x86 and AMD64
instruction formats to selectively rewrite a program <em>after</em> compilation.</p>

<h3 id="semantic-duals">Semantic duals</h3>

<p>Like every ISA, x86 (and AMD64) have multiple ways to encode the semantics of a particular
(higher level, conceptual) operation. For example, clearing a register:</p>

<div><div><pre><code><span>; xor'ing a register with itself clears all bits</span>
<span>xor</span> <span>eax</span><span>,</span> <span>eax</span>

<span>; and'ing a register with 0 also clears all bits</span>
<span>and</span> <span>eax</span><span>,</span> <span>0</span>

<span>; we could also set 0</span>
<span>mov</span> <span>eax</span><span>,</span> <span>0</span>

<span>; ...or subtract</span>
<span>sub</span> <span>eax</span><span>,</span> <span>eax</span>

<span>; ...or load a "computed" address that's always 0</span>
<span>lea</span> <span>eax</span><span>,</span> <span>[</span><span>0</span><span>]</span>
</code></pre></div></div>

<p>…among many others. Unfortunately, this doesn’t work exactly as we’d like:</p>

<ul>
  <li>Each of these alternatives may have a different length when encoded, e.g. 2 bytes for
<code>xor eax, eax</code> versus 6 for <code>lea eax, [0]</code>. Consequently, rewriting one in terms of another
might entail a length change and subsequent relocation and displacement fixups.</li>
  <li>Each of these alternatives has subtle microarchitectural differences: they set and clear
different arithmetic flags, have different implicit segmentation behavior, and so on.
We don’t know perfectly whether the original operations took advantage of those details,
so many of our potential alternatives run the risk of subtle breakage.</li>
</ul>

<p>Fortunately, this is <strong>not</strong> what <em>steg86</em> does.</p>

<p>Instead, <em>steg86</em> takes advantage of one of the quirks of the x86 instruction encoding: the ModR/M
byte:</p>

<p><img src="https://blog.yossarian.net/assets/modrm.png" alt="The ModR/M byte."></p>

<p>The ModR/M byte is normally used to encode one or more explicit register and/or memory operands
to an opcode<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>: it’s how x86 can supply the rich range of source-to-sink modes that it has
(register-to-memory, memory-to-register, register-to-register, immediate-to-register, &amp;c).</p>

<p>Because register-to-memory and memory-to-register are both valid source-sink combinations for
many opcodes, many have encoding forms like the following:</p>

<div><div><pre><code><span>; xor register with register/memory32 and store register/memory32</span>
<span>xor</span> <span>r</span><span>/</span><span>m32</span><span>,</span> <span>r32</span>

<span>; xor register/memory32 with register and store register</span>
<span>xor</span> <span>r32</span><span>,</span> <span>r</span><span>/</span><span>m32</span>
</code></pre></div></div>

<p>Consequently, there are actually <strong>two</strong> ways to encode <code>xor eax, eax</code>:</p>

<div><div><pre><code>; r/m32, r
31 C0

; r, r/m32
33 C0
</code></pre></div></div>

<p>These encodings are <em>semantic duals</em>: they have different opcodes, but belong to the same
instruction family, have the same length, and have the <em>exact same behavior and performance</em>.
We could replace every occurrence of <code>31 C0</code> with <code>33 C0</code> and our programs would happily chug along,
none the wiser.</p>

<p>As it turns out, there are actually quite a few of these duals on some of the most common
x86 instructions<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>:</p>

<div><div><pre><code>ADD r/m8, r8   &lt;=&gt; ADD r8, r/m8
ADD r/m16, r16 &lt;=&gt; ADD r16, r/m16
ADD r/m32, r32 &lt;=&gt; ADD r32, r/m32
ADD r/m64, r64 &lt;=&gt; ADD r64, r/m64

ADC r/m8, r8   &lt;=&gt; ADC r8, r/m8
ADC r/m16, r16 &lt;=&gt; ADC r16, r/m16
ADC r/m32, r32 &lt;=&gt; ADC r32, r/m32
ADC r/m64, r64 &lt;=&gt; ADC r64, r/m64

AND r/m8, r8   &lt;=&gt; AND r8, r/m8
AND r/m16, r16 &lt;=&gt; AND r16, r/m16
AND r/m32, r32 &lt;=&gt; AND r32, r/m32
AND r/m64, r64 &lt;=&gt; AND r64, r/m64

OR r/m8, r8    &lt;=&gt; OR r8, r/m8
OR r/m16, r16  &lt;=&gt; OR r16, r/m16
OR r/m32, r32  &lt;=&gt; OR r32, r/m32
OR r/m64, r64  &lt;=&gt; OR r64, r/m64

XOR r/m8, r8   &lt;=&gt; XOR r8, r/m8
XOR r/m16, r16 &lt;=&gt; XOR r16, r/m16
XOR r/m32, r32 &lt;=&gt; XOR r32, r/m32
XOR r/m64, r64 &lt;=&gt; XOR r64, r/m64

SUB r/m8, r8   &lt;=&gt; SUB r8, r/m8
SUB r/m16, r16 &lt;=&gt; SUB r16, r/m16
SUB r/m32, r32 &lt;=&gt; SUB r32, r/m32
SUB r/m64, r64 &lt;=&gt; SUB r64, r/m64

SBB r/m8, r8   &lt;=&gt; SBB r8, r/m8
SBB r/m16, r16 &lt;=&gt; SBB r16, r/m16
SBB r/m32, r32 &lt;=&gt; SBB r32, r/m32
SBB r/m64, r64 &lt;=&gt; SBB r64, r/m64

MOV r/m8, r8   &lt;=&gt; MOV r8, r/m8
MOV r/m16, r16 &lt;=&gt; MOV r16, r/m16
MOV r/m32, r32 &lt;=&gt; MOV r32, r/m32
MOV r/m64, r64 &lt;=&gt; MOV r64, r/m64

CMP r/m8, r8   &lt;=&gt; CMP r8, r/m8
CMP r/m16, r16 &lt;=&gt; CMP r16, r/m16
CMP r/m32, r32 &lt;=&gt; CMP r32, r/m32
CMP r/m64, r64 &lt;=&gt; CMP r64, r/m64
</code></pre></div></div>

<p>Each dual represents one bit of information: we can arbitrarily assign one form
to be <code>true</code> and its counterpart to be <code>false</code>. Given enough of them in a target
program, we can hide a message by translating between the forms. The result:
a binary that’s exactly the same size as the original and with the same performance
characteristics, but with a hidden message.</p>

<p>That’s all <em>steg86</em> does: it locates every semantic dual in its target program, maps
its input message to a bitstring, and translates each dual depending on its corresponding
message bit. Because it only uses duals, it’s completely agnostic to its containing format:
the <code>steg86</code> CLI currently supports ELF, Mach-O, and PE/PE32+ binaries.</p>

<p>Here’s what that actually looks like, with (some of the) differing bytes highlighted in red:</p>

<p><img src="https://blog.yossarian.net/assets/steg86_vbindiff.png" alt="vbindiff between bash and steg'd bash"></p>

<p>And, of course, try it for yourself:</p>

<div><div><pre><code><span>$ </span>steg86 profile /bin/bash
Summary <span>for</span> /bin/bash:
  175828 total instructions
  27957 potential semantic pairs
  27925 bits of information capacity <span>(</span>3490 bytes, approx. 3KB<span>)</span>

<span>$ </span>steg86 embed /bin/bash test.steg <span>&lt;&lt;&lt;</span> <span>"hello world!"</span>
<span>$ </span>file test.steg
test.steg: ELF 64-bit LSB shared object, x86-64, version 1 <span>(</span>SYSV<span>)</span>, dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]<span>=</span>a6cb40078351e05121d46daa768e271846d5cc54, <span>for </span>GNU/Linux 3.2.0, stripped

<span>$ </span>steg86 extract test.steg
hello world!
</code></pre></div></div>

<p>See the <a href="https://github.com/woodruffw/steg86/blob/master/README.md">README</a> for prior work and
future refinements to <em>steg86</em>’s informational capacity.</p>

<p><strong>Edit</strong>: An important note: because compilers and assemblers tend to select just one of the forms
above, <em>steg86</em> <strong>cannot</strong> provide deniability, even with strong encryption. Its transformations
will always look abnormal compared to a compiler-produced binary.</p>

<hr>




<hr>




  






</div>]]>
            </description>
            <link>https://blog.yossarian.net/2020/08/16/Hiding-messages-in-x86-binaries-using-semantic-duals</link>
            <guid isPermaLink="false">hacker-news-small-sites-24178188</guid>
            <pubDate>Sun, 16 Aug 2020 14:40:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Patterns of Regional Cerebral Blood Flow as a Function of Obesity in Adults]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 53 (<a href="https://news.ycombinator.com/item?id=24177679">thread link</a>) | @InInteraction
<br/>
August 16, 2020 | https://www.iospress.nl/ios_news/body-weight-has-surprising-alarming-impact-on-brain-function/ | <a href="https://web.archive.org/web/*/https://www.iospress.nl/ios_news/body-weight-has-surprising-alarming-impact-on-brain-function/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>
            <h3>Body Weight Has Surprising, Alarming Impact on Brain Function</h3> 
			<h4>Higher BMI is linked to decreased cerebral blood flow, which is associated with increased risk of Alzheimer's disease and mental illness, according to a new study in JAD</h4>		</p><div> 
						            		
						<p>
							<em>August 5, 2020</em><br>
							<strong>Amsterdam, NL and Costa Mesa, CA, USA – As a person's weight goes up, all regions of the brain go down in activity and blood flow, according to a <a href="https://content.iospress.com/articles/journal-of-alzheimers-disease/jad200655" target="_blank">new brain imaging study</a> in the <em><a href="http://j-alz.com/" target="_blank">Journal of Alzheimer's Disease</a></em>. One of the largest studies linking obesity with brain dysfunction, scientists analyzed over 35,000 functional neuroimaging scans using single-photon emission computerized tomography from more than 17,000 individuals to measure blood flow and brain activity. </strong>
						</p><p>Low cerebral blood flow is the #1 brain imaging predictor that a person will develop Alzheimer’s disease. It is also associated with depression, ADHD, bipolar disorder, schizophrenia, traumatic brain injury, addiction, suicide, and other conditions. “This study shows that being overweight or obese seriously impacts brain activity and increases the risk for Alzheimer’s disease as well as many other psychiatric and cognitive conditions,” explained Daniel G. Amen, MD, the study’s lead author and founder of Amen Clinics, one of the leading brain-centered mental health clinics in the United States</p>
<div id="attachment_48994"><p><img src="https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop.jpg" alt="" width="454" height="293" srcset="https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop.jpg 454w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop-300x194.jpg 300w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop-148x96.jpg 148w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop-333x215.jpg 333w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop-320x207.jpg 320w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image1_crop-310x200.jpg 310w" sizes="(max-width: 454px) 100vw, 454px"></p><p><em>Areas of obesity-related hypoperfusion in brain regions vulnerable to Alzheimer’s disease: Hippocampus</em></p></div>
<p>Striking patterns of progressively reduced blood flow were found in virtually all regions of the brain across categories of underweight, normal weight, overweight, obesity, and morbid obesity. These were noted while participants were in a resting state as well as while performing a concentration task. In particular, brain areas noted to be vulnerable to Alzheimer’s disease, the temporal and parietal lobes, hippocampus, posterior cingulate gyrus, and precuneus, were found to have reduced blood flow along the spectrum of weight classification from normal weight to overweight, obese, and morbidly obese.</p>
<div id="attachment_48995"><p><img src="https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop.jpg" alt="" width="500" height="333" srcset="https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop.jpg 500w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop-300x200.jpg 300w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop-148x99.jpg 148w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop-323x215.jpg 323w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop-320x213.jpg 320w, https://www.iospress.nl/wp-content/uploads/2020/08/JAD77-3_PR1_image2_crop-310x206.jpg 310w" sizes="(max-width: 500px) 100vw, 500px"></p><p><em>3D renderings of blood flow averaged across normal BMI (BMI = 23), overweight (BMI = 29), and obese (BMI = 37) men, each 40 years of age (credit: Amen Clinics)</em></p></div>
<p>Considering the latest statistics showing that 72% of Americans are overweight of whom 42% are obese, this is distressing news for America’s mental and cognitive health.</p>
<p>Commenting on this study, George Perry, PhD, Editor-in-Chief of the <em>Journal of Alzheimer’s Disease</em> and Semmes Foundation Distinguished University Chair in Neurobiology at The University of Texas at San Antonio, stated, “Acceptance that Alzheimer’s disease is a lifestyle disease, little different from other age-related diseases, that is the sum of a lifetime is the most important breakthrough of the decade. Dr. Amen and collaborators provide compelling evidence that obesity alters blood supply to the brain to shrink the brain and promote Alzheimer’s disease. This is a major advance because it directly demonstrates how the brain responds to our body.”</p>
<p>This study highlights the need to address obesity as a target for interventions designed to improve brain function, be they Alzheimer disease prevention initiatives or attempts to optimize cognition in younger populations. Such work will be crucial in improving outcomes across all age groups.</p>
<p>Although the results of this study are deeply concerning, there is hope. Dr. Amen added, “One of the most important lessons we have learned through 30 years of performing functional brain imaging studies is that brains can be improved when you put them in a healing environment by adopting brain-healthy habits, such as a healthy calorie-smart diet and regular exercise.”</p>
<p>###</p>
<p><strong>NOTES FOR EDITORS<br>
Full open access study</strong>:&nbsp;“<a href="https://content.iospress.com/articles/journal-of-alzheimers-disease/jad200655" target="_blank" rel="noopener">Patterns of Regional Cerebral Blood Flow as a Function of Obesity in Adults</a>” by Daniel G. Amen, Joseph Wu, Noble George, and Andrew Newberg (<a href="https://doi.org/10.3233/JAD-200655" target="_blank" rel="noopener">doi.org/10.3233/JAD-200655</a>). The article appears online in the <em>Journal of Alzheimer’s Disease</em> in advance of Volume 77, Issue 3 (September 2020) published by IOS Press. The study is openly available at: <a href="https://content.iospress.com/articles/journal-of-alzheimers-disease/jad200655" target="_blank" rel="noopener">content.iospress.com/articles/journal-of-alzheimers-disease/jad200655</a>.</p>
<p><strong>Contact</strong><br>
To request the full text of the article or further information please contact Diana Murray, IOS Press (+1 718-640-5678 or <a href="mailto:d.murray@iospress.com">d.murray@iospress.com</a>) To reach the authors for comment please contact. Natalie Buchoz, Amen Clinics (+1 714-421-3778 or <a href="mailto:nbuchoz@amenclinic.com">nbuchoz@amenclinic.com</a>) For questions about the <em>Journal of Alzheimer’s Disease</em>, please contact George Perry (+1 210-458-8660 or <a href="mailto:george.perry@utsa.edu">george.perry@utsa.edu</a>)</p>
<p><strong>About the Journal of Alzheimer’s Disease</strong><br>
Now in its 23d year of publication, the&nbsp;<a href="http://www.j-alz.com/" target="_blank" rel="noopener"><em>Journal of Alzheimer’s Disease</em></a>&nbsp;(JAD) is an international multidisciplinary journal to facilitate progress in understanding the etiology, pathogenesis, epidemiology, genetics, behavior, treatment, and psychology of Alzheimer’s disease. The journal publishes research reports, reviews, short communications, book reviews, and letters-to-the-editor. Groundbreaking research that has appeared in the journal includes novel therapeutic targets, mechanisms of disease, and clinical trial outcomes. JAD has a 2019 Journal Impact Factor of 3.909 according to Journal Citation Reports (Clarivate, 2020). It is&nbsp;published by IOS Press.<a href="http://j-alz.com/" target="_blank" rel="noopener">&nbsp;j-alz.com</a></p>
<p><strong>About&nbsp;IOS Press</strong><br>
IOS Press is headquartered in Amsterdam with satellite offices in the USA, Germany, India and China and serves the information needs of scientific and medical communities worldwide. IOS Press now publishes more than 80 international peer-reviewed journals and about 75 book titles each year on subjects ranging from computer science, artificial intelligence, and engineering to medicine, neuroscience, and cancer research.&nbsp;<a href="http://iospress.com/" target="_blank" rel="noopener">iospress.com</a></p>
  
					</div></div>]]>
            </description>
            <link>https://www.iospress.nl/ios_news/body-weight-has-surprising-alarming-impact-on-brain-function/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24177679</guid>
            <pubDate>Sun, 16 Aug 2020 13:39:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AKAI MPC 3000 sampler/sequencer drum machine]]>
            </title>
            <description>
<![CDATA[
Score 112 | Comments 98 (<a href="https://news.ycombinator.com/item?id=24177626">thread link</a>) | @omnibrain
<br/>
August 16, 2020 | https://audiojive.com/akai-mpc-3000/ | <a href="https://web.archive.org/web/*/https://audiojive.com/akai-mpc-3000/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
<figure><img loading="lazy" width="1024" height="655" src="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=1024%2C655&amp;ssl=1" alt="akai-mpc-3000" srcset="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=1024%2C655&amp;ssl=1 1024w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=300%2C192&amp;ssl=1 300w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=768%2C491&amp;ssl=1 768w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=1536%2C982&amp;ssl=1 1536w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=360%2C230&amp;ssl=1 360w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=545%2C349&amp;ssl=1 545w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?resize=1600%2C1023&amp;ssl=1 1600w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000.jpg?w=1911&amp;ssl=1 1911w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<p>The AKAI MPC 3000 is a classic and legendary sampler/sequencer drum machine from the early 90’s. This music instrument has been associated with everything from Hip-Hop to R&amp;B and House music.</p>



<h2>AKAI MPC 3000 Classic</h2>



<figure><img loading="lazy" width="1024" height="655" src="https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=1024%2C655&amp;ssl=1" alt="" srcset="https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=1024%2C655&amp;ssl=1 1024w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=300%2C192&amp;ssl=1 300w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=768%2C491&amp;ssl=1 768w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=1536%2C982&amp;ssl=1 1536w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=360%2C230&amp;ssl=1 360w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=545%2C349&amp;ssl=1 545w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?resize=1600%2C1023&amp;ssl=1 1600w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-classic.jpg?w=1911&amp;ssl=1 1911w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<p>The original AKAI MPC 3000 Classic released in 1994 as the step up from its predecessor, the MPC 60 (and the later MPC 60 mkII). MSRP was staggering, coming in at just under $4,000 in the early 90’s (a little over $7,000 in today’s money). It was clear that this machine was meant for professional studios and intended to be a tool for deliberate music applications.</p>



<h2>AKAI MPC 3000 LE</h2>



<figure><img loading="lazy" width="1024" height="655" src="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=1024%2C655&amp;ssl=1" alt="akai-mpc-3000-le" srcset="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=1024%2C655&amp;ssl=1 1024w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=300%2C192&amp;ssl=1 300w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=768%2C491&amp;ssl=1 768w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=1536%2C982&amp;ssl=1 1536w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=360%2C230&amp;ssl=1 360w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=545%2C349&amp;ssl=1 545w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?resize=1600%2C1023&amp;ssl=1 1600w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-le-.jpg?w=1911&amp;ssl=1 1911w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<p>The black AKAI MPC 3000 LE is a limited edition release of only 2000 individually numbered units in the year 2000. </p>



<p>There is a rumor that only 1400 units have been released to the public because as far as we know, the public has not seen any units numbered 1400 through 2000. Let it be known if you have one of these rare units.</p>



<h2>AKAI MPC 3000 Specifications</h2>



<figure><img loading="lazy" width="1024" height="400" src="https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=1024%2C400&amp;ssl=1" alt="akai-mpc-3000-specifications" srcset="https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=1024%2C400&amp;ssl=1 1024w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=300%2C117&amp;ssl=1 300w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=768%2C300&amp;ssl=1 768w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=1536%2C601&amp;ssl=1 1536w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=2048%2C801&amp;ssl=1 2048w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=360%2C141&amp;ssl=1 360w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=545%2C213&amp;ssl=1 545w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?resize=1600%2C626&amp;ssl=1 1600w, https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-specifications.jpg?w=2320&amp;ssl=1 2320w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<p><strong>Polyphony: 32 notes<br>Sampler: 16 bit, 44.1KHz, Mono and Stereo<br>Memory: 2 MB RAM (22 seconds) to upgradable 32 MB<br>Filter: Lowpass filter with resonance and envelope</strong></p>



<p><strong>General</strong><br>• Display: 320 character (240 x 64 dot graphic) LCD<br>• Disk drive: 3.5 inch HD (1.44MB formatted)<br>• CPU: V53 @ 16MHz<br>• Dimensions: 440(W) x 121(H) x 405(D) mm<br>• Weight: 9 kg<br>• Power requirements: 120 VAC 60Hz 40W<br>220-240 VAC 50Hz</p>



<p><strong>Sound Generator</strong><br>• Sampling rate: 44.1kHz (frequency response: 20Hz-20kHz)<br>• Sampling capacity: 2MB standard (21.9 seconds mono or 10.9<br>seconds stereo), expandable to 16MB (188.3 seconds mono or 94.1<br>seconds stereo).<br>• Data format: 16-bit linear<br>• Dynamic filtering: 12 dB/Octave dynamic resonant lowpass filter<br>per voice<br>• Maximum sounds in memory: 128<br>• Number of sound programs: 24<br>• Sound assignments per program: 64<br>• Simultaneous voices: 32</p>



<p><strong>Sequencer</strong><br>• Maximum notes: 75,000<br>• Resolution: 96 parts per 1/4-note (ppq)<br>• Sequences: 99<br>• Tracks per sequence: 99<br>• MIDI output channels: 64 (16 channels x 4 output ports)<br>• Song mode: 20 songs, 250 steps per song<br>• Drum pads: 16 (velocity and pressure sensitive)<br>• Drum pad banks: 4<br>• Sync modes: MIDI Time Code, MIDI clock, FSK 24, 1/4-note clicks<br>and SMPTE (optional). SMPTE frame rates supported are 24, 25,<br>29.97 drop and 30.</p>



<p><strong>Rear Panel Inputs/Outputs</strong><br>• Record input sensitivity (both L and R): (HI gain) -58dBm, 45kΩ;<br>(MID gain) -38dBm, 45kΩ; (LO gain) -18dBm, 45kΩ<br>• Digital sampling input: S/PDIF<br>• Stereo output level: 6dBm, 600Ω<br>• Level of 8 individual outputs: 6dBm, 600Ω<br>• Sync/Trigger input level: 0.5V p-p level (with input control at<br>maximum)<br>• Sync output level: 2.5V p-p, impedance 600Ω<br>• MIDI inputs: 2 (mergeable)<br>• MIDI outputs: 4 (independent)<br>• SCSI port: 1 (Apple 25-pin D-type SCSI connector)<br>• Headphone output: 1<br>• Foot Switch inputs: 2 (independently assignable)</p>



<figure><img loading="lazy" width="1024" height="404" src="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=1024%2C404&amp;ssl=1" alt="akai-mpc-3000-io-rear-back" srcset="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=1024%2C404&amp;ssl=1 1024w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=300%2C118&amp;ssl=1 300w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=768%2C303&amp;ssl=1 768w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=1536%2C606&amp;ssl=1 1536w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=360%2C142&amp;ssl=1 360w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=545%2C215&amp;ssl=1 545w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?resize=1600%2C631&amp;ssl=1 1600w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-io-rear-back.jpg?w=1800&amp;ssl=1 1800w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<h2>History of the AKAI MPC 3000</h2>



<h3>Roger Linn</h3>



<p>In 1980 Roger Linn designed and released the worlds first drum machine that uses samples called the LM-1, under his own company Linn Moffett Electronics with then partner Alex Moffett. Eventually Linn Electronics went out of business and the next chapter would be Roger Linn and Akai Professional linking up to create new products. The collaboration was a good fit because Akai needed a creative designer and Roger Linn wasn’t a fan of the manufacturing aspects of the business. In 1988 they would release the Akai MPC 60 and later followed by the updated MPC 60 MKII. In 1994 came the release of the MPC 3000, the celebrated upgrade from the MPC 60 line.</p>



<h3>MPC 60 vs MPC 3000</h3>



<ul><li>MPC 60: 12 bit audio – MPC 3000: 16 bit audio.</li><li>MPC 60 has 750KB of stock memory, expandable to 1.5 MB. The MPC 3000 has 2 MB of stock memory, expandable up to 32 MB.</li><li>The MPC 60 only has a single mono sampling input, while the MPC 3000 has dual stereo inputs.</li><li>MPC 60 has no filters whle the MPC 3000 comes equipped with a lowpass filter.</li><li>Both sound slightly different from eachother. Some prefer the 60, others prefer the 3000.</li></ul>



<p>Roger Linn’s vision for what a drum machine could be revolved around <em>grooves</em>. A drum machine had to have great groove to it. So he wanted it to be simple and be able to record exactly what you threw at it. This required the timing to be super tight in the MPC 3000. Linn made sure all the details were figured out for this on the backend and the result is a machine that grooves perfectly.</p>



<figure><div>
<p><span><iframe width="1160" height="653" src="https://www.youtube.com/embed/vubHg2379d0?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;start=80&amp;wmode=transparent" allowfullscreen="true"></iframe></span></p>
</div><figcaption>Roger Linn talks about the AKAI MPC 3000.</figcaption></figure>



<p>Hip-hop took an immediate kinship to the machine. Early notable users were producers like DJ Quik, Battlecat, Dr. Dre and J DIlla. The creation of the drum machine sampler ushered in innovation to a musical artform that was previously based around two turntables. The innovation of the sampler elevated the artform to a new frontier. Roger Linn himself has said that he did not anticipate this.</p>



<blockquote><p><em><span><b>”The AKAI MPC simultaneously </b></span><strong>digitized beat-making, progressed the pastiche musical form, and freed Hip-Hop from the confines of earlier technology.”</strong></em></p><cite><a href="https://happymag.tv/in-praise-of-the-akai-mpc-a-hip-hop-time-machine/" target="_blank" rel="noreferrer noopener">HappyMag.Tv</a></cite></blockquote>



<h3>J Dilla</h3>



<p>J Dilla started to make a name for himself in the mid 90’s with classic records like Pharcyde’s “Runnin”, Janet Jackson’s “Got Til It’s Gone”, and Q-Tip’s “Breathe &amp; Stop”.</p>



<figure><img loading="lazy" width="1024" height="797" src="https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=1024%2C797&amp;ssl=1" alt="akai-mpc-3000-j-dilla-making-a-beat" srcset="https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=1024%2C797&amp;ssl=1 1024w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=300%2C233&amp;ssl=1 300w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=768%2C597&amp;ssl=1 768w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=1536%2C1195&amp;ssl=1 1536w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=2048%2C1593&amp;ssl=1 2048w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=360%2C280&amp;ssl=1 360w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=545%2C424&amp;ssl=1 545w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-making-a-beat.jpg?resize=1600%2C1245&amp;ssl=1 1600w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>J Dilla making music on the AKAI MPC 3000 LE.</figcaption></figure>



<p>Out of Dilla’s music also spawned the R&amp;B sub-genre Neo-soul. Also connected to the MPC 3000 through J Dilla’s work.</p>



<p>It’s safe to say that nobody else has displayed the MPC 3000’s groove capablities better than J Dilla. He was turning off the quantize setting during a time when literally everybody was relying on quantization for electronic music creation. He approached the MPC 3000 like a drummer would, and in turn he is regarded as one of the best drum programmers of all time.</p>



<figure><img loading="lazy" width="1024" height="832" src="https://i1.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian.jpg?resize=1024%2C832&amp;ssl=1" alt="akai-mpc-3000-j-dilla-smithsonian" srcset="https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=1024%2C832&amp;ssl=1 1024w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=300%2C244&amp;ssl=1 300w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=768%2C624&amp;ssl=1 768w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=1536%2C1247&amp;ssl=1 1536w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=2048%2C1663&amp;ssl=1 2048w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=360%2C292&amp;ssl=1 360w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=545%2C443&amp;ssl=1 545w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?resize=1600%2C1299&amp;ssl=1 1600w, https://i2.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-j-dilla-smithsonian-scaled.jpg?w=2320&amp;ssl=1 2320w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>J Dilla’s MPC 3000 LE displayed in the Smithsonian National Museum of African American History &amp; Culture.</figcaption></figure>



<h3>Dr. Dre</h3>



<p>After the release of the landscape-changing album <em>The Chronic 2001</em> in 1999 by Dr. Dre, more of the public started to take strong notice of the machine. Dr. Dre stated in an interview with Scratch Magazine that he used four or five different MPC 3000’s at one time for the creation of the album to avoid the drawbacks of using disks and memory. He shared that during this time his beats were layed down with the drums in the 3000 first, and everything else revolving around that.</p>



<figure><img loading="lazy" width="1024" height="614" src="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?resize=1024%2C614&amp;ssl=1" alt="akai-mpc-3000-dr-dre" srcset="https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?resize=1024%2C614&amp;ssl=1 1024w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?resize=300%2C180&amp;ssl=1 300w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?resize=768%2C461&amp;ssl=1 768w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?resize=360%2C216&amp;ssl=1 360w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?resize=545%2C327&amp;ssl=1 545w, https://i0.wp.com/audiojive.com/wp-content/uploads/2020/08/akai-mpc-3000-dr-dre.jpg?w=1200&amp;ssl=1 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"></figure>



<h2>Interesting Facts About the MPC 3000</h2>



<ul><li>There are no visual waveforms. You chop on the MPC 3000 with numbers, editing the start and end time of samples with numerical digits. Others would argue that you can chop by individually sampling each chop.</li><li>There is no EQ or FX in the MPC 3000. Only a lowpass filter and attack/decay, and delay.</li><li>You can only cutoff 2 other pads per pad. Compare this with modern drum machines where you can apply choke groups, etc. and cutoff any pad.</li></ul>



<h2>Why The MPC 3000 Is The Best Drum Machine of All Time</h2>



<p>To put it plainly, the simplicity.</p>



<p>Even though technically an MPC is a computer, the ushering in of personal computers as we know them today, and the fast innovation simply changed how music was being made. Workflow started to shift from unmanageably short sample times to hoards of collections of samples on hard drives. Modern workflow now allows freely auditioning any sized samples and endless decisions to be made with endless tools at your fingertips, effectively debilitating creativity very easily.</p>



<p>If Roger Linn had not been the first to incorporate sampling into drum machines, and then successfuly iterate on it leading into the release of the 3000, its safe to say that Hip-Hop and House music would not have taken it under their wing and catapulted the machine into the history books in the way that its heralded as a classic machine and musical instrument.</p>



<p>When you take a step back, its easy to realize that the 3000 was really where things peaked for drum machines. The MPC 3000 knocked it out of the park where it mattered most…. The sound, the timing, and the workflow. Drum machines that came after the MPC 3000 started to offer new abilities and functions, but traded them for things like the beefy analog sound and streamlined workflow.</p>



<p><strong>The Timing</strong></p>



<p>When you talk to fans of the MPC 3000, you hear them talk about “The Feel.” They will tell you that the feel of the 3000 is like no other drum machine or sampler that exists. This is due to the timing that Roger Linn built into the machine. The timing on the MPC 3000 is extremely tight and about as true as you can get. It will record exactly what you play. Essentially, it is the essense of what a “groovebox” is.<br>The sequencer on the MPC 3000 is second to none.</p>



<p><strong>The Sound</strong></p>



<p>Theres something about the sound of the MPC 3000. It’s the sampling engine, but it also seems to be more than that. For drums, while subtle… enthusiasts will tell you its unmatched. While the sampling engine will color sounds in its own way, the 3000 also seems to have a unique glue which provides subtle punch. Sounds will jump off of the track and manage to coexist beautifully on the 3000.</p>



<p><em>The low end on the MPC 3000 is amazing.</em></p>



<p>The MPC 3000 is very bottom heavy and thick in sound. Roger Linn has talked about how they tried to make it sound like a tape machine. And it clearly does. You can drive this machine extremely hot and loud without ever losing any bottom end.</p>



<p>You can’t really get a computer or any DAW to sound like an MPC 3000. Some claim its a matter of boosting the low end, and cutting some of the high end frequencies to match the sound of the 3000. This is simply false. There are much more nuances happening that have yet to be achieved on a computer… mostly the digitial to analog conversion. The DAC on the MPC 3000 is extremely unique. It’s a machine and it sounds like a machine, in a good way. In the best way possible.</p>



<p><strong>The Workflow</strong></p>



<p>The workflow is unmatched. It’s minimal, but it also gives you the proper buttons for only the functions and screens that you need. </p>



<p>The addition of a number pad is brilliant and is extremely useful allowing …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://audiojive.com/akai-mpc-3000/">https://audiojive.com/akai-mpc-3000/</a></em></p>]]>
            </description>
            <link>https://audiojive.com/akai-mpc-3000/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24177626</guid>
            <pubDate>Sun, 16 Aug 2020 13:33:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Do I Need Kubernetes?]]>
            </title>
            <description>
<![CDATA[
Score 200 | Comments 168 (<a href="https://news.ycombinator.com/item?id=24177469">thread link</a>) | @mocko
<br/>
August 16, 2020 | https://mbird.biz/writing/do-i-need-kubernetes.html | <a href="https://web.archive.org/web/*/https://mbird.biz/writing/do-i-need-kubernetes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p>A question I often hear from teams - both new and established - is “should we host our stack on <a href="https://kubernetes.io/">Kubernetes</a>?”.  Given all the buzz it gets in the tech world a lot of people assume&nbsp;so.</p>
<p>I’ve been working with k8s for several years - often with very powerful and complex platforms - and I think the truth is more&nbsp;nuanced.</p>
<p>Here’s my attempt at untangling that decision.  It’s geared toward startups and self-sufficient teams in wider organisations with responsibility for hosting their own products.  It might also have value to people in more traditional <span>IT</span> departments at larger&nbsp;organisations.</p>
<h3>How Can it Help&nbsp;Me?</h3>
<p>Kubernetes isn’t just a 2018-era buzzword.  It’s a robust, highly scalable system that allows you to construct application deployments out of well-considered primitives (<code>Pod</code>, <code>Service</code>, <code>Ingress</code> etc) then does its damnedest to make the reality match.  When applications crash it restarts them.  When whole swathes of underlying machines disappear it tries to replace them.  If you’re running numerous services (probably developed as a microservice architecture) and you’re looking for efficiency, resilience and a good story for deployment it can do a lot for&nbsp;you.</p>
<p>Want a basic degree of resilience in your service? (yes you do) - run <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">multiple replicas</a> in your deployment then balance traffic between&nbsp;them.</p>
<p>If your workload is ‘bursty’ (e.g. a lot of <span>API</span> traffic) you could build on this by rigging up <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">autoscaling</a> to grow capacity when it’s needed.  This can save you a lot of money - instead of paying for peak capacity all of the time, provision a baseload amount to keep the platform ticking over and bring up more replicas when they’re needed.  And if you can <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/custom-and-external-metrics">import the queue length</a> into your system autoscaling works for queue-based workloads&nbsp;too.</p>
<p>Worried about your code breaking?  Configure liveness/readiness <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">probes</a> and Kubernetes will automatically restart your services when they fail.  Likewise for hardware failure - I’ve seen clusters who’ve lost half their nodes keep on trucking like nothing happened.  A well-configured and maintained Kubernetes cluster can be incredibly robust.  If you have the resources you might even try running a <a href="https://netflix.github.io/chaosmonkey/">chaos-monkey</a>-like <a href="https://www.gremlin.com/chaos-monkey/chaos-monkey-alternatives/kubernetes/">tool</a> to make certain your stack will tolerate regular&nbsp;failures.</p>
<p>Kubernetes has excellent features for integrating with your <span>CI</span> workflow.  The most common pattern I see for this is a build (ideally standardised across all your projects) that pushes an image to a Docker registry then kicks your k8s cluster to load it.  Depending on taste that can be done by modifying the deployment to pull a new tag, or pointing the tag to the new image and triggering k8s to reload all pods.  In most cases (migrations ruin everything) deployment can be fully automated: if you trust your tests and feature flags it can be 100% automatic (aka <a href="https://www.continuousdelivery.com/">Continuous Delivery</a>); for the less brave a manual approval step after the build eases the pain.  Either way your developers should be able to release most builds to the cluster without any help from Kubernetes&nbsp;specialists.</p>
<p>In a similar vein to <span>CI</span> it can help you standardise logging and monitoring for your applications.  Doing this is by no means unique to Kubernetes, but having a single cluster-wide system that gathers together data about all your services in the same place can greatly ease the pain of debugging.  I’ve seen particularly good results come from using <a href="https://github.com/fluent/fluentd-kubernetes-daemonset">fluentd</a> to pipe json-structured logging output from applications into <a href="https://aws.amazon.com/cloudwatch/"><span>AWS</span> CloudWatch</a> and query it through <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html">Insights</a>.</p>
<p>Finally it can do a great deal to increase efficiency - both at the hosting layer (i.e. jamming as many applications onto those expensive <span>EC2</span> instances as possible) and also reducing the time your developers need to spend deploying software.  Humans cost more than computers so for most organisations the second of these will be the biggest win.  But Kubernetes isn’t magic - I’ve seen some beautiful, efficient clusters and some total gas guzzlers.  <em>You’ll only save money with Kubernetes if you invest in using it&nbsp;correctly.</em></p>
<h3>Where Does Kubernetes&nbsp;Shine?</h3>
<p>First of all think about your workload.  What kind of applications do you need to run?  How do they talk to each other and the outside world?  From experience I’d say the following attributes make your stack a good&nbsp;fit:</p>
<ul>
<li>Broadly speaking have you followed a microservice architecture?  There’s little value in Kubernetes if your world only has one application.  You’ll need to <a href="https://docs.docker.com/engine/examples/">Dockerize</a> your applications to deploy them to k8s; doing this from day 1 of any project is a good way to make yourself think about the boundaries between&nbsp;services.</li>
<li>Are your services exposed to each other and the outside world via <span>HTTP</span>(s) (probably yes, it’s 2020)?  This will fit k8s’ model nicely and you can use one of the normal <a href="https://github.com/kubernetes/ingress-nginx">ingress controllers</a> to front&nbsp;them.</li>
<li>Are your applications suitable to be load balanced?  No local state (use PostgreSQL / Redis / whatever for that), communication over known endpoints, fast startup/shutdown.  That isn’t to say you can’t keep shortlived state like a Redis cache in your cluster but for many cases you’d do better to use the boxed services a cloud provider&nbsp;offers.</li>
<li>Kubernetes is also well suited to headless applications like batch processing (through its <code>Job</code> <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">controller</a>) and long-lived&nbsp;queue-consumers.</li>
<li>Is memory (and to a lesser extent <span>CPU</span>) usage predictable?  Kubernetes will try to host your applications on the same physical machines, so if one of them goes haywire and consumes all the <span>RAM</span> other workloads may be randomly killed.  In my experience this is the single biggest source of instability on Kubernetes clusters.  If you understand the resource usage of your applications you can declare a <code>resources.requests</code> and <code>resources.limits</code> to guarantee they’ll always get the memory they need and won’t be bad&nbsp;neighbours.</li>
</ul>
<p>Conversely here are a few kinds of workload I <em>don’t</em> think you should use it&nbsp;for:</p>
<ul>
<li>Static websites.  Typically you’d do this by baking your content into an <a href="https://hub.docker.com/_/nginx/">Nginx</a>-derived image and exposing it via your cluster’s ingress controller.  That’s a terrible way to host static content: all those little copies of Nginx need to be maintained, it’s inefficient and far less resilient / performant than it could be at the network level.  Sure you could stick it behind a <span>CDN</span> but if you’re doing that, why not have a cloud service host the content as&nbsp;well?</li>
<li>Hosting untrusted code.  That could mean applications supplied by your customers or third party code with a history of security problems, e.g. Wordpress or a dodgy library from <span>NPM</span>.  By default Kubernetes’ features for isolating workloads aren’t great.  You can add things like <a href="https://www.projectcalico.org/">Calico</a> to control network access but it’s easy to screw up and your security model is always going to be 100% dependent on the container runtime.  The default for this (Docker, based on <a href="https://en.wikipedia.org/wiki/Cgroups">Linux cgroups</a>) offers a huge attack surface to compromised applications: if a codebase your cluster is running gets hacked an attacker will not find it hard to escalate their access to the rest of the cluster.  Interesting work is being doing on alternatives to cgroups (e.g. <a href="https://katacontainers.io/">Kata Containers</a>) but it’s not yet mainstream enough to recommend to the average&nbsp;user.</li>
</ul>
<p>If your workload isn’t a good fit you may still have options for shoehorning it into Kubernetes (e.g. <a href="https://kubernetes.io/docs/concepts/storage/volumes/">volumes</a> can be used for long-lived state), but expect to spend a lot of engineering time working around the problems.  A lot of them are bad practices anyhow so your resources time might be better spent designing them out of the stack altogether - they’re only going to hurt you further down the&nbsp;line.</p>
<h3>No Magic&nbsp;Bullet</h3>
<p>Kubernetes isn’t a magic bullet.  It can help to move the complexities of hosting applications into a well-designed layer of their own but it won’t make them go away.  You’re always going to have to secure and maintain your&nbsp;platform.</p>
<p>To make a cluster useful for the average workload a menagerie of add-ons will be required.  Some of them almost everyone uses, others are somewhat niche.  Examples include (the Nginx <a href="https://github.com/kubernetes/ingress-nginx">ingress controller</a>, <a href="https://github.com/jetstack/cert-manager">cert-manager</a> and <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster-autoscaler</a> to add extra nodes when you don’t have enough capacity.  Having a bespoke set of software operating key features of the environment makes your cluster a unique snowflake and it needs to be managed as such.  Also they need to be updated regularly and can sometimes be of questionable quality.  Configuration management tools like <a href="https://helm.sh/">Helm</a> or <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">Terraform</a> are nigh-on mandatory: tinker with a cluster by hand at your peril, absent a declarative setup you’ll never be able to spin up another in exactly the same way.  I’ve seen this lead to no end of problems when operating or replacing more mature&nbsp;clusters.</p>
<p>When running any nontrivial stack on Kubernetes a degree of curation is always going to be required.  Letting anybody deploy whatever they like, however they like to your cluster can only lead to chaos.  You’ll end up with mess of inconsistently named applications scattered across dozens (or worse, one) namespaces with nobody knowing how they fit together.  Congratulations: you took your old spaghetti infrastructure and replaced it with a new flavour of spaghetti infrastructure, just served on a cooler kind of&nbsp;plate.</p>
<p>The most successful Kubernetes implementations I’ve seen are those where infrastructure specialist(s) work with developers to make sure workloads are well-configured, standardised, protected from one another and have defined patterns of communication.  They’ll handle the initial setup of an application on an infrastructure, hopefully wired up to a build/release system so dev teams can ship new versions of the code without help.  In a lot of ways this mirrors the wider culture of your organisation - if engineering is a chaotic free-for-all of bad communication and ill-defined responsibilities your hosting environment will mirror it.  At best this leads to unreliability; at worst an unreliable, unmaintainable, expensive&nbsp;mess.</p>
<h3>Cost of&nbsp;Ownership</h3>
<p>If you’re running applications on a large scale …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mbird.biz/writing/do-i-need-kubernetes.html">https://mbird.biz/writing/do-i-need-kubernetes.html</a></em></p>]]>
            </description>
            <link>https://mbird.biz/writing/do-i-need-kubernetes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24177469</guid>
            <pubDate>Sun, 16 Aug 2020 13:10:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Market for Discontinued Photographic Film]]>
            </title>
            <description>
<![CDATA[
Score 75 | Comments 57 (<a href="https://news.ycombinator.com/item?id=24177142">thread link</a>) | @leejo
<br/>
August 16, 2020 | https://leejo.github.io/2020/08/14/the_market_for_discontinued_film/ | <a href="https://web.archive.org/web/*/https://leejo.github.io/2020/08/14/the_market_for_discontinued_film/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<p>
				<center>The Market For Discontinued Photographic Film</center>
			</p>
			<div>
				<center>
				
				
				

				August 14, 2020 (
				
				
					<a href="https://leejo.github.io/2020/04/28/off_season_ibiza/">Prev</a>
				
				/
				
					Next
				
			)

			
				</center>
			</div>
			<p>My fridge was recently full of instant film. I mean that literally, it was not possible for me to put any more packets of the film into the fridge without risking damaging them:</p>

<p><img width="625px" src="https://leejo.github.io/images/2020/fuji/fridge.jpg"></p>

<p>I had acquired all of the film one day when my printer sent me an email asking if I wanted to purchase 93 packs of Fujifilm FP-100c for 2,790.- CHF. That’s 30.- CHF per box, about three times what this film used to retail for and about one third of what it currently sells for<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>. So I thought about the offer for five minutes, then said yes.</p>

<p>I figured I could sell half of the packs and recoup my costs, after various expenses, then I would have around 45 boxes that I could shoot a photo project with. So I picked the film up and thanked my printer. He even included the above fridge, three Polaroid cameras, and a bunch of accessories for free. So whichever way I spin this, even if a few packs went missing in the post, I would still be close to doubling my investment <em>at the least</em> if I decided to sell all the film. Not bad.</p>

<p>The film had been discontinued, at least announced to be, in early 2016. The film was still available on a wholesale level up until sometime in 2018 so people had started stockpiling it and the prices inevitably went up. The problem with photographic film is that it will start go off in a few years as the chemicals break down, so it needs to be cold stored. This is especially the case with instant film as there are chemicals not just to record the image but also the entire development process is included in each frame.</p>

<p><strong>Instant Film</strong></p>

<p>There’s a market for this stuff as there is a range of old cameras and backs for other cameras that can only function with it. The film comes in dedicated plastic packs that the cameras/backs are designed to work with. You load a pack, fiddle about a bit, and you are ready to shoot. Like other instant films you take the shot, pull out the frame, give it a minute or two, then “peel apart” the frame to reveal the image.</p>

<p>The film can still be bought “new” on auction sites, some analog specific photography stores, and private sellers; but all of the film is now expired as the last batch was marked to do so in early 2019, so there’s a risk involved in any purchase of this film if you want it “fresh”. To purchase a single box of the film from one of the last batches you should expect to pay around 100.- CHF. Ten times what it originally retailed for.</p>

<p>Given there are ten frames in each box, and due to the fickle nature of the cameras that shoot this type of film you can often lose one or even two frames due to mechanical or exposure issues, you’re paying in the region of 12.- CHF <em>per photo</em>.</p>

<p>This is not a case of having to learn how to load and use the film - sometimes you just have bad luck. The paper mechanism used to pull out the shot frame and prepare the next sometimes tears. Just last week I was using some and the first frame leader from the box tore, I had to open the camera to retrieve the next leader and lost another three frames in the process. Somehow the third frame managed to hide amongst the rest and turned up again between the 6th and the 7th frame, something I’ve not experienced before.</p>

<p>This is not fun, it’s quite ridiculous, and if I’m being honest I don’t see the point of this stuff and find it all very annoying. I say that as someone who is steeped in the film process. Sure I understand the aesthetic and the tangible nature of the process, but that’s not for me. I mean, if you really want to shoot it that’s fine, but at this price point I would now prefer to <a href="https://leejo.github.io/projects/bergbahnen/polaroids/">emulate it</a>, at least when the reason to do so is to have some sort of contextual point for the aesthetic. Each to their own of course.</p>

<p><img width="625px" src="https://leejo.github.io/images/2020/fuji/freezer.jpg"></p>

<p>Anyway, Fuji have got form with this. I’ve been using some of their film stocks for close to two decades so have seen product lines come and go, and my other fridge/freezer (above) contains some of those products that I do continue to use. Of course you can still purchase some of the discontinued film if you really want to. A few of those I use or used to are detailed below, and this is in no way an <a href="https://en.wikipedia.org/wiki/List_of_discontinued_photographic_films#Fujifilm">exhaustive list</a>:</p>

<p><strong>Neopan 1600</strong> - A mid to high speed black and white film that had a grain structure that I haven’t seen in other films. I used this quite a bit when shooting skate photos but it was never available in formats larger than 35mm so I stopped when I moved exclusively to medium format. You see this now and then for sale, but it won’t be good to shoot as it’s <em>at least</em> a decade expired and mid-high speed film doesn’t last more than a few years after expiry unless it’s frozen; and of course you can’t guarantee the storage history.</p>

<p>I have seen this selling for 30 to 50.- CHF for a <em>single</em> roll of 35mm, or 250.- CHF for ten packs. That’s five to ten times its original retail price.</p>

<p><strong>Fuji Fortia</strong> - a once/twice off colour slide film that was released around 2005. It was like Fuji’s Velvia 50 (detailed below) but with even more saturated reds. There were rumours that it was a batch of Velvia that didn’t pass quality control, so Fuji sold it as a special edition for the cherry blossom season in Japan. It was available for a couple of years so probably just a single run, which does backup the “it was a messed up Velvia batch” rumor.</p>

<p>You see rolls of this for sale now and then, with the price approaching ten times what it originally retailed for. This was available in medium format so I did shoot it and you can see what I mean about the saturation from the image below, which I actually had to <em>desaturate</em> a little in post as it was just too much.</p>

<p><img width="625px" src="https://leejo.github.io/images/2020/fuji/2008_poppies_cambs.jpg"></p>

<p><strong>Acros 100</strong> - Another Fuji speciality, a medium to slow speed black and white film that offered four selling points: extremely fine grain for its box speed (100 ISO), very contrasty in normal development, very sharp, and very good reciprocity failure rates. You can shoot exposures up to several minutes long with little adjustment required, which may well be unique amongst all film stocks.</p>

<p>This was discontinued a couple of years ago but then came back as Acros II, however it is no longer Fuji manufacturing it and I believe it is a co-operation with Ilford<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>. This is probably a good thing, however the 4x5 version of the film is not being made and luckily I stocked up on this a few months before its original discontinuation was announced so I have enough to last me for a few years at least. I am using this film for a specific project, which is why I bought a few boxes; my current count is 12 remaining, that’s 240 frames.</p>

<p>I rarely see this for sale in its 4x5 version, and those I do see tend to not be from the final batches, which had expiry dates of 2019-10. This was selling for about 70.- CHF a box when it was available and I suspect those final batch boxes would go for at least twice that now.</p>

<p><strong>Velvia 50</strong> - The classic landscape (and Formula 1?<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup>) photographer’s slide film. Very popular up until around, I dunno, 2005? It’s still going, but i suspect will be discontinued soon. It’s no longer available outside Japan in large format sizes, and Fuji have cut down on the number of options for it in smaller sizes. Also of note is that when I was stockpiling it the boxes had very short expiry dates, less than a year, suggesting that sales have slumped. Since then the price in Japan has gone up 50% so it’s no longer a compelling option.</p>

<p>I still shoot this, but it’s very fickle. It needs graduated neutral density filters in many cases (which I don’t have) and suffers from terrible reciprocity failure and very easy over/under exposure even if you know what you’re doing. You often have to bracket your exposures +/- a stop, which is not cheap when shooting large format film. When you get it right you understand why it was number one it its niche, again another shot that I had to pull back the saturation:</p>

<p><img width="625px" src="https://leejo.github.io/images/2020/fuji/lac_leman_2019.jpg"></p>

<p><strong>What Does This All Mean</strong></p>

<p>Discontinued film stocks are selling for twice to ten times their original retail price, so should you invest in film? Nah, not at all. Unless you discover a batch of Fuji FP-100c that’s been cold stored, is from 2018 or 2019, and selling at less than 40.- CHF a box, in which case go for it and double your investment. Clearly there is a market for discontinued photographic film emulsions, but they’re a niche within a niche and we’re now down to the products that don’t offer a unique selling point.</p>

<p>The discontinuation of the above stocks doesn’t mean much, really - you can just switch to another stock and adjust as necessary. Yeah it’s frustrating at first, but that’s the nature of film. The issue with Fuji FP-100c being discontinued is that it also renders an entire range (and more) of cameras unusable<sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup>. There are no other options, or at least limited options, to continue using the cameras<sup id="fnref:5" role="doc-noteref"><a href="#fn:5">5</a></sup>.</p>

<p>That’s what’s kind of sad about this entire thing, although I guess that is the end state of all technology designed around using refills to function. Printers, things with fixed internal batteries, liquid fuel vehicles, SodaStreams? There’s more of these than you think. We will still have 35mm film for a long time I suspect, and 120 and 4x5 and 8x10 and so on; but some of the more niche products are approaching obsolescence and thus unavailability. We can retrofit some of the technology that require these refills, but eventually the expense isn’t worth it.</p>

<p>My reasons for shooting film are because it gives me access to alternative formats and some technical controls that I can’t get elsewhere, at least not at an affordable price point<sup id="fnref:6" role="doc-noteref"><a href="#fn:6">6</a></sup>. Movements, a square or panoramic frame, an alternative way to interact with my subjects. An interesting observation about instant film is the reaction to it, compared to shooting a digital image that is effectively the same. The tangible nature of the process really is a hook.</p>

<p>To refer to a blog post I have quoted on more …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://leejo.github.io/2020/08/14/the_market_for_discontinued_film/">https://leejo.github.io/2020/08/14/the_market_for_discontinued_film/</a></em></p>]]>
            </description>
            <link>https://leejo.github.io/2020/08/14/the_market_for_discontinued_film/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24177142</guid>
            <pubDate>Sun, 16 Aug 2020 12:08:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Web by Google (TM)]]>
            </title>
            <description>
<![CDATA[
Score 694 | Comments 473 (<a href="https://news.ycombinator.com/item?id=24176898">thread link</a>) | @alangibson
<br/>
August 16, 2020 | https://landshark.io/2020/08/16/web-by-google.html | <a href="https://web.archive.org/web/*/https://landshark.io/2020/08/16/web-by-google.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<p>Looking at Mozilla’s finances, it’s reasonable to conclude that Google is keeping them on life support to keep the anti-trust hounds at bay. Mozilla’s deal with Google will account for at least 70% of their revenues going forward. That’s over $400 million to be the default search provider in 4% of browsers. For 1 year. I’ll bet my next paycheck that Google reupped this rather generous agreement to avoid a repeat of Microsoft’s troubles over an Internet Explorer monopoly. My guess is that Google will decide it’s in their interest to pull the plug right around the time this latest wave of anti-trust talk blows over.</p>

<p>With the troubles at Mozilla, Google is one step closer to replicating the WeChat and Facebook walled-garden model on the Web. A quick survey a the field of play shows just how far Google has come in capturing the once open Web.</p>

<h2 id="browser-war-is-over">(Browser) War is over</h2>

<p>Chrome’s ability to dictate web standards will only get stronger over time. Safari and Firefox have been able to apply some shame to Chrome on things like disabling third-party cookies, but soon it’ll just be Apple left with a voice.</p>

<p>But Google doesn’t even need Chrome to dictate standards since it controls the Web’s front door. AMP, a technology no one asked for, is now on over 70% of all marketing websites for no other reason than Google said so.</p>

<p>And let’s not forget that most people now access the web via mobile devices, 75% of which run Google’s Android.</p>

<h2 id="original-sins">Original Sins</h2>

<p>Two deficiencies have determined the course the Web has taken: lack of native search and lack of native payments.</p>

<p>Lacking native distributed search allowed Google to grab a monopoly position as the entry point to the web. The Web’s orignal architects were off base on hyperlinks; it turns out people just want to skip right to the answer they’re looking for. There was once the beginnings of an alternative in public bookmarking sites, but social networks came and sucked all the air out of that space.</p>

<p>Lack of a safe and easy way of exchanging funds was a big factor in advertising becoming the dominant business model on the Web. No-charge services became the norm because, for a long time, there was no reasonable way to pay for them. Enter advertising to fill the void. I think it’s safe to say that Google’s AdWords is the dominant advertising platform on the open Web, which means it holds a commanding position in the Web’s finances.</p>

<p>I’m also going to throw in an honorable mention to poor native video support (it is hyper_text_ after all). It’s beating a dead horse to point out that Google also serves up the vast majority of videos streamed on the Web.</p>

<h2 id="so-then">So then…</h2>

<p>So Google controls the Web’s search and video infrastructure. It can and does dictate standards and media formats. It also controls a huge chunk of the revenues available when publishing and selling on the Web. It even controls the operating system and browser through which most people interact with it.</p>

<p>Google’s capture of the Web is a <em>fait accompli</em>. Only legislation will keep the World Wide Web from finally becoming Web by Google (TM).</p>


</div></div>]]>
            </description>
            <link>https://landshark.io/2020/08/16/web-by-google.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24176898</guid>
            <pubDate>Sun, 16 Aug 2020 11:23:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What I've learned about scaling OSPF in Datacenters]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24176872">thread link</a>) | @signa11
<br/>
August 16, 2020 | https://elegantnetwork.github.io/posts/What-Ive-learned-about-OSPF/ | <a href="https://web.archive.org/web/*/https://elegantnetwork.github.io/posts/What-Ive-learned-about-OSPF/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
	
	<h4>The wisdom is that BGP is the only Datacenter protocol, but is it? Do we know?</h4>
	<p>
		Published on Aug 14, 2020 by Justin Pietsch
	</p>
	<ul></ul>
</div>
</div><div>
    <div>
      <div>
          <p>I worked at Amazon for 17 years as a network engineer. Now I’m out of Amazon and looking at the industry, I’m learning new things. <strong>One of the things I’ve learned recently is that OSPF shouldn’t be used for Clos leaf-spine networks because of scale.</strong> I’ve heard that it can’t keep up with the size and scale of the modern datacenter networks. <a href="https://www.juniper.net/documentation/en_US/release-independent/solutions/topics/task/configuration/ip-fabric-underlay-cloud-dc-configuring.html">Juniper IP Fabric Underlay Network Design and Implementation</a> is an example of the recommendation.  <a href="https://tools.ietf.org/html/rfc7938">https://tools.ietf.org/html/rfc7938</a> is what as used as the reference for using eBGP to be the routing protocol inside of large Clos networks.</p>

<p><strong>This is a shock to me</strong> since we built large OSPF Clos networks a decade ago. We built networks this way with hundreds of routers (even thousands) speaking OSPF. I’m not saying it is easy or that we didn’t have to understand OSPF well; If you don’t design it well, you get a flooding disaster. But if you do set it up right, it works really well. It converges quickly and doesn’t collapse under failure. The key thing is to be sure you understand how flooding works and you focus on areas and summarization. There are different things you have to be aware of when you are designing a Clos network with BGP vs OSPF. With BGP you have to be figure out how to mitigate path hunting. With OSPF need to work on how to avoid congestion collapse.</p>

<p>There is also work on building new protocols for these large Clos or Clos-like networks, such as <a href="https://datatracker.ietf.org/wg/rift/documents/">https://datatracker.ietf.org/wg/rift/documents/</a>. I don’t understand why OSPF or ISIS isn’t good enough so I don’t understand why these are necessary. There <a href="https://pc.nanog.org/static/published/meetings/NANOG74/1763/20181003_Martin_Routing_In_Dense_v1.pdf">are some good ideas in RIFT</a>, I’m just haven’t seen a reason to use it.</p>

<p>When we did this we were using CPUs from 2003 and we were using a lesser known OSPF stack. It’s OSPF implementation was okay, but we were strongly warned away from using their BGP implementation by the developers. <strong>We had to understand and mitigate risk</strong>. We had to test it as well as we could and be very careful in our first deployments. The stack had never been battle tested like this before. We still ran into some scary performance issues but got to work with great software engineers on the protocol stack to work through problems and figure out our scale.</p>

<p>Others agree with me, <a href="https://www.youtube.com/watch?v=Qmvg2mnbcPg">https://www.youtube.com/watch?v=Qmvg2mnbcPg</a>, even though they are mostly focusing on smaller networks than I was working on. But either way, OSPF can be made to work just fine on very large (or small) Clos networks.</p>

<p>As an example of the scale, this is a generic 24 port 3-tier Clos. It is possible to make OSPF work on this. This is smaller than what we were working on in 2012.
<img src="https://elegantnetwork.github.io/assets/images/24port-3tier-clos-resize.png" alt="24 port 3-tier clos"></p>

<p>To understand and then mitigate the risks an unknown OSPF stack, at scale, is why I started building a configuration management system and started <a href="https://elegantnetwork.github.io/posts/Network-Validation-with-Vagrant/">simulating our network design</a>. We needed to understand how any OSPF protocol stack would work. Could we make the protocol scale at all? We needed to see what would actually happen with a real OSPF implementation. <strong>I cannot stress enough how important this simulation was.</strong> We did not understand the problem of flooding and we did not understand the solution until we had tried it out in simulation.</p>

<p>Before that, though, we spent a lot of time on white boards arguing about topology and OSPF. <strong>I think whiteboards are the most important tool for network design</strong> currently available, which makes me sad. I wish that wasn’t true, I want much better tools. I can’t even tell you the number of disasters averted by 2-3 great network engineers arguing over a whiteboard. This works really well, but there are important reasons it’s not good enough. One is that for many of these discussions, it’s extremely hard to get data to make the decisions. In this case, a comparison of convergence time and failure detection in a 3-tier Clos with eBGP, iBGP, and OSPF should be required, but we didn’t have that capability.</p>

<p>In a dense mesh-like network such as a Clos network, the concerns around OSPF swirled around four main issues: the effect of flooding, the size of the link state database, the speed of SPF calculations, and the ability for OSPF to carry a large number of prefixes. We were concerned about OSPF scaling w.r.t. its flooding and it’s the flooding that is expensive with a Link State Database protocol, not the Shortest Path First (SPF) calculations. Which means you have to be very careful about what is flooded, what are the areas, and how things are summarized. I didn’t understand this until I had a simulation and I could try things out with areas or without, and the effect was dramatic. I think it’s often true that there is a part of a design that you don’t understand, which is one of the big reasons to do simulation. We didn’t have the ability to lab up a network as big as we were going to be building, so the simulation was critical. I can’t even imagine what we would have done without it. Based on the simulation, we created targeted tests for real hardware to understand how it would deal with the scale.</p>

<p>The point of this article is not that OSPF is better than BGP, but that it does work in very large size Clos networks. I’m frustrated that so much of the industry went a direction without great data or analysis, just following one set of opinions and design choices. <strong>I wish there were easy ways for people to be able to design, simulate and test different designs</strong> like this so that they could decide for themselves the tradeoffs they’d like to make, rather than have to rely on some type of industry hype. There needs to be a better way to be able to describe topologies and designs so that it’s really easy to try out different ideas.</p>

<p>If you have other considerations, such as you want to run IPv6, or EVPN, a single instance of eBGP such as the one popularized by the open source routing suite, FRR, maybe the simplest or elegant. But with proper design, scale is not the reason to use BGP over OSPF. If you are running a small Clos network and don’t have IPv6 or EVPN, OSPF is more straightforward and generally faster, as was the common practice before the advent of BGP in the data center.</p>

<p>No matter what, you need to understand the choices that you are making and understand the tradeoffs. This is hard to do, and especially hard in networking because we lack design patterns and good tools to help us understand the implications of our choices.</p>

<p>If anybody knows about tools or design patterns or even somebody who’s actually compared OSPF vs BGP in Clos networks with data, I’d love to hear about it.</p>

      </div>

        <!-- Configure Disqus -->
        
        </div>
  </div></div>]]>
            </description>
            <link>https://elegantnetwork.github.io/posts/What-Ive-learned-about-OSPF/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24176872</guid>
            <pubDate>Sun, 16 Aug 2020 11:16:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are CRDTs suitable for shared editing?]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 39 (<a href="https://news.ycombinator.com/item?id=24176455">thread link</a>) | @signa11
<br/>
August 16, 2020 | https://blog.kevinjahns.de/are-crdts-suitable-for-shared-editing/ | <a href="https://web.archive.org/web/*/https://blog.kevinjahns.de/are-crdts-suitable-for-shared-editing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        
<div>
  <main>
      <article>
  
  <div>
      <p><a href="https://crdt.tech/">CRDTs</a> are often praised as the "holy grail" for building collaborative applications because they don't require a central authority to resolve sync conflicts. They open up new possibilities to scale the backend infrastructure and are also well-suited as a data-model for distributed apps that don't require a server at all.</p><p>However, several text editor developers report not to use them because they impose a too significant overhead.</p><p>Just recently, Marijn Haverbeke wrote about his considerations against using CRDTs as a data model for <a href="https://codemirror.net/6/">CodeMirror 6</a>:</p><blockquote>[..] the cost of such a representation is significant, and in the end, I judged the requirement for converging positions to be too obscure to justify that level of extra complexity and memory use. <a href="https://marijnhaverbeke.nl/blog/collaborative-editing-cm.html">(source)</a></blockquote><p>The <a href="https://github.com/xi-editor/xi-editor">Xi Editor</a> used a CRDT as its data model to allow different processes (syntax highlighter, type checker, ..) to concurrently access the editor state without blocking the process. They reverted to a synchronous model because ..</p><blockquote>[..] CRDT is not pulling its (considerable) weight. <a href="https://github.com/xi-editor/xi-editor/issues/1187#issuecomment-491473599">(source)</a></blockquote><p>Apparently, everyone recognizes that CRDTs have a lot of potential, but concluded that the memory overhead of using them must be too expensive for real-world applications.</p><p>They bring up a fair point. Most CRDTs assign a unique ID to <em>every character</em> that was ever created in the document. In order to ensure that documents can always converge, the CRDT model preserves this metadata even when characters are deleted.</p><p>This seems to be particularly expensive in dynamic languages like JavaScript. Other languages allow you to efficiently represent all those characters and IDs in-memory using <a href="https://en.wikipedia.org/wiki/Struct_(C_programming_language)">structs</a> (e.g. C or Rust). In JavaScript, everything is represented as an <code>Object</code> - basically a key-value map that needs to keep track of all its keys and values. CRDTs assign multiple properties to every single character in the document to ensure conflict resolution. The memory overhead of merely representing a document as a CRDT could be immense.</p><p>Every user-interaction creates more metadata that the CRDT needs to retain in order to ensure conflict resolution. It is not uncommon that CRDTs create millions of objects to store all this metadata. The JavaScript engine manages these objects on the heap, checks if they are referenced, and, if possible, garbage collects them. Another major problem is that with an increasing number of object creations, the cost of creating additional objects increases exponentially. This is shown in the following graphic:</p><figure><img src="https://blog.kevinjahns.de/content/images/2020/07/scatter-plot.png" alt="" srcset="https://blog.kevinjahns.de/content/images/2020/07/scatter-plot.png 600w"><figcaption>Average time to create one object increases with the number of objects on the heap <a href="https://jsperf.com/cost-of-objects">(Data Source)</a></figcaption></figure><p>So the question arises if CRDTs are actually suitable for shared editing on the web, or if they impose a too significant cost to be viable in practice.</p><p>In case you don't know me, I'm the author of a CRDT implementation named <a href="https://github.com/yjs/yjs">Yjs</a>, that is specifically designed for building shared editing applications on the web.</p><p>In this article, I will introduce you to a simple optimization for CRDTs and examine the exact performance trade-off of using Yjs for shared editing. I hope to convince you that the overhead is actually very small even for large documents with long edit histories.</p><h2 id="yjs">Yjs</h2><p>Yjs is a framework for building collaborative applications using CRDTs as the data model. It has a growing ecosystem of extensions that enable shared editing using different editors (<a href="https://docs.yjs.dev/ecosystem/editor-bindings/yjs-prosemirror"></a><a href="https://docs.yjs.dev/ecosystem/editor-bindings/prosemirror">ProseMirror</a>, <a href="https://www.npmjs.com/package/@remirror/extension-yjs"></a><a href="https://docs.yjs.dev/ecosystem/editor-bindings/remirror"></a><a href="https://docs.yjs.dev/ecosystem/editor-bindings/remirror">Remirror</a>, <a href="https://docs.yjs.dev/ecosystem/editor-bindings/yjs-quilljs"></a><a href="https://docs.yjs.dev/ecosystem/editor-bindings/quill">Quill</a>, <a href="https://docs.yjs.dev/ecosystem/editor-bindings/codemirror">CodeMirror</a>, ..), different networking technologies (<a href="https://docs.yjs.dev/ecosystem/connection-provider/y-websocket">WebSocket</a>, <a href="https://docs.yjs.dev/ecosystem/connection-provider/y-webrtc">WebRTC</a>, <a href="https://docs.yjs.dev/ecosystem/connection-provider/y-hyper">Hyper</a>, ..), and different persistence layers (<a href="https://docs.yjs.dev/ecosystem/database-provider/y-indexeddb">IndexedDB</a>, <a href="https://docs.yjs.dev/ecosystem/database-provider/y-leveldb">LevelDB</a>, <a href="https://docs.yjs.dev/ecosystem/database-provider/y-redis">Redis</a>, ..). Most shared editing solutions are tied to a specific editor and to a specific backend. With Yjs, you can make any of the supported editors collaborative and exchange document updates through your custom communication channel, over a peer-to-peer WebRTC network, or over a scalable server infrastructure. My vision for this project is that you can simply compose your collaborative application using the technologies that make sense for your project. </p><figure><a href="https://github.com/yjs/yjs"><div><p>yjs/yjs</p><p>Peer-to-peer shared types. Contribute to yjs/yjs development by creating an account on GitHub.</p><p><img src="https://github.githubassets.com/favicons/favicon.svg"><span>GitHub</span></p></div><p><img src="https://repository-images.githubusercontent.com/22392639/419c6c00-995d-11ea-87c6-dc7e997553d4"></p></a></figure><p>This is not just some cool prototypical side-project. Yjs is a battle-proven technology that is used by several companies to enable collaboration. I'm only mentioning my sponsors here:</p><ul><li><a href="https://nimbusweb.me/note.php">Nimbus Note</a> scales collaborative note editing horizontally using Yjs.</li><li><a href="https://room.sh/">Room.sh</a> is a meeting software that allows collaborative editing &amp; drawing over WebRTC.</li></ul><p>I maintain a reproducible set of benchmarks that compares different CRDT implementations. Yjs is by far the fastest web-based CRDT implementation with the most efficient encoding. In this article I will often refer to specific benchmarks contained in the <code>crdt-benchmarks</code> repository as, for example, "<a href="https://github.com/dmonad/crdt-benchmarks/#b1-no-conflicts">[B1.11]</a>".</p><figure><a href="https://github.com/dmonad/crdt-benchmarks"><div><p>dmonad/crdt-benchmarks</p><p>A collection of CRDT benchmarks. Contribute to dmonad/crdt-benchmarks development by creating an account on GitHub.</p><p><img src="https://github.githubassets.com/favicons/favicon.svg"><span>dmonad</span><span>GitHub</span></p></div><p><img src="https://repository-images.githubusercontent.com/185183228/89ad9780-b8e8-11ea-998a-91c689c4a511"></p></a></figure><h2 id="data-representation">Data Representation</h2><p>Chances are you're already familiar with the general concepts of how CRDTs work. If not, and you want to dive into this rabbit hole, I recommend this fun interactive series:</p><figure><a href="https://lars.hupel.info/topics/crdt/01-intro/"><div><p>CRDTs: Part 1</p><p><img src="https://lars.hupel.info/img/profile.jpg"><span>Lars Hupel</span><span>Lars Hupel</span></p></div><p><img src="https://lars.hupel.info/img/topics/crdt/world.jpg"></p></a><figcaption>A fun interactive series about CRDTs</figcaption></figure><figure><a href="https://crdt.tech/resources"><div><p>CRDT Resources • Conflict-free Replicated Data Types</p><p>Resources and community around CRDT technology — papers, blog posts, code and more.</p><p><img src="https://crdt.tech/assets/favicons/favicon-192x192.png"><span>Conflict-free Replicated Data Types</span></p></div><p><img src="https://crdt.tech/assets/img/crdt-favicon.png"></p></a><figcaption>Good entry point to find more CRDT related resources</figcaption></figure><p>The concept paper that describes Yjs' conflict resolution algorithm YATA is available on <a href="https://www.researchgate.net/publication/310212186_Near_Real-Time_Peer-to-Peer_Shared_Editing_on_Extensible_Data_Types">Researchgate</a>. The concepts discussed here are pretty generic and could be extended to almost any CRDT.</p><p>In order to determine the performance cost, we are are going to examine how Yjs maintains data. Please bear with me while I describe how the CRDT model is represented using JavaScript objects. This will be relevant later.</p><p>Similarly to other CRDTs, the YATA CRDT assigns a unique ID to every character. These characters are then maintained in a doubly linked list.</p><p>The unique IDs are<a href="https://en.wikipedia.org/wiki/Lamport_timestamp"> Lamport Timestamps</a>. They consist of a unique user identifier and a logical clock that increases with each character insertion.</p><p>When a user types the content "ABC" from left to right, it will perform the following operations: <code>insert(0, "A") • insert(1, "B") • insert(2, "C")</code>. The linked list of the YATA CRDT, which models the textual content, will look like this:</p><figure><img src="https://blog.kevinjahns.de/content/images/2020/07/crdt-datamodel.svg" alt=""><figcaption>CRDT model of inserting content "ABC" (assuming the user has the unique client identifier "1")</figcaption></figure><p>Notice how every character can be uniquely identified by the combination of the unique client-id and an ever-increasing <code>clock</code> counter.</p><p>Yjs represents the items of the linked list as <code>Item</code> objects that contain some content (in this case a String), a unique ID, the links to the adjacent <code>Item</code> objects, and additional metadata that is relevant for the CRDT algorithm.</p><p>All CRDTs assign some kind of unique ID and additional metadata to every character, which is very memory-consuming for large documents. We can't get rid of metadata as it is necessary for conflict resolution. Yjs also uniquely identifies each character and assigns metadata, but represents this information efficiently. Larger document insertions are represented as a single <code>Item</code> object using the character-offset to uniquely identify each character individually. The below <code>Item</code> uniquely identifies character <code>"A"</code> as <code>{ client: 1, clock: 0 }</code>, character <code>"B"</code> as <code>{ client: 1, clock: 1 }</code>, and so on..</p><figure><pre><code>Item {
    id: { client: 1, clock: 0 },
    content: 'ABC',
    ...
}</code></pre><figcaption>Yjs's internal representation of items in the linked list</figcaption></figure><p>If a user copy/pastes a lot of content into the document, the inserted content is represented by a single <code>Item</code>. Furthermore, single-character insertions that are written from left to right can be merged into a single <code>Item</code>. It is just important that we are able to split and merge items without losing any metadata.</p><p>This type of compound representation of the CRDT model and its split-functionality have first been described in <a href="https://kundoc.com/pdf-a-string-wise-crdt-algorithm-for-smart-and-large-scale-collaborative-editing-sys.html">"A string-wise CRDT algorithm for smart and large-scale collaborative systems"</a>. Yjs adapts this approach for YATA and also includes functionality to merge <code>Item</code> objects.</p><h2 id="the-cost-of-operations">The Cost of Operations</h2><p>With this simple optimization in mind, let's have a look at how the number of modifications on a document relates to the amount of metadata that needs to be retained. We will measure metadata by the amount of <code>Item</code> objects created and later examine what the cost of a single <code>Item</code> is.</p><p>Every user interaction with a text editor can be expressed as either an <strong>insert</strong> or a <strong>delete</strong> operation.</p><ul><li><code>insert(index: number, content: string)</code><strong> </strong>Insertions of any size create a single <code>Item</code> that is integrated into the document. In some cases the integration requires to split an existing <code>Item</code>. So we have a maximum of two <code>Item</code> creations per insertion.</li><li><code>delete(index: number, length: number)</code> Deleting an <code>Item</code> only marks it as deleted. I.e <code>item.deleted = true</code>. Therefore, <code>Item</code> deletions are free and will reduce the amount of memory used because the <code>Item.content</code> can be removed. The <code>Item</code> doesn't need to retain the content for doing conflict resolution. But deleting a range of content might require to split two existing items. Therefore, the cost of deletions is a maximum of two <code>Item</code> creations also.</li></ul><p>By using the compound representation of CRDTs, the amount of metadata only relates to the amount of operations produced by a user. Not to the amount of characters inserted. This makes a huge difference in practice. Most larger documents are created by copy-pasting existing content or by moving paragraphs to other positions. Any kind of operation, even copy-paste and undo/redo, only create at most two <code>Item</code> objects.</p><p>Yjs also supports rich-text and structured documents. The above statement, that the amount of metadata only relates to the amount of operations, still holds true for these kinds of documents. But measuring operation-cost for more complex operations is out of scope of this article. An interesting observation in practice is that the document size of long-running …</p></div></article></main></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.kevinjahns.de/are-crdts-suitable-for-shared-editing/">https://blog.kevinjahns.de/are-crdts-suitable-for-shared-editing/</a></em></p>]]>
            </description>
            <link>https://blog.kevinjahns.de/are-crdts-suitable-for-shared-editing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24176455</guid>
            <pubDate>Sun, 16 Aug 2020 09:36:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hello “Hello World”]]>
            </title>
            <description>
<![CDATA[
Score 58 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24175174">thread link</a>) | @hermanradtke
<br/>
August 15, 2020 | https://blog.jfo.click/hello-hello-world/ | <a href="https://web.archive.org/web/*/https://blog.jfo.click/hello-hello-world/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p>Languages are often judged initially on their "<a href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program">Hello,
world!</a>" program.
How easy is it to write? To run? How easy is it to understand? It's a very
simple program, of course, one of the simplest, even... just produce a little
text, and display it, what could be simpler?</p>
<p>It's really not fair to judge a language by such a cursory impression, but it
<em>can</em> give you an idea of what a language <em>values</em> and how it works. What does
the syntax look like? Is it typed? Is it interpreted? You can usually tell a
lot at a glance.</p>
<p>For example, One of Ruby's (many) hello worlds is so simple, it's also Python!</p>
<pre><code>print(<span>'Hello world!'</span>)
</code></pre>
<p>Often, people coming from interpreted languages experience compiled, systems
languages to be more complicated right off the bat. There is the obvious added
complexity of compiling and running being separate steps, as opposed to simply
pointing an executable at some source code and seeing a result right away, but
there are often syntactical constructs to go along with that...</p>
<p>At first glance, Rust's hello world looks fairly inert, as well:</p>
<pre><code><span><span>fn</span> <span>main</span></span>() {
    <span>println!</span>(<span>"Hello World!"</span>);
}
</code></pre>
<p>But <code>println!</code> is actually a macro, what does it look like expanded?</p>
<pre><code><span>macro_rules!</span> println {
    () =&gt; ($crate::<span>print!</span>(<span>"\n"</span>));
    ($($arg:tt)*) =&gt; ({
        $crate::io::_print($crate::format_args_nl!($($arg)*));
    })
}
</code></pre>
<p>You'll notice that this <em>also</em> has a macro <em>inside of it.</em> This matches on the
second case (because there is an argument present) and calls into
<code>$crate::format_args_nl!</code> and passes the result of that to <code>$crate::io::_print</code></p>
<pre><code><span>pub</span> <span><span>fn</span> <span>_print</span></span>(args: fmt::Arguments&lt;<span>'_</span>&gt;) {
    print_to(args, &amp;LOCAL_STDOUT, stdout, <span>"stdout"</span>);
}
</code></pre>
<p>which calls into <code>print_to</code>, which looks like</p>
<pre><code><span><span>fn</span> <span>print_to</span></span>&lt;T&gt;(
    args: fmt::Arguments&lt;<span>'_</span>&gt;,
    local_s: &amp;<span>'static</span> LocalKey&lt;RefCell&lt;<span>Option</span>&lt;<span>Box</span>&lt;<span>dyn</span> Write + <span>Send</span>&gt;&gt;&gt;&gt;,
    global_s: <span><span>fn</span></span>() -&gt; T,
    label: &amp;<span>str</span>,
) <span>where</span>
    T: Write,
{
    <span>let</span> result = local_s
        .try_with(|s| {
            
            
            
            <span>let</span> prev = s.borrow_mut().take();
            <span>if</span> <span>let</span> <span>Some</span>(<span>mut</span> w) = prev {
                <span>let</span> result = w.write_fmt(args);
                *s.borrow_mut() = <span>Some</span>(w);
                <span>return</span> result;
            }
            global_s().write_fmt(args)
        })
        .unwrap_or_else(|_| global_s().write_fmt(args));

    <span>if</span> <span>let</span> <span>Err</span>(e) = result {
        <span>panic!</span>(<span>"failed printing to {}: {}"</span>, label, e);
    }
}

</code></pre>
<p>Which is, uh, well let's just say it's not exactly <em>simple</em> looking now? There
is a lot going on here!</p>
<p>To be clear, I'm not faulting Rust here at all, my point is exactly the
opposite actually, in that there is <em>always</em> necessarily more going on in a
"Hello world!" than <code>puts "la de da"</code> or similar would have you believe on its
face. Speaking of Ruby's <code>puts</code>, what <em>is</em> the code that runs <code>puts</code> in the
Ruby interpreter itself, which is written in C?</p>
<p>Well it looks like <a href="https://github.com/ruby/ruby/blob/7c2bbd1c7d40a30583844d649045824161772e36/io.c#L7727-L7758">this</a></p>
<pre><code><span>VALUE
<span>rb_io_puts</span><span>(<span>int</span> argc, <span>const</span> VALUE *argv, VALUE out)</span>
</span>{
    <span>int</span> i, n;
    VALUE <span>line</span>, args[<span>2</span>];

    
    <span>if</span> (argc == <span>0</span>) {
        rb_io_write(out, rb_default_rs);
        <span>return</span> Qnil;
    }
    <span>for</span> (i=<span>0</span>; i&lt;argc; i++) {
        <span>if</span> (RB_TYPE_P(argv[i], T_STRING)) {
            <span>line</span> = argv[i];
            <span>goto</span> <span>string</span>;
        }
        <span>if</span> (rb_exec_recursive(io_puts_ary, argv[i], out)) {
            <span>continue</span>;
        }
        <span>line</span> = rb_obj_as_string(argv[i]);
      <span>string</span>:
        n = <span>0</span>;
        args[n++] = <span>line</span>;
        <span>if</span> (RSTRING_LEN(<span>line</span>) == <span>0</span> ||
            !rb_str_end_with_asciichar(<span>line</span>, <span>'\n'</span>)) {
            args[n++] = rb_default_rs;
        }
        rb_io_writev(out, n, args);
    }

    <span>return</span> Qnil;
}
</code></pre>
<p>Hello world!</p>
<p>We all know that a languages like Ruby or Python are designed explicitly to
hide this sort of complexity from us and let us get on with the dirty business
of munging data blobs or serving web requests or solving sudokus or whatever,
and thank goodness for that, but wow that is <em>quite</em> a lot, isn't it?</p>
<hr>
<p>When people come from languages that were designed to be ergonomic to more
systems oriented languages, they're often jarred by what they perceive to be
code thatis inelegant, ugly, and verbose. To be sure, it sometimes <em>is</em> exactly
that... (although anyone who has worked with a "pretty" language in a production
codebase knows that those are not immune to these descriptors either).</p>
<p>Usually, the tradeoff is explicit: elegance and simplicity for
<em>control</em>...  specific and granular <em>control</em>, over the program that will
eventually be run.  It isn't always necessary, in fact it is almost always _un_necessary,
to have <em>that</em> much control over your program. Obviously, productivity matters,
and if your business is <em><em>insert viable business</em></em>, well it's likely that your
goals are not going to be optimally met by futzing with manual memory
management all day (<a href="https://danluu.com/sounds-easy/">at least from the macro level, in the general
sense</a>).</p>
<p>But what if you <em>do</em> need that control? Well then, <em>you need it</em>. When every
ounce of performance actually is necessary, or on embedded systems with hard
memory constraints, or when writing code for some bespoke or otherwise uncommon
processor.</p>
<p>I'm going to choose one language, Zig, and dive deep into its hello world, but
it is important to note here that my point is not primarily about Zig, it's
about how <em>all</em> languages have to contend with an enormous amount of complexity
in order to do <em>anything</em>, even the simplest of tasks like a hello world
program. Complexity that is, for the most part, hidden from us in our day to
day. So what in the hello world is <em>actually</em> going on then?</p>
<blockquote>
<p>I'll be using the most current minor release version of Zig: 0.6.0.</p>
</blockquote>
<h2>Let's take a walk</h2>
<p><a href="https://ziglang.org/">Zig</a>'s hello world looks like this, from the docs.:</p>
<pre><code><span>const</span> std = <span>@import</span>(<span>"std"</span>);

<span>pub</span> <span><span>fn</span> <span>main</span></span>() !<span>void</span> {
    <span>const</span> stdout = std.io.getStdOut().outStream();
    <span>try</span> stdout.print(<span>"Hello, {}!\n"</span>, .{<span>"world"</span>});
}
</code></pre>
<p>If you are new to zig, a quick word on this syntax before I get into the gritty
details.</p>
<pre><code><span>const</span> std = <span>@import</span>(<span>"std"</span>);
</code></pre>
<p><code>@import</code> is a <a href="https://ziglang.org/documentation/0.6.0/#Builtin-Functions"><em>compiler
builtin</em></a> function
that assigns the namespace of the file it is referencing to the <code>const</code>
variable on the left hand side.</p>
<pre><code><span>pub</span> <span><span>fn</span> <span>main</span></span>() !<span>void</span> {
  
}
</code></pre>
<p>Just like in C, <code>main</code> is a special function that marks the entry point to a
program after it has been compiled as an executable. Unlike in C, it accepts no
arguments (C's main function has a variety of vagaries that make it a bit
<a href="https://stackoverflow.com/a/4207223">unique</a>) and command line input is
available through utility functions to allow easier cross platform use.</p>
<p>It is marked <a href="https://ziglang.org/documentation/0.6.0/#Keyword-pub"><code>pub</code></a> so
that it is accessible from outside of the immediate module ('module' here
referring to nothing more than the top level scope of the current namespace...
i.e., the file), this is a necessary step since, as the program's entry point,
<code>main</code> would <em>have</em> to be accessible from outside the immediate scope.</p>
<p><code>fn</code> is the function keyword.</p>
<p><code>main()</code> is the name of the function (and where the argument list <em>would</em> be)
and <code>!void</code> is the return type. Looking a little closer at that return type:</p>
<p>In C, the return type of a function is declared <em>before</em> anything else. This
makes a certain amount of sense: it's congruent with how variables are
declared, after all, and scanning the file you can see clearly "calling this
will get you that."</p>
<p>In Zig, the return type comes after the function declaration but before the
function body. This also makes sense! It's the same in Rust and Go, and seems
to be generally a more modern approach. The reason is actually pretty simple:
doing it this way makes it possible to have a context-free grammar! C and C++
put the parser in a position where it <a href="https://stackoverflow.com/questions/14589346/is-c-context-free-or-context-sensitive">has to understand semantics to even just
<em>parse</em> the source
code.</a></p>
<p>In Zig, <code>main</code> returns <code>void</code> (well, actually, it can return a variety of
things, and if it returns void (which is just a way of saying it doesn't return
anything at all)), it's actually returning
<a href="https://github.com/jfo/zig/blob/7381aaf70e0cad92fc52b79f3aa2a0abb7c3ee04/lib/std/start.zig#L241-L244"><code>0</code></a>
as a success code, but) there is a wrinkle!  <code>void</code> is preceded by an
exclamation mark. This means: "This function is supposed to return <code>void</code>, but
it <em>could</em> fail and return an error." This is an <a href="https://ziglang.org/documentation/0.6.0/#Inferred-Error-Sets">inferred error
set</a>, and
whenever a function that <em>could</em> fail is called, the compiler will enforce that
you handle that error at the call site. More on Zig's error handling some other
time, for now it is enough to understand what the <code>!</code> in front of the return
type declaration means. I want to move on to the body of the function, let's go
line by line.</p>
<pre><code><span>const</span> stdout = std.io.getStdOut().outStream();
</code></pre>
<p>So, we can see that this is a call into a standard library function (<code>std</code>)
that returns something that we assign to <code>const stdout</code>. Standard out (stdout) and
standard error (stderr) may be familiar concepts from the shell, but what does it
mean to be referring to <code>stdout</code> here in this program? What exactly <em>is</em>
<code>stdout</code>? Whatever it is, it's being returned by the call to <code>outStream()</code>,
which is a method called on the return value of <code>std.io.getStdOut()</code>, so we
first need to know what <em>that</em> is.</p>
<p>To the source! In the Zig source tree, <code>std</code> lives in <code>lib/std/std.zig</code>, which
is a file that makes a wide variety of functionality available. It includes the line:</p>
<pre><code><span>pub</span> <span>const</span> io = <span>@import</span>(<span>"io.zig"</span>);
</code></pre>
<p>Which is referred to on the <code>std</code> variable as <code>std.io</code> (again, notice the <code>pub</code>
keyword, without which this declared constant would be inaccesible outside of
this immediate scope). Going deeper, into <code>lib/std/io.zig</code>...</p>
<pre><code><span>pub</span> <span><span>fn</span> <span>getStdOut</span></span>() File {
    <span>return</span> File{
        .handle = getStdOutHandle(),
        .capable_io_mode = .blocking,
        .intended_io_mode = default_mode,
    };
}
</code></pre>
<p>So, <code>stdout</code> is a <em>File</em> struct. Let's look at that. It is imported at the top
of <code>io.zig</code> as</p>
<pre><code><span>const</span> File = std.fs.File;
</code></pre>
<p>and lives in the source, perhaps unsurprisingly, at <code>lib/std/fs/File.zig</code>. This
struct definition is quite long, so I'll focus on what we want to look at, the
<code>outStream()</code> method.</p>
<h2>An aside: methods vs functions</h2>
<p>Zig doesn't <em>really</em> have methods, but it's useful to talk about a special
class of functions <em>as</em> methods, since the calling convention supports implicit
passing of <code>self</code> when called on a struct "instance" using dot syntax. Let me
show you what I mean.</p>
<pre><code><span>const</span> std = <span>@import</span>(<span>"std"</span>);

<span>con…</span></code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.jfo.click/hello-hello-world/">https://blog.jfo.click/hello-hello-world/</a></em></p>]]>
            </description>
            <link>https://blog.jfo.click/hello-hello-world/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24175174</guid>
            <pubDate>Sun, 16 Aug 2020 04:39:44 GMT</pubDate>
        </item>
    </channel>
</rss>
