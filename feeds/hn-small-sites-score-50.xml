<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 50]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 50. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 21 Oct 2020 04:31:14 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-50.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 21 Oct 2020 04:31:14 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Discord Desktop App RCE]]>
            </title>
            <description>
<![CDATA[
Score 195 | Comments 50 (<a href="https://news.ycombinator.com/item?id=24822755">thread link</a>) | @Wingy
<br/>
October 18, 2020 | https://mksben.l0.cm/2020/10/discord-desktop-rce.html | <a href="https://web.archive.org/web/*/https://mksben.l0.cm/2020/10/discord-desktop-rce.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-6054437144226106686" itemprop="description articleBody">
<p>A few months ago, I discovered a remote code execution issue in the <a href="https://discord.com/">Discord</a> desktop application and I reported it via their <a href="https://discord.com/security">Bug Bounty Program</a>.</p><p>The RCE I found was an interesting one because it is achieved by combining multiple bugs. In this article, I'd like to share the details.</p><h3>Why I chose Discord for the target</h3><p>I kind of felt like finding for vulnerabilities of the Electron app, so I was looking for a bug bounty program which pays the bounty for an Electron app and I found Discord. Also, I am a Discord user and simply wanted to check if the app I'm using is secure, so I decided to investigate.</p><h3>Bugs I found</h3><p>Basically I found the following three bugs and achieved RCE&nbsp;by combining them.</p><ol><li>Missing contextIsolation</li><li>XSS in iframe embeds</li><li>Navigation restriction bypass (CVE-2020-15174)</li></ol><p>I'll explain these bugs one by one.</p><h3>Missing contextIsolation</h3><p>When I test Electron app, first I always check the options of the <a href="https://www.electronjs.org/docs/api/browser-window">BrowserWindow API</a>, which is used to create a browser window. By checking it, I think about how RCE can be achieved when arbitrary JavaScript execution on the renderer is possible.</p><p>The Discord's Electron app is not an open source project but the Electron's JavaScript code is saved locally with the asar format and I was able to read it just by extracting it.</p><p>In the main window, the following options are used:&nbsp;</p><blockquote>const mainWindowOptions = {<br>&nbsp;&nbsp;title: 'Discord',<br>&nbsp;&nbsp;backgroundColor: getBackgroundColor(),<br>&nbsp;&nbsp;width: DEFAULT_WIDTH,<br>&nbsp;&nbsp;height: DEFAULT_HEIGHT,<br>&nbsp;&nbsp;minWidth: MIN_WIDTH,<br>&nbsp;&nbsp;minHeight: MIN_HEIGHT,<br>&nbsp;&nbsp;transparent: false,<br>&nbsp;&nbsp;frame: false,<br>&nbsp;&nbsp;resizable: true,<br>&nbsp;&nbsp;show: isVisible,<br>&nbsp;&nbsp;webPreferences: {<br>&nbsp;&nbsp;&nbsp;&nbsp;blinkFeatures: 'EnumerateDevices,AudioOutputDevices',<br>&nbsp;&nbsp;&nbsp;&nbsp;<span><b>nodeIntegration: false</b></span>,<br>&nbsp;&nbsp;&nbsp;&nbsp;preload: _path2.default.join(__dirname, 'mainScreenPreload.js'),<br>&nbsp;&nbsp;&nbsp;&nbsp;nativeWindowOpen: true,<br>&nbsp;&nbsp;&nbsp;&nbsp;enableRemoteModule: false,<br>&nbsp;&nbsp;&nbsp;&nbsp;spellcheck: true<br>&nbsp;&nbsp;}<br>};</blockquote><p>The important options which we should check here are especially <i>nodeIntegration</i> and <i>contextIsolation</i>. From the above code, I found that the <i>nodeIntegration</i> option is set to false and the <i>contextIsolation</i> option is set to false (the default of the used version) in the Discord's main window.</p><p>If the nodeIntegration is set to true, a web page's JavaScript can use Node.js features easily just by calling the <code>require()</code>. For example, the way to execute the calc application on Windows is:</p><blockquote>&lt;script&gt;<br>&nbsp; require('child_process').exec('calc');<br>&lt;/script&gt;</blockquote><p>In this time, the <i>nodeIntegration</i> was set to false, so I couldn't use Node.js features by calling the <code>require()</code> directly.</p><p>However, there is still a possibility of access to Node.js features. The <i>contextIsolation</i>, another important option, was set to false. This option should not be set to false if you want to eliminate the possibility of RCE on your app.</p><p>If the <i>contextIsolation</i> is disabled, a web page's JavaScript can affect the execution of the <a href="https://github.com/electron/electron/tree/83bb065b4f6ed512d545c46389a7fdc114c94a54/lib/renderer">Electron's internal JavaScript code on the renderer</a>, and preload scripts (In the following, these JavaScript will be referred to as the JavaScript code outside web pages).&nbsp;For example, if you override&nbsp; <code>Array.prototype.join</code>, one of the JavaScript built-in methods, with another function from a web page's JavaScript, the JavaScript code outside web pages also will use the overridden function when the <code>join</code> is called.</p><p>This behavior is dangerous because Electron allows the JavaScript code outside web pages to use the Node.js features regardless the <i>nodeIntegration</i> option and by interfering with them from the function overridden in the web page, it could be possible to achieve RCE even if the <i>nodeIntegration</i> is set to false.</p><p>By the way, a such trick was previously not known. It was first discovered in a pentest by Cure53, which I also joined in, in 2016. After that, we reported it to Electron team and the <i>contextIsolation</i> was introduced.</p><p>Recently, that pentest report was published. If you are interested, you can read it from the following link:</p><p>Pentest-Report Ethereum Mist 11.2016 - 10.2017<br><a href="https://drive.google.com/file/d/1LSsD9gzOejmQ2QipReyMXwr_M0Mg1GMH/view">https://drive.google.com/file/d/1LSsD9gzOejmQ2QipReyMXwr_M0Mg1GMH/view</a></p><p>You can also read the slides which I used at a CureCon event:</p><p>The <i>contextIsolation</i> introduces the separated contexts between the web page and the JavaScript code outside web pages so that the JavaScript execution of each code does not affect each. This is a necessary faeture to eliminate the possibility of RCE, but this time it was disabled in Discord.</p><p>Now I found that the <i>contextIsolation</i> is disabled, so I started looking for a place where I could execute arbitrary code by interfering with the JavaScript code outside web pages.</p><p>Usually, when I create a PoC for RCE in the Electron's pentests, I first try to achieve RCE by using the Electron's internal JavaScript code on the renderer. This is because the Electron's internal JavaScript code on the renderer can be executed in any Electron app, so basically I can reuse the same code to achieve RCE and it's easy.</p><p>In my slides, <a href="https://speakerdeck.com/masatokinugawa/electron-abusing-the-lack-of-context-isolation-curecon-en?slide=41">I introduced</a> that RCE can be achieved by using the code which Electron executes at the navigation timing. It's not only possible from that code but there are such code in some places. (I'd like to publish examples of the PoC in the future.)</p><p>However, depending on the version of Electron used, or the <i>BrowserWindow</i> option which is set, because the code has been changed or the affected code can't be reached correctly, sometimes PoC via the Electron's code does not work well. In this time, it did not work, so I decided to change the target to the preload scripts.</p><div><p>When checking the preload scripts, I found that Discord exposes the function, which allows some allowed modules to be called via <code>DiscordNative.nativeModules.requireModule('MODULE-NAME')</code>, into the web page.</p></div><p>Here, I couldn't use modules that can be used for RCE directly, such as <i>child_process</i> module, but I found a code where RCE can be achieved by overriding the JavaScript built-in methods and interfering with the execution of the exposed module.</p><p>The following is the PoC. I was able to confirm that the calc application is popped up when I call the <code>getGPUDriverVersions</code> function which is defined in the module called "<i>discord_utils</i>" from devTools, while overriding the <code>RegExp.prototype.test</code> and <code>Array.prototype.join</code>.</p><blockquote>RegExp.prototype.test=function(){<br>&nbsp;&nbsp;&nbsp;&nbsp;return false;<br>}<br>Array.prototype.join=function(){<br>&nbsp;&nbsp;&nbsp;&nbsp;return "calc";<br>}<br>DiscordNative.nativeModules.requireModule('discord_utils').getGPUDriverVersions();</blockquote><p>The <code>getGPUDriverVersions</code> function tries to execute the program by using the "<i>execa</i>" library, like the following:</p><blockquote>module.exports.getGPUDriverVersions = async () =&gt; {<br>&nbsp;&nbsp;if (process.platform !== 'win32') {<br>&nbsp;&nbsp;&nbsp;&nbsp;return {};<br>&nbsp;&nbsp;}<p>&nbsp;&nbsp;const result = {};<br>&nbsp;&nbsp;const nvidiaSmiPath = `${process.env['ProgramW6432']}/NVIDIA Corporation/NVSMI/nvidia-smi.exe`;</p><p>&nbsp;&nbsp;try {<br>&nbsp;&nbsp;&nbsp;&nbsp;result.nvidia = parseNvidiaSmiOutput(await execa(nvidiaSmiPath, []));<br>&nbsp;&nbsp;} catch (e) {<br>&nbsp;&nbsp;&nbsp;&nbsp;result.nvidia = {error: e.toString()};<br>&nbsp;&nbsp;}</p><p>&nbsp;&nbsp;return result;<br>};</p></blockquote><p>Usually the <i>execa</i> tries to execute "<i>nvidia-smi.exe</i>", which is specified in the <code>nvidiaSmiPath</code> variable, however, due to the overridden <code>RegExp.prototype.test</code> and <code>Array.prototype.join</code>, the argument is replaced to "<i>calc</i>" in the <i>execa</i>'s internal processing.</p><p>Specifically, the argument is replaced by changing the following two parts.</p><p><a href="https://github.com/moxystudio/node-cross-spawn/blob/16feb534e818668594fd530b113a028c0c06bddc/lib/parse.js#L36">https://github.com/moxystudio/node-cross-spawn/blob/16feb534e818668594fd530b113a028c0c06bddc/lib/parse.js#L36</a></p><p><a href="https://github.com/moxystudio/node-cross-spawn/blob/16feb534e818668594fd530b113a028c0c06bddc/lib/parse.js#L55">https://github.com/moxystudio/node-cross-spawn/blob/16feb534e818668594fd530b113a028c0c06bddc/lib/parse.js#L55</a></p><p>The remaining work is to find a way to execute JavaScript on the application. If I can find it, it leads to actual RCE.</p><h3>XSS in iframe embeds</h3><p>As explained above, I found that RCE could happen from arbitrary JavaScript execution, so I was trying to find an XSS vulnerability. The app supports the autolink or Markdown feature, but looked like it is good. So I turned my attention to the iframe embeds feature. The iframe embeds is the feature which automatically displays the video player on the chat when the YouTube URL is posted, for example.</p><p>When the URL is posted, Discord tries to get the <a href="https://ogp.me/">OGP</a> information of that URL and if there is the OGP information, it displays the page's title, description, thumbnail image, associated video and so on in the chat.</p><p>The Discord extracts the video URL from the OGP and only if the video URL is allowed domain and the URL has actually the URL format of the embeds page, the URL is embedded in the iframe.</p><p>I couldn't find the documentation about which services can be embedded in the iframe, so I tried to get a hint by checking the CSP's <i>frame-src</i> directive. At that time, the following CSP was used:</p><blockquote>Content-Security-Policy: [...] ; frame-src https://*.youtube.com https://*.twitch.tv https://open.spotify.com https://w.soundcloud.com https://sketchfab.com https://player.vimeo.com https://www.funimation.com https://twitter.com https://www.google.com/recaptcha/ https://recaptcha.net/recaptcha/ https://js.stripe.com https://assets.braintreegateway.com https://checkout.paypal.com https://*.watchanimeattheoffice.com</blockquote><p>Obviously, some of them are listed to allow iframe embeds (e.g. YouTube, Twitch, Spotify).&nbsp;I tried to check if the URL can be embeded in the iframe by specifying the domain into the OGP information one by one and tried to find XSS on the embedded domains. After some attempts, I found that the&nbsp;<a href="https://sketchfab.com/">sketchfab.com</a>, which is one of the domains listed in the CSP, can be embedded in the iframe and found XSS on the embeds page.&nbsp;I didn't know about Sketchfab at that time, but it seems that it is a platform in which users can publish, buy and sell 3D models. There was a simple DOM-based XSS in the footnote of the 3D model.</p><p>The following is the PoC, which has the crafted OGP. When I posted this URL to the chat, the Sketchfab was embedded into the iframe on the chat, and after a few clicks on the iframe, arbitrary JavaScript was executed.</p><p><a href="https://l0.cm/discord_rce_og.html">https://l0.cm/discord_rce_og.html</a></p><blockquote>&lt;head&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta charset="utf-8"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta property="og:title" content="RCE DEMO"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;[...]<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta property="<span><b>og:video:url</b></span>" content="https://<span><b>sketchfab.com</b></span>/models/2b198209466d43328169d2d14a4392bb/embed"&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta ‚Ä¶</blockquote></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mksben.l0.cm/2020/10/discord-desktop-rce.html">https://mksben.l0.cm/2020/10/discord-desktop-rce.html</a></em></p>]]>
            </description>
            <link>https://mksben.l0.cm/2020/10/discord-desktop-rce.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24822755</guid>
            <pubDate>Mon, 19 Oct 2020 02:00:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gravity is not a force ‚Äì free-fall parabolas are straight lines in spacetime]]>
            </title>
            <description>
<![CDATA[
Score 641 | Comments 334 (<a href="https://news.ycombinator.com/item?id=24821141">thread link</a>) | @tim_hutton
<br/>
October 18, 2020 | https://timhutton.github.io/GravityIsNotAForce/ | <a href="https://web.archive.org/web/*/https://timhutton.github.io/GravityIsNotAForce/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>For full functionality of this site it is necessary to enable JavaScript.
    Here are the <a href="http://www.enable-javascript.com/" target="_blank">
    instructions how to enable JavaScript in your web browser</a>.
    </p>



    <p>
    <canvas id="canvas" width="1360" height="480">(Canvas drawing not supported by your browser.)</canvas>
    </p><p>
      Change the frame acceleration: 
    </p>
    <p>
      Move the time window: 
    </p>
    

    <h4>Description:</h4>
    <p>
    Under general relativity, gravity is not a force. Instead it is a distortion of spacetime. Objects in free-fall move along geodesics (straight lines) in spacetime, as seen in the inertial frame of reference on the right. When standing on Earth we experience a frame of reference that is accelerating upwards, causing objects in free-fall to move along parabolas, as seen on the left.
    </p>

    <p>
    In this system there is only one space dimension, shown on the vertical axis and labeled in meters. The time dimension is the horizontal axis and labeled in seconds. The gravitational field is constant within the area of interest. 
    </p>

    <p>
    Use the first slider to change the acceleration of the frame of reference in the middle. When the frame has zero acceleration it is said to be an inertial frame of reference.
    </p>

    <p>
    Use the second slider to move the time window. Note that all the trajectories remain as straight lines in the inertial frame of reference.
    </p>

    <p>
    You can drag the start and end position of each object to change their trajectories. All free-fall trajectories in the inertial frame of reference are straight lines.
    </p>

    <p>
    Code, more details, feedback: <a href="https://github.com/timhutton/GravityIsNotAForce">https://github.com/timhutton/GravityIsNotAForce</a>
    </p>

    <p>
    More on these concepts: <a href="https://youtu.be/XRr1kaXKBsU">https://youtu.be/XRr1kaXKBsU</a>
    </p>





</div>]]>
            </description>
            <link>https://timhutton.github.io/GravityIsNotAForce/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24821141</guid>
            <pubDate>Sun, 18 Oct 2020 21:07:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The local timeline is the key to enjoying Mastodon]]>
            </title>
            <description>
<![CDATA[
Score 163 | Comments 93 (<a href="https://news.ycombinator.com/item?id=24819387">thread link</a>) | @carlesfe
<br/>
October 18, 2020 | https://cfenollosa.com/blog/you-may-be-using-mastodon-wrong.html | <a href="https://web.archive.org/web/*/https://cfenollosa.com/blog/you-may-be-using-mastodon-wrong.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="divbodyholder">

<div id="divbody"><div>
<!-- entry begin -->
<h3><a href="https://cfenollosa.com/blog/you-may-be-using-mastodon-wrong.html">
You may be using Mastodon wrong
</a></h3>
<!-- bashblog_timestamp: #202010181913.37# -->
<p>October 18, 2020 ‚Äî 
Carlos Fenollosa
</p>
<!-- text begin -->

<p>I'm sure you have already heard about <a href="https://joinmastodon.org/">Mastodon</a>,
typically marketed as <em>a Twitter alternative</em>.</p>

<p>I will try to convince you that the word <em>alternative</em> doesn't mean here what you think it means,
and why you may be using Mastodon wrong if you find it boring.</p>

<h4>An alternative community</h4>

<p>You should not expect to "migrate from Twitter to Mastodon."</p>

<p>Forget about the privacy angle for now. Mastodon is an alternative community, where people behave
differently.</p>

<p><strong>It's your chance to make new internet friends.</strong></p>

<p>There may be some people for whom Mastodon is a safe haven. Yes, some users really do migrate
there to avoid censorship or bullying but, for most of us, that will not be the case.</p>

<p>Let's put it this way: Mastodon is to Twitter what Linux is to Windows.</p>

<p>Linux is libre software. But that's not why most people use it. Linux users mostly want 
to get their work done, and Linux is an excellent platform.
There is no Microsoft Word, no Adobe Photoshop, no Starcraft. If you need to use these tools, honestly,
you'd better stick with Windows. You can use emulation, in the same way that there are
utilities to post to Twitter from Mastodon, but that would miss the point.</p>

<p>The bottom line is, you can perform the same tasks, but the process will be different.
You can post <em>toots</em> on Mastodon, upload gifs, send DMs... but it's not Twitter, and that is fine.</p>

<h4>The Local Timeline is Mastodon's greatest invention</h4>

<p>The problem most people have with Mastodon is that they "get bored" with it quickly. I've seen it a lot, and
it means one thing: <strong>the person created their account on the wrong server</strong>.</p>

<p>"But," they say, "isn't Mastodon federated? Can't I chat with everybody, regardless of their server?"
Yes, of course. But discoverability works differently on Mastodon.</p>

<p>Twitter has only two discoverability layers: your network and the whole world. Either a small group of
contacts, or everybody in the whole world. That's crazy.</p>

<p>They try very hard to show you tweets from outside your network so you can discover new people.
And, at the same time, they show your tweets to third parties, so you can get new followers.
This is the way that
they try to keep you engaged once your network is more or less stable and starts getting stale.</p>

<p>Mastodon, instead, has an extra layer between your network and the whole world:
messages from <em>people on your server</em>. This is called the <em>local timeline</em>.</p>

<p><strong>The local timeline is the key to enjoying Mastodon.</strong></p>

<h4>How long it's been since you made a new internet friend?</h4>

<p>If you're of a certain age you may remember BBSs, Usenet, the IRC, or early internet forums.
Do you recall how exciting it was to log into the unknown and realize that there were people
all around the world who shared your interests?</p>

<p>It was an amazing feeling which got lost on the modern internet. Now you have a chance to relive it.</p>

<p>The local timeline dynamics are very different. There is a lot of respectful interactions among total strangers,
because there is this feeling of community, of being in a neighborhood. Twitter is just the opposite, strangers
shouting at each other.</p>

<p>Furthermore, since the local timeline is more or less limited in the amount of users, you have the chance
to recognize usernames, and being recognized. You start interacting with strangers, mentioning them, sending them
links they may like. You discover new websites, rabbit holes, new approaches to your hobbies.</p>

<p>I've made quite a few new <em>internet friends</em> on my Mastodon server, and I don't mean followers or contacts.
I'm talking about human beings who I have never met in person but feel close to.</p>

<p>People are humble and respectful. And, for less nice users, admins enforce codes of conduct and, 
on extreme cases, users may get kicked off a server. But they are not being banned by a faceless corporation
due to mass reports, everybody is given a chance.</p>

<h4>How to choose the right server</h4>

<p>The problem with "generalist" Mastodon servers like <a href="https://mastodon.social/">mastodon.social</a>
is that users have just too diverse interests and backgrounds.
Therefore, there is no community feeling. For some people, that may be exactly what they're looking for. But, 
for most of us, there is more value on the smaller servers.</p>

<p>So, how can you choose the right server? Fortunately, you can do a bit of research. 
There is an official <a href="https://joinmastodon.org/communities">directory of Mastodon servers</a> categorized by
interests and regions. </p>

<p>Since you're reading my blog, start by taking a look at these:</p>

<ul>
<li><a href="http://bsd.network/">bsd.network</a>, for fans of BSD systems</li>
<li><a href="http://linuxrocks.online/">linuxrocks.online</a>, for Linux fans</li>
<li><a href="http://fosstodon.org/">fosstodon.org</a>, for free software in general</li>
<li><a href="http://tilde.zone/">tilde.zone</a>, for oldschool internet users</li>
<li><a href="https://merveilles.town/">merveilles.town</a>, with a very particular mixture of art and technology</li>
<li><a href="https://metalhead.club/">metalhead.club</a>, to enjoy those classic riffs</li>
</ul>

<p>And the regionals</p>

<ul>
<li><a href="https://mastodont.cat/">mastodont.cat</a> for catalans</li>
<li><a href="https://mastodon.madrid/">mastodon.madrid</a> for madrile√±os</li>
</ul>

<p>There are many more. Simply search online for "mastodon server MY_FAVORITE_HOBBY." And believe me, servers
between 500 and 5,000 people are the best.</p>

<h4>Final tips</h4>

<p>Before clicking on "sign up", always browse the local timeline,
the about page, and the most active users list. You will get a pretty good idea of the kind of people
who chat there.
Once you feel right at home you can continue your adventure and start following users from other servers.</p>

<p>Mastodon has an option to only display toots in specific languages. It can be very useful to avoid being
flooded by toots that you just have no chance of understanding or even getting what they're about.</p>

<p>You can also filter your notifications by types: replies, mentions, favorites, reposts, and more.
This makes catching up much more manageable than on Twitter.</p>

<p>Finally, Mastodon has a built-in "Content Warning" feature. It allows you to hide
text behind a short explanation, in case you want to talk about sensible topics or just about spoiling
a recent movie.</p>

<p>Good luck with your search, and see you on the Fediverse! I'm at
<a href="https://mastodon.sdf.org/@cfenollosa">@cfenollosa@mastodon.sdf.org</a></p>

<p>Tags: <a href="https://cfenollosa.com/blog/tag_internet.html">internet</a></p>
<!-- text end -->
<p id="twitter"><a href="http://twitter.com/intent/tweet?url=http://cfenollosa.com/blog/you-may-be-using-mastodon-wrong.html&amp;text=%3CType%20your%20comment%20here%20but%20please%20leave%20the%20URL%20so%20that%20other%20people%20can%20follow%20the%20comments%3E&amp;via=cfenollosa">Comments? Tweet</a> 
<a href="https://twitter.com/search?q=http://cfenollosa.com/blog/you-may-be-using-mastodon-wrong.html"><span id="count-18287"></span></a>&nbsp;</p>
<!-- entry end -->
</div>

</div></div></div>]]>
            </description>
            <link>https://cfenollosa.com/blog/you-may-be-using-mastodon-wrong.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24819387</guid>
            <pubDate>Sun, 18 Oct 2020 17:16:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cursed Elixir]]>
            </title>
            <description>
<![CDATA[
Score 121 | Comments 45 (<a href="https://news.ycombinator.com/item?id=24818706">thread link</a>) | @udfalkso
<br/>
October 18, 2020 | https://evuez.github.io/posts/cursed-elixir.html | <a href="https://web.archive.org/web/*/https://evuez.github.io/posts/cursed-elixir.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      
        <time datetime="2020-08-12">2020-08-12</time>
      <p>Let's write some Elixir.</p>
<pre><code>defmodule FooBar do
  def foo(a) do
    if a &lt; 0 do
      bar(a, -1)
    else
      bar(a, 1)
    end
  end

  defp bar(a, b) do
    IO.inspect(a * b)
  end
end
</code></pre>
<p>Not very useful, but that's good enough for our purpose.</p>
<p>I like Elixir, but I think most of the time it just looks like functional Ruby. I want to make this code look like Elixir.</p>
<p>First, this code is lacking every Elixir developer's best friend: <code>|&gt;</code>. Let's add some <code>|&gt;</code>s.</p>
<pre><code>defmodule FooBar do
  def foo(a) do
    if a &lt; 0 do
      a |&gt; bar(-1)
    else
      a |&gt; bar(1)
    end
  end

  defp bar(a, b) do
    (a * b) |&gt; IO.inspect()
  end
end
</code></pre>
<p>Meh. It's definitely better, but I mean, that's only 3 <code>|&gt;</code>s. I want more <code>|&gt;</code>s.</p>
<p>We have an <code>if</code> in there, so maybe we can do something with it?</p>
<pre><code>defmodule FooBar do
  def foo(a) do
    (a &lt; 0) |&gt; if do
      a |&gt; bar(-1)
    else
      a |&gt; bar(1)
    end
  end

  defp bar(a, b) do
    (a * b) |&gt; IO.inspect()
  end
end
</code></pre>
<p>We sure can! That's one more <code>|&gt;</code>. Can we do better than this?</p>
<p>Well... <code>&gt;</code> and <code>*</code> are <code>Kernel</code> functions, so maybe...</p>
<pre><code>defmodule FooBar do
  def foo(a) do
    a |&gt; Kernel.&lt;(0) |&gt; if do
      a |&gt; bar(-1)
    else
      a |&gt; bar(1)
    end
  end

  defp bar(a, b) do
    a |&gt; Kernel.*(b) |&gt; IO.inspect()
  end
end
</code></pre>
<p>This is great, can we keep going?</p>
<p>The Elixir docs say <code>defmodule</code> is just a macro. Does that mean I can just <code>|&gt;</code> into <code>defmodule</code>?</p>
<pre><code>FooBar |&gt; defmodule do
  def foo(a) do
    a |&gt; Kernel.&lt;(0) |&gt; if do
      a |&gt; bar(-1)
    else
      a |&gt; bar(1)
    end
  end

  defp bar(a, b) do
    a |&gt; Kernel.*(b) |&gt; IO.inspect()
  end
end
</code></pre>
<p>Yes you can!</p>
<p><code>def</code> and <code>defp</code> are macros too right?</p>
<pre><code>FooBar |&gt; defmodule do
  a |&gt; foo() |&gt; def do
    a |&gt; Kernel.&lt;(0) |&gt; if do
      a |&gt; bar(-1)
    else
      a |&gt; bar(1)
    end
  end

  a |&gt; bar(b) |&gt; defp do
    a |&gt; Kernel.*(b) |&gt; IO.inspect()
  end
end
</code></pre>
<p>So many pipes! üòç</p>
<p>We're getting somewhere, but something still doesn't feel right. This module really isn't doing much, so maybe it should not be that long? Also, I think we need more <code>:</code>. Atoms are very Elixir-y, so let's do more of that:</p>
<pre><code>FooBar |&gt; defmodule(do: (
  a |&gt; foo() |&gt; def(do: a |&gt; Kernel.&lt;(0) |&gt; if(do: a |&gt; bar(-1), else: a |&gt; bar(1)))

  a |&gt; bar(b) |&gt; defp(do: a |&gt; Kernel.*(b) |&gt; IO.inspect())
))
</code></pre>
<p>We're <code>:do</code>ing great!</p>
<p>You know what's also very Elixir-y? Lists. Lists and tuples.</p>
<pre><code>FooBar |&gt; defmodule([{:do, (
  a
  |&gt; foo()
  |&gt; def([{:do, a |&gt; Kernel.&lt;(0) |&gt; if([{:do, a |&gt; bar(-1)}, {:else, a |&gt; bar(1)}])}])

  a |&gt; bar(b) |&gt; defp([{:do, a |&gt; Kernel.*(b) |&gt; IO.inspect()}])
)}])
</code></pre>
<p>Who's going to say this looks like Ruby now? ‚öóÔ∏è</p>
<hr>
<p><a href="https://news.ycombinator.com/item?id=24818706">discussion on hackernews</a> /
<a href="https://www.reddit.com/r/elixir/comments/jd2hr4/cursed_elixir/">discussion on reddit</a></p>

    </article></div>]]>
            </description>
            <link>https://evuez.github.io/posts/cursed-elixir.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24818706</guid>
            <pubDate>Sun, 18 Oct 2020 15:49:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[EU shoots for ‚Ç¨10B ‚Äòindustrial cloud‚Äô to rival US]]>
            </title>
            <description>
<![CDATA[
Score 167 | Comments 320 (<a href="https://news.ycombinator.com/item?id=24817290">thread link</a>) | @colinjoy
<br/>
October 18, 2020 | https://www.politico.eu/article/eu-pledges-e10-billion-to-power-up-industrial-cloud-sector/ | <a href="https://web.archive.org/web/*/https://www.politico.eu/article/eu-pledges-e10-billion-to-power-up-industrial-cloud-sector/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
									
							<div id="amazon-polly-audio-table">
				<p>Press play to listen to this article</p>
				
		</div><p>BERLIN ‚Äî The European Union aims to spend up to ‚Ç¨10 billion over the next seven years to help build up a homegrown cloud computing sector that could rival foreign corporations such as Amazon, Google and Alibaba.</p>
<p>Twenty-five EU countries signed a <a href="https://ec.europa.eu/digital-single-market/en/news/towards-next-generation-cloud-europe" target="_blank">joint declaration</a> Thursday pledging public money to power up the cloud sector and establishing the "European Alliance on Industrial Data and Cloud," a partnership geared toward facilitating such projects.</p>
<p>The alliance ‚Äî whose funding is to be drawn from existing EU programs and hoped-for pledges from industry and national capitals ‚Äî will be launched by the end of the year. Cyprus and Denmark were the only EU member countries not to sign the declaration due to ‚Äútechnical reasons.‚Äù</p>
<p>The declaration ‚Äúis a foundation stone for the establishment of European cloud technology, which will be very high performing,‚Äù said Internal Market Commissioner Thierry Breton, following a meeting of European telecoms ministers organized by the German government, which currently holds the EU‚Äôs rotating Council presidency.</p>
<p>‚ÄúContrary to the prejudices, we are not late [on cloud development]. We are the first to get involved in the industrial cloud,‚Äù he added.</p>
<blockquote><p>‚ÄúEU governments and the public sector need to be fully committed to the initiative by fully shifting to cloud services‚Äù ‚Äî <em>Lise Fuhr, ETNO‚Äôs director general</em></p></blockquote>
<p>The cloud alliance is a key part of the European Commission‚Äôs data strategy, which aims to create a single market for industrial data. Commissioner Breton in particular has lobbied hard to make the EU a worldwide data hub, and to develop data processing capabilities that would give Europe an edge over foreign rivals that currently dominate the cloud business.</p>
<p>It also fits in with broader efforts by European policymakers to make the Continent less dependent on foreign technology. Currently, U.S. technology firms dominate the global market for cloud storage.</p>
<p>In the same vein, the bloc is set to unveil by December a rulebook for platforms dubbed the ‚ÄúDigital Services Act," as well as binding laws for artificial intelligence that are set to be released early next year.</p>
<p>The new alliance will have the mandate to develop business, investment and implementation plans for European cloud technologies in the public and private sectors.</p>
<p>Signatories also pledge to create common European standards and policy norms to create pan-European cloud services, and help small and medium-sized businesses, startups and the public sector embrace cloud technology.</p>
<p>‚ÄúIn order to achieve digital sovereignty, we need to start approaching data processing the way major American and Chinese companies ‚Äî the hyper-scalers ‚Äî approach it,‚Äù said German Economy Minister Peter Altmaier.</p>
<p>‚ÄúThis is an area where we‚Äôre far from being equals,‚Äù he added.</p>
<h3>Money, money, money</h3>
<p>The Commission‚Äôs plan is to invest up to ‚Ç¨10 billion to develop Europe's cloud and data infrastructures.</p>
<p>The EU‚Äôs executive arm would invest ‚Ç¨2 billion from programs in its long-term budget such as the Digital Europe Programme, Connecting Europe Facility 2 and InvestEU.</p>
<p>The rest of the money will come from both industry and member countries. National governments will be able to fund these projects through the EU‚Äôs coronavirus recovery plan, which has earmarked 20 percent toward digital projects.</p>
<p>The joint cloud declaration also issues demands to non-European cloud companies.</p>
<p>Cloud providers must ‚Äúguarantee European standards in terms of security, data protection, consumer protection, data portability and energy efficiency and contribute to European digital sovereignty.‚Äù</p>
<p>The companies must offer ‚Äúadequate assurance‚Äù that the EU will maintain control over its strategic and sensitive data.</p>
<p>‚ÄúWhile all cloud providers are welcome in European cloud federation, the resulting cloud capacities should not be subject to laws of foreign jurisdictions,‚Äù the declaration read.</p>
<p>One of the first initiatives to come out of Europe‚Äôs cloud push is Gaia-X, a much-hyped European effort spearheaded by Germany and France to build up a European platform that sets common standards for cloud technology. Gaia-X has <a href="https://www.politico.eu/?p=1449266">limited</a> the voting rights of non-European cloud computing companies, and they cannot become directors of the association.</p>
<p>ETNO, the association representing Europe‚Äôs leading telecom operators, applauded the move.</p>
<p>‚ÄúEU governments and the public sector need to be fully committed to the initiative by fully shifting to cloud services. We call for EU targets and commitments that reflect the demand side of the cloud investment story,‚Äù said Lise Fuhr, ETNO‚Äôs director general.</p>
<p>Tech lobby DigitalEurope was equally supportive, and announced it was applying for membership in Gaia-X.</p>
<p><em>Want more analysis from </em><span>POLITICO</span><em>? </em><span>POLITICO</span><em> Pro is our premium intelligence service for professionals. From financial services to trade, technology, cybersecurity and more, Pro delivers real time intelligence, deep insight and breaking scoops you need to keep one step ahead. Email <a href="https://www.politico.eu/cdn-cgi/l/email-protection#6515170a25150a090c110c060a4b0010" target="_blank"><span data-cfemail="3d4d4f527d4d52515449545e52135848">[email&nbsp;protected]</span></a> to request a complimentary trial.</em></p>
 <div> <h3>  Also On POLITICO  </h3>   <div data-block-attributes="[]" data-page="0"> <div> <div> <p><a href="https://www.politico.eu/article/eu-cloud-new-front-with-us-tech-giants/"> <img src="https://www.politico.eu/wp-content/uploads/2020/09/iStock-1160479733-765x540.jpg" sizes="(max-width: 765px) 100vw, 765px" alt="EU cloud regulation opens new front with US tech giants" width="765" height="540" data-thumbnail-size="ev-pro-lead" loading="lazy"></a> </p>   </div><div> <p><a href="https://www.politico.eu/article/beyond-tiktok-us-chinese-app-crackdown/"> <img src="https://www.politico.eu/wp-content/uploads/2020/08/GettyImages-1068921922-1-765x540.jpg" sizes="(max-width: 765px) 100vw, 765px" alt="Beyond TikTok, US eyes Chinese apps and cloud for crackdown" width="765" height="540" data-thumbnail-size="ev-pro-lead" loading="lazy"></a> </p>   </div> </div> </div>    </div> 								</div></div>]]>
            </description>
            <link>https://www.politico.eu/article/eu-pledges-e10-billion-to-power-up-industrial-cloud-sector/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24817290</guid>
            <pubDate>Sun, 18 Oct 2020 12:20:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Object Detection at 1840 FPS with TorchScript, TensorRT and DeepStream]]>
            </title>
            <description>
<![CDATA[
Score 159 | Comments 54 (<a href="https://news.ycombinator.com/item?id=24817173">thread link</a>) | @briggers
<br/>
October 18, 2020 | https://paulbridger.com/posts/video-analytics-deepstream-pipeline/ | <a href="https://web.archive.org/web/*/https://paulbridger.com/posts/video-analytics-deepstream-pipeline/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  
  
  <h5>October 17, 2020</h5>



  
  
  

  


  <h2 id="intro">
  Intro
  <a href="#intro">#</a>
</h2>
<p>Previously, we took a <a href="https://paulbridger.com/posts/video-analytics-pytorch-pipeline/">simple video pipeline</a> and made it as fast as we could without sacrificing the flexibility of the Python runtime. It‚Äôs amazing how far you can go ‚Äî <a href="https://paulbridger.com/posts/video-analytics-pipeline-tuning/">9 FPS to 650 FPS</a> ‚Äî but we did not reach full hardware utilization and the pipeline did not scale linearly beyond a single GPU. There is evidence (measured using <a href="https://github.com/chrisjbillington/gil_load">gil_load</a>) that we were throttled by a fundamental Python limitation with multiple threads fighting over the <a href="https://wiki.python.org/moin/GlobalInterpreterLock">Global Interpreter Lock</a> (GIL).</p>
<p>In this article we‚Äôll take performance of the same <a href="https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/">SSD300 model</a> even further, leaving Python behind and moving towards true production deployment technologies:</p>
<ul>
<li>
<p><a href="https://pytorch.org/docs/stable/jit.html"><strong>TorchScript.</strong></a> Instead of running directly in the Pytorch runtime, we‚Äôll export our model using TorchScript tracing into a form that can be executed portably using the <code>libtorch</code> C++ runtime.</p>
</li>
<li>
<p><a href="https://developer.nvidia.com/tensorrt"><strong>TensorRT.</strong></a> This toolset from Nvidia includes a ‚Äúdeep learning inference optimizer‚Äù ‚Äî a compiler for optimizing CUDA-based computational graphs. We‚Äôll use this to squeeze out every drop of inference efficiency.</p>
</li>
<li>
<p><a href="https://developer.nvidia.com/deepstream-sdk"><strong>DeepStream.</strong></a> While <a href="https://gstreamer.freedesktop.org/">Gstreamer</a> gives us an extensive library of elements to build media pipelines with, DeepStream expands this library with a set of GPU-accelerated elements specialized for machine learning.</p>
</li>
</ul>
<p>These technologies fit together like this:</p>
<p><img src="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/deepstream_hybrid.svg" alt="DeepStream Hybrid Architecture"></p>
<p>This article will not be a step-by-step tutorial with code examples, but will show what is possible when these technologies are combined. The associated repository is here: <a href="https://github.com/pbridger/deepstream-video-pipeline">github.com/pbridger/deepstream-video-pipeline</a>.</p>
<h3 id="torchscript-vs-tensorrt">
  üî•TorchScript vs TensorRTüî•
  <a href="#torchscript-vs-tensorrt">#</a>
</h3>
<p>Both TorchScript and TensorRT can produce a deployment-ready form of our model, so why do we need both? These great tools may eventually be competitors but in 2020 they are complementary ‚Äî they each have weaknesses that are compensated for by the other.</p>
<p><strong>TorchScript.</strong> With a few lines of <code>torch.jit</code> code we can generate a deployment-ready asset from essentially any Pytorch model that will run anywhere libtorch runs. It‚Äôs not inherently faster (it is submitting approximately the same sequence of kernels) but the libtorch runtime will perform better under high concurrency. However, without care TorchScript output may have performance and portability surprises (I‚Äôll cover some of these in a later article).</p>
<p><strong>TensorRT.</strong> An unparalleled model compiler for Nvidia hardware, but for Pytorch or <a href="https://onnx.ai/">ONNX</a>-based models it has incomplete support and suffers from poor portability. There is a plugin system to add arbitrary layers and postprocessing, but this low-level work is out of reach for groups without specialized deployment teams. TensorRT also doesn‚Äôt support cross-compilation so models must be optimized directly on the target hardware ‚Äî not great for embedded platforms or highly diverse compute ecosystems.</p>
<p>Let‚Äôs begin with a baseline from the previous post in this series ‚Äî <a href="https://paulbridger.com/posts/video-analytics-pipeline-tuning/">Object Detection from 9 FPS to 650 FPS in 6 Steps</a>.</p>
<h2 id="stage-0-python-baseline">
  Stage 0: Python Baseline
  <a href="#stage-0-python-baseline">#</a>
</h2>
<table>
<thead>
<tr>
<th>Code</th>
<th>Nsight Systems Trace</th>
<th>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/pbridger/pytorch-video-pipeline/blob/master/tuning_postprocess_2.py">tuning_postprocess_2.py</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-pipeline-tuning/logs/tuning_postprocess_2.qdrep">tuning_postprocess_2.qdrep</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-pipeline-tuning/logs/tuning_postprocess_2.pipeline.dot.png">tuning_postprocess_2.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>The <a href="https://paulbridger.com/posts/video-analytics-pipeline-tuning/#stage-2-postprocessing-on-gpu">Postprocessing on GPU</a> stage from my previous post is logically closest to our first DeepStream pipeline. This was a fairly slow, early stage in the Python-based optimization journey but limitations in DeepStream around batching and memory transfer make this the best comparison.</p>
<p>This Python-based pipeline runs at around 80 FPS:</p>








<p>After we get a basic DeepStream pipeline up and running we‚Äôll empirically understand and then remove the limitations we see.</p>
<h2 id="stage-1-normal-deepstream-mdash-100-torchscript">
  Stage 1: Normal DeepStream ‚Äî 100% TorchScript
  <a href="#stage-1-normal-deepstream-mdash-100-torchscript">#</a>
</h2>
<table>
<thead>
<tr>
<th>Code</th>
<th>Nsight Systems Trace</th>
<th>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_1.py">ds_trt_1.py</a>, <a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_1.py">ds_tsc_1.py</a>, <a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_1.py">ds_ssd300_1.py</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/logs/ds_1_1gpu_batch16_host.qdrep">ds_1_1gpu_batch16_host.qdrep</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/logs/ds_1_1gpu_batch16_host.pipeline.dot.png">ds_1_1gpu_batch16_host.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>Our approach to using TorchScript and TensorRT together in a DeepStream pipeline will be to construct a hybrid model with two sequential components ‚Äî a TensorRT frontend passing results to a TorchScript backend which completes the calculation.</p>
<h3 id="hybrid-deepstream-pipeline">
  Hybrid DeepStream Pipeline
  <a href="#hybrid-deepstream-pipeline">#</a>
</h3>
<p>Our hybrid pipeline will eventually use the <code>nvinfer</code> element of DeepStream to serve a TensorRT-compiled form of the SSD300 model directly in the media pipeline. Since TensorRT cannot compile the entire model (due to unsupported <a href="https://onnx.ai/">ONNX</a> ops) we‚Äôll run the remaining operations as a TorchScript module (via <a href="https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream%20Plugins%20Development%20Guide/deepstream_plugin_details.html#wwpID0E0TDB0HA">the <code>parse-bbox-func-name</code> hook</a>).</p>
<p>However, the first pipeline will be the simplest possible while still following the hybrid pattern. The TensorRT model does no processing and simply passes frames to the TorchScript model, which does all preprocessing, inference, and postprocessing. 0% TensorRT, 100% TorchScript.</p>
<p>This pipeline runs at 110 FPS without tracing overhead. However, this TorchScript model has already been converted to <code>fp16</code> precision so a direct comparison to the Python-based pipeline is a bit misleading.</p>








<p>Let‚Äôs drill into the trace with <a href="https://developer.nvidia.com/nsight-systems">Nvidia‚Äôs Nsight Systems</a> to understand the patterns of execution. I have zoomed in to the processing for two 16-frame batches:</p>








<a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_batch.png">
    <figure>
        <img src="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_batch_hu85cb8d526cf1fda403db89df3e60bf80_432723_896x520_fill_box_top_2.png" width="896" height="520">
        <figcaption><small></small></figcaption>
    </figure>
</a>

<p>Looking at the red NVTX ranges on the <code>GstNvInfer</code> line we can see overlapping ranges where batches of 16 frames are being processed. However, the pattern of processing on the GPU is quite clear from the 16 utilisation spikes ‚Äî it is processing frame-by-frame.  We also see constant memory transfers between device and host.</p>
<p>Drilling in to see just two frames of processing, the pattern is even more clear:</p>








<a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_frame.png">
    <figure>
        <img src="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_normal_ds_two_frame_hudc758a7e69d7157937e6a1d7caab6946_421239_896x540_fill_box_top_2.png" width="896" height="540">
        <figcaption><small></small></figcaption>
    </figure>
</a>

<p>With a little knowledge of how DeepStream works the problem is clear:</p>
<ul>
<li><code>nvinfer</code> sends batches of frames to the configured model engine (our empty TensorRT component) ‚Äî great.</li>
<li><code>nvinfer</code> then sends the model output <em>frame by frame</em> to the postprocessing hook (our TorchScript component).</li>
</ul>
<p>Since we have put our entire model into a TorchScript postprocessing hook we are now processing frame by frame with no batching, and this is causing very low GPU utilisation. (This is why we are comparing against a Python pipeline with no batching).</p>
<p><strong>We are using DeepStream contrary to the design</strong>, but to build a truly hybrid TensorRT and TorchScript pipeline we need batched postprocessing.</p>
<blockquote>
  <p><strong>DeepStream Limitation: Postprocessing Hooks are Frame-by-Frame</strong></p>
<p>The design of <code>nvinfer</code> assumes model output will be postprocessed frame-by-frame. This makes writing postprocessing code a tiny bit easier but is inefficient by default. Preprocessing, inference and postprocessing logic should always assume a batch dimension is present.</p>

</blockquote>

<p>The Nsight Systems view above also shows a pointless sequence of device-to-host then host-to-device transfers. The purple device-to-host memory transfer is due to <code>nvinfer</code> sending tensors to system memory, ready for the postprocessing code to use it. The green host-to-device transfers are me putting this memory back on the GPU where it belongs.</p>
<blockquote>
  <p><strong>DeepStream Limitation: Postprocessing is Assumed to Happen on Host</strong></p>
<p>This is a legacy of early machine learning approaches. Modern deep learning pipelines keep data on the GPU end-to-end, including data augmentation and postprocessing. See Nvidia‚Äôs <a href="https://developer.nvidia.com/DALI">DALI library</a> for an example of this.</p>

</blockquote>

<p>Okay, time to hack DeepStream and remove these limitations.</p>
<h2 id="stage-2-hacked-deepstream-mdash-100-torchscript">
  Stage 2: Hacked DeepStream ‚Äî 100% TorchScript
  <a href="#stage-2-hacked-deepstream-mdash-100-torchscript">#</a>
</h2>
<table>
<thead>
<tr>
<th>Code</th>
<th>Nsight Systems Trace</th>
<th>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_2.py">ds_trt_2.py</a>, <a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_2.py">ds_tsc_2.py</a>, <a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_2.py">ds_ssd300_2.py</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/logs/ds_2_1gpu_batch16_device.qdrep">ds_2_1gpu_batch16_device.qdrep</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/logs/ds_2_1gpu_batch16_device.pipeline.dot.png">ds_2_1gpu_batch16_device.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>Thankfully, Nvidia have provided source for the <code>nvinfer</code> pipeline element. I‚Äôve made two changes to better support our approach of doing significant work in the postprocessing hook and fix the above limitations:</p>
<ul>
<li><code>nvinfer</code> model engine output is now sent in a single batch to the postprocessing hook.</li>
<li>Model output tensors are no-longer copied to host, but are left on the device.</li>
</ul>
<blockquote>
  These <code>nvinfer</code> changes are unreleased and are not present in the companion repository (<a href="https://github.com/pbridger/deepstream-video-pipeline">github.com/pbridger/deepstream-video-pipeline</a>) because they are clearly derivative of <code>nvinfer</code> and I‚Äôm unsure of the licensing. Nvidia people, feel free to get in touch: <a href="mailto:paul@paulbridger.com">paul@paulbridger.com</a>.
</blockquote>

<p>With hacked DeepStream and no model changes at all this pipeline now hits 350 FPS when measured with no tracing overhead. This is up from 110 FPS with regular DeepStream. I think we deserve a chart:</p>








<p>The <code>Concurrency 1x2080Ti</code> stage from the Python pipeline is now the closest comparison both in terms of FPS and optimizations applied. Both pipelines have batched inference, video frames decoded and processed on GPU end-to-end, and concurrency at the batch level (note the overlapping NVTX ranges below). One additional level of concurrency in the Python pipeline is multiple overlapping CUDA streams.</p>
<p>The Nsight Systems view shows processing for several 16-frame batches:</p>








<a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_hacked_ds_two_batch.png">
    <figure>
        <img src="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/images/ds_100pc_torchscript_hacked_ds_two_batch_hudc758a7e69d7157937e6a1d7caab6946_445767_896x520_fill_box_top_2.png" width="896" height="520">
        <figcaption><small></small></figcaption>
    </figure>
</a>

<p>We now have good GPU utilization and very few needless memory transfers, so the path forward is to optimize the TorchScript model. Until now the TensorRT component has been entirely pass-through and everything from preprocessing, inference and postprocessing has been in TorchScript.</p>
<p>It‚Äôs time to start using the TensorRT optimizer, so get ready for some excitement.</p>
<h2 id="stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript">
  Stage 3: Hacked DeepStream ‚Äî 80% TensorRT, 20% TorchScript
  <a href="#stage-3-hacked-deepstream-mdash-80-tensorrt-20-torchscript">#</a>
</h2>
<table>
<thead>
<tr>
<th>Code</th>
<th>Nsight Systems Trace</th>
<th>Gstreamer Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_trt_3.py">ds_trt_3.py</a>, <a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_tsc_3.py">ds_tsc_3.py</a>, <a href="https://github.com/pbridger/deepstream-video-pipeline/blob/master/ds_ssd300_3.py">ds_ssd300_3.py</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/logs/ds_3_1gpu_batch16_device.qdrep">ds_3_1gpu_batch16_device.qdrep</a></td>
<td><a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/logs/ds_3_1gpu_batch16_device.pipeline.dot.png">ds_3_1gpu_batch16_device.pipeline.dot.png</a></td>
</tr>
</tbody>
</table>
<p>According to Nvidia, TensorRT <a href="https://developer.nvidia.com/tensorrt">‚Äúdramatically accelerates deep learning inference performance‚Äù</a> so why not compile 100% of our model with TensorRT?</p>
<p>The Pytorch export to TensorRT consists of a couple of steps, and both provide an opportunity for incomplete support:</p>
<ol>
<li>Export the Pytorch model to the <a href="https://onnx.ai/">ONNX</a> interchange representation via <a href="https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting">tracing or scripting</a>.</li>
<li>Compile the ONNX representation into a TensorRT engine, the optimized form of the model.</li>
</ol>
<p>If you try to create an optimized TensorRT engine for this entire model (SSD300 including postprocessing), the first problem you will run into is the export to ONNX of the ‚Ä¶</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://paulbridger.com/posts/video-analytics-deepstream-pipeline/">https://paulbridger.com/posts/video-analytics-deepstream-pipeline/</a></em></p>]]>
            </description>
            <link>https://paulbridger.com/posts/video-analytics-deepstream-pipeline/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24817173</guid>
            <pubDate>Sun, 18 Oct 2020 11:54:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Samsung phones force Mainland China DNS service upon Hong Kong WiFi users]]>
            </title>
            <description>
<![CDATA[
Score 201 | Comments 51 (<a href="https://news.ycombinator.com/item?id=24816764">thread link</a>) | @signa11
<br/>
October 18, 2020 | http://blog.headuck.com/2020/10/12/samsung-phones-force-mainland-china-dns-service-upon-hong-kong-wifi-users/ | <a href="https://web.archive.org/web/*/http://blog.headuck.com/2020/10/12/samsung-phones-force-mainland-china-dns-service-upon-hong-kong-wifi-users/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-1387">

	
<!-- .entry-header -->

	<div>

		<div>

			<p>This is a technical write up of the author‚Äôs investigation on how users of Samsung phones in Hong Kong (and Macau), using firmware released in September 2020, would be forced to use a public DNS service in Mainland China, which caused unease and privacy concerns among some of its users.</p>
<p>While this was investigated on a variant of a Galaxy Note 10+ phone targetting the Hong Kong market, it was reported that the issue exists for a wide range of recent Samsung phones, including those sold in other places when used in Hong Kong.</p>
<p>(Update: The firmware update released in Mid-October 2020 has fixed the DNS issue discussed in this Part, but the issue of DNS queries for <code>qq.com</code> discussed in <a href="http://blog.headuck.com/2020/10/15/samsung-phones-force-mainland-china-dns-service-upon-hong-kong-wifi-users-2/">Part 2</a> remained unchanged.)</p>

<blockquote><p>Shameless plug (for Hong Kong users): The author is the developer of Headuck Call blocker (useful only in Hong Kong). Earlier it got mistakenly flagged as malware by Google and lost many of its users, but the appeal was successful and the App is <a href="https://play.google.com/store/apps/details?id=com.headuck.headuckblocker.dev">on Google Play</a> again. Please re-consider the App if you got scared by the earlier Google Play malware warning.</p></blockquote>
<h3>Background</h3>
<p>In early October 2020, a Samsung phone user in Hong Kong, local forum (HKEPC) user dingwinslow209, <a href="https://www.hkepc.com/forum/viewthread.php?fid=168&amp;tid=2586830">reported that (link to forum post in Chinese)</a> an extra DNS server entry, <code>114.114.114.114</code> was added to the DNS setting of his Samsung mobile phone, which was updated with the latest firmware, whenever he was using a WiFi connection.&nbsp; This happened both when the DNS setting is static, or dynamic using DHCP (but not when VPN / mobile network was used).&nbsp; Even when both DNS1 and DNS2 were set to valid DNS servers (e.g. Google public DNS service using <code>DNS1 = 8.8.8.8</code> and <code>DNS2 = 8.8.4.4</code>), a new DNS3 entry pointing to <code>114.114.114.114</code> would appear in some utility app.</p>
<p>This immediately raised privacy concerns among some Samsung users in Hong Kong, as the public DNS server <code>114.114.114.114</code> is owned by Cogent Communications, under <a href="https://www.whois.com/whois/114.114.114.114">Nanjing XinFeng Information Technologies Inc</a>, in Mainland China.</p>
<p>Subsequently, in another local forum (lihkg), users captured DNS requests to <code>114.114.114.114</code>, and observed queries for ‚Äú<code>qq.com</code>‚Äù&nbsp; (domain owned by Chinese tech giant Tencent), even when no software from Tencent is installed in the devices.&nbsp; There were reports that these DNS queries were sent once per minute, so long as the phone screen remained on.</p>
<p>There are further reports that when DNS queries to qq.com were blocked, the phone would report no internet connectivity via the WiFi connection.</p>
<p>The observation of the extra DNS entry was later independently confirmed by other forum users and the local media, using recent Samsung phones which have been updated in recent months.&nbsp; (See the links in the HKPEC post above, in Chinese).&nbsp; The issue persisted even when users reset their Samsung phone to factory settings. This showed that the issue originates from Samsung firmware instead of some third-party software malware.</p>
<p>The following documents the technical investigation and verification of the issue directly from analysing the code from firmware.&nbsp; The information should be sufficient for the issue and its extent to be independently verified.</p>
<h3>Getting and extracting the firmware</h3>
<p>To confirm the issue, a recent Samsung Galaxy Note 10+ firmware (SM-N9750 TGY, Hong Kong version), at Security Patch level 2020-09-01 was downloaded from one of the Samsung firmware download sites. Galaxy Note 10+ is one of the Samsung models reportedly&nbsp;affected.</p>
<p>After unzipping the downloaded firmware (in zip format), expanding the tar file beginning with ‚Äú<code>AP_N9750ZSU3CTH1</code>‚Äú, decompressing the file <code>system.img.ext4.lz4</code> using <code>lz4</code>, converting it to ext4 format using <code>simg2img</code>, and mounting it as ext4 volume under Linux, one has access to the system image which would be installed when Note 10+ users update their phone to that patch level. (The same is accessible if a Note 10+ phone is rooted).</p>
<p>After some research, the culprit was found ‚Äì a vendor specific service level component, added by Samsung to the Android framework, located at <code>/system/framework/wifi-service.jar</code>. This seems to work at the android system service level, supplementing the usual <code>services.jar</code>.</p>
<p>The decompiled source of the jar file (using <a href="http://www.javadecompilers.com/apk">this site</a>, with Jadx decompiler) has been uploaded to <a href="https://github.com/headuck/SM-N9750-TGY">https://github.com/headuck/SM-N9750-TGY</a>.&nbsp; The following is a walkthrough of the relevant code when a user connects to a WiFi network, showing how the DNS entries were modified.</p>
<h3>Relevant flow of DNS entry addition</h3>
<p>The main culprit is the class <code>com.android.server.wifi.WifiConnectivityMonitor</code>, located in the file <a href="https://github.com/headuck/SM-N9750-TGY/blob/main/com/android/server/wifi/WifiConnectivityMonitor.java">WifiConnectivityMonitor.java</a>. The decompiled code contains the following:</p>
<p>line 129: the hardcoded address <code>114.114.114.114</code></p>
<pre>private static final String CHN_PUBLIC_DNS_IP = "114.114.114.114";</pre>
<p>line 878: addresses used for DNS probe (to be covered in next part).</p>
<pre>public final String DEFAULT_URL = "http://www.google.com";
public final String DEFAULT_URL_CHINA = "http://www.qq.com";
public String DEFAULT_URL_STRING = "www.google.com";
public final String DEFAULT_URL_STRING_CHINA = "www.qq.com";</pre>
<p>This large class is mainly a state machine of the various WiFi states. The state hierarchy are defined, and initial state set, at lines 1185-1197.</p>
<pre>addState(this.mDefaultState);
addState(this.mNotConnectedState, this.mDefaultState);
addState(this.mConnectedState, this.mDefaultState);
addState(this.mCaptivePortalState, this.mConnectedState);
addState(this.mEvaluatedState, this.mConnectedState);
....
setInitialState(this.mNotConnectedState);</pre>
<p>The base class for the StateMachine can be found under AOSP source (<a href="https://cs.android.com/android/platform/superproject/+/master:frameworks/base/core/java/com/android/internal/util/StateMachine.java?q=StateMachine.java&amp;ss=android%2Fplatform%2Fsuperproject">StateMachine.java</a>).</p>
<p>When the device is connected to WiFi, it would enter <code>ConnectedState</code>.</p>
<p>line 1976 (under <code>processMessage()</code> of the initial <code>NotConnectedState</code>) would be invoked when a new WiFi connection is detected.</p>
<pre>wifiConnectivityMonitor.transitionTo(wifiConnectivityMonitor.mConnectedState);</pre>
<p>The <code>mConnectedState</code> variable is of class <code>ConnectedState</code>, defined from line 1988. The <code>enter()</code> method of <code>ConnectedState</code> contains the following code (from line 2090), which uses <code>CHN_PUBLIC_DNS_IP</code> (i.e. the Mainland Chinese controlled DNS server):</p>
<pre>if (WifiConnectivityMonitor.this.mWifiManager != null &amp;&amp; WifiConnectivityMonitor.this.inChinaNetwork()) {
    Message msg = new Message();
    msg.what = 330;
    Bundle args = new Bundle();
    args.putString("publicDnsServer", WifiConnectivityMonitor.CHN_PUBLIC_DNS_IP);
    msg.obj = args;
    WifiConnectivityMonitor.this.mWifiManager.callSECApi(msg);
}
</pre>
<p>From the code it seems to add the Mainland Chinese DNS service to the user‚Äôs list of DNS server automatically, when the device is connected to Chinese mobile network. There seems no option to disable the behaviour.</p>
<p><code>WifiConnectivityMonitor.inChinaNetwork()</code> is at line 11199.&nbsp; As suggested by its name, it should obtain the ISO code and return true only if the device is connected to a mobile network in China:</p>
<pre>public boolean inChinaNetwork() {
    String str = this.mCountryIso;
    if (str == null || str.length() != 2) {
        updateCountryIsoCode();
    }
    if (!isChineseIso(this.mCountryIso)) {
        return false;
    }
    if (!DBG) {
        return true;
    }
    Log.d(TAG, "Need to skip captive portal check. CISO: " + this.mCountryIso);
    return true;
}
</pre>
<p>Digging deeper, this is how the ISO code (<code>mCountryIso</code>) is obtained, under <code>updateCountryIsoCode()</code> at line 11219 (fallback skipped).</p>
<pre>public void updateCountryIsoCode() {
    if (this.mTelephonyManager == null) {
        try {
            this.mTelephonyManager = (TelephonyManager) this.mContext.getSystemService("phone");
        } catch (Exception e) {
            Log.e(TAG, "Exception occured at updateCountryIsoCode(), while retrieving Context.TELEPHONY_SERVICE");
        }
    }
    TelephonyManager telephonyManager = this.mTelephonyManager;
    if (telephonyManager != null) {
        this.mCountryIso = telephonyManager.getNetworkCountryIso();
        Log.i(TAG, "updateCountryIsoCode() via TelephonyManager : mCountryIso: " + this.mCountryIso);
    }
    /* fallback when there is no mobile network skipped. The fallback is to read the CountryISO setting from a Samsung config file (cscfeature.xml) */
    ....
}
</pre>
<p>(While not shown here, this code is also invoked when initializing and when change in ISO code of telephone network is detected, so it need not be called during each check.) The country code is get from <code>TelephonyManager.getNetworkCountryIso()</code> which is a standard Android API, documented <a href="https://developer.android.com/reference/android/telephony/TelephonyManager#getNetworkCountryIso()">here</a>. It returns the ISO-3166-1 alpha-2 country code equivalent of the MCC (Mobile Country Code) of the mobile operator. In Hong Kong, this is ‚ÄúHK‚Äù, and in Mainland China this is ‚ÄúCN‚Äù.</p>
<p>As one might suspect at this point, the problem lies in <code>isChineseIso()</code>, at line 11214:</p>
<pre>private boolean isChineseIso(String countryIso) {
    return "cn".equalsIgnoreCase(countryIso) || "hk".equalsIgnoreCase(countryIso) || "mo".equalsIgnoreCase(countryIso);
}</pre>
<p>This means that you are treated as being connected to a Chinese mobile network if you are connected to a Hong Kong mobile network for the purpose of adding the 114 DNS service.&nbsp; (BTW, ‚ÄúMO‚Äù is the ISO-3166-1 code for Macau.) Perhaps Samsung might want to address cases when people travel to Mainland China while forgetting to reset their hardcoded DNS settings, and kindly ‚Äúadd‚Äù a DNS service which works within the Great Firewall of China. But they seemed to forget that both Hong Kong and Macau are outside the Great Firewall, at least so far.</p>
<p>Back to the DNS setting code above (line 2090). It makes a binder call to <code>WifiManager.callSECApi()</code>, with message code = 330, with a <code>Bundle</code> setting <code>publicDnsServer</code> to our friend <code>114.114.114.114</code>. While <code>WifiManager</code> is a standard Android class the method <code>callSECApi()</code>, as suggested by its name, is Samsung specific.</p>
<p>The remote call to <code>WifiManager</code> would end up in the service class implementation at <code>com.android.server.wifi.WifiServiceImpl</code>, implementing WifiService, at <a href="https://github.com/headuck/SM-N9750-TGY/blob/main/com/android/server/wifi/WifiServiceImpl.java">WifiServiceImpl.java</a>. (The class is Samsung‚Äôs extension to the AOSP service class of the same ‚Ä¶</p></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.headuck.com/2020/10/12/samsung-phones-force-mainland-china-dns-service-upon-hong-kong-wifi-users/">http://blog.headuck.com/2020/10/12/samsung-phones-force-mainland-china-dns-service-upon-hong-kong-wifi-users/</a></em></p>]]>
            </description>
            <link>http://blog.headuck.com/2020/10/12/samsung-phones-force-mainland-china-dns-service-upon-hong-kong-wifi-users/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24816764</guid>
            <pubDate>Sun, 18 Oct 2020 10:20:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Audio‚Äôs opportunity and who will capture it]]>
            </title>
            <description>
<![CDATA[
Score 82 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24815888">thread link</a>) | @hunglee2
<br/>
October 17, 2020 | https://www.matthewball.vc/all/audiotech | <a href="https://web.archive.org/web/*/https://www.matthewball.vc/all/audiotech">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page" role="main">
        
          <article data-page-sections="5d8e94500fa50d2aaaa7c406" id="sections">
  
    <section data-section-id="5d8e94500fa50d2aaaa7c408" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;video&quot;: {
  &quot;playbackSpeed&quot;: 0.5,
  &quot;filter&quot;: 1,
  &quot;filterStrength&quot;: 0,
  &quot;zoom&quot;: 0
},
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;customSectionHeight&quot;: 10,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;customContentWidth&quot;: 50,
&quot;sectionTheme&quot;: &quot;&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-content-field="main-content" data-item-id="">
  <article id="article-">
  
    <div>
      

      <div>
        <div><div data-layout-label="Post Body" data-type="item" id="item-5f88a99f19d4cd6d64631928"><div><div><div data-block-type="5" id="block-yui_3_17_2_1_1602791840799_3755"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5d8e9007bc3d0e18a4c49673/1602792859307-U0J0NC5X6RECD3KT88YX/ke17ZwdGBToddI8pDm48kM4_kVKk9l_w74w-snZK7Fx7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0sfmLVeynSYYXBTMYB-wzcE4Rlu1L95vgCX6mg-kkKYPXXkbScjoF_1N2dt8jg_pvQ/Edison.png" data-image="https://images.squarespace-cdn.com/content/v1/5d8e9007bc3d0e18a4c49673/1602792859307-U0J0NC5X6RECD3KT88YX/ke17ZwdGBToddI8pDm48kM4_kVKk9l_w74w-snZK7Fx7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0sfmLVeynSYYXBTMYB-wzcE4Rlu1L95vgCX6mg-kkKYPXXkbScjoF_1N2dt8jg_pvQ/Edison.png" data-image-dimensions="2500x1456" data-image-focal-point="0.5,0.5" alt="Edison.png" data-load="false" data-image-id="5f88ad99ea51f67834f2495a" data-type="image" src="https://www.matthewball.vc/all/Edison.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-ff1f1677bf36496f03d8"><div><p>As most of the major media categories ‚Äî music, video and video games ‚Äî have existed for decades, we tend to forget that media is technology. Instead, we think of technology as being used to express media, rather than media itself. Spotify, for example, is an <em>internet</em> <em>streaming </em>music service, while iTunes is a <em>download </em>music service, SiriusXM is <em>satellite</em> <em>broadcast</em> music service, and radio is a <em>terrestrial broadcast</em> technology. This focus on delivery ignores the classic definition of media: ‚Äú<span>outlets</span> or <span>tools</span> used to <span>store</span> and <span>deliver</span> information or data.‚Äù</p><p>While the above might seem preoccupied with theory and philosophy, all analysis of the past and future of a given media category must start from the fact that media is technology. This is because technology not only enables content categories, it defines their business models and shapes the content, too. And as we know, technology is in a constant process of change. </p><p><strong>Chapter 1: How Technology Created Recorded Media, Then Continually Redefined It</strong></p><p>Music offers a great view into the interplay between technology, business model and content. Consider the following triptych, which covers seven decades, two decades and one year, respectively.&nbsp;</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1602797899091_5458"><div><p>&nbsp;When the flat record first emerged in the 1850s, it standardized around the 78. The 78 (as in 78 rotations per minute) came in a 10-inch version that held three minutes of music and a 12-inch version that held four. This meant that after centuries of variability, music suddenly had a defined run-time.&nbsp;</p><p>This length was reaffirmed by the first mass market standard for consumer media: the 45 RPM vinyl single, which launched in 1948 and held roughly three minutes. The music industry coalesced around this format (and its runtime) for a variety of tech-based reasons. The 45 was far cheaper for consumers than a 78 album, which was important given the high cost of record players and the ubiquity of free (singles-focused) radio. The 45‚Äôs cost advantage also meant it was the primary way labels delivered singles to thousands of radio stations across the country for local airplay. In addition, RCA quickly figured out how to make a stackable version of 45s, which was important to jukebox manufacturers. The rise of the 45 naturally led the length of the average song to decline; a four-minute song simply couldn‚Äôt fit on the most important audio format in the world.</p><p>As the physical and financial limitations of the 78 and 45 were relieved, and the far more flexible cassette and CD emerged, the length of the average single grew rapidly, adding nearly two minutes (or 78%) from 1959 to 1992. Still, almost all tracks conformed to the three-to-four minute standard. After decades, the West had become used to the idea that a song was roughly between three minutes and 20 seconds and four minutes and 10 seconds long.</p><p>On its surface, the shift to digital audio should have led to further increases in song length. After all, there was no longer any limitation to run-time. However, the reverse occurred. Technology might have relaxed its grip on music‚Äôs length, but it had strengthened its hold on business models.</p><p>As is well known, iTunes unbundled the physical album in individually downloadable (and bought) tracks. But in doing so, it penalized artists for bundling a multi-part song into a single track. Pink Floyd‚Äôs decision to split the 26-minute and nine-part Shine on You Crazy Diamond into two discrete tracks didn‚Äôt matter in 1975; all nine parts fit on a single record and no one wanted to buy just a single half let alone a single part. But in 2005, such a move could mean missing out on 75% of revenues ‚Äî why sell two things when you could sell nine? And why would a consumer buy an entire $10 album if all they wanted was two $1 portions of Shine on You Crazy Diamond? These incentives naturally led to artists that were publishing new music to split their longer/multi-section songs into separate ‚Äî and shorter ‚Äî preludes, interludes and segments.</p><p>This behaviour has been greatly exacerbated by the advent of a new and even more disruptive digital music technology: on-demand streaming. While iTunes was technically innovative, its business model was not. Consumers, after all, primarily owned copies of individual tracks in the 1950s and 1960s. Spotify and Apple Music, meanwhile, meant consumers adopted not just a new music technology, but also bought an entirely different product: ongoing access to all music ever created.</p><p>But as technology has shifted consumers away from discrete and attributable transactions (buying record A on date B) to ongoing and general ones (subscribing to service C in perpetuity), musical talent needed a new compensation model. Spotify, therefore, decided to pay talent as and to the degree consumers listened to their works. Matching revenue with usage is intuitive, but it was never before possible in music. There was no way to track at-home record spins or CD plays, let alone charge for them. Nor was it practical for iTunes to ask users to download an individual song to their devices and pay several pennies per play when they later synched their iPod to iTunes. (This would have been rife with abuse, too.)</p><p>Engagement-based monetization is arguably more fair. Consider, for example, that the Beatles‚Äô <em>Yesterday </em>and Psy‚Äôs <em>Gangnam Style</em> would each generate $1 when sold on iTunes, even if the former was played 2,000 times over ten years and the latter 30 times in the month it was bought and then never again. But the more that business models change, the more that incentives and content change, too.</p><p>To support engagement-based monetization, Spotify and its label suppliers had to define engagement. And they chose to do this on a per stream basis with a minimum stream time of 30 seconds (to avoid accidental plays, track skipping, etc.). However, this meant that a 10-minute track, five-minute track and 31-second track generated the same royalties.&nbsp;</p><p>So as the music industry has transitioned the majority of its revenues from CDs and downloads to streaming, major artists have relentlessly shortened and split their tracks. Why release a five-minute song if you can make it a two and a half-minute song that‚Äôs played twice? Or two different two and a half-minute songs? This meant artists had yet another reason to reduce track lengths</p><p>All of this helps to explain the extraordinary success of the 2019‚Äôs top track, <em>Old Town Road </em>by Lil Nas X, which is also Billboard‚Äôs longest running #1 ever, at 19 consecutive weeks. While the song is awesome, it‚Äôs also only one minute and 53 seconds ‚Äî roughly half of 2019‚Äôs average song length. This means that four minutes of listening generated two times the average revenue and charting lift of every other hit song that year.</p><p><em>Old Town Road </em>isn‚Äôt an exception, either. Up until 2017, Billboard‚Äôs Hot 100 Chart has never had a year with more than 2% of its charting tracks shorter than two minutes and 30 seconds (most years had none). In the past three years, this sum has skyrocketed to over 12%, or roughly one in every eight tracks.</p><p>Notably, labels are also encouraging artists to simplify the name of their songs and albums in order to ensure they‚Äôre optimized for voice-controlled speakers and touchscreen-based searches. A track with five words is more likely to be misunderstood or suffer from autocorrect than one with two. Similarly, voice assistants are known to struggle with <a href="https://www.wired.com/2017/03/voice-is-the-next-big-platform-unless-you-have-an-accent/">accents</a>, such as Irish or even Texan. Being hard to say means you might not get played.</p><p><em>Old Town Road </em>isn‚Äôt the first time technology made a hit. In fact, the modern day dominance of rap and R&amp;B comes from how changes in technology ‚Äì not for delivery, but sales recognition ‚Äì&nbsp; afforded Lil Nas X the opportunity to top the charts in the first place.</p><p>Prior to the 1990s, Black artists and music fans had spent decades arguing the record industry conspired against ‚Äúurban contemporary‚Äù music by refusing it radio play and ignoring its sales. It took only five weeks after Billboard adopted SoundScan, a computerized sales database, to prove this theory right.</p><p>Until 1991, Billboard charts weren‚Äôt based on actual unit sales or radio play. Instead, it was assembled using (white) retail clerk estimates of what was selling best and what (white) DJs considered to be ‚Äú<a href="https://www.washingtonpost.com/archive/lifestyle/1991/06/19/charting-soundscans-shake-up/3b8187fb-2332-4096-95e9-1f8db7b66b2e/">hottest</a>‚Äù each week. According to <em>The Atlantic</em>, both groups had<a href="https://www.washingtonpost.com/archive/lifestyle/1991/06/19/charting-soundscans-shake-up/3b8187fb-2332-4096-95e9-1f8db7b66b2e/"> reasons to lie</a>. For example, labels would pressure radio stations to favour ‚Äúhand-picked hits‚Äù if they wanted to keep receiving the newest single on time (stations sometimes<a href="https://en.wikipedia.org/wiki/Payola"> received bribes to play specific tracks</a>, too). Meanwhile, labels would force inventory on their retailers, who would then overreport sales to convince music fans to buy excess inventory.</p><p>Naturally, those who ran the music industry saw little need to overhaul how it worked. And thus while the book and film industries had shifted to computerized sales databases in the 1980s, not one of the top six record distributors signed onto SoundScan before its release in June 1991. But this resistance didn‚Äôt stop N.W.A.‚Äôs <em>N***az4life</em> from debuting #2 on the Billboard Top 100 the very next month under SoundScan. This was the highest charting performance in rap history ‚Äì and happened without any radio airplay, music video airings on MTV, or a concert tour. The failings of the old honour system were further demonstrated by the fact that N.W.A. debuted at only #21 on Billboard‚Äôs R&amp;B chart, which wasn‚Äôt yet on SoundScan. Somehow it was possible that <em>N***az4life</em> was the second biggest album in the country by units purchased, but 21st in its own genre when it came to what was ‚Äúselling‚Äù and ‚Äúhottest.‚Äù One week after it‚Äôs release, the album hit #1 on the Billboard chart (displacing R.E.M) as hundreds ‚Ä¶</p></div></div></div></div></div></div></div></div></article></div></div></div></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.matthewball.vc/all/audiotech">https://www.matthewball.vc/all/audiotech</a></em></p>]]>
            </description>
            <link>https://www.matthewball.vc/all/audiotech</link>
            <guid isPermaLink="false">hacker-news-small-sites-24815888</guid>
            <pubDate>Sun, 18 Oct 2020 06:22:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chaos Ink]]>
            </title>
            <description>
<![CDATA[
Score 381 | Comments 80 (<a href="https://news.ycombinator.com/item?id=24813921">thread link</a>) | @parisianka
<br/>
October 17, 2020 | https://dev.scottdarby.com/chaos-ink/ | <a href="https://web.archive.org/web/*/https://dev.scottdarby.com/chaos-ink/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://dev.scottdarby.com/chaos-ink/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24813921</guid>
            <pubDate>Sat, 17 Oct 2020 22:27:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We deleted the production database by accident]]>
            </title>
            <description>
<![CDATA[
Score 419 | Comments 406 (<a href="https://news.ycombinator.com/item?id=24813795">thread link</a>) | @caspii
<br/>
October 17, 2020 | https://keepthescore.co/blog/posts/deleting_the_production_database/ | <a href="https://web.archive.org/web/*/https://keepthescore.co/blog/posts/deleting_the_production_database/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

          


<article>
  <header>
    <h2 dir="auto"><a href="https://keepthescore.co/blog/posts/deleting_the_production_database/">We deleted the production database by accident üí•</a></h2>
    <p><time datetime="2020-10-17T00:00:00+02:00">Sat Oct 17, 2020</time> by Caspar</p>
  </header>
  

<p>Today at around 10:45pm CET, after a couple of glasses of red wine, we deleted the production database by accident üò®. Over 300.00 scoreboards and their associated data were vaporised in an instant.</p>

<p>Thankfully our database is a managed database from DigitalOcean, which means that DigitalOcean automatically do backups once a day. After 5 minutes of hand-wringing and panic, we took the website into maintenance mode and worked on restoring a backup. At around 11:15pm CET, 30 minutes after the disaster, we went back online, however 7 hours of scoreboard data was gone forever üòµ.</p>

<p>To be precise, any scoreboards created or scores added on the 17th October 2020 between 15:47 CET and 23:21 CET have been lost. We are extremely sorry about this.</p>

<p><img src="https://keepthescore.co/blog/disaster.jpg" alt="Blue green tennis"></p>

<h2 id="what-happened">What happened?</h2>

<p>It‚Äôs tempting to blame the disaster on the couple of glasses of red wine. However, the function that wiped the database was written whilst sober. It‚Äôs a function that deletes the local database and creates all the required tables from scratch. This evening, whilst doing some late evening coding, the function connected to the production database and wiped it. Why? This is something we‚Äôre still trying to figure out.</p>

<p>Here is the code that caused the disaster:</p>
<div><pre><code data-lang="python"><span>def</span> <span>database_model_create</span>():
    <span>"""Only works on localhost to prevent catastrophe"""</span>
    database <span>=</span> config<span>.</span>DevelopmentConfig<span>.</span>DB_DATABASE
    user <span>=</span> config<span>.</span>DevelopmentConfig<span>.</span>DB_USERNAME
    password <span>=</span> config<span>.</span>DevelopmentConfig<span>.</span>DB_PASSWORD
    port <span>=</span> config<span>.</span>DevelopmentConfig<span>.</span>DB_PORT
    local_db <span>=</span> PostgresqlDatabase(database<span>=</span>database, user<span>=</span>user, password<span>=</span>password, host<span>=</span><span>'localhost'</span>, port<span>=</span>port)
    local_db<span>.</span>drop_tables([Game, Player, Round, Score, Order])
    local_db<span>.</span>create_tables([Game, Player, Round, Score, Order])
    <span>print</span>(<span>'Initialized the local database.'</span>)</code></pre></div>
<p>Note that <code>host</code> is hardcoded to <code>localhost</code>. This means it should <strong>never connect to any machine other than the developer machine</strong>.  Also: <strong>of course</strong> we use different passwords and users for development and production. We‚Äôre too tired to figure it out right now.</p>

<h2 id="what-have-we-learned-why-won-t-this-happen-again">What have we learned? Why won‚Äôt this happen again?</h2>

<p>We‚Äôve learned that having a function that deletes your database is too dangerous to have lying around. The problem is, you can never really test the safety mechanisms properly, because testing it would mean pointing a gun at the production database.</p>

<p>We‚Äôve learned that having a backup which allows a quick recovery is absolutely essential. Thanks DigitalOcean, for making this part reliable and simple.</p>

<p>We‚Äôve learned that even a disaster can have some up-sides. This blog post generated a lot of interest. When life gives you citrus fruits, and so on.</p>

<p>The truth is, we can never be 100% sure that something like this won‚Äôt happen again. Computers are just too complex and there are days when the complexity gremlins win. However, we will figure out what went wrong and ensure that this <em>particular</em> error doesn‚Äôt happen again.</p>

<h2 id="some-perspective">Some perspective</h2>

<p>Thankfully nobody‚Äôs job is at risk due to this disaster. The founder is not going to fire the developer ‚Äì because they are one and the same person.</p>

<p>Also, this webapp is just a side-project. It‚Äôs not the software that‚Äôs running a power-plant. Nonetheless, we have many users, some of them paying customers, and we try our very best to make them happy. Today we let those users down and that hurts.</p>

<p>The wonderful irony is that not 4 days earlier we tweeted a <em>hilarious</em> meme about deleting your production database:
</p><blockquote><p lang="und" dir="ltr"><a href="https://t.co/mOlFqWal08">pic.twitter.com/mOlFqWal08</a></p>‚Äî Keepthescore.co (@keep_the_score) <a href="https://twitter.com/keep_the_score/status/1315552102299598851?ref_src=twsrc%5Etfw">October 12, 2020</a></blockquote>



<p>Again, we are very sorry. Good night.</p>

<p>PS This generated some <a href="https://news.ycombinator.com/item?id=24813795">great discussion on Hackernews</a>.</p>

<p><span>Photo by <a href="https://unsplash.com/@darmfield?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Dawn Armfield</a> on <a href="https://unsplash.com/s/photos/disaster?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span></p>


  

  
  <hr>
  
  

</article> 



        </div> <!-- /.blog-main -->

        


      </div> <!-- /.row -->
    </div></div>]]>
            </description>
            <link>https://keepthescore.co/blog/posts/deleting_the_production_database/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24813795</guid>
            <pubDate>Sat, 17 Oct 2020 22:09:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Themed days, Timeboxing and why you should use them]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 50 (<a href="https://news.ycombinator.com/item?id=24813440">thread link</a>) | @jamalx31
<br/>
October 17, 2020 | https://www.jamalx31.com/post/themed-days-timeboxing-and-why-you-should-use-them | <a href="https://web.archive.org/web/*/https://www.jamalx31.com/post/themed-days-timeboxing-and-why-you-should-use-them">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Have you ever wondered how Elon Musk is running two billion-dollar companies at once? Musk is an interesting example of someone who manages his time so well that he can work 100 hours a week and still manage to take time out for his hobbies, family, and even Twitter! So, how does he do it?<br></p><p>Musk is known for using timeboxing methods to manage his time effectively. He uses timeboxing from the moment he wakes up in the morning where he assigns each time block to a certain task that he needs to accomplish on a given day. This timeboxing method can be used for any tasks ranging from writing emails, scheduling meetings, meditating or anything that you would like to do on a particular day.<br></p><p>Before I began using timeboxing, I learned about another technique called <strong>Themed days</strong>. I decided to combine both to maximize productivity. Themed days are <strong>strategically planned days in your calendar, which are entirely dedicated to one single thing or tasks from the same category</strong>. While timeboxing is <strong>to allocate a certain amount of time to a task in advance and then complete it within that time frame</strong>. The idea behind these two techniques is to eliminate context switching or minimize it.<br></p><p>How to use them? Very simple.<br></p><p>Make a list of 4-5 things you want to focus on this week. For example:</p><ol role="list"><li>‚òéÔ∏è Talk to 5 customers</li><li>üíªÔ∏è Implement features X,Y &amp; Z</li><li>üé® Redesign landing page</li><li>‚úçÔ∏è Write a blog post about productivity<br></li></ol><p>These are going to be your ‚Äúthemes‚Äù. Now open your calendar and assign a day for each one. Let‚Äôs say you want to work Monday and Tuesday on coding the missing features. Use Wednesday to interview customers, etc, you got the idea. (Tip: give each theme a color or an emoji üòâÔ∏è)<br></p><p>Themed days could be super useful if you already have a day job and are trying to make time for your side hustle. Instead of trying to work on it one hour each day after your day job when you already feel exhausted, you dedicate a whole day for your side project.</p><p>With the one hour a day approach, the chance you could skip the scheduled hour is higher; <strong>with themed days, maintaining self-discipline is much easier.</strong></p><h2>Timeboxing</h2><p>To spice things up a little bit, you should use time timeboxing. Once you finished assigning themes and each day has one:</p><ol role="list"><li>Zoom in on each day</li><li>Add sub-tasks for that day‚Äôs theme</li><li>Add a time estimate to each task, and frame it with a start and end time.<br>‚Äç<br></li></ol><p>You want to avoid any tasks without a clear deadline, even if the deadline is not accurate. According to Parkinson‚Äôs Law, if you have more time to finish a task, the overall time that you will take to finish the task will also expand. This holds true for many people and timeboxing greatly helps in dealing with that as well.<br></p><p>It‚Äôs important that as much as possible in Step 3 above, you avoid the <strong>mental optimistic bias</strong> of underestimating how long the completion of a task would require. When we‚Äôre too optimistic about how long a given task is going to take, we fail to completely follow through on what we set out to do.This bias, can be avoided by simply using a <strong>feedback loop, </strong>where you're constantly thinking about what you've done and how you could be doing it better. At the end of each day or at least once a week, reflect on your progress and review the things you worked on that week and think to yourself how you could improve.&nbsp;<br></p><p>Timeboxing is amazing if you learn to do it effectively. In simple words, it is a process to merge your to-do list with your calendar. Another benefit of timeboxing is that it reduces the number of choices you‚Äôd have to make in any given moment ‚Äî boosting your willpower for peak productivity.</p><h2>Productivity as an indie hacker</h2><p>To maximize your productivity, you must set your priorities straight. For me as a developer, I find myself investing the majority of my time coding, but as an indie hacker who wants to build a profitable business, I must focus on other stuff too; marketing, promoting, writing, and talking to customers. Themed days made me realize that <a href="https://www.jamalx31.com/post/is-coding-a-procrastination-trap">coding could be a procrastination trap</a>.<br></p><p>Applying theme days helped me break out of the dev-hamster-wheel (aka coding) and forced me to do more of what needed to be done. So I decided to structure my themed days to look like this:<br></p><p><strong>Monday</strong>: Coding <br>‚Äç<strong>Tuesday</strong>: Coding<strong><br>Wednesday</strong>: Writing / Learning<strong><br>Thursday</strong>: Coding&nbsp;<strong><br>Friday</strong>: Housekeeping / Reflecting&nbsp;<strong><br>Saturday</strong>: Marketing / SEO&nbsp;<strong><br>Sunday</strong>: Writing&nbsp;</p><p>As you see, I only code three days a week, so I can focus on other equally or sometimes even more important things.</p><p>I also found themed days to be very effective when it comes to dealing with new or unexpected tasks. Depending on the task‚Äôs "theme", I know precisely on which day it should be scheduled. It takes me a fraction of the time to add it to my system.<br></p><h2>Themed Inbox</h2><p>I took themed days an extra step and applied it to my Gmail inbox. With a <a href="https://www.axllent.org/docs/gmail-snooze-script/">super cool trick using Google Apps Script</a>, I have control over when emails hit my inbox. Instead of checking my email everyday and having dozens of unrelated emails, now emails show up in my inbox on the right day. For example, all unimportant emails are scheduled for Friday (my Housekeeping day), while all marketing &amp; SEO related emails come in on Saturday, and so on. This way I make sure I read the right email at the right time when I‚Äôm ready to act on it. So cool, right?<br></p><figure id="w-node-01d3452ce5a9-4885bece"><p><img src="https://uploads-ssl.webflow.com/5eed2ff36d85f3277043492b/5f8b5b8eb994e7219ad5c7a6_aacavebgfxMyaKY5fTJeYgq5iGSUGD-5kW-_E5b-80Oo0IdiLDzEmok9W3WkT5_U-_woBP59Bk2TJg2xCFos_w3fsecvjQiGpCcArpMwuSTAgkbBQkGZUL0fY1dDm_AIU3Vs5NiL.png" alt=""></p><figcaption>The script automatically label emails</figcaption></figure><h2>Building a system: find a tool that works for you.</h2><p>When it comes to productivity in general, it‚Äôs important to have an efficient and flexible system that can adapt as your needs change without investing countless hours rebuilding everything from scratch.<br></p><p>Notion has been the core part of my system for the last five months. I‚Äôve been experimenting a lot with different productivity techniques and approaches and it would have been so hard to do that without a super flexible and elegant tool like Notion. It's my second brain and where I store everything, so I‚Äôm using it to implement themed days, with Notion build-in templates.<br></p><p>I have a database where I have designed a template for each day and theme. Every morning, after preparing my coffee, I create a new page for that day using the proper template. The new page gets populated with everything I need to be as productive as possible on that day.<br></p><p>For example, on Saturday, which is a marketing/SEO day, I have a page with all the resources and reminders related to that subject.<br></p><figure id="w-node-5a939054f819-4885bece"><p><img src="https://uploads-ssl.webflow.com/5eed2ff36d85f3277043492b/5f8b5b8e51775d3896dd2ca8_IR7wRnLd7_xn6THm_98T9fff1sIPhOhPUMI39ZsvQZckACxBvIC6BGD4oOj2ke_tcSEynYyETcdgfdGl_6kFQibk_no8hh-5oawKs_-YBQTp8GeLS8dstn5kCNhbzySBkC7BNe7X.png" alt="my Notion templates"></p><figcaption>Notion template for each day of the week</figcaption></figure><figure id="w-node-fb4ef460147c-4885bece"><p><img src="https://uploads-ssl.webflow.com/5eed2ff36d85f3277043492b/5f8b5b8e1cd4becfc1e46ca2_RaH23NZlbiaNZOdoL6Pg-aIbRYxrnuE3L3A2-ttTweWCPEe_vbzW6XUu0d-Ycspa372e7ZdciYXHQsFhw_3jadOX_Soqdvqd_ohmn1ikovesCXtAMPrdljxuWiv9Gt15cxeWtB9x.png" alt="Saturday template"></p><figcaption>Saturday's template, focus on marketing &amp;&nbsp;SEO</figcaption></figure><p>What I found really nice about this approach is that my templates are not static, and as I keep updating them, they keep evolving and improving.</p><h2>Next level: Themed weeks</h2><p>If you really want to level up your game and accelerate your progress even further, you can implement themed weeks.</p><p>One of my favourite indie makers, <a href="https://twitter.com/yongfook">Jon Yongfook</a>, has implemented themed weeks for his workflow. Jon works in a 2 weeks sprint, one week of coding, followed by one week of marking, and then wraps it up by writing a newsletter and changelog. When I asked him if he tried shorter sprints, he said that he did but found shorter sprints lead to too much context switching, and two weeks sprint works very well for him.<br></p><p>You can also use the same concept to devote the whole week to a single project, and really make huge advancements, or to a certain business function, and today, with all the rapid prototyping tools available, you could take a week to build an MVP/prototype and test your idea on the market.</p><h2>Give it a try and see for yourself.&nbsp;&nbsp;</h2><p>The best way to see what themed days can do for your productivity is merely trying it out. Here's your homework:</p><ul role="list"><li>Make a list of 3-4 categories/themes you want to focus on (e.g writing, coding, marketing)</li><li>Break them down to atomic tasks, ideally each sub-task should take one hour or less</li><li>Map each theme to a day of the week</li><li>If you can‚Äôt find time on working days, try to do it on a Saturday.</li><li>Open your calendar</li><li>Timebox the sub-tasks by scheduling them on their dedicated day</li><li>Get excited -message me to hold you accountable- and do it!&nbsp;<br></li></ul><p>I promise that afterwards, you will feel a sense of achievement and see the progress that can</p><p>be made with this approach. It‚Äôs time to make room in your calendar for what matters!</p><p>‚Äç<br></p></div></div>]]>
            </description>
            <link>https://www.jamalx31.com/post/themed-days-timeboxing-and-why-you-should-use-them</link>
            <guid isPermaLink="false">hacker-news-small-sites-24813440</guid>
            <pubDate>Sat, 17 Oct 2020 21:15:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Canadian government invests in small nuclear reactors to help meet net-zero 2050]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24811840">thread link</a>) | @goodcanadian
<br/>
October 17, 2020 | https://www.cbc.ca/news/politics/bains-small-modular-reactors-net-zero-1.5763762 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/politics/bains-small-modular-reactors-net-zero-1.5763762">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The federal government says it's investing $20 million in the nuclear industry to help Canada meet its target of net-zero greenhouse gas emissions by 2050.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5764372.1602800777!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/small-modular-reactor.jpg"></p></div><figcaption>An architectural rendering of Terrestrial Energy's Integral Molten Salt Reactor power plant that it expects to have in production by the end of the decade. <!-- --> <!-- -->(Terrestrial Energy)</figcaption></figure><p><span><p>The federal government says it's investing $20 million in the nuclear industry to help Canada meet its target of net-zero greenhouse gas emissions by 2050.</p>  <p>The investment in&nbsp;Oakville Ontario's Terrestrial Energy is meant to help the firm bring&nbsp;small modular nuclear reactors to market.</p>  <p>"By helping to bring these small reactors to market, we&nbsp;are supporting significant environmental and economic benefits, including generating energy with reduced&nbsp;emissions, highly skilled&nbsp;job creation and Canadian intellectual property development," said Innovation Minister&nbsp;Navdeep Bains&nbsp;in a media&nbsp;statement.</p>  <p>Small modular reactors ‚Äî&nbsp;SMRs ‚Äî&nbsp;are smaller than a conventional&nbsp;nuclear power plant and can be built in one location before being&nbsp;transported and&nbsp;assembled elsewhere.</p>  <p>Atomic Energy of Canada Limited says it sees three major uses for SMRs in Canada:</p>  <ul>   <li>Helping utilities replace energy capacity lost&nbsp;to closures&nbsp;of coal fired power plants.</li>   <li>Providing power and heat to off-grid industrial projects such as mines and oilsands developments.&nbsp;</li>   <li>Replacing diesel fuel as a source of energy and heat in remote communities.</li>  </ul>  <p>The reactor that Terrestrial Energy hopes to have in production by the end of the decade is an&nbsp;Integral Molten Salt Reactor. The company says the reactor&nbsp;can&nbsp;provide additional utility power and power for industrial projects.&nbsp;</p>  <p>The company says that the reactor can produce up to 195 megawatts ‚Äî enough to power a&nbsp;city the size of Regina ‚Äî&nbsp;likely&nbsp;making it too powerful for use in remote communities.</p>    <p>Bains said nuclear energy is&nbsp;part of the energy mix Canada&nbsp;must have&nbsp;to reach its climate targets.&nbsp;</p>  <p>Another part of that mix, Bains said, was the recently announced $590 million investment ‚Äî split evenly&nbsp;between the Ontario and federal governments ‚Äî to help the Ford Motor Company upgrade its assembly plant in Oakville&nbsp;and&nbsp;start making electric vehicles there.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5764415.1602802493!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/small-modular-reactor.jpg 300w,https://i.cbc.ca/1.5764415.1602802493!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/small-modular-reactor.jpg 460w,https://i.cbc.ca/1.5764415.1602802493!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/small-modular-reactor.jpg 620w,https://i.cbc.ca/1.5764415.1602802493!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/small-modular-reactor.jpg 780w,https://i.cbc.ca/1.5764415.1602802493!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/small-modular-reactor.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5764415.1602802493!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/small-modular-reactor.jpg"></p></div><figcaption> <!-- -->(CBC News)</figcaption></figure></span></p>  <h2>Recycling nuclear waste</h2>  <p>Natural Resources Minister&nbsp;Seamus O'Regan said&nbsp;the federal government is reviewing its radioactive waste program to ensure it adheres to the "highest international standards."</p>  <p>"We do have to make sure that Canadians trust the power system," O'Regan said. "SMR technology allows us to minimize the amount of waste and in some cases has the potential to recycle nuclear waste."</p>  <p>The federal government says that&nbsp;Terrestrial Energy has committed to creating and maintaining 186 jobs and creating 52 co-op placements nationally.</p>  <p>The government says the&nbsp;company also has&nbsp;promised to undertake gender equity and diversity initiatives to, among other things, boost the number of women&nbsp;working&nbsp;in science, technology, engineering and mathematics&nbsp;fields.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5187788.1561419324!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/how-a-nuclear-reactor-works.jpg 300w,https://i.cbc.ca/1.5187788.1561419324!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/how-a-nuclear-reactor-works.jpg 460w,https://i.cbc.ca/1.5187788.1561419324!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/how-a-nuclear-reactor-works.jpg 620w,https://i.cbc.ca/1.5187788.1561419324!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/how-a-nuclear-reactor-works.jpg 780w,https://i.cbc.ca/1.5187788.1561419324!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/how-a-nuclear-reactor-works.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5187788.1561419324!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/how-a-nuclear-reactor-works.jpg"></p></div><figcaption>How a nuclear reactor works<!-- --> <!-- -->(This diagram, which should be read clockwise from bottom left, shows how a nuclear reactor works.)</figcaption></figure></span></p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/politics/bains-small-modular-reactors-net-zero-1.5763762</link>
            <guid isPermaLink="false">hacker-news-small-sites-24811840</guid>
            <pubDate>Sat, 17 Oct 2020 18:01:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Banks help scammers with their bad UI]]>
            </title>
            <description>
<![CDATA[
Score 96 | Comments 76 (<a href="https://news.ycombinator.com/item?id=24810870">thread link</a>) | @ZainRiz
<br/>
October 17, 2020 | https://www.zainrizvi.io/blog/how-banks-help-scammers-with-their-bad-ui/ | <a href="https://web.archive.org/web/*/https://www.zainrizvi.io/blog/how-banks-help-scammers-with-their-bad-ui/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
			<!-- .cover -->


				<div>
					<p>My sister Sana just wanted to earn some money before returning to college. She was looking for a covid-friendly job on craigslist, something to cut down on the student loans she'd have to take out for the year. After searching and scrounging, Sana found someone who needed logistical help taking care of their pets. She applied and was excited to get hired.</p><p>First, they needed help buying pet food. They had ordered $1,800 worth of pet food (they bought high quality stuff in bulk apparently) but the seller would only accept payments via Zelle. Her employer was a bit old fashioned and didn't know how to use that service, so they offered a different arrangement: ‚ÄúI‚Äôll mail you a check for $2,000‚Äù they said, ‚Äúplease deposit it in your own bank account, and then use $1,800 of it to Zelle the payment to the sellers. The remaining $200 is your fee.‚Äù</p><p>Sana was a bit suspicious, but she deposited the check into her Chase bank account and it cleared a few days later. Chase‚Äôs site showed that the money had been deposited successfully and was available to use, no strings attached. </p><p>Seemed like it was legit. So she did the honest thing and Zelle'd the money &nbsp;to the seller.</p><p>Happy with the job well done, her "employer" sent her another $2,000 check to deposit and transfer. This person really liked their pets. The same process happened again, and her employer was happy with the job well done.</p><p>Two weeks later, Sana‚Äôs debit card stopped working. Confused, she checked her account to see what was wrong. </p><p>Her heart sank. </p><p>Chase showed it as overdrawn.</p><p>By more than three thousand dollars.</p><p>The checks had bounced, two weeks after Chase‚Äôs site had indicated they had ‚Äúcleared‚Äù. Before this whole thing had started, Sana had $300 in her bank account. Now she had negative $3,300.</p><p>She had just been scammed out of $3,600.</p><h2 id="what-happened">What happened?</h2><p>Turns out checks can bounce weeks after they're deposited. It can take that long for the bank to verify they‚Äôre legitimate.</p><p>Why does it take so long? Here‚Äôs a peek behind the scenes:</p><ol><li>You deposit your check, and it shows up as pending in your account.</li><li>Your bank sends the check to the check-writer‚Äôs bank to request the funds.</li><li>Their bank verifies if the check is legitimate and the account actually contains the funds. At small or international banks this step is often manual and could take <em>weeks</em>.</li><li>Their bank lets your bank know it's legit. That's when the check has "cleared".</li></ol><figure><img src="https://www.zainrizvi.io/content/images/2020/10/Check-lifecycle-large.jpeg" alt="" srcset="https://www.zainrizvi.io/content/images/size/w600/2020/10/Check-lifecycle-large.jpeg 600w, https://www.zainrizvi.io/content/images/size/w1000/2020/10/Check-lifecycle-large.jpeg 1000w, https://www.zainrizvi.io/content/images/size/w1600/2020/10/Check-lifecycle-large.jpeg 1600w, https://www.zainrizvi.io/content/images/size/w2400/2020/10/Check-lifecycle-large.jpeg 2400w" sizes="(min-width: 720px) 720px"><figcaption>The life cycle of a check</figcaption></figure><p>I skipped one step though: </p><p>It take can than more than <strong>six months</strong> for a check to clear, but banks remove that ‚Äúpending‚Äù status just a couple days after it's deposited and allow people to spend the money. This might sound a bit iffy, but it's actually a good thing. Most checks are legitimate and people might need the money quickly. But that assumption breaks down with fraudulent checks.</p><p>By removing the ‚Äúpending‚Äù status before fully verifying the check‚Äôs legitimacy, Chase gave the illusion that the check had already cleared. <em>This is what the scammers count on.</em> &nbsp;</p><p>Today there is <em>no way</em> for a Chase customer to be sure that a check has fully cleared. But unless you have the inside scoop on how banks work, or a finely honed sense of when someone is trying to swindle you, you won‚Äôt catch it.</p><p>When Sana called up Chase, the bank heard her out. But instead of stopping the scammers, their only concern was making sure she pays up the remaining $3,000 that they claimed she owed them.</p><p><em>Edit: Chase customer support had one clarification to make. I'd originally said it might take weeks for the check to clear, but they explained it could actually take more than six months. And you'll have no idea when that magic moment occurs.</em></p><!--kg-card-begin: html--><p><img src="https://www.zainrizvi.io/content/images/2020/10/IMG_1399.jpg" alt="" srcset="https://www.zainrizvi.io/content/images/size/w600/2020/10/IMG_1399.jpg 600w, https://www.zainrizvi.io/content/images/2020/10/IMG_1399.jpg 828w" sizes="(min-width: 525px) 525px"></p><!--kg-card-end: html--><h2 id="and-she-s-not-the-only-one-who-s-been-hit-by-this-scam-">And she's not the only one who's been hit by this scam.</h2><p>My friend David Vargas was caught by <a href="https://davidvargas.me/blog/i-got-scammed-out-of-5k/">pretty much the exact same scam</a>. He was sent checks which included his fee and payment that was supposed to be made to someone else. And they only accepted Zelle.</p><p>In fact, similar<a href="https://techcrunch.com/2018/02/16/zelle-users-are-finding-out-the-hard-way-theres-no-fraud-protection/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly9uZXdzLnljb21iaW5hdG9yLmNvbS8&amp;guce_referrer_sig=AQAAAEVQ_LYXhqL8UBL2vmnP9kphsW_QtokZbglthvs6MgR-ZvqTSF7pcgQMl05VmPjal00O9pBtatU8paO6pmmAYyt91oRIxVEX7L32zEu7v3WKlKOYDV-RVDEHjYe3Co_ugzyWZPRq3oFD-D2giDL1H-Dvj7jmdSbg6NLoAjBnnyWt"> scams</a> <a href="https://news.cardnotpresent.com/news/zelle-scam-emerging">have</a> <a href="https://chicago.cbslocal.com/2019/11/19/how-to-avoid-zelle-scams-and-fraud/">been</a> <a href="https://medium.com/@eirurueta/i-was-scammed-by-using-zelle-for-ethereum-crypto-online-2fa3a38d1817">going on</a> <a href="https://www.wsj.com/articles/you-accidentally-venmoed-149-to-a-stranger-good-luck-getting-it-back-1531411133">for years</a>, yet most banks don‚Äôt seem to care as long as they're not the ones losing money.</p><h2 id="what-should-chase-have-done-here">What should Chase have done here?</h2><h3 id="-1-take-responsibility-their-system-is-broken">#1 Take responsibility: their system is broken</h3><p>Is Chase bank directly responsible for scamming people? Of course not. Did they create an environment where scammers could thrive? Absolutely.</p><p>The harm was unintentional, but they played a hand in it. </p><p>Involuntary manslaughter.</p><p>What have other companies done when someone abused their products to deliberately cause harm? They took responsibility for it and fixed the problem at a systematic level.</p><p><strong>The Tylenol murders: </strong>In 1982, someone started <a href="https://www.pbs.org/newshour/health/tylenol-murders-1982">spiking bottles of Tylenol</a> with poison, killing seven people. This was clearly not Tylenol‚Äôs fault, but they still immediately paid to recalled all bottles in the stores, offered anyone who had already purchased the pills free replacements. They took it a step further and <strong>developed tamper-proof packaging </strong>to prevent anyone from contaminating the medicine again in the future.</p><p><strong>Credit Card fraud services</strong>: Online payments are inherently risky. Credit card companies know this, and they‚Äôve committed to helping their customers when the inevitable fraud occurs. If you pay someone with your Discover card and the product turns out to be a scam, <em>Discover will accept responsibility and refund you the money</em>, taking a loss if necessary. This dynamic keeps Discover on their toes working to prevent scams before they even happen.</p><h3 id="-2-how-should-they-fix-it">#2 How should they fix it?</h3><p>Make the status of the deposited check abundantly clear. This is how Chase can tamper-proof their bottles.</p><p>If a check has been deposited but not fully cleared, if it hasn't had the actual funds transferred over to the bank, then Chase needs to make that fact clear to customers! This missing piece of UX is what scammers depend on. That‚Äôs the systemic flaw here.</p><p>If the bank actually made the status of the funds clear to its customers, then it would have a much stronger leg to stand on when claiming innocence.</p><p>But as things stand right now, the only way a person could know the check was still at risk is if they already knew about the long clearing time and how banks hide it. Before the results come back from the external bank, Chase itself doesn‚Äôt know if the check is valid. Yet they offer customers no hint of that uncertainty.</p><p>And instead of taking responsibility, what did Chase actually do?</p><h3 id="chase-took-option-3-shakedown-their-customers">Chase took option #3: Shakedown their customers</h3><figure><img src="https://www.zainrizvi.io/content/images/2020/09/Ya340FffaeszEwIQ0ZPdxC9WQbRbN4kGKmcgTWiXLUiifzyqYvwzQSr4nwR3-jR--TS_Z-WrSDyVCcx4G37knBt171ds1zqYgfJ9hB4m-rUHKsDrCMG_N_2u6uTVCOreTRlc32iw.png" alt="" srcset="https://www.zainrizvi.io/content/images/size/w600/2020/09/Ya340FffaeszEwIQ0ZPdxC9WQbRbN4kGKmcgTWiXLUiifzyqYvwzQSr4nwR3-jR--TS_Z-WrSDyVCcx4G37knBt171ds1zqYgfJ9hB4m-rUHKsDrCMG_N_2u6uTVCOreTRlc32iw.png 600w, https://www.zainrizvi.io/content/images/size/w1000/2020/09/Ya340FffaeszEwIQ0ZPdxC9WQbRbN4kGKmcgTWiXLUiifzyqYvwzQSr4nwR3-jR--TS_Z-WrSDyVCcx4G37knBt171ds1zqYgfJ9hB4m-rUHKsDrCMG_N_2u6uTVCOreTRlc32iw.png 1000w, https://www.zainrizvi.io/content/images/2020/09/Ya340FffaeszEwIQ0ZPdxC9WQbRbN4kGKmcgTWiXLUiifzyqYvwzQSr4nwR3-jR--TS_Z-WrSDyVCcx4G37knBt171ds1zqYgfJ9hB4m-rUHKsDrCMG_N_2u6uTVCOreTRlc32iw.png 1192w" sizes="(min-width: 720px) 720px"></figure><p>Chase acted like the mafia, shaking down whoever they could to get their money back.</p><p>My sister had created this bank account back when she was in high school. For minors, Chase requires a guarantor, someone who would make sure her debts are paid. My dad had listed himself, he was also a Chase customer.</p><p>And now Chase came after him for the remaining three grand.</p><p>He spent hours on the phone with Chase‚Äôs customer support trying to get the scam resolved. The reps may have been empathetic, they weren‚Äôt empowered to help.</p><p>From the bank's perspective, the money they had already pulled from Sana's account was theirs. Sorry, no negotiation possible. The computer won't let us. And Chase did their darned best to not have to take a hair cut on the rest of the amount they enabled scammers to steal.</p><p>The most those reps could share was to suggest my dad could try not paying the remaining three thousand and wait until the debt goes to collections. And then pray that the collections department has more leniency. It might hurt his credit rating though, they warned. How much? They didn‚Äôt know. &nbsp;The account was scheduled to go to collections at the end of September.</p><p>But a month before that date, my dad noticed his own account was now three thousand dollars lighter. Chase had just helped itself to those funds without warning.</p><p>And now that Chase is no longer losing money on this scam, customer support tells him ‚ÄúSorry, there‚Äôs nothing we can do.‚Äù</p><p>With every step Chase made sure it got paid, one way or another. Vito Corleone would be proud.</p><figure><img src="https://www.zainrizvi.io/content/images/2020/09/image-3-1.png" alt="" srcset="https://www.zainrizvi.io/content/images/size/w600/2020/09/image-3-1.png 600w, https://www.zainrizvi.io/content/images/2020/09/image-3-1.png 669w"></figure><p>And if the bank has no skin in the game, why worry about fixing some misleading UI? They seem to think people don‚Äôt bother switching banks, and that it can treat its customers as captive users.</p><h2 id="how-widespread-must-this-be">How widespread must this be?</h2><p>When a person gets scammed they‚Äôre usually hesitant to speak up about it. </p><p>They're ashamed of having been duped, afraid of being scorned for their foolishness, and so their story rarely gets told. </p><p>Yet I know two people who‚Äôve admitted to being hurt by this scam. What does that imply about the scale of this problem?</p><p>And Chase allows it to continue happening, punishing their customers for not having intimate knowledge of how banking works.</p><h2 id="is-your-blood-boiling-yet">Is your blood boiling yet?</h2><p>I get furious every time I think about it. David Vargas tried to pass his situation off as his own fault, but I was outraged on his behalf.</p><p>I want everyone at Chase to see this story. I want every other bank that‚Äôs enabling scammers to see this. And I want them to fix their system to let people know when shady checks have not yet been deposited.</p><p>When I showed this article to a friend, he got restless, he wanted to take action. ‚ÄúHow can I help?‚Äù he asked, ‚ÄúTell me what to do!‚Äù At the time I wasn‚Äôt sure what could be done, but there is one thing:</p><p><strong>My ask to you:</strong> Can you help spread the word and make Chase notice this problem? Maybe the internet outrage machine can turn this into a priority and protect the thousands of other people who are being scammed by this.</p><p>You could share this article (<a href="https://twitter.com/ZainRzv/status/1311701855148236800">retweet the Tweet</a>, or post on FB, Reddit, HN, whatever your usual channels are). Get this shared widely. And maybe, just maybe, some executive at Chase will notice and take the lead in transforming the banking industry. My sincerest thank you to everyone who helps.</p><p>So far, Chase hasn‚Äôt been interested in accepting an ounce of responsibility for their part in this scheme. Why bother fixing the system when you can just pull money from your customer‚Äôs bank accounts? But maybe they can change. Who wants to be known as an unwitting accomplice to scammers?</p><p>And even if they don't change, remember Chase's missteps and ‚Ä¶</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.zainrizvi.io/blog/how-banks-help-scammers-with-their-bad-ui/">https://www.zainrizvi.io/blog/how-banks-help-scammers-with-their-bad-ui/</a></em></p>]]>
            </description>
            <link>https://www.zainrizvi.io/blog/how-banks-help-scammers-with-their-bad-ui/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24810870</guid>
            <pubDate>Sat, 17 Oct 2020 15:52:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lisp from Nothing]]>
            </title>
            <description>
<![CDATA[
Score 175 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24809293">thread link</a>) | @nils-m-holm
<br/>
October 17, 2020 | http://t3x.org/lfn/index.html | <a href="https://web.archive.org/web/*/http://t3x.org/lfn/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Lulu Press, 2020 - 301 pages - 17 figures - 6"&nbsp;x&nbsp;9" format - <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a> code</p></div><p>
This text plays with the theme of minimal LISP by providing
several implementations from a simple metacircular evaluator to
a full compiler that emits a single, self-contained C program.
The discussion is embedded in reflections on what hacking looked
like in the early days of LISP.
</p></div>]]>
            </description>
            <link>http://t3x.org/lfn/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24809293</guid>
            <pubDate>Sat, 17 Oct 2020 11:48:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Clean Text Data at the Command Line]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 45 (<a href="https://news.ycombinator.com/item?id=24808603">thread link</a>) | @ethink
<br/>
October 17, 2020 | https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line | <a href="https://web.archive.org/web/*/https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><figure><p><img src="https://uploads-ssl.webflow.com/5f359c073455c743bc873ee4/5f875bfccc04b8153e57b1c2_cleaning-data.jpg" loading="lazy" alt=""></p><figcaption>Photo by <a href="https://unsplash.com/@jeshoots?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">JESHOOTS.COM</a> on <a href="https://unsplash.com/s/photos/cleaning-kitchen?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><p>Cleaning data is like cleaning the walls in your house, you clear any scribble, remove the dust, and filter out what is unnecessary that makes your walls ugly and get rid of it. The same thing happens when cleaning your data, it‚Äôs filtering what we want and removing what we don‚Äôt want to make the raw data useful and not raw anymore. You can do the cleaning with Python, R, or whatever language you prefer but in this tutorial, I‚Äôm going to explain how you can clean your text files at the command line &nbsp;files by giving insights from a paper researching clickbait and non-clickbait data.</p><p>This tutorial is mainly motivated by <a href="https://amzn.to/33y5EkR">Data Science at the Command Line</a></p><h6>Disclosure: <em>The two Amazon links for the book (in this section) are paid links so if you buy the book, I will have a small commission</em></h6><p>This book tries to catch your attention on the ability of the command line when you do data science tasks - meaning you can obtain your data, manipulate it, explore it, and make your prediction on it using the command line. If you are a data scientist, aspiring to be, or want to know more about it, I&nbsp;highly recommend this book. You can read it online for free from <a href="https://www.datascienceatthecommandline.com/">its website</a> or order an <a href="https://amzn.to/33y5EkR">ebook or paperback</a>.&nbsp;In this tutorial we're gonna focus on using the command line to clean our data.</p><h2>Pulling and running the docker image</h2><p>To remove the hassle of downloading files we deal with and dependencies we need, I've made a docker image for you that has all you need. You just pull it from docker hub and you'll find what you need to play with to let you focus on the cleaning part.&nbsp;So let's<a href="https://hub.docker.com/r/ezzeddin/clean-data"> pull that image</a> and then run it interactively to enter the shell and write some command-lines.</p><div><pre>$ docker pull ezzeddin/clean-data
$ docker run --rm -it ezzeddin/clean-data
</pre></div><ul role="list"><li><strong>docker run </strong>is a command to run the docker image</li><li>the option <strong>--rm </strong>is set to remove the container after it exists</li><li>the option <strong>-it</strong> which is a combination of <strong>-i </strong>and <strong>-t </strong>is set for an interactive process (shell)</li><li><strong>ezzeddin/clean-data </strong>is the docker image name</li></ul><p>If using docker is still unclear for you, you can see <a href="https://www.ezzeddinabdullah.com/posts/penguins-in-docker-a-tutorial-on-why-we-use-docker">why we use docker tutorial</a></p><h2>Cleaning text files</h2><p>Let's clean two text files containing <a href="https://github.com/bhargaviparanjape/clickbait/tree/master/dataset">clickbait and non clickbait</a> headlines for 16,000 articles each.&nbsp;This data is used from a paper titled: <a href="https://people.mpi-sws.org/~achakrab/papers/chakraborty_clickbait_asonam16.pdf"><strong>Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media</strong></a> at <em>2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). </em>Our goal here is to get the most common words used in both clickbait and non-clickbait headlines.</p><p>If you list what's inside the container you'll see two text files called <em>clickbait_data </em>and <em>non_clickbait_data</em>. Let's see what the final output first that we want to get. For the clickbait data, we want the most common 20 words to be represented like this with their counts:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f359c073455c743bc873ee4/5f89e590d4bb2d5e8b98f68d_clickbait_20_words.png" loading="lazy" alt=""></p><figcaption>Image by the&nbsp;Author</figcaption></figure><p>And for the most 20 common words of non-clickbait headlines:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f359c073455c743bc873ee4/5f89e5ff1f4c25700461fbc8_non_clickbait_20_words.png" loading="lazy" alt=""></p><figcaption>Image by the Author</figcaption></figure><p>Let's see how we can get these histograms through the command line by taking it step by step. After running the docker image, we're now in a new shell with the new environment Let's first see what the <em>clickbait_data </em>file<em> </em>has by getting the first 10 lines of it:</p><p>So it seems this file has headlines that are labeled as clickbait as you can see:</p><div><pre>Should I Get Bings

Which TV Female Friend Group Do You Belong In

The New "Star Wars: The Force Awakens" Trailer Is Here To Give You Chills

This Vine Of New York On "Celebrity Big Brother" Is Fucking Perfect

A Couple Did A Stunning Photo Shoot With Their Baby After Learning She Had An Inoperable Brain Tumor

</pre></div><p>And if you use <strong>head</strong> for getting the first lines of <em>non_clickbait_data </em>you'll find:</p><div><pre>Bill Changing Credit Card Rules Is Sent to Obama With Gun Measure Included
In Hollywood, the Easy-Money Generation Toughens Up
1700 runners still unaccounted for in UK's Lake District following flood

Yankees Pitchers Trade Fielding Drills for Putting Practice
Large earthquake rattles Indonesia; Seventh in two days

Coldplay's new album hits stores worldwide this week

U.N. Leader Presses Sri Lanka on Speeding Relief to War Refugees in Camps
</pre></div><p>We're interested in words here not phrases, so we can get words starting from 3 letters to more with:</p><div><pre>$ head clickbait_data | grep -oE '\w{3,}'
</pre></div><p><strong>head clickbait_data </strong>is used here because we're doing statistics here on the couple of headlines at the top of the file which is piped to the next grep command <strong>grep -oE '\w{3,}'</strong></p><p><strong>grep </strong></p><p><strong> &nbsp;&nbsp;&nbsp;-oE -o </strong>for getting only matching words and <strong>-E </strong>for using extended regular expression which is the next pattern</p><p><strong>&nbsp;&nbsp;&nbsp;&nbsp;'\w{3,}' </strong>this pattern is like <strong>'\w\w\w+' </strong>which matches whole words with 3 letters or more</p><p>In order to get the counts of each word we need to first get the unique words which we can get by <strong>uniq </strong>command with the option <strong>-c </strong>to give you counts, but to let <strong>uniq &nbsp;</strong>delete duplicate words you need to sort first:</p><div><pre>$ head clickbait_data | grep -oE '\w{3,}' | sort | uniq -c
</pre></div><p>This command is done on the first 10 lines, let's do it across the entire clickbait headlines:</p><div><pre>$ cat clickbait_data | grep -oE '\w{3,}' | sort | uniq -c | sort -nr | head
</pre></div><ul role="list"><li><strong>cat clickbait_data | grep -oE '\w{3,}' | sort | uniq -c</strong> we're now putting this command (to get all the words across the clickbait data)&nbsp;into the standard input of the next command</li><li><strong>sort -nr</strong> to sort numerically in a reverse order to get the highest count first</li><li><strong>head </strong>to get the first 10 common words</li></ul><p>Here is the output of the previous command:</p><div><pre>   5538 You
   4983 The
   2538 Your
   1945 That
   1942 Are
   1812 This
   1645 And
   1396 For
   1326 What
   1242 Will
</pre></div><p>Looks like we're close now to be in a good shape, let's see what we can do to better clean that up.</p><p>If we get a deeper look</p><figure><p><img src="https://uploads-ssl.webflow.com/5f359c073455c743bc873ee4/5f89fb80aeaee61427802c66_deeper-look.jpg" loading="lazy" alt=""></p><figcaption>Photo by <a href="https://unsplash.com/@dtravisphd?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">David Travis</a> on <a href="https://unsplash.com/s/photos/glasses?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></figcaption></figure><p>We can see we're missing small letters and all caps letters. For example, for the 'You' word we're missing 'you' and we're also missing 'YOU'. Let's try to see if these words already exist:</p><div><pre>$ cat clickbait_data | grep -oE '\w{3,}' | sort | uniq -c | sort -nr | grep you
$ cat clickbait_data | grep -oE '\w{3,}' | sort | uniq -c | sort -nr | grep YOU
</pre></div><p>So as we can see:</p><p>We're missing 2 words that each can contribute to our counts to the 'You' occurrence and 'Your' to make them 5540 and 2540 respectively.</p><p>What we need to do first is to convert each capital letter into small ones using <strong>tr </strong>which is a command-line utility that translates characters:</p><div><pre>$ cat clickbait_data | tr '[:upper:]' '[:lower:]'| grep -oE '\w{3,}' \
| sort | uniq -c | sort -nr | head
</pre></div><p><strong>tr '[:upper:]' '[:lower:]' </strong>here translates the contents of <em>clickbait_data </em>into lower-case. <strong>['upper'] </strong>is a character class that represents all upper case characters and <strong>['lower'] </strong>is a character class that represents all lower case characters.</p><p>To prepend these values with the header, we can use <strong>sed </strong>to put two column names to represent each column:</p><div><pre>$ cat clickbait_data | tr '[:upper:]' '[:lower:]'| grep -oE '\w{3,}' \
| sort | uniq -c | sort -nr | sed '1i count,word' | head 
</pre></div><p><strong>sed '1i count,word' </strong>so we‚Äôre having <em>count</em> representing the number of occurrences and <em>word</em> obviously representing the word</p><p><strong>1i </strong>is used here to write these two words at the first line and the change in the file will be in-place</p><p>outputting:</p><div><pre>count,word 
5540 you
4992 the
2540 your
1950 that
1944 are
1812 this
1653 and
1397 for
1326 what
</pre></div><p>In order to print that in a pretty shape we can use <strong>csvlook </strong>which will get us this:</p><div><pre>| count        | word |
| ------------ | ---- |
|    5540 you  |      |
|    4992 the  |      |
|    2540 your |      |
|    1950 that |      |
|    1944 are  |      |
|    1812 this |      |
|    1653 and  |      |
|    1397 for  |      |
|    1326 what |      |
</pre></div><p>which is not pretty at all. The reason this happened is that <strong>csvlook </strong>works as its name indicates to a better look for a CSV&nbsp;file so we should have a CSV (Comma Separated&nbsp;Value)&nbsp;file first. We should then find a way to separate each value at each line with a comma. At this point, we can use <strong>awk </strong>which is a pattern-directed scanning and processing language:</p><div><pre>$ cat clickbait_data | tr '[:upper:]' '[:lower:]'| grep -oE '\w{3,}' \
| sort | uniq -c | sort -nr | awk '{print $1","$2}' | sed '1i count,word' | head | csvlook
</pre></div><p><strong>awk '{print $1","$2}' </strong></p><p><strong>'{</strong></p><p><strong>&nbsp;&nbsp;&nbsp;print $1 </strong>here prints the first field (which is the count column) followed by...</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>"," </strong>a comma followed by...</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>$2 </strong>the second field which is the word column</p><p><strong>}'</strong></p><p>It seems we're in much better shape now:</p><div><pre>| count | word |
| ----- | ---- |
| 5,540 | you  |
| 4,992 | the  |
| 2,540 | your |
| 1,950 | that |
| 1,944 | are  |
| 1,812 | this |
| 1,653 | and  |
| 1,397 | for  |
| 1,326 | what |
</pre></div><p>If we want to get the word column in the first field and the count column in the second field, we just need to reverse the order in the <strong>awk </strong>and <strong>sed </strong>command:</p><div><pre>$ cat clickbait_data | tr '[:upper:]' '[:lower:]'| grep -oE '\w{3,}' \
| sort | uniq -c | sort -nr | awk '{print $2","$1}' | sed '1i word,count' | head | csvlook
</pre></div><p>In order to get the same output for the non-clickbait data, we just need to change the filename:</p><div><pre>$ cat non_clickbait_data | tr '[:upper:]' '[:lower:]'| grep -oE '\w{3,}' \
| sort | uniq -c | sort -nr | awk '{print $2","$1}' | sed '1i word,count' | head | csvlook
</pre></div><h2>Getting insight into the clickbait study</h2><p>In this study as reported by the <a href="https://people.mpi-sws.org/~achakrab/papers/chakraborty_clickbait_asonam16.pdf">paper</a>, it addresses the clickbait and non-clickbait headlines to be able to detect both as</p><blockquote>Most of the online news media outlets rely heavily on the revenues generated from the clicks made by their readers, and due to the presence of numerous such outlets, they need to compete with each other for reader attention.</blockquote><p>So what's in this tutorial is a way to clean up data through the command-line so that we can get some insights about the result of this paper and see if we can get some points claimed by the paper through their research.</p><p>Again, let's see the final distribution of the most 20 common words for clickbait headlines for this data is:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f359c073455c743bc873ee4/5f89e590d4bb2d5e8b98f68d_clickbait_20_words.png" loading="lazy" alt=""></p><figcaption>Image by the Author</figcaption></figure><p>we can obviously see the excessive use of the possessive case <em>you </em>and not using the third person references like <em>he, she, </em>or a specific name</p><p>and ‚Ä¶</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line">https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line</a></em></p>]]>
            </description>
            <link>https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line</link>
            <guid isPermaLink="false">hacker-news-small-sites-24808603</guid>
            <pubDate>Sat, 17 Oct 2020 08:57:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a 10BASE5 ‚ÄúThick Ethernet‚Äù Network (2012)]]>
            </title>
            <description>
<![CDATA[
Score 66 | Comments 23 (<a href="https://news.ycombinator.com/item?id=24807542">thread link</a>) | @RicardoLuis0
<br/>
October 16, 2020 | http://tech.mattmillman.com/projects/10base5/ | <a href="https://web.archive.org/web/*/http://tech.mattmillman.com/projects/10base5/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>I can hardly claim to have worked professionally with 10BASE5 as when I first started building my own 10BASE2 networks in 1997, 10BASE5 was long obsolete. Back then, I knew about 10BASE5, but it wasn‚Äôt practical, affordable or interesting enough for me to attempt building one.</p>
<p>Fast forward to 2012, and 10BASE5 is now truly a vintage technology. Anyone studying something&nbsp;I.T. related likely will at some point have been told about this stuff, because it‚Äôs very important in the history of computing. This was the first standardised, commercially used form of Ethernet, and today, almost the entire Internet is Ethernet, but heck, who‚Äôs ever actually seen a working 10BASE5 setup? Not me, that‚Äôs for sure, nor or anyone I‚Äôve ever met.</p>
<p>So‚Ä¶ can I build a working setup in 2012? Read on‚Ä¶</p>

<p>I‚Äôve written this page primarily for people like myself, who came into the computing scene just a bit too late to have ever seen this technology in use. A 1980s network engineer can safely skip this page.</p>

<p>In order to connect to a true 10BASE5 network, network adapters with AUI are required. AUI stands for Attachment Unit Interface. It is in effect, a media independent form of all of the physical 10Mbit ethernet variants, which I shall call 10BASE-X. AUI was to 10BASE-X what MII is to 100BASE-X, with the exception that MII is rarely found as an external connector.</p>
<p>Unfortunately for this project, networking equipment with external AUI connectors is becoming rather difficult to find.</p>
<p>As I see it, there‚Äôs 2 options:</p>
<ul>
<li>Build true 10BASE5 nodes using equipment natively supporting AUI</li>
<li>Bridge to something else i.e. 10BASE-T</li>
</ul>
<h2>Building true 10BASE5 nodes</h2>
<p>Not many options remain for connecting to modern PCs. Here‚Äôs a few:</p>
<h3>ISA Cards</h3>
<p>While this can be done, modern mainboards with ISA slots are niche and very expensive. I ruled out this option.</p>
<p><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c509.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c509.jpg" alt="3c509" width="717" height="453" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c509.jpg 717w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c509-150x95.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c509-300x190.jpg 300w" sizes="(max-width: 717px) 100vw, 717px"></a></p>

<h3>PCI Cards</h3>
<p>Fortunately, some modern PCs still have 32-Bit/5 volt parallel PCI slots, also, PCI and 10BASE5 had enough overlap that a small number of products were manufactured. A couple of examples are the 3Com 3C900B-COMBO, and 3C905B-COMBO. At the time of writing there were a fair few of these on eBay, but often for outrageous sums of money. Fortunately I managed to pick up a couple for a few quid. 64-Bit Windows drivers do not exist, making Windows 7 32-bit likely the last version of Windows able to make use of them, short of writing some new drivers. All versions of Linux will work OK with these.</p>
<figure id="attachment_1354" aria-describedby="caption-attachment-1354"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb.jpg" alt="3Com 3c900-COMBO NIC" width="2575" height="1615" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb.jpg 2575w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb-150x94.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb-300x188.jpg 300w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb-768x482.jpg 768w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/3c900-cmb-800x502.jpg 800w" sizes="(max-width: 2575px) 100vw, 2575px"></a><figcaption id="caption-attachment-1354">3Com 3c900-COMBO NIC With AUI Connector</figcaption></figure>
<figure id="attachment_32" aria-describedby="caption-attachment-32"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/10BASE5Node.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/10BASE5Node-800x600.jpg" alt="10BASE5 Node" width="474" height="356" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/10BASE5Node-800x600.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/10BASE5Node-150x113.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/10BASE5Node-300x225.jpg 300w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/10BASE5Node.jpg 1113w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-32">A true 10BASE5 node. Using a 3Com 3c905B-COMBO NIC with AUI</figcaption></figure>
<h3>USB or other solutions</h3>
<p>As far as I can tell no USB product was ever made. With 10BASE5 MAU‚Äôs requiring 12 volts and some 5 or more watts of power, over and above what USB can provide, this would be a problematic proposition. Also, no single chip ‚ÄúUSB with AUI‚Äù solutions exist. From my research, it seems theoretically possible to create a 2-chip adapter, with external power supply, but apparently there is no market for such as a thing.</p>
<figure id="attachment_71" aria-describedby="caption-attachment-71"><a href="http://tech.mattmillman.com/projects/usbaui/"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/axaui_proto_tn-800x534.jpg" alt="USB AUI Ethernet adapter proof of concept, based on a D-Link DUB-E100, NQ8502 connected to MII" width="474" height="316" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/axaui_proto_tn-800x534.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/axaui_proto_tn-150x100.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/axaui_proto_tn-300x200.jpg 300w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/axaui_proto_tn.jpg 880w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-71">A D-Link DUB-E100 with its 100BASE-TX PHY removed, and an NQ8502 frankensteined in its place.<br> This image illustrates my key design objective: To give an AUI interface to a USB Ethernet MAC, instead of converting from 10BASE-T.</figcaption></figure>
<p>But that didn‚Äôt stop me from building one. Above is what is quite possibly one of the worlds only USB Ethernet Adapters with AUI interface. I have a seperate project page for this&nbsp;<a href="http://tech.mattmillman.com/usbaui/">here</a>.</p>
<h3>Routers</h3>
<p>A Cisco NM-1E2W module could be used if one has a large ISR kicking around.</p>
<h2>Bridging 10BASE5 to modern twisted pair networks</h2>
<h3>10BASE-T Hubs equipped with AUI</h3>
<p>Using a 10BASE-T hub with AUI to bridge to a twisted pair network is probably the easiest approach. There‚Äôs thousands of these for sale on eBay for peanuts, but in my case, having one of these on each node would make the setup rather clunky.</p>
<h3>10BASE-T to AUI Media converters</h3>
<p>What I was really looking for was a few of these little boxes. Easily mistaken for a 10BASE-T MAU, these (unlike the former which tend to be spilling out of every I.T. junk box in the land) are fairly uncommon.</p>
<figure id="attachment_17" aria-describedby="caption-attachment-17"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/MC12T.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/MC12T-800x475.jpg" alt="Allied Telesis MC12T" width="474" height="281" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/MC12T-800x475.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/MC12T-150x89.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/MC12T-300x178.jpg 300w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/MC12T.jpg 872w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-17">An Allied Telesis MC12T next to an ordinary 10BASE-T MAU. Spot the difference?</figcaption></figure>

<p>To keep the cost of this project to a minimum, I decided my network would only have 3-4 nodes as this was sufficient to demonstrate a proper shared media network as opposed to a 2 node effective ‚Äúpoint to point‚Äù link. For this I sourced the following components:</p>
<ul>
<li>2x 3Com 3C900B-COMBO PCI NICs with AUI</li>
<li>3x MiLAN MIL-130A AUI-to-10BASE-T bridges</li>
<li>1x MiLAN MIL-10P 10BASE-T Transceiver (for testing AUI nodes)</li>
<li>1x Allied Telesis MC12T</li>
<li>6x Cabletron ST-500 MAU‚Äôs</li>
<li>3x AMP 228752-1 Vampire taps</li>
<li>2x AMP 221914-1 Intrusive N connector taps</li>
<li>5x AUI Drop cables</li>
<li>15M of Belden #9880 cable</li>
<li>2x 50ohm N terminators</li>
<li>2x N female to female couplers</li>
<li>10x UNC 4-40 x 1.1/2‚Ä≥ countersunk screws (to replace missing ones on my MAUs)</li>
</ul>

<p>A common belief persists that 10BASE5 used RG-8/U cable. This, for the most part, appears to be untrue. The cable used was purpose designed for 10BASE5 and manufactured by Belden under the part number 9880 (A few other compatible cables may exist). The reason #9880 cable was specified is that several variants of RG-8 exist with varying dimensions and manufacturing standards. Given that the main form of connection of MAU‚Äôs to 10BASE5 networks was by the precisely designed prongs in ‚ÄúVampire taps‚Äù, cable dimensions and dielectric consistency had to be spot on.</p>
<p>A network designed using only intrusive N connector taps such as AMP 221914-1 could get away with using pretty much any 50 ohm coax, but then the sum total of insertion losses created by each node may prevent the full 500M segment length from being achieved. Also, this approach wouldn‚Äôt permit addition of new nodes without bringing down the network. After an evening of wrestling with RG-8/U cable (why oh why didn‚Äôt I just buy RG-58) while attempting to make a short test lead, I think I‚Äôll stick with the Vampire Taps.</p>
<p>For someone like me who hadn‚Äôt encountered it before, no amount of looking at pictures could prepare for how big this stuff is. Short of high power transmission cables, it‚Äôs the largest coaxial cable I‚Äôve ever seen. It is also very heavy, rigid and the bend radius is absurdly large.</p>
<p>Another interesting feature of this cable is the black bands which appear every 2.5M. In 10BASE5 networks, MAUs can only be attached at 2.5M intervals, this is to ensure that signal reflections generated by taps stay out of phase, which stops them compounding across the whole segment. In a small network such as mine, it‚Äôs probably not so important to respect this tap placement rule, but I‚Äôll respect it anyway.</p>
<figure id="attachment_1325" aria-describedby="caption-attachment-1325"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/BELDEN_9880.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/BELDEN_9880-800x423.jpg" alt="Belden 9880" width="474" height="251" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/BELDEN_9880-800x423.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/BELDEN_9880-150x79.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/BELDEN_9880-300x159.jpg 300w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-1325">Belden 9880 next to standard CAT5E cable. Either side of this frame is 10 kilos of steel, about the minimum required to hold this stuff down for the photo</figcaption></figure>

<p>Unlike the BNC connectors found on 10BASE2, the official connector used by 10BASE5 (where connectors are required) is the N connector. While still used extensively in lab/specialist environments, this large coaxial connector isn‚Äôt something the average ‚Äòtech dude‚Äô is likely to encounter these days. They perform exceptionally, have an intrinsic impedance of 50 ohms, are fairly common/inexpensive and easily handle the large sized #9880 cable.</p>
<figure id="attachment_1324" aria-describedby="caption-attachment-1324"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/N.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/N-800x449.jpg" alt="10BASE5 N connectors" width="474" height="266" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/N-800x449.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/N-150x84.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/N-300x169.jpg 300w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/N.jpg 1789w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-1324">N Connector and joiner with CAT5E patch cable for comparison</figcaption></figure>

<p>These cables sporting male and female 15-pin D-sub connectors, connect a node to the MAU. Since the MAU is bolted to a rigid behemoth piece of coax, it wasn‚Äôt exactly convenient to try and drag that lot down to the back of a PC, so these cables bridged that gap. With a maximum length of 50 metres, that gap can be very large indeed. Also, given that taps could only be placed at 2.5M intervals, a world without drop cables would lend its self to much humour around the office, at the expense of well-intended PC and desk placement considerations.</p>
<p>One notable feature is the slide locking mechanism used in place of the usual UNC 4-40 screws. It is my personal opinion that the design of this mechanism is a bit daft. In theory it‚Äôs a good idea but in practice I think it was implemented too flimsily. AUI drop cables are often very large and rigid (likely thanks to a half a dozen layers of shielding), so much so that a mild sideways nudge on a cable can easily rip a latched connector straight out of the socket, bending/breaking the mechanism. They also have a feature that the screws in the female end often work their way out over time, inevitably to be lost forever. Even during this small scale project, I have suffered some grief over these locking mechanisms. I can imagine they were a source of many support calls in the 80s and early 90s.</p>
<figure id="attachment_1322" aria-describedby="caption-attachment-1322"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/AUI_DROP.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/AUI_DROP-800x447.jpg" alt="1M AUI drop cable" width="474" height="265" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/AUI_DROP-800x447.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/AUI_DROP-150x84.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/AUI_DROP-300x168.jpg 300w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/AUI_DROP.jpg 1605w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-1322">AUI drop cable, with a one-up-from-useless slide locking mechanism</figcaption></figure>

<p>These metal boxes are one of the key defining pieces of kit in a 10BASE5 network. They function as the transceiver, converting the primitive signals from the AUI interface to what‚Äôs required on the physical media, as well as performing collision detection. They came in 4 flavours:</p>
<ul>
<li>With a tap for 10BASE5 networks, using a Vampire Tap</li>
<li>With a tap for 10BASE5 networks, using N connectors (Rare)</li>
<li>With a tap for 10BASE2 networks, using BNC connectors</li>
<li>Without a tap (Buy your own)</li>
</ul>
<p>I chose Cabletron ST-500‚Äôs, because they have blinky lights. The ST-500 (90 Series) would have been one of the last and most modern 10BASE5 MAUs manufactured. The internal component count (when compared with, say, a DEC h4000) is low.</p>
<figure id="attachment_1314" aria-describedby="caption-attachment-1314"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-01.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-01-800x542.jpg" alt="Cabletron ST500-01" width="474" height="321" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-01-800x542.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-01-150x102.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-01-300x203.jpg 300w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-1314">Cabletron ST500-01</figcaption></figure>
<figure id="attachment_1315" aria-describedby="caption-attachment-1315"><a href="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-02a.jpg"><img src="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-02a-800x659.jpg" alt="" width="474" height="390" srcset="http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-02a-800x659.jpg 800w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-02a-150x124.jpg 150w, http://techmattmillman.s3.amazonaws.com/wp-content/uploads/2014/04/ST500-02a-300x247.jpg 300w" sizes="(max-width: 474px) 100vw, 474px"></a><figcaption id="caption-attachment-1315">Cabletron ST500-02</figcaption></figure>

<p>By far the most common means of connection to the coax segment. The bottom half is plastic, I assume to eliminate ground loops by ensuring the braid remains electrically isolated from the metal case of the MAU. The top half is extruded aluminium. Each contains three prongs (two to connect to the braid, and one to the centre conductor). To me, the idea of using this style of connection to coaxial cable is ambitious, but it has to be said that a few spikes in a coaxial cable will equate to significantly less reflection and insertion loss ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://tech.mattmillman.com/projects/10base5/">http://tech.mattmillman.com/projects/10base5/</a></em></p>]]>
            </description>
            <link>http://tech.mattmillman.com/projects/10base5/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24807542</guid>
            <pubDate>Sat, 17 Oct 2020 04:47:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What they don‚Äôt tell you about demand paging in school]]>
            </title>
            <description>
<![CDATA[
Score 69 | Comments 24 (<a href="https://news.ycombinator.com/item?id=24806246">thread link</a>) | @luu
<br/>
October 16, 2020 | https://offlinemark.com/2020/10/14/demand-paging/ | <a href="https://web.archive.org/web/*/https://offlinemark.com/2020/10/14/demand-paging/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>This post details my adventures with the Linux virtual memory subsystem, and my discovery of a creative way to taunt the OOM (out of memory) killer by accumulating memory in the kernel, rather than in userspace.</p>



<p>Keep reading and you‚Äôll learn:</p>



<ul><li>Internal details of the Linux kernel‚Äôs demand paging implementation</li><li>How to exploit virtual memory to implement highly efficient sparse data structures</li><li>What page tables are and how to calculate the memory overhead incurred by them</li><li>A cute way to get killed by the OOM killer while appearing to consume very little memory (great for parties)</li></ul>



<h2>Part 1: Demand paging is nuanced</h2>



<p>As usual, the story begins with me asking questions about implementation details. This time, about the Linux kernel‚Äôs demand paging implementation.</p>



<p>In Operating Systems 101 we learn that operating systems are ‚Äúlazy‚Äù when they allocate memory to processes. When you mmap() an anonymous page, the kernel slyly returns a pointer immediately. It then waits until you trigger a page fault by ‚Äútouching‚Äù that memory before doing the real memory allocation work. This is called ‚Äúdemand paging‚Äù.</p>



<p>This is efficient ‚Äî if the memory is never touched, no physical memory is ever allocated. This also means you can allocate virtual memory in vast excess of what is physically available (‚Äúovercommit‚Äù), which can be useful.<span id="easy-footnote-1-496"></span><span><a href="#easy-footnote-bottom-1-496" title="Some examples of applications that allocate immense virtual memory are Address Sanitizer and Webkit."><sup>1</sup></a></span>. You just can‚Äôt touch it all.</p>



<p>Let‚Äôs dive deeper. Barring execution, ‚Äútouching‚Äù memory means reading or writing. Writes to a new mmap‚Äôd region require the kernel to perform a full memory allocation. You need memory, you need it <em>now</em>, and the kernel can‚Äôt push it off any longer.</p>



<p>Now, the question: <strong>What about reads?</strong></p>



<p>Unlike writes, reads to a new mmap‚Äôd region do <em>not</em> trigger a memory allocation. The kernel continues to push off the allocation by exploiting how new anonymous mappings must be zero initialized. Instead of allocating memory, the kernel services the page fault using the ‚Äúzero page‚Äù: a pre-allocated page of physical memory, completely filled with zeros. In theory this is ‚Äúfree‚Äù ‚Äî a single physical frame can back all zero-initialized pages.</p>



<p>The point? <strong>Demand paging is nuanced ‚Äî not all ways of accessing a new mapping require the kernel to allocate memory</strong>.</p>



<p>Let‚Äôs see what this looks like in the <a href="https://elixir.bootlin.com/linux/v5.8.12/source/mm/memory.c#L4191">source</a>. The core page fault handler, <code>handle_mm_fault</code> is in <code>mm/memory.c</code>. A few calls deep via <code>__handle_mm_fault</code> and <code>handle_pte_fault</code>, we hit this block:</p>



<div><pre data-lang="C"><code>static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
{
	// ...

	if (!vmf-&gt;pte) {
		if (vma_is_anonymous(vmf-&gt;vma))
			return do_anonymous_page(vmf);
		else
			// ...
	}

	// ...
}</code></pre></div>



<p>Which leads us to <code>do_anonymous_page</code> ‚Äî the core page fault handler for anonymous pages. </p>



<div><pre data-lang="C"><code>static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
{
	// ...
	
	if (pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd))
		return VM_FAULT_OOM;
	// ...

	/* Use the zero-page for reads */
	if (!(vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp; // (1)
			!mm_forbids_zeropage(vma-&gt;vm_mm)) {
		entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf-&gt;address), // (2)
						vma-&gt;vm_page_prot));
		vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd,
				vmf-&gt;address, &amp;vmf-&gt;ptl);
		// ...
		goto setpte;
	}
	// ...
setpte:
	set_pte_at(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte, entry); // (3)
	// ...
	return ret;

	// ...
}</code></pre></div>



<p>Bingo. It checks whether a read caused the fault (1), then maps the virtual page to the zero page (2 and 3).</p>



<p>Note that this all happens in the page fault handler, which is an implementation choice. The mmap <a href="https://elixir.bootlin.com/linux/v5.8.12/source/mm/mmap.c#L1687">core logic</a> does not touch the page tables at all, and only records the presence of the new mapping. It leaves the mapping‚Äôs page table entry non-present (present bit = 0) which will trigger a page fault on access.</p>



<p>Alternatively, mmap could proactively allocate the page table entry, and initialize it to the zero page. This would avoid a page fault on the first read, but at the cost of initializing (potentially many) page table entries up front. Given that it‚Äôs most efficient to be maximally lazy, the current implementation is best.</p>



<h2>Part 2: Allocating infinite memory, and touching it too</h2>



<p>This led me to another question. Since reads from anonymous mappings are ‚Äúfree‚Äù, in addition to allocating excessive virtual memory, can‚Äôt you actually touch all of it? As long as that ‚Äútouch‚Äù is a read?</p>



<p>Time for an experiment. Here‚Äôs some code that allocates 100 GB of linear memory, and tries to read from the first byte of each page. It allocates 512 MB at a time because I <a href="https://twitter.com/damageboy/status/1317676450393300994">incorrectly thought</a> you couldn‚Äôt mmap 100GB at once <s>you can‚Äôt directly ask mmap for 100 GB üôÇ</s>.<span id="easy-footnote-2-496"></span><span><a href="#easy-footnote-bottom-2-496" title="Thanks to <a href=&quot;https://twitter.com/damageboy&quot;>@damageboy</a> for pointing this out."><sup>2</sup></a></span><span id="easy-footnote-3-496"></span><span><a href="#easy-footnote-bottom-3-496" title="This is lazy, dirty research code :) It is not sound or portable, and only just works because Linux happens to place the mmaps contiguously, growing down in memory."><sup>3</sup></a></span> My test system was a x64 Ubuntu 20.04 VPS.</p>



<div><pre data-lang="C++"><code>#include &lt;sys/mman.h&gt;
#include &lt;iostream&gt;

const size_t MB = 1024 * 1024;
const size_t GB = MB * 1024;

int main() {
  size_t alloc_size = 512 * MB;
  size_t total_alloc = 100 * GB;
  size_t num_allocs = total_alloc / alloc_size;

  // std::cout &lt;&lt; "alloc_size (MB)" &lt;&lt; alloc_size / (1024*1024)&lt;&lt; "\n";
  // std::cout &lt;&lt; "total_alloc " &lt;&lt; total_alloc &lt;&lt; "\n";
  // std::cout &lt;&lt; "num_allocs " &lt;&lt; num_allocs &lt;&lt; "\n";

  std::cout &lt;&lt; "Allocating mem...\n";

  char* base = nullptr;

  // Allocate a ton of memory
  for (size_t i = 0; i &lt; num_allocs; i++) {
    // Unsound alert - assuming allocations are contiguous and grow down.
    base = (char*)mmap(NULL, alloc_size, PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (base == MAP_FAILED) {
      perror(NULL);
      throw std::runtime_error("Fail");
    }
    std::cout &lt;&lt; (void*)base &lt;&lt; " " &lt;&lt; i &lt;&lt; "\n";
  }

  std::cout &lt;&lt; "Allocated Virtual Mem (GB): " &lt;&lt; total_alloc / GB &lt;&lt; "\n";
  std::cout &lt;&lt; "Base Addr: " &lt;&lt; (void*)base &lt;&lt; "\n";
  std::cout &lt;&lt; "Press enter to start reading.\n";
  getchar();
  std::cout &lt;&lt; "Reading each page...\n";

  // Read the first byte of each page
  for (size_t i = 0; i &lt; total_alloc; i += 0x1000) {
    auto x = base[i];
  }

  std::cout &lt;&lt; "Done!\n";
  getchar();
}</code></pre></div>



<p>When we run it, here‚Äôs what we see:</p>



<div><pre data-lang="Bash"><code>$ ./demo
Allocating mem...
0x7f3f6d300000 1
0x7f3f4d300000 2
0x7f3f2d300000 3
...
0x7f26cd300000 198
0x7f26ad300000 199
0x7f268d300000 200
Allocated Virtual Mem (GB): 100
Base Addr: 0x7f268d300000
Press enter to start reading.</code></pre></div>



<p>It successfully allocated 100 GB of linear virtual memory in 512 MB chunks. We can confirm this with <code>pmap</code>, which shows a 100GB region of anonymous virtual memory at the base address printed.</p>



<div><pre><code>$ pmap `pidof demo`
485209:   ./demo
00005600e1d0c000      4K r---- demo
00005600e1d0d000      4K r-x-- demo
00005600e1d0e000      4K r---- demo
00005600e1d0f000      4K r---- demo
00005600e1d10000      4K rw--- demo
00005600e2a47000    132K rw---   [ anon ]
00007f268d300000 104857600K r----   [ anon ] &lt;&lt;&lt;&lt; 100 GB region
00007f3f8d300000     16K rw---   [ anon ]
00007f3f8d304000     60K r---- libm-2.31.so
00007f3f8d313000    668K r-x-- libm-2.31.so
...</code></pre></div>



<p>What does htop say?</p>



<figure><img loading="lazy" width="625" height="87" src="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=625%2C87&amp;ssl=1" alt="" srcset="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=1024%2C142&amp;ssl=1 1024w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=300%2C42&amp;ssl=1 300w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=768%2C107&amp;ssl=1 768w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=1536%2C213&amp;ssl=1 1536w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=624%2C87&amp;ssl=1 624w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?w=1916&amp;ssl=1 1916w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?w=1250&amp;ssl=1 1250w" sizes="(max-width: 625px) 100vw, 625px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=1024%2C142&amp;ssl=1 1024w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=300%2C42&amp;ssl=1 300w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=768%2C107&amp;ssl=1 768w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=1536%2C213&amp;ssl=1 1536w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=624%2C87&amp;ssl=1 624w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?w=1916&amp;ssl=1 1916w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?w=1250&amp;ssl=1 1250w" data-lazy-src="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-9.png?resize=625%2C87&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>htop confirms that 100 GB of virtual memory is allocated (VIRT column), but a much more reasonable 1540 KB of resident memory (RES) is actually occupying RAM. Note the MINFLT column ‚Äî this is the number of ‚Äúminor‚Äù page faults that have occurred. A minor page fault is one that does not require loading from disk. We will be triggering lots of these and should expect to see this number grow dramatically.</p>



<p>Here‚Äôs what happens after I press enter to trigger reading.</p>



<div><pre><code>0x7f26cd300000 198
0x7f26ad300000 199
0x7f268d300000 200
Allocated Virtual Mem (GB): 100
Base Addr: 0x7f268d300000
Press enter to start reading.

Reading each page...
Done!</code></pre></div>



<p>The process hits the ‚ÄúDone!‚Äù print. This means it successfully touched every page of the 100 GB allocation!</p>



<figure><img loading="lazy" width="625" height="79" src="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=625%2C79&amp;ssl=1" alt="" srcset="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=1024%2C130&amp;ssl=1 1024w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=300%2C38&amp;ssl=1 300w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=768%2C98&amp;ssl=1 768w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=1536%2C195&amp;ssl=1 1536w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=624%2C79&amp;ssl=1 624w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?w=1888&amp;ssl=1 1888w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?w=1250&amp;ssl=1 1250w" sizes="(max-width: 625px) 100vw, 625px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=1024%2C130&amp;ssl=1 1024w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=300%2C38&amp;ssl=1 300w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=768%2C98&amp;ssl=1 768w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=1536%2C195&amp;ssl=1 1536w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=624%2C79&amp;ssl=1 624w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?w=1888&amp;ssl=1 1888w, https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?w=1250&amp;ssl=1 1250w" data-lazy-src="https://i0.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-10.png?resize=625%2C79&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>htop confirms that many minor faults have occurred. We would expect it to cause exactly 26214400 faults (100 GB / 4 KB), and indeed, 26214552 ‚Äì 26214400 = 152, the number we started with. Intriguingly, the resident memory appears to have also increased, which should not have happened. See the <a href="#rss-spike">Appendix A</a> for discussion of this.</p>



<p>Here‚Äôs a video to see it all in action:</p>



<figure><p>
<iframe title="Allocating 100 GB and touching it all (Demand Paging Pt 1)" width="625" height="469" src="https://www.youtube.com/embed/HYNMt6FWsI4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>



<p><strong>So the theory is confirmed! You can apparently ‚ÄúAllocate your memory, and touch it too‚Äù (as long as that touch is a read).</strong><span id="easy-footnote-4-496"></span><span><a href="#easy-footnote-bottom-4-496" title="Or if you prefer, the famous Henry Ford <a href=&quot;https://www.goodreads.com/quotes/23494-any-customer-can-have-a-car-painted-any-colour-that&quot;>quote</a>:  &amp;#8220;You can touch as much memory as you want, as long as you are only reading&amp;#8221;."><sup>4</sup></a></span></p>



<p>If your application, for some reason, benefits from the ability to have a 100 GB array of zeros then this is perfect for you. What about the rest of us?</p>



<p>A closer-to-real-life application is a <a href="https://en.wikipedia.org/wiki/Sparse_matrix">sparse array</a>. A sparse array is a (typically very large) array, whose elements are mostly zero. By exploiting demand paging, you can implement a memory efficient sparse array, where the majority of the array is backed by the zero page (or not even mapped). You get the fast indexing benefits of an array while avoiding the memory overhead.</p>



<h2>Part 3: Playing with a killer</h2>



<p>I have a confession. Remember when I said not all ways of accessing memory require the kernel to allocate memory? Yeah, that‚Äôs a lie.</p>



<p>Even though a read can be serviced by the shared zero page, that doesn‚Äôt mean no memory is allocated. Even the process of mapping a zero page requires allocating  memory. And here‚Äôs where we get into the nitty gritty.</p>



<h3>The hidden overhead of page tables</h3>



<p>The overhead comes from the virtual memory infrastructure itself ‚Äî the page tables. Page tables are data structures that power the virtual memory subsystem. Like regular data structures, they occupy memory, only their overhead is easy to overlook, since it‚Äôs hidden from userspace.</p>



<p>This is what page tables on x86_64 look like:</p>



<figure><img loading="lazy" width="625" height="463" src="https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=625%2C463&amp;ssl=1" alt="" srcset="https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=1024%2C758&amp;ssl=1 1024w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=300%2C222&amp;ssl=1 300w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=768%2C568&amp;ssl=1 768w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=624%2C462&amp;ssl=1 624w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?w=1276&amp;ssl=1 1276w" sizes="(max-width: 625px) 100vw, 625px" data-recalc-dims="1" data-lazy-srcset="https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=1024%2C758&amp;ssl=1 1024w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=300%2C222&amp;ssl=1 300w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=768%2C568&amp;ssl=1 768w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=624%2C462&amp;ssl=1 624w, https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?w=1276&amp;ssl=1 1276w" data-lazy-src="https://i1.wp.com/offlinemark.com/wp-content/uploads/2020/09/image-11.png?resize=625%2C463&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>x86_64 Page Tables: Intel Manual Vol 3A Chapter 4.5</figcaption></figure>



<p>They‚Äôre a tree data structure that is 4 levels deep, with each node (table) being an array of 512 8-byte entries. Together these tables offer an efficient way to represent a mapping between every virtual page in the address space and a physical frame.</p>



<p>Here‚Äôs where the overhead comes from. Each page touched requires 1 Page Table Entry (PTE) to be allocated. However, PTEs aren‚Äôt allocated individually. They‚Äôre allocated in blocks of 512 called Page Tables. Each Page Table requires 1 Page Directory Entry to be allocated. But ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://offlinemark.com/2020/10/14/demand-paging/">https://offlinemark.com/2020/10/14/demand-paging/</a></em></p>]]>
            </description>
            <link>https://offlinemark.com/2020/10/14/demand-paging/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24806246</guid>
            <pubDate>Sat, 17 Oct 2020 00:09:58 GMT</pubDate>
        </item>
    </channel>
</rss>
